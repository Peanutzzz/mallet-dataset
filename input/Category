cassandra connector i t case suppress warning serial category fail on java11 class public class cassandra connector i t case extend write ahead sink test base tuple3 string integer integer cassandra tuple write ahead sink tuple3 string integer integer private static final logger log logger factory get logger cassandra connector i t case class private static final boolean embed true private static embed cassandra service cassandra private static final string host 127.0 0.1 private static final int port private static cluster builder builder new cluster builder override protect cluster build cluster cluster builder builder return builder add contact point with port new inet socket address host port with query option new query option set consistency level consistency level one set serial consistency level consistency level local serial without j m x reporting without metric build private static cluster cluster private static session session private static final string table name prefix flink private static final string table name variable table private static final string create keyspace query create keyspace flink with replication class simple strategy replication factor private static final string create table query create table flink table name variable id text primary key counter int batch id int private static final string insert datum query insert into flink table name variable id counter batch id value private static final string select datum query select from flink table name variable private static final random random new random private int table i d private static final array list tuple3 string integer integer collection new array list private static final array list row row collection new array list private static final type information field type basic type info string type info basic type info int type info basic type info int type info static for int i i i collection add new tuple3 uuid random u u i d to string i row collection add row of uuid random u u i d to string i private static class embed cassandra service cassandra daemon cassandra daemon public void start throw i o exception this cassandra daemon new cassandra daemon this cassandra daemon init null this cassandra daemon start public void stop this cassandra daemon stop class rule public static final temporary folder temporary folder new temporary folder before class public static void start cassandra throw i o exception class loader class loader cassandra connector i t case class get class loader file file new file class loader get resource cassandra yaml get file file tmp temporary folder new file cassandra yaml try buffer writer b new buffer writer new file writer tmp copy cassandra yaml inject absolute path into cassandra yaml scanner scanner new scanner file while scanner have next line string line scanner next line line line replace path tmp get parent file b write line n b flush tell cassandra where the configuration file be use the test configuration file system set property cassandra config tmp get absolute file to u be i to string if embed cassandra new embed cassandra service cassandra start start establish a connection within seconds long start system nano time long deadline start 000 l while true try cluster builder get cluster session cluster connect break catch exception e if system nano time deadline throw e try thread sleep catch interrupt exception ignore log debug connection establish after ms system current time millis start session execute create keyspace query session execute create table query replace table name variable table name prefix initial before public void create table table i d random next int integer max value session execute inject table name create table query after class public static void close cassandra if session null session close if cluster null cluster close if cassandra null cassandra stop exactly once test override protect cassandra tuple write ahead sink tuple3 string integer integer create sink throw exception return new cassandra tuple write ahead sink inject table name insert datum query type extractor get for object new tuple3 create serializer new execution config builder new cassandra committer builder override protect tuple type info tuple3 string integer integer create type info return tuple type info get basic tuple type info string class integer class integer class override protect tuple3 string integer integer generate value int counter int checkpoint i d return new tuple3 uuid random u u i d to string counter checkpoint i d override protect void verify result ideal circumstance cassandra tuple write ahead sink tuple3 string integer integer sink result set result session execute inject table name select datum query array list integer list new array list for int x x x list add x for com datastax driver core row s result list remove new integer s get int counter assert assert true the follow id s be not find in the result set list to string list be empty override protect void verify result datum persistence upon miss notify cassandra tuple write ahead sink tuple3 string integer integer sink result set result session execute inject table name select datum query array list integer list new array list for int x x x list add x for com datastax driver core row s result list remove new integer s get int counter assert assert true the follow id s be not find in the result set list to string list be empty override protect void verify result datum discard upon restore cassandra tuple write ahead sink tuple3 string integer integer sink result set result session execute inject table name select datum query array list integer list new array list for int x x x list add x for int x x x list add x for com datastax driver core row s result list remove new integer s get int counter assert assert true the follow id s be not find in the result set list to string list be empty override protect void verify result when re scale cassandra tuple write ahead sink tuple3 string integer integer sink int start element counter int end element counter important note for cassandra we always have to start from because all operator will share the same final db array list integer expect new array list for int i i end element counter i expect add i array list integer actual new array list result set result session execute inject table name select datum query for com datastax driver core row s result actual add s get int counter collection sort actual assert assert array equal expect to array actual to array test public void test cassandra committer throw exception string job i d new job i d to string cassandra committer cc1 new cassandra committer builder flink auxiliary cc cc1 set job id job i d cc1 set operator id operator cassandra committer cc2 new cassandra committer builder flink auxiliary cc cc2 set job id job i d cc2 set operator id operator cassandra committer cc3 new cassandra committer builder flink auxiliary cc cc3 set job id job i d cc3 set operator id operator1 cc1 create resource cc1 open cc2 open cc3 open assert assert false cc1 be checkpoint commit assert assert false cc2 be checkpoint commit assert assert false cc3 be checkpoint commit cc1 commit checkpoint assert assert true cc1 be checkpoint commit verify that other sub task aren t affect assert assert false cc2 be checkpoint commit verify that other task aren t affect assert assert false cc3 be checkpoint commit assert assert false cc1 be checkpoint commit cc1 close cc2 close cc3 close cc1 new cassandra committer builder flink auxiliary cc cc1 set job id job i d cc1 set operator id operator cc1 open verify that checkpoint data be not destroy within open close and not reliant on internally cache datum assert assert true cc1 be checkpoint commit assert assert false cc1 be checkpoint commit cc1 close at least once test test public void test cassandra tuple at least once sink throw exception cassandra tuple sink tuple3 string integer integer sink new cassandra tuple sink inject table name insert datum query builder try sink open new configuration for tuple3 string integer integer value collection sink send value finally sink close result set rs session execute inject table name select datum query assert assert equal rs all size test public void test cassandra row at least once sink throw exception cassandra row sink sink new cassandra row sink field type length inject table name insert datum query builder try sink open new configuration for row value row collection sink send value finally sink close result set rs session execute inject table name select datum query assert assert equal rs all size test public void test cassandra pojo at least once sink throw exception session execute create table query replace table name variable test cassandra pojo sink pojo sink new cassandra pojo sink pojo class builder try sink open new configuration for int x x x sink send new pojo uuid random u u i d to string x finally sink close result set rs session execute select datum query replace table name variable test assert assert equal rs all size test public void test cassandra pojo no annotated keyspace at least once sink throw exception session execute create table query replace table name variable test pojo no annotated keyspace cassandra pojo sink pojo no annotated keyspace sink new cassandra pojo sink pojo no annotated keyspace class builder flink try sink open new configuration for int x x x sink send new pojo no annotated keyspace uuid random u u i d to string x finally sink close result set rs session execute select datum query replace table name variable test pojo no annotated keyspace assert assert equal rs all size test public void test cassandra table sink throw exception stream execution environment env stream execution environment get execution environment env set parallelism stream table environment t env stream table environment create env datum stream source row source env from collection row collection t env create temporary view test flink table source table environment internal t env register table sink internal cassandra table new cassandra append table sink builder inject table name insert datum query configure new string f0 f1 f2 new type information type string type int type int table result table result t env sql query select from test flink table execute insert cassandra table table result get job client get get job execution result thread current thread get context class loader get result set rs session execute inject table name select datum query validate that all input be correctly write to cassandra list row input new array list row collection list com datastax driver core row output rs all for com datastax driver core row o output row cmp new row cmp set field o get string cmp set field o get int cmp set field o get int assert assert true row cmp be write to cassandra but not in input input remove cmp assert assert true the input datum be not completely write to cassandra input be empty test public void test cassandra batch pojo format throw exception session execute create table query replace table name variable custom cassandra annotate pojo table name output format custom cassandra annotate pojo sink new cassandra pojo output format builder custom cassandra annotate pojo class new mapper option mapper option save null field true list custom cassandra annotate pojo custom cassandra annotate pojo int stream range map to obj x new custom cassandra annotate pojo uuid random u u i d to string x collect collector to list try sink configure new configuration sink open for custom cassandra annotate pojo custom cassandra annotate pojo custom cassandra annotate pojo sink write record custom cassandra annotate pojo finally sink close result set rs session execute select datum query replace table name variable custom cassandra annotate pojo table name assert assert equal rs all size input format custom cassandra annotate pojo input split source new cassandra pojo input format select datum query replace table name variable batch builder custom cassandra annotate pojo class list custom cassandra annotate pojo result new array list try source configure new configuration source open null while source reach end custom cassandra annotate pojo temp source next record null result add temp finally source close assert assert equal result size result sort comparator compare int custom cassandra annotate pojo get counter custom cassandra annotate pojo sort comparator compare int custom cassandra annotate pojo get counter assert that result same property value as custom cassandra annotate pojo test public void test cassandra batch tuple format throw exception output format tuple3 string integer integer sink new cassandra output format inject table name insert datum query builder try sink configure new configuration sink open for tuple3 string integer integer value collection sink write record value finally sink close sink new cassandra tuple output format inject table name insert datum query builder try sink configure new configuration sink open for tuple3 string integer integer value collection sink write record value finally sink close input format tuple3 string integer integer input split source new cassandra input format inject table name select datum query builder list tuple3 string integer integer result new array list try source configure new configuration source open null while source reach end result add source next record new tuple3 string integer integer finally source close assert assert equal result size test public void test cassandra batch row format throw exception output format row sink new cassandra row output format inject table name insert datum query builder try sink configure new configuration sink open for row value row collection sink write record value finally sink close result set rs session execute inject table name select datum query list com datastax driver core row row rs all assert assert equal row collection size row size private string inject table name string target return target replace table name variable table name prefix table i d test public void test cassandra scala tuple at least once sink builder detection throw exception class scala tuple1 string c class scala tuple1 string new scala tuple1 hello get class seq type information type info java converter as scala buffer converter collection type information singleton list basic type info string type info as scala seq string field name java converter as scala buffer converter collection singleton list as scala case class type info scala tuple1 string type info new case class type info scala tuple1 string c null type info field name override public type serializer scala tuple1 string create serializer execution config config return null stream execution environment env stream execution environment get execution environment datum stream scala tuple1 string input env from element new scala tuple1 hello return type info cassandra sink cassandra sink builder scala tuple1 string sink builder cassandra sink add sink input assert true sink builder instanceof cassandra sink cassandra scala product sink builder test public void test cassandra scala tuple at least sink throw exception cassandra scala product sink scala tuple3 string integer integer sink new cassandra scala product sink inject table name insert datum query builder list scala tuple3 string integer integer scala tuple collection new array list for int i i i scala tuple collection add new scala tuple3 uuid random u u i d to string i try sink open new configuration for scala tuple3 string integer integer value scala tuple collection sink invoke value sink context util for timestamp finally sink close result set rs session execute inject table name select datum query list com datastax driver core row row rs all assert assert equal scala tuple collection size row size for com datastax driver core row row row scala tuple collection remove new scala tuple3 row get string id row get int counter row get int batch id assert assert equal scala tuple collection size test public void test cassandra scala tuple partial column update throw exception cassandra sink base config config cassandra sink base config new builder set ignore null field true build cassandra scala product sink scala tuple3 string integer integer sink new cassandra scala product sink inject table name insert datum query builder config string id uuid random u u i d to string integer counter integer batch id send partial record across multiple request scala tuple3 string integer integer scala tuple record first new scala tuple3 id counter null scala tuple3 string integer integer scala tuple record second new scala tuple3 id null batch id try sink open new configuration sink invoke scala tuple record first sink context util for timestamp sink invoke scala tuple record second sink context util for timestamp finally sink close result set rs session execute inject table name select datum query list com datastax driver core row row rs all assert assert equal row size since null be ignore we should be read one complete record for com datastax driver core row row row assert assert equal new scala tuple3 id counter batch id new scala tuple3 row get string id row get int counter row get int batch id 
kafka010 secure run i t case category fail on java11 class public class kafka010 secure run i t case extend kafka consumer test base protect static final logger log logger factory get logger kafka010 secure run i t case class before class public static void prepare throw exception log info log info start kafka010 secure run i t case log info secure test environment prepare temp folder secure test environment populate flink secure configuration get flink configuration start cluster true false after class public static void shut down service throw exception shutdown cluster secure test environment cleanup timeout interval be large since in travis zk connection timeout occur frequently the timeout for the test case be time timeout of zk connection test timeout public void test multiple topic throw exception run produce consume multiple topic true 
s q l client kafka i t case run with parameterized class category value travi group1 class fail on java11 class public class s q l client kafka i t case extend test logger private static final logger log logger factory get logger s q l client kafka i t case class private static final string kafka e2 e sql kafka e2e sql parameterized parameter name index kafka version kafka sql version public static collection object datum return array as list new object 0.10 2.2 0.10 kafka 0.10 kafka 0.10 jar 0.11 0.2 0.11 kafka 0.11 kafka 0.11 jar 2.4 universal kafka kafka jar private static configuration get configuration we have to enable checkpoint to trigger flush for filesystem sink final configuration flink config new configuration flink config set string execution checkpoint interval 5 return flink config rule public final flink resource flink new local standalone flink resource factory create flink resource setup builder add configuration get configuration build rule public final kafka resource kafka rule public final temporary folder tmp new temporary folder private final string kafka version private final string kafka s q l version private final string kafka identifier private path result class rule public static final download cache download cache download cache get private static final path sql avro jar test util get resource avro jar private static final path sql tool box jar test util get resource sql toolbox jar private final list path apache avro jar new array list private final path sql connector kafka jar public s q l client kafka i t case string kafka version string kafka s q l version string kafka identifier string kafka s q l jar pattern this kafka kafka resource get kafka version this kafka version kafka version this kafka s q l version kafka s q l version this kafka identifier kafka identifier this sql connector kafka jar test util get resource kafka s q l jar pattern before public void before throw exception download cache before path tmp path tmp get root to path log info the current temporary path tmp path this result tmp path resolve result apache avro jar add download cache get or download http repo1 maven org maven2 org apache avro avro 1.8 avro 1.8 jar tmp path apache avro jar add download cache get or download http repo1 maven org maven2 org codehaus jackson jackson core asl 1.9 jackson core asl 1.9 jar tmp path apache avro jar add download cache get or download http repo1 maven org maven2 org codehaus jackson jackson mapper asl 1.9 jackson mapper asl 1.9 jar tmp path test public void test kafka throw exception try cluster controller cluster controller flink start cluster create topic and send message string test json topic test json kafka version uuid random u u i d to string string test avro topic test avro kafka version uuid random u u i d to string kafka create topic test json topic string message new string rowtime 2018 - 03 12 08 00:00 user alice event type warn message this be a warning rowtime 2018 - 03 12 08 10:00 user alice event type warn message this be a warning rowtime 2018 - 03 12 09 00:00 user bob event type warn message this be another warning rowtime 2018 - 03 12 09 10:00 user alice event type info message this be a info rowtime 2018 - 03 12 09 20:00 user steve event type info message this be another info rowtime 2018 - 03 12 09 30:00 user steve event type info message this be another info rowtime 2018 - 03 12 09 30:00 user null event type warn message this be a bad message because the user be miss rowtime 2018 - 03 12 10 40:00 user bob event type error message this be a error kafka send message test json topic message create topic test avro kafka create topic test avro topic initialize the sql statement from kafka e2e sql file map string string var map new hash map var map put kafka identifier this kafka identifier var map put topic json name test json topic var map put topic avro name test avro topic var map put result this result to absolute path to string var map put kafka bootstrap server string util join kafka get bootstrap server address to array list string sql line initialize sql line var map execute sql statement in kafka e2e sql file execute sql statement cluster controller sql line wait until all the result flush to the csv file log info verify the csv result check csv result file log info the kafka sql client test run successfully this kafka s q l version private void execute sql statement cluster controller cluster controller list string sql line throw i o exception log info execute kafka end to end sql statement kafka s q l version cluster controller submit s q l job new s q l job submission s q l job submission builder sql line add jar sql avro jar add jar apache avro jar add jar sql connector kafka jar add jar sql tool box jar build private list string initialize sql line map string string var throw i o exception url url s q l client kafka i t case class get class loader get resource kafka e2 e sql if url null throw new file not find exception kafka e2 e sql list string line file read all line new file url get file to path list string result new array list for string line line for map entry string string var var entry set line line replace var get key var get value result add line return result private void check csv result file throw exception boolean success false final deadline deadline deadline from now duration of seconds while deadline have time leave if file exist result list string line read csv result file result if line size success true assert that line to array new string array contain in any order 2018 - 03 12 08 00:00 alice this be a warning success constant folding 2018 - 03 12 09 00:00 bob this be another warning success constant folding 2018 - 03 12 09 00:00 steve this be another info success constant folding 2018 - 03 12 09 00:00 alice this be a info success constant folding break else log info the target csv do not contain enough record current record leave time s result line size deadline time leave get seconds else log info the target csv do not exist now result thread sleep assert assert true do not get expect result before timeout success private static list string read csv result file path path throw i o exception file file path path to file list all the non hide file file csv file file path list file dir name name start with list string result new array list if csv file null for file file csv file result add all file read all line file to path return result 
stream kafka i t case run with parameterized class category value travi group1 class pre commit class fail on java11 class public class streaming kafka i t case extend test logger private static final logger log logger factory get logger streaming kafka i t case class parameterize parameter name index kafka version public static collection object datum return array as list new object flink streaming kafka010 test 0.10 2.2 flink streaming kafka011 test 0.11 0.2 flink streaming kafka test 2.4 private final path kafka example jar private final string kafka version rule public final kafka resource kafka rule public final flink resource flink flink resource get flink resource setup builder add configuration get configuration build private static configuration get configuration modify configuration to have enough slot final configuration flink config new configuration flink config set integer task manager option num task slot return flink config public streaming kafka i t case final string kafka example jar pattern final string kafka version this kafka example jar test util get resource kafka example jar pattern this kafka kafka resource get kafka version this kafka version kafka version test public void test kafka throw exception try final cluster controller cluster controller flink start cluster final string input topic test input kafka version uuid random u u i d to string final string output topic test output kafka version uuid random u u i d to string create the require topic kafka create topic input topic kafka create topic output topic run the flink job detach mode cluster controller submit job new job submission job submission builder kafka example jar set detached true add argument input topic input topic add argument output topic output topic add argument prefix prefix add argument bootstrap server kafka get bootstrap server address stream map address address get host string address get port collect collector join add argument group id myconsumer add argument auto offset reset earliest add argument transaction timeout ms add argument flink partition discovery interval milli build log info send message to kafka topic input topic send some datum to kafka kafka send keyed message input topic t key telephant 5,45218 key tsquirrel 12,46213 key tbee 3,51348 key tsquirrel 22,52444 key tbee 10,53412 key telephant 9,54867 log info verify message from kafka topic output topic final list string message kafka read message kafka e2e driver output topic final list string elephant filter message message elephant final list string squirrel filter message message squirrel final list string bee filter message message bee check all key assert assert equal array as list elephant 5,45218 elephant 14,54867 elephant assert assert equal array as list squirrel 12,46213 squirrel 34,52444 squirrel assert assert equal array as list bee 3,51348 bee 13,53412 bee now we add a new partition to the topic log info repartition kafka topic input topic kafka set num partition input topic assert assert equal fail add a partition to input topic kafka get num partition input topic send some more message to kafka log info send more message to kafka topic input topic kafka send keyed message input topic t key telephant 13,64213 key tgiraffe 9,65555 key tbee 5,65647 key tsquirrel 18,66413 verify that we assumption that the new partition actually have write message be correct assert assert not equal the newly create partition do not have any new message and therefore partition discovery can not be verify 0 l kafka get partition offset input topic log info verify message from kafka topic output topic final list string message kafka read message kafka e2e driver output topic final list string elephant filter message message elephant final list string squirrel filter message message squirrel final list string bee filter message message bee final list string giraffe filter message message giraffe assert assert equal string format message from kafka s s kafka version message array as list elephant 27,64213 elephant assert assert equal string format message from kafka s s kafka version message array as list squirrel 52,66413 squirrel assert assert equal string format message from kafka s s kafka version message array as list bee 18,65647 bee assert assert equal string format message from kafka s s kafka version message array as list giraffe 9,65555 giraffe private static list string filter message final list string message final string keyword return message stream filter msg msg contain keyword collect collector to list 
s q l client h base i t case category value travi group1 class pre commit class fail on java11 class public class s q l client h base i t case extend test logger private static final logger log logger factory get logger s q l client h base i t case class private static final string hbase e2 e sql hbase e2e sql rule public final h base resource hbase rule public final flink resource flink new local standalone flink resource factory create flink resource setup builder build rule public final temporary folder tmp new temporary folder class rule public static final download cache download cache download cache get private static final path sql tool box jar test util get resource sql toolbox jar private static final path sql connector h base jar test util get resource hbase jar private static final path hadoop classpath test util get resource hadoop classpath private list path hadoop classpath jar public s q l client h base i t case this hbase h base resource get before public void before throw exception download cache before path tmp path tmp get root to path log info the current temporary path tmp path prepare all hadoop jar to mock hadoop classpath use hadoop classpath which contain all hadoop jar file hadoop classpath file new file hadoop classpath to absolute path to string if hadoop classpath file exist throw new file not find exception file that contain hadoop classpath hadoop classpath to string do not exist string class path content file util read file utf8 hadoop classpath file hadoop classpath jar array stream class path content split map jar path get jar collect collector to list test public void test h base throw exception try cluster controller cluster controller flink start cluster create table and put datum hbase create table source family1 family2 hbase create table sink family1 family2 hbase put datum source row1 family1 f1c1 v1 hbase put datum source row1 family2 f2c1 v2 hbase put datum source row1 family2 f2c2 v3 hbase put datum source row2 family1 f1c1 v4 hbase put datum source row2 family2 f2c1 v5 hbase put datum source row2 family2 f2c2 v6 initialize the sql statement from hbase e2e sql file list string sql line initialize sql line execute sql statement in hbase e2e sql file execute sql statement cluster controller sql line log info verify the sink table result wait until all the result flush to the h base sink table check h base sink result log info the h base sql client test run successfully private void check h base sink result throw exception boolean success false final deadline deadline deadline from now duration of seconds while deadline have time leave final list string line hbase scan table sink if line size success true assert that line to array new string array contain in any order core matcher all of contain string row1 contain string family1 contain string f1c1 contain string value1 core matcher all of contain string row1 contain string family2 contain string f2c1 contain string v2 core matcher all of contain string row1 contain string family2 contain string f2c2 contain string v3 core matcher all of contain string row2 contain string family1 contain string f1c1 contain string value4 core matcher all of contain string row2 contain string family2 contain string f2c1 contain string v5 core matcher all of contain string row2 contain string family2 contain string f2c2 contain string v6 break else log info the h base sink table do not contain enough record current record leave time s line size deadline time leave get seconds thread sleep assert assert true do not get expect result before timeout success private list string initialize sql line throw i o exception url url s q l client h base i t case class get class loader get resource hbase e2 e sql if url null throw new file not find exception hbase e2 e sql return file read all line new file url get file to path private void execute sql statement cluster controller cluster controller list string sql line throw i o exception log info execute sql h base source table h base sink table cluster controller submit s q l job new s q l job submission s q l job submission builder sql line add jar sql tool box jar add jar sql connector h base jar add jar hadoop classpath jar build 
metric availability i t case category travi group1 class public class metric availability i t case extend test logger private static final string host localhost private static final int port rule public final flink resource dist new local standalone flink resource factory create flink resource setup builder build nullable private static schedule executor service schedule executor service null before class public static void start executor schedule executor service executor new schedule thread pool after class public static void shutdown executor if schedule executor service null schedule executor service shutdown test public void test reporter throw exception try cluster controller ignore dist start cluster final rest client rest client new rest client rest client configuration from configuration new configuration schedule executor service check job manager metric availability rest client final collection resource i d task manager id get task manager id rest client for final resource i d task manager id task manager id check task manager metric availability rest client task manager id private static void check job manager metric availability final rest client rest client throw exception final job manager metric header header job manager metric header get instance final job manager metric message parameter parameter header get unresolved message parameter parameter metric filter parameter resolve collection singleton list num register task manager fetch metric rest client send request host port header parameter empty request body get instance get metric name predicate num register task manager private static collection resource i d get task manager id final rest client rest client throw exception final task manager header header task manager header get instance final task manager info response fetch metric rest client send request host port header empty message parameter get instance empty request body get instance task manager info task manager info get task manager info be empty return response get task manager info stream map task manager info get resource id collect collector to list private static void check task manager metric availability final rest client rest client final resource i d task manager id throw exception final task manager metric header header task manager metric header get instance final task manager metric message parameter parameter header get unresolved message parameter parameter task manager id parameter resolve task manager id parameter metric filter parameter resolve collection singleton list status network total memory segment fetch metric rest client send request host port header parameter empty request body get instance get metric name predicate status network total memory segment private static x x fetch metric final supplier with exception completable future x i o exception client operation final predicate x predicate throw interrupted exception execution exception timeout exception final completable future x response future future util retry successful with delay try return client operation get catch i o exception e throw new runtime exception e time seconds deadline from now duration of seconds predicate new schedule executor service adapter schedule executor service return response future get time unit seconds private static predicate metric collection response body get metric name predicate final string metric name return response response get metric stream any match metric metric get id equal metric name 
prometheus reporter end to end i t case category travi group1 class run with parameterized class public class prometheus reporter end to end i t case extend test logger private static final logger log logger factory get logger prometheus reporter end to end i t case class private static final object mapper object mapper new object mapper private static final string prometheus version 2.4 private static final string prometheus file name private static final string prometheus jar prefix flink metric prometheus static final string base prometheus prometheus version final string os final string platform switch operate system get current operating system case mac os os darwin break case window os window break default os linux break switch processor architecture get processor architecture case x86 platform break case a m d64 platform amd64 break case a r mv7 platform armv7 break case a a r c h64 platform arm64 break default platform unknown break prometheus file name base os platform private static final pattern log reporter port pattern pattern compile start prometheus reporter http server on port 0 - 9 before class public static void check o s assume assume false this test do not run on window operate system be window parameterized parameter name index public static collection test param test parameter return array as list test param from jar in lib builder builder move jar prometheus jar prefix jar location plugin jar location lib reflection test param from jar in lib builder builder move jar prometheus jar prefix jar location plugin jar location lib factory test param from jar in plugin builder reflection test param from jar in plugin builder factory test param from jar in lib and plugin builder builder copy jar prometheus jar prefix jar location plugin jar location lib reflection test param from jar in lib and plugin builder builder copy jar prometheus jar prefix jar location plugin jar location lib factory rule public final flink resource dist public prometheus reporter end to end i t case test param param final flink resource setup flink resource setup builder builder flink resource setup builder param get builder setup accept builder builder add configuration get flink config param get instantiation type dist new local standalone flink resource factory create builder build rule public final temporary folder tmp new temporary folder rule public final download cache download cache download cache get private static configuration get flink config test param instantiation type instantiation type final configuration config new configuration switch instantiation type case factory config set string config constant metric reporter prefix prom config constant metric reporter factory class suffix prometheus reporter factory class get name break case reflection config set string config constant metric reporter prefix prom config constant metric reporter class suffix prometheus reporter class get canonical name config set string config constant metric reporter prefix prom port 9000 - 9100 return config test public void test reporter throw exception final path tmp prometheus dir tmp new folder to path resolve prometheus final path prometheus bin dir tmp prometheus dir resolve prometheus file name final path prometheus config prometheus bin dir resolve prometheus yml final path prometheus binary prometheus bin dir resolve prometheus file create directory tmp prometheus dir final path prometheus archive download cache get or download http github com prometheus prometheus release download v prometheus version prometheus file name tar gz tmp prometheus dir log info unpack prometheus run block command line wrapper tar prometheus archive extract zipped target dir tmp prometheus dir build log info set prometheus scrape interval run block command line wrapper sed s scrape interval 1 1 s prometheus config in place build try cluster controller ignore dist start cluster final list integer port dist search all log log reporter port pattern matcher matcher group map integer value of collect collector to list final string scrape target port stream map port localhost port collect collector join log info set prometheus scrape target to scrape target run block command line wrapper sed s target scrape target prometheus config in place build log info start prometheus server try auto closable process prometheus run non block prometheus binary to absolute path to string config file prometheus config to absolute path storage tsdb path prometheus bin dir resolve datum to absolute path final ok http client client new ok http client check metric availability client flink jobmanager num register task manager check metric availability client flink taskmanager status network total memory segment private static void check metric availability final ok http client client final string metric throw interrupted exception final request job manager request new request builder get url http localhost api v1 query query metric build exception report exception null for int x x x try response response client new call job manager request execute if response be successful final string json response body string sample response status success datum result type vector result metric name flink jobmanager num register task manager host localhost instance localhost job prometheus value 1540548500.107 object mapper read tree json get datum get result get get value get as int if we reach this point some value for the give metric be report to prometheus return else log info retrieve metric fail retry response code response message thread sleep catch exception e report exception exception util first or suppress e report exception thread sleep throw new assertion error could not retrieve metric metric from prometheus report exception static class test param private final string jar location description private final consumer flink resource setup flink resource setup builder builder setup private final instantiation type instantiation type private test param string jar location description consumer flink resource setup flink resource setup builder builder setup instantiation type instantiation type this jar location description jar location description this builder setup builder setup this instantiation type instantiation type public static test param from string jar location desription consumer flink resource setup flink resource setup builder builder setup instantiation type instantiation type return new test param jar location desription builder setup instantiation type public consumer flink resource setup flink resource setup builder get builder setup return builder setup public instantiation type get instantiation type return instantiation type override public string to string return jar location description instantiate via instantiation type name to lower case public enum instantiation type reflection factory 
