public  evolving  public  class    client  options  public  static  final    config  option    duration  client  timeout    config  options  key  client  timeout  duration  type  default  value    duration  of  seconds    with  deprecated  keys  akka  client  timeout  the  deprecated    akka  options  client  timeout  with  description    timeout  on  the  client  side  public  static  final    config  option    duration  client  retry  period    config  options  key  client  retry  period  duration  type  default  value    duration  of  millis    with  description    the  interval  in  ms  between  consecutive  retries  of  failed  attempts  to  execute  commands  through  the  cli  or    flink  s  clients  wherever  retry  is  supported  default  2sec  
public  evolving  public  class    application  execution  exception  extends    flink  exception  public    application  execution  exception    string  message  super  message  public    application  execution  exception    throwable  cause  super  cause  public    application  execution  exception    string  message    throwable  cause  super  message  cause  
public  evolving  public  class    web  submission  job  client  implements    job  client  private  final    job  i  d  job  id  public    web  submission  job  client  final    job  i  d  job  id  this  job  id  check  not  null  job  id    override  public    job  i  d  get  job  i  d  return  job  id    override  public    completable  future    job  status  get  job  status  throw  new    flink  runtime  exception    the    job    status  cannot  be  requested  when  in    web    submission    override  public    completable  future    void  cancel  throw  new    flink  runtime  exception    cancelling  the  job  is  not  supported  by  the    job    client  when  in    web    submission    override  public    completable  future    string  stop  with  savepoint  boolean  advance  to  end  of  event  time    nullable    string  savepoint  directory  throw  new    flink  runtime  exception    stop  with    savepoint  is  not  supported  by  the    job    client  when  in    web    submission    override  public    completable  future    string  trigger  savepoint    nullable    string  savepoint  directory  throw  new    flink  runtime  exception  a  savepoint  cannot  be  taken  through  the    job    client  when  in    web    submission    override  public    completable  future    map    string    object  get  accumulators    class  loader  class  loader  throw  new    flink  runtime  exception    the    accumulators  cannot  be  fetched  through  the    job    client  when  in    web    submission    override  public    completable  future    job  execution  result  get  job  execution  result    class  loader  user  classloader  throw  new    flink  runtime  exception    the    job    result  cannot  be  fetched  through  the    job    client  when  in    web    submission  
public  evolving  public  class    stream  context  environment  extends    stream  execution  environment  private  static  final    logger  log    logger  factory  get  logger    execution  environment  class  private  final  boolean  suppress  sysout  private  final  boolean  enforce  single  job  execution  private  int  job  counter  public    stream  context  environment  final    pipeline  executor  service  loader  executor  service  loader  final    configuration  configuration  final    class  loader  user  code  class  loader  final  boolean  enforce  single  job  execution  final  boolean  suppress  sysout  super  executor  service  loader  configuration  user  code  class  loader  this  suppress  sysout  suppress  sysout  this  enforce  single  job  execution  enforce  single  job  execution  this  job  counter      override  public    job  execution  result  execute    stream  graph  stream  graph  throws    exception  final    job  client  job  client  execute  async  stream  graph  final    list    job  listener  job  listeners  get  job  listeners  try  final    job  execution  result  job  execution  result  get  job  execution  result  job  client  job  listeners  for  each  job  listener  job  listener  on  job  executed  job  execution  result  null  return  job  execution  result  catch    throwable  t  job  listeners  for  each  job  listener  job  listener  on  job  executed  null    exception  utils  strip  execution  exception  t    exception  utils  rethrow  exception  t  never  reached  only  make  javac  happy  return  null  private    job  execution  result  get  job  execution  result  final    job  client  job  client  throws    exception  check  not  null  job  client    job  execution  result  job  execution  result  if  get  configuration  get  boolean    deployment  options  attached    completable  future    job  execution  result  job  execution  result  future  job  client  get  job  execution  result  get  user  classloader  if  get  configuration  get  boolean    deployment  options  shutdown  if  attached    thread  shutdown  hook    shutdown  hook  util  add  shutdown  hook  wait  a  smidgen  to  allow  the  async  request  to  go  through  before  the  jvm  exits  job  client  cancel  get      time  unit  seconds    stream  context  environment  class  get  simple  name  log  job  execution  result  future  when  complete  ignored  throwable    shutdown  hook  util  remove  shutdown  hook  shutdown  hook    stream  context  environment  class  get  simple  name  log  job  execution  result  job  execution  result  future  get    system  out  println  job  execution  result  else  job  execution  result  new    detached  job  execution  result  job  client  get  job  i  d  return  job  execution  result    override  public    job  client  execute  async    stream  graph  stream  graph  throws    exception  validate  allowed  execution  final    job  client  job  client  super  execute  async  stream  graph  if  suppress  sysout    system  out  println    job  has  been  submitted  with    job  i  d  job  client  get  job  i  d  return  job  client  private  void  validate  allowed  execution  if  enforce  single  job  execution  job  counter    throw  new    flink  runtime  exception    cannot  have  more  than  one  execute  or  execute  async  call  in  a  single  environment  job  counter  public  static  void  set  as  context  final    pipeline  executor  service  loader  executor  service  loader  final    configuration  configuration  final    class  loader  user  code  class  loader  final  boolean  enforce  single  job  execution  final  boolean  suppress  sysout    stream  execution  environment  factory  factory  new    stream  context  environment  executor  service  loader  configuration  user  code  class  loader  enforce  single  job  execution  suppress  sysout  initialize  context  environment  factory  public  static  void  unset  as  context  reset  context  environment  
public  evolving  public  class    stream  plan  environment  extends    stream  execution  environment  private    pipeline  pipeline  public    pipeline  get  pipeline  return  pipeline  public    stream  plan  environment    configuration  configuration    class  loader  user  class  loader  int  parallelism  super  configuration  user  class  loader  if  parallelism    set  parallelism  parallelism    override  public    job  client  execute  async    stream  graph  stream  graph  pipeline  stream  graph  do  not  go  on  with  anything  now  throw  new    program  abort  exception  public  void  set  as  context    stream  execution  environment  factory  factory  this  initialize  context  environment  factory  public  void  unset  as  context  reset  context  environment  
public  evolving  public  interface    cassandra  failure  handler  extends    serializable    handle  a  failed  link    throwable  param  failure  the  cause  of  failure  throws    i  o  exception  if  the  sink  should  fail  on  this  failure  the  implementation  should  rethrow  the  throwable  or  a  custom  one  void  on  failure    throwable  failure  throws    i  o  exception  
public  evolving  public    cassandra  sink  in  uid    string  uid  if  use  data  stream  sink  get  sink  transformation  set  uid  uid  else  get  transformation  set  uid  uid  return  this  
public  evolving  public    cassandra  sink  in  set  uid  hash    string  uid  hash  if  use  data  stream  sink  get  sink  transformation  set  uid  hash  uid  hash  else  get  transformation  set  uid  hash  uid  hash  return  this  
public  evolving  public  interface    action  request  failure  handler  extends    serializable    handle  a  failed  link    action  request  param  action  the  link    action  request  that  failed  due  to  the  failure  param  failure  the  cause  of  failure  param  rest  status  code  the  rest  status  code  of  the  failure    if  none  can  be  retrieved  param  indexer  request  indexer  to  re  add  the  failed  action  if  intended  to  do  so  throws    throwable  if  the  sink  should  fail  on  this  failure  the  implementation  should  rethrow  the  exception  or  a  custom  one  void  on  failure    action  request  action    throwable  failure  int  rest  status  code    request  indexer  indexer  throws    throwable  
public  evolving  public  interface    elasticsearch  sink  function  t  extends    serializable    function    initialization  method  for  the  function    it  is  called  once  before  the  actual  working  process  methods  default  void  open  throws    exception    tear  down  method  for  the  function    it  is  called  when  the  sink  closes  default  void  close  throws    exception    process  the  incoming  element  to  produce  multiple  link    action  request    actions  requests    the  produced  requests  should  be  added  to  the  provided  link    request  indexer  param  element  incoming  element  to  process  param  ctx  runtime  context  containing  information  about  the  sink  instance  param  indexer  request  indexer  that  code    action  request  should  be  added  to  void  process  t  element    runtime  context  ctx    request  indexer  indexer  
public  evolving  public  interface    request  indexer    add  multiple  link    action  request  to  the  indexer  to  prepare  for  sending  requests  to    elasticsearch  param  action  requests    the  multiple  link    action  request  to  add  deprecated  use  the  link    delete  request  link    index  request  or  link    update  request    deprecated  default  void  add    action  request  action  requests  for    action  request  action  request  action  requests  if  action  request  instanceof    index  request  add    index  request  action  request  else  if  action  request  instanceof    delete  request  add    delete  request  action  request  else  if  action  request  instanceof    update  request  add    update  request  action  request  else  throw  new    illegal  argument  exception    request  indexer  only  supports    index    delete  and    update  requests    add  multiple  link    delete  request  to  the  indexer  to  prepare  for  sending  requests  to    elasticsearch  param  delete  requests    the  multiple  link    delete  request  to  add  void  add    delete  request  delete  requests    add  multiple  link    index  request  to  the  indexer  to  prepare  for  sending  requests  to    elasticsearch  param  index  requests    the  multiple  link    index  request  to  add  void  add    index  request  index  requests    add  multiple  link    update  request  to  the  indexer  to  prepare  for  sending  requests  to    elasticsearch  param  update  requests    the  multiple  link    update  request  to  add  void  add    update  request  update  requests  
public  evolving  public  class    retry  rejected  execution  failure  handler  implements    action  request  failure  handler  private  static  final  long  serial  version  u  i  d    l  private  static  final    logger  log    logger  factory  get  logger    retry  rejected  execution  failure  handler  class    override  public  void  on  failure    action  request  action    throwable  failure  int  rest  status  code    request  indexer  indexer  throws    throwable  log  error    failed    elasticsearch  item  request  failure  get  message  failure  if    exception  utils  find  throwable  failure    es  rejected  execution  exception  class  is  present  indexer  add  action  else  rethrow  all  other  failures  throw  failure  
public  evolving  public  class    elasticsearch  extends    connector  descriptor  private    descriptor  properties  internal  properties  new    descriptor  properties  true  private    list    host  hosts  new    array  list    connector  descriptor  for  the    elasticsearch  search  engine  public    elasticsearch  super  connector  type  value  elasticsearch    true    sets  the    elasticsearch  version  to  be  used    required  param  version    elasticsearch  version  e  g    public    elasticsearch  version    string  version  internal  properties  put  string  connector  version  version  return  this    adds  an    elasticsearch  host  to  connect  to    required  p    multiple  hosts  can  be  declared  by  calling  this  method  multiple  times  param  hostname  connection  hostname  param  port  connection  port  param  protocol  connection  protocol  e  g  http  public    elasticsearch  host    string  hostname  int  port    string  protocol  final    host  host  new    host    preconditions  check  not  null  hostname  port    preconditions  check  not  null  protocol  hosts  add  host  return  this    declares  the    elasticsearch  index  for  every  record    required  param  index    elasticsearch  index  public    elasticsearch  index    string  index  internal  properties  put  string  connector  index  index  return  this    declares  the    elasticsearch  document  type  for  every  record    required  param  document  type    elasticsearch  document  type  public    elasticsearch  document  type    string  document  type  internal  properties  put  string  connector  document  type  document  type  return  this    sets  a  custom  key  delimiter  in  case  the    elasticsearch  id  needs  to  be  constructed  from  multiple  fields    optional  param  key  delimiter  key  delimiter  e  g  would  result  in    i  ds    k  e  y1    k  e  y2    k  e  y3  public    elasticsearch  key  delimiter    string  key  delimiter  internal  properties  put  string  connector  key  delimiter  key  delimiter  return  this    sets  a  custom  representation  for  null  fields  in  keys    optional  param  key  null  literal  key  null  literal  string  e  g  n  a  would  result  in    i  ds    k  e  y1  n  a    k  e  y3  public    elasticsearch  key  null  literal    string  key  null  literal  internal  properties  put  string  connector  key  null  literal  key  null  literal  return  this    configures  a  failure  handling  strategy  in  case  a  request  to    elasticsearch  fails  p    this  strategy  throws  an  exception  if  a  request  fails  and  thus  causes  a  job  failure  public    elasticsearch  failure  handler  fail  internal  properties  put  string  connector  failure  handler    elasticsearch  validator  connector  failure  handler  value  fail  return  this    configures  a  failure  handling  strategy  in  case  a  request  to    elasticsearch  fails  p    this  strategy  ignores  failures  and  drops  the  request  public    elasticsearch  failure  handler  ignore  internal  properties  put  string  connector  failure  handler    elasticsearch  validator  connector  failure  handler  value  ignore  return  this    configures  a  failure  handling  strategy  in  case  a  request  to    elasticsearch  fails  p    this  strategy  re  adds  requests  that  have  failed  due  to  queue  capacity  saturation  public    elasticsearch  failure  handler  retry  rejected  internal  properties  put  string  connector  failure  handler    elasticsearch  validator  connector  failure  handler  value  retry  return  this    configures  a  failure  handling  strategy  in  case  a  request  to    elasticsearch  fails  p    this  strategy  allows  for  custom  failure  handling  using  a  link    action  request  failure  handler  public    elasticsearch  failure  handler  custom    class  extends    action  request  failure  handler  failure  handler  class  internal  properties  put  string  connector  failure  handler    elasticsearch  validator  connector  failure  handler  value  custom  internal  properties  put  class  connector  failure  handler  class  failure  handler  class  return  this    disables  flushing  on  checkpoint    when  disabled  a  sink  will  not  wait  for  all  pending  action  requests  to  be  acknowledged  by    elasticsearch  on  checkpoints  p    note    if  flushing  on  checkpoint  is  disabled  a    elasticsearch  sink  does  not  provide  any  strong  guarantees  for  at  least  once  delivery  of  action  requests  public    elasticsearch  disable  flush  on  checkpoint  internal  properties  put  boolean  connector  flush  on  checkpoint  false  return  this    configures  how  to  buffer  elements  before  sending  them  in  bulk  to  the  cluster  for  efficiency  p    sets  the  maximum  number  of  actions  to  buffer  for  each  bulk  request  param  max  actions  the  maximum  number  of  actions  to  buffer  per  bulk  request  public    elasticsearch  bulk  flush  max  actions  int  max  actions  internal  properties  put  int  connector  bulk  flush  max  actions  max  actions  return  this    configures  how  to  buffer  elements  before  sending  them  in  bulk  to  the  cluster  for  efficiency  p    sets  the  maximum  size  of  buffered  actions  per  bulk  request  using  the  syntax  of  link    memory  size  public    elasticsearch  bulk  flush  max  size    string  max  size  internal  properties  put  memory  size  connector  bulk  flush  max  size    memory  size  parse  max  size    memory  size    memory  unit  bytes  return  this    configures  how  to  buffer  elements  before  sending  them  in  bulk  to  the  cluster  for  efficiency  p    sets  the  bulk  flush  interval  in  milliseconds  param  interval  bulk  flush  interval  in  milliseconds  public    elasticsearch  bulk  flush  interval  long  interval  internal  properties  put  long  connector  bulk  flush  interval  interval  return  this    configures  how  to  buffer  elements  before  sending  them  in  bulk  to  the  cluster  for  efficiency  p    sets  a  constant  backoff  type  to  use  when  flushing  bulk  requests  public    elasticsearch  bulk  flush  backoff  constant  internal  properties  put  string  connector  bulk  flush  backoff  type    elasticsearch  validator  connector  bulk  flush  backoff  type  value  constant  return  this    configures  how  to  buffer  elements  before  sending  them  in  bulk  to  the  cluster  for  efficiency  p    sets  an  exponential  backoff  type  to  use  when  flushing  bulk  requests  public    elasticsearch  bulk  flush  backoff  exponential  internal  properties  put  string  connector  bulk  flush  backoff  type    elasticsearch  validator  connector  bulk  flush  backoff  type  value  exponential  return  this    configures  how  to  buffer  elements  before  sending  them  in  bulk  to  the  cluster  for  efficiency  p    sets  the  maximum  number  of  retries  for  a  backoff  attempt  when  flushing  bulk  requests  p    make  sure  to  enable  backoff  by  selecting  a  strategy  link  bulk  flush  backoff  constant  or  link  bulk  flush  backoff  exponential  param  max  retries  the  maximum  number  of  retries  public    elasticsearch  bulk  flush  backoff  max  retries  int  max  retries  internal  properties  put  int  connector  bulk  flush  backoff  max  retries  max  retries  return  this    configures  how  to  buffer  elements  before  sending  them  in  bulk  to  the  cluster  for  efficiency  p    sets  the  amount  of  delay  between  each  backoff  attempt  when  flushing  bulk  requests  in  milliseconds  p    make  sure  to  enable  backoff  by  selecting  a  strategy  link  bulk  flush  backoff  constant  or  link  bulk  flush  backoff  exponential  param  delay  delay  between  each  backoff  attempt  in  milliseconds  public    elasticsearch  bulk  flush  backoff  delay  long  delay  internal  properties  put  long  connector  bulk  flush  backoff  delay  delay  return  this    sets  connection  properties  to  be  used  during  rest  communication  to    elasticsearch  p    sets  the  maximum  timeout  in  milliseconds  in  case  of  multiple  retries  of  the  same  request  param  max  retry  timeout  maximum  timeout  in  milliseconds  public    elasticsearch  connection  max  retry  timeout  int  max  retry  timeout  internal  properties  put  int  connector  connection  max  retry  timeout  max  retry  timeout  return  this    sets  connection  properties  to  be  used  during  rest  communication  to    elasticsearch  p    adds  a  path  prefix  to  every  rest  communication  param  path  prefix  prefix  string  to  be  added  to  every  rest  communication  public    elasticsearch  connection  path  prefix    string  path  prefix  internal  properties  put  string  connector  connection  path  prefix  path  prefix  return  this    override  protected    map    string    string  to  connector  properties  final    descriptor  properties  properties  new    descriptor  properties  properties  put  properties  internal  properties  if  hosts  size    properties  put  string  connector  hosts  hosts  stream  map    host  to  string  collect    collectors  joining  return  properties  as  map  
public  evolving  public  class    elasticsearch  sink  t  extends    elasticsearch  sink  base  t    transport  client  private  static  final  long  serial  version  u  i  d  1  l    creates  a  new  code    elasticsearch  sink  that  connects  to  the  cluster  using  a  link    transport  client  param  user  config    the  map  of  user  settings  that  are  used  when  constructing  the  link    transport  client  and  link    bulk  processor  param  transport  addresses    the  addresses  of    elasticsearch  nodes  to  which  to  connect  using  a  link    transport  client  param  elasticsearch  sink  function    this  is  used  to  generate  multiple  link    action  request  from  the  incoming  element  public    elasticsearch  sink    map    string    string  user  config    list    inet  socket  address  transport  addresses    elasticsearch  sink  function  t  elasticsearch  sink  function  this  user  config  transport  addresses  elasticsearch  sink  function  new    no  op  failure  handler    creates  a  new  code    elasticsearch  sink  that  connects  to  the  cluster  using  a  link    transport  client  param  user  config    the  map  of  user  settings  that  are  used  when  constructing  the  link    transport  client  and  link    bulk  processor  param  transport  addresses    the  addresses  of    elasticsearch  nodes  to  which  to  connect  using  a  link    transport  client  param  elasticsearch  sink  function    this  is  used  to  generate  multiple  link    action  request  from  the  incoming  element  param  failure  handler    this  is  used  to  handle  failed  link    action  request  public    elasticsearch  sink    map    string    string  user  config    list    inet  socket  address  transport  addresses    elasticsearch  sink  function  t  elasticsearch  sink  function    action  request  failure  handler  failure  handler  super  new    elasticsearch5  api  call  bridge  transport  addresses  user  config  elasticsearch  sink  function  failure  handler  
public  evolving  final  class    elasticsearch6  dynamic  sink  implements    dynamic  table  sink    visible  for  testing  static  final    elasticsearch6  request  factory  request  factory  new    elasticsearch6  request  factory  private  final    encoding  format    serialization  schema    row  data  format  private  final    table  schema  schema  private  final    elasticsearch6  configuration  config  public    elasticsearch6  dynamic  sink    encoding  format    serialization  schema    row  data  format    elasticsearch6  configuration  config    table  schema  schema  this  format  config  schema    elasticsearch  sink    builder  new    hack  to  make  configuration  testing  possible    the  code  in  this  block  should  never  be  used  outside  of  tests    having  a  way  to  inject  a  builder  we  can  assert  the  builder  in  the  test    we  can  not  assert  everything  though  e  g  it  is  not  possible  to  assert  flushing  on  checkpoint  as  it  is  configured  on  the  sink  itself  private  final    elastic  search  builder  provider  builder  provider    functional  interface  interface    elastic  search  builder  provider    elasticsearch  sink    builder    row  data  create  builder    list    http  host  http  hosts    row  elasticsearch  sink  function  upsert  sink  function    elasticsearch6  dynamic  sink    encoding  format    serialization  schema    row  data  format    elasticsearch6  configuration  config    table  schema  schema    elastic  search  builder  provider  builder  provider  this  format  format  this  schema  schema  this  config  config  this  builder  provider  builder  provider    end  of  hack  to  make  configuration  testing  possible    override  public    changelog  mode  get  changelog  mode    changelog  mode  requested  mode    changelog  mode    builder  builder    changelog  mode  new  builder  for    row  kind  kind  requested  mode  get  contained  kinds  if  kind    row  kind  update  before  builder  add  contained  kind  kind  return  builder  build    override  public    sink  function  provider  get  sink  runtime  provider    context  context  return    serialization  schema    row  data  format  this  format  create  runtime  encoder  context  schema  to  row  data  type  final    row  elasticsearch  sink  function  upsert  function  new    row  elasticsearch  sink  function    index  generator  factory  create  index  generator  config  get  index  schema  config  get  document  type  format    x  content  type  json  request  factory    key  extractor  create  key  extractor  schema  config  get  key  delimiter  final    elasticsearch  sink    builder    row  data  builder  builder  provider  create  builder  config  get  hosts  upsert  function  builder  set  failure  handler  config  get  failure  handler  builder  set  bulk  flush  max  actions  config  get  bulk  flush  max  actions  builder  set  bulk  flush  max  size  mb  int  config  get  bulk  flush  max  byte  size    builder  set  bulk  flush  interval  config  get  bulk  flush  interval  builder  set  bulk  flush  backoff  config  is  bulk  flush  backoff  enabled  config  get  bulk  flush  backoff  type  if  present  builder  set  bulk  flush  backoff  type  config  get  bulk  flush  backoff  retries  if  present  builder  set  bulk  flush  backoff  retries  config  get  bulk  flush  backoff  delay  if  present  builder  set  bulk  flush  backoff  delay  we  must  overwrite  the  default  factory  which  is  defined  with  a  lambda  because  of  a  bug  in  shading  lambda  serialization  shading  see  flink    if  config  get  username  is  present  config  get  password  is  present    string  utils  is  null  or  whitespace  only  config  get  username  get    string  utils  is  null  or  whitespace  only  config  get  password  get  builder  set  rest  client  factory  new    auth  rest  client  factory  config  get  path  prefix  or  else  null  config  get  username  get  config  get  password  get  else  builder  set  rest  client  factory  new    default  rest  client  factory  config  get  path  prefix  or  else  null  final    elasticsearch  sink    row  data  sink  builder  build  if  config  is  disable  flush  on  checkpoint  sink  disable  flush  on  checkpoint  return  sink    override  public    dynamic  table  sink  copy  return  this    override  public    string  as  summary  string  return    elasticsearch6    serializable  link    rest  client  factory  used  by  the  sink    visible  for  testing  static  class    default  rest  client  factory  implements    rest  client  factory  private  final    string  path  prefix  public    default  rest  client  factory    nullable    string  path  prefix  this  path  prefix  path  prefix    override  public  void  configure  rest  client  builder    rest  client  builder  rest  client  builder  if  path  prefix  null  rest  client  builder  set  path  prefix  path  prefix    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    default  rest  client  factory  that    default  rest  client  factory  o  return    objects  equals  path  prefix  that  path  prefix    override  public  int  hash  code  return    objects  hash  path  prefix    serializable  link    rest  client  factory  used  by  the  sink  which  enable  authentication    visible  for  testing  static  class    auth  rest  client  factory  implements    rest  client  factory  private  final    string  path  prefix  private  final    string  username  private  final    string  password  private  transient    credentials  provider  credentials  provider  public    auth  rest  client  factory    nullable    string  path  prefix    string  username    string  password  this  path  prefix  path  prefix  this  password  password  this  username  username    override  public  void  configure  rest  client  builder    rest  client  builder  rest  client  builder  if  path  prefix  null  rest  client  builder  set  path  prefix  path  prefix  if  credentials  provider  null  credentials  provider  new    basic  credentials  provider  credentials  provider  set  credentials    auth  scope  any  new    username  password  credentials  username  password  rest  client  builder  set  http  client  config  callback  http  async  client  builder  http  async  client  builder  set  default  credentials  provider  credentials  provider    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    auth  rest  client  factory  that    auth  rest  client  factory  o  return    objects  equals  path  prefix  that  path  prefix    objects  equals  username  that  username    objects  equals  password  that  password    override  public  int  hash  code  return    objects  hash  path  prefix  username  password    version  specific  creation  of  link  org  elasticsearch  action    action  request  s  used  by  the  sink  private  static  class    elasticsearch6  request  factory  implements    request  factory    override  public    update  request  create  update  request    string  index    string  doc  type    string  key    x  content  type  content  type  byte  document  return  new    update  request  index  doc  type  key  doc  document  content  type  upsert  document  content  type    override  public    index  request  create  index  request    string  index    string  doc  type    string  key    x  content  type  content  type  byte  document  return  new    index  request  index  doc  type  key  source  document  content  type    override  public    delete  request  create  delete  request    string  index    string  doc  type    string  key  return  new    delete  request  index  doc  type  key    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    elasticsearch6  dynamic  sink  that    elasticsearch6  dynamic  sink  o  return    objects  equals  format  that  format    objects  equals  schema  that  schema    objects  equals  config  that  config    objects  equals  builder  provider  that  builder  provider    override  public  int  hash  code  return    objects  hash  format  schema  config  builder  provider  
public  evolving  public  class    elasticsearch  sink  t  extends    elasticsearch  sink  base  t    rest  high  level  client  private  static  final  long  serial  version  u  i  d  1  l  private    elasticsearch  sink    map    string    string  bulk  requests  config    list    http  host  http  hosts    elasticsearch  sink  function  t  elasticsearch  sink  function    action  request  failure  handler  failure  handler    rest  client  factory  rest  client  factory  super  new    elasticsearch6  api  call  bridge  http  hosts  rest  client  factory  bulk  requests  config  elasticsearch  sink  function  failure  handler  a  builder  for  creating  an  link    elasticsearch  sink  param  t    type  of  the  elements  handled  by  the  sink  this  builder  creates    public  evolving  public  static  class    builder  t  private  final    list    http  host  http  hosts  private  final    elasticsearch  sink  function  t  elasticsearch  sink  function  private    map    string    string  bulk  requests  config  new    hash  map  private    action  request  failure  handler  failure  handler  new    no  op  failure  handler  private    rest  client  factory  rest  client  factory  rest  client  builder    creates  a  new  code    elasticsearch  sink  that  connects  to  the  cluster  using  a  link    rest  high  level  client  param  http  hosts    the  list  of  link    http  host  to  which  the  link    rest  high  level  client  connects  to  param  elasticsearch  sink  function    this  is  used  to  generate  multiple  link    action  request  from  the  incoming  element  public    builder    list    http  host  http  hosts    elasticsearch  sink  function  t  elasticsearch  sink  function  this  http  hosts    preconditions  check  not  null  http  hosts  this  elasticsearch  sink  function    preconditions  check  not  null  elasticsearch  sink  function    sets  the  maximum  number  of  actions  to  buffer  for  each  bulk  request  param  num  max  actions  the  maxinum  number  of  actions  to  buffer  per  bulk  request  public  void  set  bulk  flush  max  actions  int  num  max  actions  this  bulk  requests  config  put  config  key  bulk  flush  max  actions    string  value  of  num  max  actions    sets  the  maximum  size  of  buffered  actions  in  mb  per  bulk  request  param  max  size  mb  the  maximum  size  of  buffered  actions  in  mb  public  void  set  bulk  flush  max  size  mb  int  max  size  mb  this  bulk  requests  config  put  config  key  bulk  flush  max  size  mb    string  value  of  max  size  mb    sets  the  bulk  flush  interval  in  milliseconds  param  interval  millis  the  bulk  flush  interval  in  milliseconds  public  void  set  bulk  flush  interval  long  interval  millis  this  bulk  requests  config  put  config  key  bulk  flush  interval  ms    string  value  of  interval  millis    sets  whether  or  not  to  enable  bulk  flush  backoff  behaviour  param  enabled  whether  or  not  to  enable  backoffs  public  void  set  bulk  flush  backoff  boolean  enabled  this  bulk  requests  config  put  config  key  bulk  flush  backoff  enable    string  value  of  enabled    sets  the  type  of  back  of  to  use  when  flushing  bulk  requests  param  flush  backoff  type  the  backoff  type  to  use  public  void  set  bulk  flush  backoff  type    flush  backoff  type  flush  backoff  type  this  bulk  requests  config  put  config  key  bulk  flush  backoff  type    preconditions  check  not  null  flush  backoff  type  to  string    sets  the  maximum  number  of  retries  for  a  backoff  attempt  when  flushing  bulk  requests  param  max  retries  the  maximum  number  of  retries  for  a  backoff  attempt  when  flushing  bulk  requests  public  void  set  bulk  flush  backoff  retries  int  max  retries    preconditions  check  argument  max  retries      max  number  of  backoff  attempts  must  be  larger  than    this  bulk  requests  config  put  config  key  bulk  flush  backoff  retries    string  value  of  max  retries    sets  the  amount  of  delay  between  each  backoff  attempt  when  flushing  bulk  requests  in  milliseconds  param  delay  millis  the  amount  of  delay  between  each  backoff  attempt  when  flushing  bulk  requests  in  milliseconds  public  void  set  bulk  flush  backoff  delay  long  delay  millis    preconditions  check  argument  delay  millis      delay  in  milliseconds  between  each  backoff  attempt  must  be  larger  than  or  equal  to    this  bulk  requests  config  put  config  key  bulk  flush  backoff  delay    string  value  of  delay  millis    sets  a  failure  handler  for  action  requests  param  failure  handler    this  is  used  to  handle  failed  link    action  request  public  void  set  failure  handler    action  request  failure  handler  failure  handler  this  failure  handler    preconditions  check  not  null  failure  handler    sets  a  rest  client  factory  for  custom  client  configuration  param  rest  client  factory  the  factory  that  configures  the  rest  client  public  void  set  rest  client  factory    rest  client  factory  rest  client  factory  this  rest  client  factory    preconditions  check  not  null  rest  client  factory    creates  the    elasticsearch  sink  return  the  created    elasticsearch  sink  public    elasticsearch  sink  t  build  return  new    elasticsearch  sink  bulk  requests  config  http  hosts  elasticsearch  sink  function  failure  handler  rest  client  factory    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    builder  builder    builder  o  return    objects  equals  http  hosts  builder  http  hosts    objects  equals  elasticsearch  sink  function  builder  elasticsearch  sink  function    objects  equals  bulk  requests  config  builder  bulk  requests  config    objects  equals  failure  handler  builder  failure  handler    objects  equals  rest  client  factory  builder  rest  client  factory    override  public  int  hash  code  return    objects  hash  http  hosts  elasticsearch  sink  function  bulk  requests  config  failure  handler  rest  client  factory  
public  evolving  public  interface    rest  client  factory  extends    serializable    configures  the  rest  client  builder  param  rest  client  builder  the  configured  rest  client  builder  void  configure  rest  client  builder    rest  client  builder  rest  client  builder  
public  evolving  public  class    elasticsearch  sink  t  extends    elasticsearch  sink  base  t    rest  high  level  client  private  static  final  long  serial  version  u  i  d  1  l  private    elasticsearch  sink    map    string    string  bulk  requests  config    list    http  host  http  hosts    elasticsearch  sink  function  t  elasticsearch  sink  function    action  request  failure  handler  failure  handler    rest  client  factory  rest  client  factory  super  new    elasticsearch7  api  call  bridge  http  hosts  rest  client  factory  bulk  requests  config  elasticsearch  sink  function  failure  handler  a  builder  for  creating  an  link    elasticsearch  sink  param  t    type  of  the  elements  handled  by  the  sink  this  builder  creates    public  evolving  public  static  class    builder  t  private  final    list    http  host  http  hosts  private  final    elasticsearch  sink  function  t  elasticsearch  sink  function  private    map    string    string  bulk  requests  config  new    hash  map  private    action  request  failure  handler  failure  handler  new    no  op  failure  handler  private    rest  client  factory  rest  client  factory  rest  client  builder    creates  a  new  code    elasticsearch  sink  that  connects  to  the  cluster  using  a  link    rest  high  level  client  param  http  hosts    the  list  of  link    http  host  to  which  the  link    rest  high  level  client  connects  to  param  elasticsearch  sink  function    this  is  used  to  generate  multiple  link    action  request  from  the  incoming  element  public    builder    list    http  host  http  hosts    elasticsearch  sink  function  t  elasticsearch  sink  function  this  http  hosts    preconditions  check  not  null  http  hosts  this  elasticsearch  sink  function    preconditions  check  not  null  elasticsearch  sink  function    sets  the  maximum  number  of  actions  to  buffer  for  each  bulk  request  param  num  max  actions  the  maxinum  number  of  actions  to  buffer  per  bulk  request  public  void  set  bulk  flush  max  actions  int  num  max  actions    preconditions  check  argument  num  max  actions      max  number  of  buffered  actions  must  be  larger  than    this  bulk  requests  config  put  config  key  bulk  flush  max  actions    string  value  of  num  max  actions    sets  the  maximum  size  of  buffered  actions  in  mb  per  bulk  request  param  max  size  mb  the  maximum  size  of  buffered  actions  in  mb  public  void  set  bulk  flush  max  size  mb  int  max  size  mb    preconditions  check  argument  max  size  mb      max  size  of  buffered  actions  must  be  larger  than    this  bulk  requests  config  put  config  key  bulk  flush  max  size  mb    string  value  of  max  size  mb    sets  the  bulk  flush  interval  in  milliseconds  param  interval  millis  the  bulk  flush  interval  in  milliseconds  public  void  set  bulk  flush  interval  long  interval  millis    preconditions  check  argument  interval  millis      interval  in  milliseconds  between  each  flush  must  be  larger  than  or  equal  to    this  bulk  requests  config  put  config  key  bulk  flush  interval  ms    string  value  of  interval  millis    sets  whether  or  not  to  enable  bulk  flush  backoff  behaviour  param  enabled  whether  or  not  to  enable  backoffs  public  void  set  bulk  flush  backoff  boolean  enabled  this  bulk  requests  config  put  config  key  bulk  flush  backoff  enable    string  value  of  enabled    sets  the  type  of  back  of  to  use  when  flushing  bulk  requests  param  flush  backoff  type  the  backoff  type  to  use  public  void  set  bulk  flush  backoff  type    flush  backoff  type  flush  backoff  type  this  bulk  requests  config  put  config  key  bulk  flush  backoff  type    preconditions  check  not  null  flush  backoff  type  to  string    sets  the  maximum  number  of  retries  for  a  backoff  attempt  when  flushing  bulk  requests  param  max  retries  the  maximum  number  of  retries  for  a  backoff  attempt  when  flushing  bulk  requests  public  void  set  bulk  flush  backoff  retries  int  max  retries    preconditions  check  argument  max  retries      max  number  of  backoff  attempts  must  be  larger  than    this  bulk  requests  config  put  config  key  bulk  flush  backoff  retries    string  value  of  max  retries    sets  the  amount  of  delay  between  each  backoff  attempt  when  flushing  bulk  requests  in  milliseconds  param  delay  millis  the  amount  of  delay  between  each  backoff  attempt  when  flushing  bulk  requests  in  milliseconds  public  void  set  bulk  flush  backoff  delay  long  delay  millis    preconditions  check  argument  delay  millis      delay  in  milliseconds  between  each  backoff  attempt  must  be  larger  than  or  equal  to    this  bulk  requests  config  put  config  key  bulk  flush  backoff  delay    string  value  of  delay  millis    sets  a  failure  handler  for  action  requests  param  failure  handler    this  is  used  to  handle  failed  link    action  request  public  void  set  failure  handler    action  request  failure  handler  failure  handler  this  failure  handler    preconditions  check  not  null  failure  handler    sets  a  rest  client  factory  for  custom  client  configuration  param  rest  client  factory  the  factory  that  configures  the  rest  client  public  void  set  rest  client  factory    rest  client  factory  rest  client  factory  this  rest  client  factory    preconditions  check  not  null  rest  client  factory    creates  the    elasticsearch  sink  return  the  created    elasticsearch  sink  public    elasticsearch  sink  t  build  return  new    elasticsearch  sink  bulk  requests  config  http  hosts  elasticsearch  sink  function  failure  handler  rest  client  factory    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    builder  builder    builder  o  return    objects  equals  http  hosts  builder  http  hosts    objects  equals  elasticsearch  sink  function  builder  elasticsearch  sink  function    objects  equals  bulk  requests  config  builder  bulk  requests  config    objects  equals  failure  handler  builder  failure  handler    objects  equals  rest  client  factory  builder  rest  client  factory    override  public  int  hash  code  return    objects  hash  http  hosts  elasticsearch  sink  function  bulk  requests  config  failure  handler  rest  client  factory  
public  evolving  public  interface    rest  client  factory  extends    serializable    configures  the  rest  client  builder  param  rest  client  builder  the  configured  rest  client  builder  void  configure  rest  client  builder    rest  client  builder  rest  client  builder  
public  evolving  public  class    h  base  extends    connector  descriptor  private    descriptor  properties  properties  new    descriptor  properties  public    h  base  super  connector  type  value  hbase    true    set  the    apache    h  base  version  to  be  used    required  param  version    h  base  version  e  g  1.4    public    h  base  version    string  version  properties  put  string  connector  version  version  return  this    set  the    h  base  table  name    required  param  table  name    name  of    h  base  table  e  g  test  namespace  test  table  test  default  table  public    h  base  table  name    string  table  name  properties  put  string  connector  table  name  table  name  return  this    set  the  zookeeper  quorum  address  to  connect  the    h  base  cluster    required  param  zookeeper  quorum  zookeeper  quorum  address  to  connect  the    h  base  cluster  e  g  localhost    localhost    localhost    public    h  base  zookeeper  quorum    string  zookeeper  quorum  properties  put  string  connector  zk  quorum  zookeeper  quorum  return  this    set  the  zookeeper  node  parent  path  of    h  base  cluster    default  to  use  hbase    optional  param  zookeeper  node  parent  zookeeper  node  path  of  hbase  cluster  e  g  hbase  example  root  znode  public    h  base  zookeeper  node  parent    string  zookeeper  node  parent  properties  put  string  connector  zk  node  parent  zookeeper  node  parent  return  this    set  threshold  when  to  flush  buffered  request  based  on  the  memory  byte  size  of  rows  currently  added    default  to  code  2mb  code    optional  param  max  size  the  maximum  size  using  the  syntax  of  link    memory  size  public    h  base  write  buffer  flush  max  size    string  max  size  properties  put  memory  size  connector  write  buffer  flush  max  size    memory  size  parse  max  size    memory  size    memory  unit  bytes  return  this    set  threshold  when  to  flush  buffered  request  based  on  the  number  of  rows  currently  added    defaults  to  not  set  i  e  won  t  flush  based  on  the  number  of  buffered  rows    optional  param  write  buffer  flush  max  rows  number  of  added  rows  when  begin  the  request  flushing  public    h  base  write  buffer  flush  max  rows  int  write  buffer  flush  max  rows  properties  put  int  connector  write  buffer  flush  max  rows  write  buffer  flush  max  rows  return  this    set  an  interval  when  to  flushing  buffered  requesting  if  the  interval  passes  in  milliseconds    defaults  to  not  set  i  e  won  t  flush  based  on  flush  interval    optional  param  interval  flush  interval    the  string  should  be  in  format  length  value  time  unit  label  e  g    ms    s    if  no  time  unit  label  is  specified  it  will  be  considered  as  milliseconds    for  more  details  about  the  format  please  see  link    time  utils  parse  duration    string  public    h  base  write  buffer  flush  interval    string  interval  properties  put  string  connector  write  buffer  flush  interval  interval  return  this    override  protected    map    string    string  to  connector  properties  return  properties  as  map  
public  evolving  public  class    flink  hive  exception  extends    runtime  exception  public    flink  hive  exception    string  message  super  message  public    flink  hive  exception    throwable  cause  super  cause  public    flink  hive  exception    string  message    throwable  cause  super  message  cause  
public  evolving  public  class    jdbc  catalog  extends    abstract  jdbc  catalog  private  static  final    logger  log    logger  factory  get  logger    jdbc  catalog  class  private  final    abstract  jdbc  catalog  internal  public    jdbc  catalog    string  catalog  name    string  default  database    string  username    string  pwd    string  base  url  super  catalog  name  default  database  username  pwd  base  url  internal    jdbc  catalog  utils  create  catalog  catalog  name  default  database  username  pwd  base  url  databases    override  public    list    string  list  databases  throws    catalog  exception  return  internal  list  databases    override  public    catalog  database  get  database    string  database  name  throws    database  not  exist  exception    catalog  exception  return  internal  get  database  database  name  tables  and  views    override  public    list    string  list  tables    string  database  name  throws    database  not  exist  exception    catalog  exception  return  internal  list  tables  database  name    override  public    catalog  base  table  get  table    object  path  table  path  throws    table  not  exist  exception    catalog  exception  return  internal  get  table  table  path    override  public  boolean  table  exists    object  path  table  path  throws    catalog  exception  try  return  database  exists  table  path  get  database  name  list  tables  table  path  get  database  name  contains  table  path  get  object  name  catch    database  not  exist  exception  e  return  false  getters    visible  for  testing  public    abstract  jdbc  catalog  get  internal  return  internal  
public  evolving  public  class    jdbc  connection  options  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l  protected  final    string  url  protected  final    string  driver  name    nullable  protected  final    string  username    nullable  protected  final    string  password  protected    jdbc  connection  options    string  url    string  driver  name    string  username    string  password  this  url    preconditions  check  not  null  url  jdbc  url  is  empty  this  driver  name    preconditions  check  not  null  driver  name  driver  name  is  empty  this  username  username  this  password  password  public    string  get  db  u  r  l  return  url  public    string  get  driver  name  return  driver  name  public    optional    string  get  username  return    optional  of  nullable  username  public    optional    string  get  password  return    optional  of  nullable  password    builder  for  link    jdbc  connection  options  public  static  class    jdbc  connection  options  builder  private    string  url  private    string  driver  name  private    string  username  private    string  password  public    jdbc  connection  options  builder  with  url    string  url  this  url  url  return  this  public    jdbc  connection  options  builder  with  driver  name    string  driver  name  this  driver  name  driver  name  return  this  public    jdbc  connection  options  builder  with  username    string  username  this  username  username  return  this  public    jdbc  connection  options  builder  with  password    string  password  this  password  password  return  this  public    jdbc  connection  options  build  return  new    jdbc  connection  options  url  driver  name  username  password  
public  evolving  public  class    jdbc  execution  options  implements    serializable  public  static  final  int  default  max  retry  times    private  static  final  int  default  interval  millis    public  static  final  int  default  size    private  final  long  batch  interval  ms  private  final  int  batch  size  private  final  int  max  retries  private    jdbc  execution  options  long  batch  interval  ms  int  batch  size  int  max  retries    preconditions  check  argument  max  retries    this  batch  interval  ms  batch  interval  ms  this  batch  size  batch  size  this  max  retries  max  retries  public  long  get  batch  interval  ms  return  batch  interval  ms  public  int  get  batch  size  return  batch  size  public  int  get  max  retries  return  max  retries    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    jdbc  execution  options  that    jdbc  execution  options  o  return  batch  interval  ms  that  batch  interval  ms  batch  size  that  batch  size  max  retries  that  max  retries    override  public  int  hash  code  return    objects  hash  batch  interval  ms  batch  size  max  retries  public  static    builder  builder  return  new    builder  public  static    jdbc  execution  options  defaults  return  builder  build    builder  for  link    jdbc  execution  options  public  static  final  class    builder  private  long  interval  ms  default  interval  millis  private  int  size  default  size  private  int  max  retries  default  max  retry  times  public    builder  with  batch  size  int  size  this  size  size  return  this  public    builder  with  batch  interval  ms  long  interval  ms  this  interval  ms  interval  ms  return  this  public    builder  with  max  retries  int  max  retries  this  max  retries  max  retries  return  this  public    jdbc  execution  options  build  return  new    jdbc  execution  options  interval  ms  size  max  retries  
public  evolving  public  class    jdbc  sink    create  a  jdbc  sink  with  the  default  link    jdbc  execution  options  see  sink    string    jdbc  statement  builder    jdbc  execution  options    jdbc  connection  options  public  static  t    sink  function  t  sink    string  sql    jdbc  statement  builder  t  statement  builder    jdbc  connection  options  connection  options  return  sink  sql  statement  builder    jdbc  execution  options  defaults  connection  options    create  a  jdbc  sink  p    note  the  objects  passed  to  the  return  sink  can  be  processed  in  batch  and  retried    therefore  objects  can  not  be  link  org  apache  flink  api  common    execution  config  enable  object  reuse  reused  p  param  sql  arbitrary  dml  query  e  g  insert  update  upsert  param  statement  builder  sets  parameters  on  link  java  sql    prepared  statement  according  to  the  query  param  t  type  of  data  in  link  org  apache  flink  streaming  runtime  streamrecord    stream  record    stream  record  param  execution  options  parameters  of  execution  such  as  batch  size  and  maximum  retries  param  connection  options  parameters  of  connection  such  as  jdbc  url  public  static  t    sink  function  t  sink    string  sql    jdbc  statement  builder  t  statement  builder    jdbc  execution  options  execution  options    jdbc  connection  options  connection  options  return  new    generic  jdbc  sink  function  new    jdbc  batching  output  format  new    simple  jdbc  connection  provider  connection  options  execution  options  context    preconditions  check  state  context  get  execution  config  is  object  reuse  enabled  objects  can  not  be  reused  with  jdbc  sink  function  return    jdbc  batch  statement  executor  simple  sql  statement  builder    function  identity    jdbc  batching  output  format    record  extractor  identity  private    jdbc  sink  
public  evolving  public  interface    jdbc  statement  builder  t  extends    bi  consumer  with  exception    prepared  statement  t    s  q  l  exception    serializable  
public  evolving  public  class    flink  kafka  consumer  t  extends    flink  kafka  consumer  base  t    configuration  key  to  change  the  polling  timeout  public  static  final    string  key  poll  timeout  flink  poll  timeout    from    kafka  s    javadoc    the  time  in  milliseconds  spent  waiting  in  poll  if  data  is  not  available    if    returns  immediately  with  any  records  that  are  available  now  public  static  final  long  default  poll  timeout    l    user  supplied  properties  for    kafka  protected  final    properties  properties    from    kafka  s    javadoc    the  time  in  milliseconds  spent  waiting  in  poll  if  data  is  not  available    if    returns  immediately  with  any  records  that  are  available  now  protected  final  long  poll  timeout    creates  a  new    kafka  streaming  source  consumer  param  topic    the  name  of  the  topic  that  should  be  consumed  param  value  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props  public    flink  kafka  consumer    string  topic    deserialization  schema  t  value  deserializer    properties  props  this    collections  singleton  list  topic  value  deserializer  props    creates  a  new    kafka  streaming  source  consumer  p    this  constructor  allows  passing  a  see    kafka  deserialization  schema  for  reading  key  value  pairs  offsets  and  topic  names  from    kafka  param  topic    the  name  of  the  topic  that  should  be  consumed  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props  public    flink  kafka  consumer    string  topic    kafka  deserialization  schema  t  deserializer    properties  props  this    collections  singleton  list  topic  deserializer  props    creates  a  new    kafka  streaming  source  consumer  p    this  constructor  allows  passing  multiple  topics  to  the  consumer  param  topics    the    kafka  topics  to  read  from  param  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props  public    flink  kafka  consumer    list    string  topics    deserialization  schema  t  deserializer    properties  props  this  topics  new    kafka  deserialization  schema  wrapper  deserializer  props    creates  a  new    kafka  streaming  source  consumer  p    this  constructor  allows  passing  multiple  topics  and  a  key  value  deserialization  schema  param  topics    the    kafka  topics  to  read  from  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props  public    flink  kafka  consumer    list    string  topics    kafka  deserialization  schema  t  deserializer    properties  props  this  topics  null  deserializer  props    creates  a  new    kafka  streaming  source  consumer    use  this  constructor  to  subscribe  to  multiple  topics  based  on  a  regular  expression  pattern  p    if  partition  discovery  is  enabled  by  setting  a  non  negative  value  for  link    flink  kafka  consumer  key  partition  discovery  interval  millis  in  the  properties  topics  with  names  matching  the  pattern  will  also  be  subscribed  to  as  they  are  created  on  the  fly  param  subscription  pattern    the  regular  expression  for  a  pattern  of  topic  names  to  subscribe  to  param  value  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props  public    flink  kafka  consumer    pattern  subscription  pattern    deserialization  schema  t  value  deserializer    properties  props  this  null  subscription  pattern  new    kafka  deserialization  schema  wrapper  value  deserializer  props    creates  a  new    kafka  streaming  source  consumer    use  this  constructor  to  subscribe  to  multiple  topics  based  on  a  regular  expression  pattern  p    if  partition  discovery  is  enabled  by  setting  a  non  negative  value  for  link    flink  kafka  consumer  key  partition  discovery  interval  millis  in  the  properties  topics  with  names  matching  the  pattern  will  also  be  subscribed  to  as  they  are  created  on  the  fly  p    this  constructor  allows  passing  a  see    kafka  deserialization  schema  for  reading  key  value  pairs  offsets  and  topic  names  from    kafka  param  subscription  pattern    the  regular  expression  for  a  pattern  of  topic  names  to  subscribe  to  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props  public    flink  kafka  consumer    pattern  subscription  pattern    kafka  deserialization  schema  t  deserializer    properties  props  this  null  subscription  pattern  deserializer  props  private    flink  kafka  consumer    list    string  topics    pattern  subscription  pattern    kafka  deserialization  schema  t  deserializer    properties  props  super  topics  subscription  pattern  deserializer  get  long  check  not  null  props  props  key  partition  discovery  interval  millis  partition  discovery  disabled  get  boolean  props  key  disable  metrics  false  this  properties  props  set  deserializer  this  properties  configure  the  polling  timeout  try  if  properties  contains  key  key  poll  timeout  this  poll  timeout    long  parse  long  properties  get  property  key  poll  timeout  else  this  poll  timeout  default  poll  timeout  catch    exception  e  throw  new    illegal  argument  exception    cannot  parse  poll  timeout  for  key  poll  timeout  e    override  protected    abstract  fetcher  t  create  fetcher    source  context  t  source  context    map    kafka  topic  partition    long  assigned  partitions  with  initial  offsets    serialized  value    watermark  strategy  t  watermark  strategy    streaming  runtime  context  runtime  context    offset  commit  mode  offset  commit  mode    metric  group  consumer  metric  group  boolean  use  metrics  throws    exception  make  sure  that  auto  commit  is  disabled  when  our  offset  commit  mode  is  on  checkpoints  this  overwrites  whatever  setting  the  user  configured  in  the  properties  adjust  auto  commit  config  properties  offset  commit  mode  return  new    kafka  fetcher  source  context  assigned  partitions  with  initial  offsets  watermark  strategy  runtime  context  get  processing  time  service  runtime  context  get  execution  config  get  auto  watermark  interval  runtime  context  get  user  code  class  loader  runtime  context  get  task  name  with  subtasks  deserializer  properties  poll  timeout  runtime  context  get  metric  group  consumer  metric  group  use  metrics    override  protected    abstract  partition  discoverer  create  partition  discoverer    kafka  topics  descriptor  topics  descriptor  int  index  of  this  subtask  int  num  parallel  subtasks  return  new    kafka  partition  discoverer  topics  descriptor  index  of  this  subtask  num  parallel  subtasks  properties    override  protected    map    kafka  topic  partition    long  fetch  offsets  with  timestamp    collection    kafka  topic  partition  partitions  long  timestamp    map    topic  partition    long  partition  offsets  request  new    hash  map  partitions  size  for    kafka  topic  partition  partition  partitions  partition  offsets  request  put  new    topic  partition  partition  get  topic  partition  get  partition  timestamp  final    map    kafka  topic  partition    long  result  new    hash  map  partitions  size  use  a  short  lived  consumer  to  fetch  the  offsets  this  is  ok  because  this  is  a  one  time  operation  that  happens  only  on  startup  try    kafka  consumer  consumer  new    kafka  consumer  properties  for    map    entry    topic  partition    offset  and  timestamp  partition  to  offset  consumer  offsets  for  times  partition  offsets  request  entry  set  result  put  new    kafka  topic  partition  partition  to  offset  get  key  topic  partition  to  offset  get  key  partition  partition  to  offset  get  value  null  null  partition  to  offset  get  value  offset  return  result    override  protected  boolean  get  is  auto  commit  enabled  return  get  boolean  properties    consumer  config  enable  auto  commit  config  true    properties  util  get  long  properties    consumer  config  auto  commit  interval  ms  config        makes  sure  that  the    byte  array  deserializer  is  registered  in  the    kafka  properties  param  props    the    kafka  properties  to  register  the  serializer  in  private  static  void  set  deserializer    properties  props  final    string  de  ser  name    byte  array  deserializer  class  get  name    object  key  de  ser  props  get    consumer  config  key  deserializer  class  config    object  val  de  ser  props  get    consumer  config  value  deserializer  class  config  if  key  de  ser  null  key  de  ser  equals  de  ser  name  log  warn    ignoring  configured  key    de  serializer    consumer  config  key  deserializer  class  config  if  val  de  ser  null  val  de  ser  equals  de  ser  name  log  warn    ignoring  configured  value    de  serializer    consumer  config  value  deserializer  class  config  props  put    consumer  config  key  deserializer  class  config  de  ser  name  props  put    consumer  config  value  deserializer  class  config  de  ser  name  
public  evolving  public  enum    flink  kafka  error  code  producers  pool  empty  external  error  
public  evolving  public  class    flink  kafka  exception  extends    flink  exception  private  static  final  long  serial  version  u  i  d    l  private  final    flink  kafka  error  code  error  code  public    flink  kafka  exception    flink  kafka  error  code  error  code    string  message  super  message  this  error  code  error  code  public    flink  kafka  exception    flink  kafka  error  code  error  code    string  message    throwable  cause  super  message  cause  this  error  code  error  code  public    flink  kafka  error  code  get  error  code  return  error  code  
public  evolving  public  class    flink  kafka  producer  in  extends    two  phase  commit  sink  function  in    flink  kafka  producer    kafka  transaction  state    flink  kafka  producer    kafka  transaction  context    semantics  that  can  be  chosen  li  link  exactly  once  li  li  link  at  least  once  li  li  link  none  li  public  enum    semantic    semantic  exactly  once  the    flink  producer  will  write  all  messages  in  a    kafka  transaction  that  will  be  committed  to    kafka  on  a  checkpoint  p    in  this  mode  link    flink  kafka  producer  sets  up  a  pool  of  link    flink  kafka  internal  producer    between  each  checkpoint  a    kafka  transaction  is  created  which  is  committed  on  link    flink  kafka  producer  notify  checkpoint  complete  long    if  checkpoint  complete  notifications  are  running  late  link    flink  kafka  producer  can  run  out  of  link    flink  kafka  internal  producer  s  in  the  pool    in  that  case  any  subsequent  link    flink  kafka  producer  snapshot  state    function  snapshot  context  requests  will  fail  and  link    flink  kafka  producer  will  keep  using  the  link    flink  kafka  internal  producer  from  the  previous  checkpoint    to  decrease  the  chance  of  failing  checkpoints  there  are  four  options  li  decrease  number  of  max  concurrent  checkpoints  li  li  make  checkpoints  more  reliable  so  that  they  complete  faster  li  li  increase  the  delay  between  checkpoints  li  li  increase  the  size  of  link    flink  kafka  internal  producer  s  pool  li  exactly  once    semantic  at  least  once  the    flink  producer  will  wait  for  all  outstanding  messages  in  the    kafka  buffers  to  be  acknowledged  by  the    kafka  producer  on  a  checkpoint  at  least  once    semantic  none  means  that  nothing  will  be  guaranteed    messages  can  be  lost  and  or  duplicated  in  case  of  failure  none  private  static  final    logger  log    logger  factory  get  logger    flink  kafka  producer  class  private  static  final  long  serial  version  u  i  d  1  l    this  coefficient  determines  what  is  the  safe  scale  down  factor  p    if  the    flink  application  previously  failed  before  first  checkpoint  completed  or  we  are  starting  new  batch  of  link    flink  kafka  producer  from  scratch  without  clean  shutdown  of  the  previous  one  link    flink  kafka  producer  doesn  t  know  what  was  the  set  of  previously  used    kafka  s  transactional  id  s    in  that  case  it  will  try  to  play  safe  and  abort  all  of  the  possible  transactional  ids  from  the  range  of  code    get  number  of  parallel  subtasks  kafka  producers  pool  size  safe  scale  down  factor  p    the  range  of  available  to  use  transactional  ids  is  code    get  number  of  parallel  subtasks  kafka  producers  pool  size  p    this  means  that  if  we  decrease  code  get  number  of  parallel  subtasks  by  a  factor  larger  than  code  safe  scale  down  factor  we  can  have  a  left  some  lingering  transaction  public  static  final  int  safe  scale  down  factor      default  number  of    kafka  producers  in  the  pool    see  link    flink  kafka  producer    semantic  exactly  once  public  static  final  int  default  kafka  producers  pool  size      default  value  for  kafka  transaction  timeout  public  static  final    time  default  kafka  transaction  timeout    time  hours      configuration  key  for  disabling  the  metrics  reporting  public  static  final    string  key  disable  metrics  flink  disable  metrics    descriptor  of  the  transactional    i  ds  list    note    this  state  is  serialized  by    kryo    serializer  and  it  has  compatibility  problem  that  will  be  removed  later    please  use  next  transactional  id  hint  descriptor    v2    deprecated  private  static  final    list  state  descriptor    flink  kafka  producer    next  transactional  id  hint  next  transactional  id  hint  descriptor  new    list  state  descriptor  next  transactional  id  hint    type  information  of    next  transactional  id  hint  class  private  static  final    list  state  descriptor    flink  kafka  producer    next  transactional  id  hint  next  transactional  id  hint  descriptor    v2  new    list  state  descriptor  next  transactional  id  hint  v2  new    next  transactional  id  hint  serializer    state  for  next  transactional  id  hint  private  transient    list  state    flink  kafka  producer    next  transactional  id  hint  next  transactional  id  hint  state    generator  for    transactional    i  ds  private  transient    transactional  ids  generator  transactional  ids  generator    hint  for  picking  next  transactional  id  private  transient    flink  kafka  producer    next  transactional  id  hint  next  transactional  id  hint    user  defined  properties  for  the    producer  protected  final    properties  producer  config    the  name  of  the  default  topic  this  producer  is  writing  data  to  protected  final    string  default  topic  id    serializable    serialization  schema  for  turning  objects  used  with    flink  into  byte  for    kafka    nullable  private  final    keyed  serialization  schema  in  keyed  schema    serializable  serialization  schema  for  serializing  records  to  link    producer  record    producer  records    nullable  private  final    kafka  serialization  schema  in  kafka  schema    user  provided  partitioner  for  assigning  an  object  to  a    kafka  partition  for  each  topic    nullable  private  final    flink  kafka  partitioner  in  flink  kafka  partitioner    partitions  of  each  topic  protected  final    map    string  int  topic  partitions  map    max  number  of  producers  in  the  pool    if  all  producers  are  in  use  snapshoting  state  will  throw  an  exception  private  final  int  kafka  producers  pool  size    pool  of  available  transactional  ids  private  final    blocking  deque    string  available  transactional  ids  new    linked  blocking  deque    flag  controlling  whether  we  are  writing  the    flink  record  s  timestamp  into    kafka  protected  boolean  write  timestamp  to  kafka  false    flag  indicating  whether  to  accept  failures  and  log  them  or  to  fail  on  failures  private  boolean  log  failures  only    semantic  chosen  for  this  instance  protected    flink  kafka  producer    semantic  semantic    runtime  fields    the  callback  than  handles  error  propagation  or  logging  callbacks    nullable  protected  transient    callback  callback    errors  encountered  in  the  async  producer  are  stored  here    nullable  protected  transient  volatile    exception  async  exception    number  of  unacknowledged  records  protected  final    atomic  long  pending  records  new    atomic  long    cache  of  metrics  to  replace  already  registered  metrics  instead  of  overwriting  existing  ones  private  final    map    string    kafka  metric  mutable  wrapper  previously  created  metrics  new    hash  map    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  param  broker  list    comma  separated  addresses  of  the  brokers  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  keyless  serialization  schema  public    flink  kafka  producer    string  broker  list    string  topic  id    serialization  schema  in  serialization  schema  this  topic  id  serialization  schema  get  properties  from  broker  list  broker  list    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer    string    serialization  schema    properties    optional  instead  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  key  less  serialization  schema  param  producer  config    properties  with  the  producer  configuration  public    flink  kafka  producer    string  topic  id    serialization  schema  in  serialization  schema    properties  producer  config  this  topic  id  serialization  schema  producer  config    optional  of  new    flink  fixed  partitioner    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  key  less  link    serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    since  a  key  less  link    serialization  schema  is  used  all  records  sent  to    kafka  will  not  have  an  attached  key    therefore  if  a  partitioner  is  also  not  provided  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  topic  id    the  topic  to  write  data  to  param  serialization  schema  a  key  less  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  a  partitioner  is  not  provided  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  public    flink  kafka  producer    string  topic  id    serialization  schema  in  serialization  schema    properties  producer  config    optional    flink  kafka  partitioner  in  custom  partitioner  this  topic  id  serialization  schema  producer  config  custom  partitioner  or  else  null    semantic  at  least  once  default  kafka  producers  pool  size    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  key  less  link    serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    since  a  key  less  link    serialization  schema  is  used  all  records  sent  to    kafka  will  not  have  an  attached  key    therefore  if  a  partitioner  is  also  not  provided  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  topic  id    the  topic  to  write  data  to  param  serialization  schema  a  key  less  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  a  partitioner  is  not  provided  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  semantic    defines  semantic  that  will  be  used  by  this  producer  see  link    flink  kafka  producer    semantic  param  kafka  producers  pool  size    overwrite  default    kafka  producers  pool  size  see  link    flink  kafka  producer    semantic  exactly  once  public    flink  kafka  producer    string  topic  id    serialization  schema  in  serialization  schema    properties  producer  config    nullable    flink  kafka  partitioner  in  custom  partitioner    flink  kafka  producer    semantic  semantic  int  kafka  producers  pool  size  this  topic  id  null  null  new    kafka  serialization  schema  wrapper  topic  id  custom  partitioner  false  serialization  schema  producer  config  semantic  kafka  producers  pool  size    key    value  serialization  schema  constructors    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer    string    keyed  serialization  schema    properties    optional  instead  param  broker  list    comma  separated  addresses  of  the  brokers  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  deprecated  use  link    flink  kafka  producer    string    kafka  serialization  schema    properties    flink  kafka  producer    semantic    deprecated  public    flink  kafka  producer    string  broker  list    string  topic  id    keyed  serialization  schema  in  serialization  schema  this  topic  id  serialization  schema  get  properties  from  broker  list  broker  list    optional  of  new    flink  fixed  partitioner  in    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer    string    keyed  serialization  schema    properties    optional  instead  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  param  producer  config    properties  with  the  producer  configuration  deprecated  use  link    flink  kafka  producer    string    kafka  serialization  schema    properties    flink  kafka  producer    semantic    deprecated  public    flink  kafka  producer    string  topic  id    keyed  serialization  schema  in  serialization  schema    properties  producer  config  this  topic  id  serialization  schema  producer  config    optional  of  new    flink  fixed  partitioner  in    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  param  producer  config    properties  with  the  producer  configuration  param  semantic    defines  semantic  that  will  be  used  by  this  producer  see  link    flink  kafka  producer    semantic  deprecated  use  link    flink  kafka  producer    string    kafka  serialization  schema    properties    flink  kafka  producer    semantic    deprecated  public    flink  kafka  producer    string  topic  id    keyed  serialization  schema  in  serialization  schema    properties  producer  config    flink  kafka  producer    semantic  semantic  this  topic  id  serialization  schema  producer  config    optional  of  new    flink  fixed  partitioner  in  semantic  default  kafka  producers  pool  size    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  keyed  link    keyed  serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    if  a  partitioner  is  not  provided  written  records  will  be  partitioned  by  the  attached  key  of  each  record  as  determined  by  link    keyed  serialization  schema  serialize  key    object    if  written  records  do  not  have  a  key  i  e  link    keyed  serialization  schema  serialize  key    object  returns  code  null  they  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  default  topic  id    the  default  topic  to  write  data  to  param  serialization  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  a  partitioner  is  not  provided  records  will  be  partitioned  by  the  key  of  each  record  determined  by  link    keyed  serialization  schema  serialize  key    object    if  the  keys  are  code  null  then  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  deprecated  use  link    flink  kafka  producer    string    kafka  serialization  schema    properties    flink  kafka  producer    semantic    deprecated  public    flink  kafka  producer    string  default  topic  id    keyed  serialization  schema  in  serialization  schema    properties  producer  config    optional    flink  kafka  partitioner  in  custom  partitioner  this  default  topic  id  serialization  schema  producer  config  custom  partitioner    flink  kafka  producer    semantic  at  least  once  default  kafka  producers  pool  size    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  keyed  link    keyed  serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    if  a  partitioner  is  not  provided  written  records  will  be  partitioned  by  the  attached  key  of  each  record  as  determined  by  link    keyed  serialization  schema  serialize  key    object    if  written  records  do  not  have  a  key  i  e  link    keyed  serialization  schema  serialize  key    object  returns  code  null  they  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  default  topic  id    the  default  topic  to  write  data  to  param  serialization  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  a  partitioner  is  not  provided  records  will  be  partitioned  by  the  key  of  each  record  determined  by  link    keyed  serialization  schema  serialize  key    object    if  the  keys  are  code  null  then  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  semantic    defines  semantic  that  will  be  used  by  this  producer  see  link    flink  kafka  producer    semantic  param  kafka  producers  pool  size    overwrite  default    kafka  producers  pool  size  see  link    flink  kafka  producer    semantic  exactly  once  deprecated  use  link    flink  kafka  producer    string    kafka  serialization  schema    properties    flink  kafka  producer    semantic    deprecated  public    flink  kafka  producer    string  default  topic  id    keyed  serialization  schema  in  serialization  schema    properties  producer  config    optional    flink  kafka  partitioner  in  custom  partitioner    flink  kafka  producer    semantic  semantic  int  kafka  producers  pool  size  this  default  topic  id  serialization  schema  custom  partitioner  or  else  null  null  kafka  serialization  schema  producer  config  semantic  kafka  producers  pool  size    creates  a  link    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  link    kafka  serialization  schema  for  serializing  records  to  a  link    producer  record  including  partitioning  information  param  default  topic    the  default  topic  to  write  data  to  param  serialization  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  semantic    defines  semantic  that  will  be  used  by  this  producer  see  link    flink  kafka  producer    semantic  public    flink  kafka  producer    string  default  topic    kafka  serialization  schema  in  serialization  schema    properties  producer  config    flink  kafka  producer    semantic  semantic  this  default  topic  serialization  schema  producer  config  semantic  default  kafka  producers  pool  size    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  link    kafka  serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  param  default  topic    the  default  topic  to  write  data  to  param  serialization  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  semantic    defines  semantic  that  will  be  used  by  this  producer  see  link    flink  kafka  producer    semantic  param  kafka  producers  pool  size    overwrite  default    kafka  producers  pool  size  see  link    flink  kafka  producer    semantic  exactly  once  public    flink  kafka  producer    string  default  topic    kafka  serialization  schema  in  serialization  schema    properties  producer  config    flink  kafka  producer    semantic  semantic  int  kafka  producers  pool  size  this  default  topic  null  null  keyed  schema  and    flink  kafka  partitioner  serialization  schema  producer  config  semantic  kafka  producers  pool  size    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  link    kafka  serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    if  a  partitioner  is  not  provided  written  records  will  be  partitioned  by  the  attached  key  of  each  record  as  determined  by  link    keyed  serialization  schema  serialize  key    object    if  written  records  do  not  have  a  key  i  e  link    keyed  serialization  schema  serialize  key    object  returns  code  null  they  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  default  topic    the  default  topic  to  write  data  to  param  keyed  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  a  partitioner  is  not  provided  records  will  be  partitioned  by  the  key  of  each  record  determined  by  link    keyed  serialization  schema  serialize  key    object    if  the  keys  are  code  null  then  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  kafka  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  semantic    defines  semantic  that  will  be  used  by  this  producer  see  link    flink  kafka  producer    semantic  param  kafka  producers  pool  size    overwrite  default    kafka  producers  pool  size  see  link    flink  kafka  producer    semantic  exactly  once  private    flink  kafka  producer    string  default  topic    keyed  serialization  schema  in  keyed  schema    flink  kafka  partitioner  in  custom  partitioner    kafka  serialization  schema  in  kafka  schema    properties  producer  config    flink  kafka  producer    semantic  semantic  int  kafka  producers  pool  size  super  new    flink  kafka  producer    transaction  state  serializer  new    flink  kafka  producer    context  state  serializer  this  default  topic  id  check  not  null  default  topic  default  topic  is  null  if  kafka  schema  null  this  keyed  schema  null  this  kafka  schema  kafka  schema  this  flink  kafka  partitioner  null    closure  cleaner  clean  this  kafka  schema    execution  config    closure  cleaner  level  recursive  true  if  custom  partitioner  null  throw  new    illegal  argument  exception    customer  partitioner  can  only  be  used  when  using  a    keyed  serialization  schema  or    serialization  schema  else  if  keyed  schema  null  this  kafka  schema  null  this  keyed  schema  keyed  schema  this  flink  kafka  partitioner  custom  partitioner    closure  cleaner  clean  this  flink  kafka  partitioner    execution  config    closure  cleaner  level  recursive  true    closure  cleaner  clean  this  keyed  schema    execution  config    closure  cleaner  level  recursive  true  else  throw  new    illegal  argument  exception    you  must  provide  either  a    kafka  serialization  schema  or  a    keyed  serialization  schema  this  producer  config  check  not  null  producer  config  producer  config  is  null  this  semantic  check  not  null  semantic  semantic  is  null  this  kafka  producers  pool  size  kafka  producers  pool  size  check  state  kafka  producers  pool  size    kafka  producers  pool  size  must  be  non  empty  set  the  producer  configuration  properties  for  kafka  record  key  value  serializers  if  producer  config  contains  key    producer  config  key  serializer  class  config  this  producer  config  put    producer  config  key  serializer  class  config    byte  array  serializer  class  get  name  else  log  warn    overwriting  the  is  not  recommended    producer  config  key  serializer  class  config  if  producer  config  contains  key    producer  config  value  serializer  class  config  this  producer  config  put    producer  config  value  serializer  class  config    byte  array  serializer  class  get  name  else  log  warn    overwriting  the  is  not  recommended    producer  config  value  serializer  class  config  eagerly  ensure  that  bootstrap  servers  are  set  if  this  producer  config  contains  key    producer  config  bootstrap  servers  config  throw  new    illegal  argument  exception    producer  config  bootstrap  servers  config  must  be  supplied  in  the  producer  config  properties  if  producer  config  contains  key    producer  config  transaction  timeout  config  long  timeout  default  kafka  transaction  timeout  to  milliseconds  check  state  timeout    integer  max  value  timeout    timeout  does  not  fit  into    bit  integer  this  producer  config  put    producer  config  transaction  timeout  config  int  timeout  log  warn    property  not  specified    setting  it  to    producer  config  transaction  timeout  config  default  kafka  transaction  timeout    enable  transaction  timeout  warnings  to  avoid  silent  data  loss    see  kafka    affects  versions  0.11  0.0  and  0.11  0.1    the    kafka  producer  may  not  throw  an  exception  if  the  transaction  failed  to  commit  if  semantic    flink  kafka  producer    semantic  exactly  once  final    object  object  this  producer  config  get    producer  config  transaction  timeout  config  final  long  transaction  timeout  if  object  instanceof    string    string  utils  is  numeric    string  object  transaction  timeout    long  parse  long    string  object  else  if  object  instanceof    number  transaction  timeout    number  object  long  value  else  throw  new    illegal  argument  exception    producer  config  transaction  timeout  config  must  be  numeric  was  object  super  set  transaction  timeout  transaction  timeout  super  enable  transaction  timeout  warnings  0.8  this  topic  partitions  map  new    hash  map    properties    if  set  to  true    flink  will  write  the  event  time  timestamp  attached  to  each  record  into    kafka    timestamps  must  be  positive  for    kafka  to  accept  them  param  write  timestamp  to  kafka    flag  indicating  if    flink  s  internal  timestamps  are  written  to    kafka  public  void  set  write  timestamp  to  kafka  boolean  write  timestamp  to  kafka  this  write  timestamp  to  kafka  write  timestamp  to  kafka  if  kafka  schema  instanceof    kafka  serialization  schema  wrapper    kafka  serialization  schema  wrapper  in  kafka  schema  set  write  timestamp  write  timestamp  to  kafka    defines  whether  the  producer  should  fail  on  errors  or  only  log  them    if  this  is  set  to  true  then  exceptions  will  be  only  logged  if  set  to  false  exceptions  will  be  eventually  thrown  and  cause  the  streaming  program  to  fail  and  enter  recovery  param  log  failures  only    the  flag  to  indicate  logging  only  on  exceptions  public  void  set  log  failures  only  boolean  log  failures  only  this  log  failures  only  log  failures  only    disables  the  propagation  of  exceptions  thrown  when  committing  presumably  timed  out    kafka  transactions  during  recovery  of  the  job    if  a    kafka  transaction  is  timed  out  a  commit  will  never  be  successful    hence  use  this  feature  to  avoid  recovery  loops  of  the    job    exceptions  will  still  be  logged  to  inform  the  user  that  data  loss  might  have  occurred  p    note  that  we  use  link    system  current  time  millis  to  track  the  age  of  a  transaction    moreover  only  exceptions  thrown  during  the  recovery  are  caught  i  e  the  producer  will  attempt  at  least  one  commit  of  the  transaction  before  giving  up  p    override  public    flink  kafka  producer  in  ignore  failures  after  transaction  timeout  super  ignore  failures  after  transaction  timeout  return  this    utilities    initializes  the  connection  to    kafka    override  public  void  open    configuration  configuration  throws    exception  if  log  failures  only  callback  new    callback    override  public  void  on  completion    record  metadata  metadata    exception  e  if  e  null  log  error    error  while  sending  record  to    kafka  e  get  message  e  acknowledge  message  else  callback  new    callback    override  public  void  on  completion    record  metadata  metadata    exception  exception  if  exception  null  async  exception  null  async  exception  exception  acknowledge  message  if  kafka  schema  null  kafka  schema  open  get  runtime  context  get  metric  group  add  group  user  super  open  configuration    override  public  void  invoke    flink  kafka  producer    kafka  transaction  state  transaction  in  next    context  context  throws    flink  kafka  exception  check  erroneous    producer  record  byte  byte  record  if  keyed  schema  null  byte  serialized  key  keyed  schema  serialize  key  next  byte  serialized  value  keyed  schema  serialize  value  next    string  target  topic  keyed  schema  get  target  topic  next  if  target  topic  null  target  topic  default  topic  id    long  timestamp  null  if  this  write  timestamp  to  kafka  timestamp  context  timestamp  int  partitions  topic  partitions  map  get  target  topic  if  null  partitions  partitions  get  partitions  by  topic  target  topic  transaction  producer  topic  partitions  map  put  target  topic  partitions  if  flink  kafka  partitioner  null  record  new    producer  record  target  topic  flink  kafka  partitioner  partition  next  serialized  key  serialized  value  target  topic  partitions  timestamp  serialized  key  serialized  value  else  record  new    producer  record  target  topic  null  timestamp  serialized  key  serialized  value  else  if  kafka  schema  null  if  kafka  schema  instanceof    kafka  context  aware    suppress  warnings  unchecked    kafka  context  aware  in  context  aware  schema    kafka  context  aware  in  kafka  schema    string  target  topic  context  aware  schema  get  target  topic  next  if  target  topic  null  target  topic  default  topic  id  int  partitions  topic  partitions  map  get  target  topic  if  null  partitions  partitions  get  partitions  by  topic  target  topic  transaction  producer  topic  partitions  map  put  target  topic  partitions  context  aware  schema  set  partitions  partitions  record  kafka  schema  serialize  next  context  timestamp  else  throw  new    runtime  exception    we  have  neither    kafka  serialization  schema  nor    keyed  serialization  schema  this  is  a  bug  pending  records  increment  and  get  transaction  producer  send  record  callback    override  public  void  close  throws    flink  kafka  exception    first  close  the  producer  for  current  transaction  try  final    kafka  transaction  state  current  transaction  current  transaction  if  current  transaction  null  to  avoid  exceptions  on  aborting  transactions  with  some  pending  records  flush  current  transaction  normal  abort  for  at  least  once  and  none  do  not  clean  up  resources  because  of  producer  reusing  thus  we  need  to  close  it  manually  switch  semantic  case  exactly  once  break  case  at  least  once  case  none  current  transaction  producer  flush  current  transaction  producer  close    duration  of  seconds    break  super  close  catch    exception  e  async  exception    exception  utils  first  or  suppressed  e  async  exception  finally    we  may  have  to  close  producer  of  the  current  transaction  in  case  some  exception  was  thrown  before  the  normal  close  routine  finishes  if  current  transaction  null  try  current  transaction  producer  close    duration  of  seconds    catch    throwable  t  log  warn    error  closing  producer  t    make  sure  all  the  producers  for  pending  transactions  are  closed  pending  transactions  for  each  transaction  try  transaction  get  value  producer  close    duration  of  seconds    catch    throwable  t  log  warn    error  closing  producer  t  make  sure  we  propagate  pending  errors  check  erroneous    logic  for  handling  checkpoint  flushing    override  protected    flink  kafka  producer    kafka  transaction  state  begin  transaction  throws    flink  kafka  exception  switch  semantic  case  exactly  once    flink  kafka  internal  producer  byte  byte  producer  create  transactional  producer  producer  begin  transaction  return  new    flink  kafka  producer    kafka  transaction  state  producer  get  transactional  id  producer  case  at  least  once  case  none    do  not  create  new  producer  on  each  begin  transaction  if  it  is  not  necessary  final    flink  kafka  producer    kafka  transaction  state  current  transaction  current  transaction  if  current  transaction  null  current  transaction  producer  null  return  new    flink  kafka  producer    kafka  transaction  state  current  transaction  producer  return  new    flink  kafka  producer    kafka  transaction  state  init  non  transactional  producer  true  default  throw  new    unsupported  operation  exception    not  implemented  semantic    override  protected  void  pre  commit    flink  kafka  producer    kafka  transaction  state  transaction  throws    flink  kafka  exception  switch  semantic  case  exactly  once  case  at  least  once  flush  transaction  break  case  none  break  default  throw  new    unsupported  operation  exception    not  implemented  semantic  check  erroneous    override  protected  void  commit    flink  kafka  producer    kafka  transaction  state  transaction  if  transaction  is  transactional  try  transaction  producer  commit  transaction  finally  recycle  transactional  producer  transaction  producer    override  protected  void  recover  and  commit    flink  kafka  producer    kafka  transaction  state  transaction  if  transaction  is  transactional    flink  kafka  internal  producer  byte  byte  producer  null  try  producer  init  transactional  producer  transaction  transactional  id  false  producer  resume  transaction  transaction  producer  id  transaction  epoch  producer  commit  transaction  catch    invalid  txn  state  exception    producer  fenced  exception  ex    that  means  we  have  committed  this  transaction  before  log  warn    encountered  error  while  recovering  transaction    presumably  this  transaction  has  been  already  committed  before  ex  transaction  finally  if  producer  null  producer  close      time  unit  seconds    override  protected  void  abort    flink  kafka  producer    kafka  transaction  state  transaction  if  transaction  is  transactional  transaction  producer  abort  transaction  recycle  transactional  producer  transaction  producer    override  protected  void  recover  and  abort    flink  kafka  producer    kafka  transaction  state  transaction  if  transaction  is  transactional    flink  kafka  internal  producer  byte  byte  producer  null  try  producer  init  transactional  producer  transaction  transactional  id  false  producer  init  transactions  finally  if  producer  null  producer  close      time  unit  seconds  b  attention  to  subclass  implementors  b    when  overriding  this  method  please  always  call  code  super  acknowledge  message  to  keep  the  invariants  of  the  internal  bookkeeping  of  the  producer    if  not  be  sure  to  know  what  you  are  doing  protected  void  acknowledge  message  pending  records  decrement  and  get    flush  pending  records  param  transaction  private  void  flush    flink  kafka  producer    kafka  transaction  state  transaction  throws    flink  kafka  exception  if  transaction  producer  null  transaction  producer  flush  long  pending  records  count  pending  records  get  if  pending  records  count    throw  new    illegal  state  exception    pending  record  count  must  be  zero  at  this  point  pending  records  count  if  the  flushed  requests  has  errors  we  should  propagate  it  also  and  fail  the  checkpoint  check  erroneous    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception  super  snapshot  state  context  next  transactional  id  hint  state  clear    to  avoid  duplication  only  first  subtask  keeps  track  of  next  transactional  id  hint    otherwise  all  of  the  subtasks  would  write  exactly  same  information  if  get  runtime  context  get  index  of  this  subtask    semantic    flink  kafka  producer    semantic  exactly  once  check  state  next  transactional  id  hint  null  next  transactional  id  hint  must  be  set  for  exactly  once  long  next  free  transactional  id  next  transactional  id  hint  next  free  transactional  id    if  we  scaled  up  some  unknown  subtask  must  have  created  new  transactional  ids  from  scratch    in  that  case  we  adjust  next  free  transactional  id  by  the  range  of  transactional  ids  that  could  be  used  for  this  scaling  up  if  get  runtime  context  get  number  of  parallel  subtasks  next  transactional  id  hint  last  parallelism  next  free  transactional  id  get  runtime  context  get  number  of  parallel  subtasks  kafka  producers  pool  size  next  transactional  id  hint  state  add  new    flink  kafka  producer    next  transactional  id  hint  get  runtime  context  get  number  of  parallel  subtasks  next  free  transactional  id    override  public  void  initialize  state    function  initialization  context  context  throws    exception  if  semantic    flink  kafka  producer    semantic  none    streaming  runtime  context  this  get  runtime  context  is  checkpointing  enabled  log  warn    using  semantic  but  checkpointing  is  not  enabled    switching  to  semantic  semantic    flink  kafka  producer    semantic  none  semantic    flink  kafka  producer    semantic  none  next  transactional  id  hint  state  context  get  operator  state  store  get  union  list  state  next  transactional  id  hint  descriptor    v2  if  context  get  operator  state  store  get  registered  state  names  contains  next  transactional  id  hint  descriptor  migrate  next  transactional  id  hind  state  context  transactional  ids  generator  new    transactional  ids  generator  get  runtime  context  get  task  name    streaming  runtime  context  get  runtime  context  get  operator  unique  i  d  get  runtime  context  get  index  of  this  subtask  get  runtime  context  get  number  of  parallel  subtasks  kafka  producers  pool  size  safe  scale  down  factor  if  semantic    flink  kafka  producer    semantic  exactly  once  next  transactional  id  hint  null  else    array  list    flink  kafka  producer    next  transactional  id  hint  transactional  id  hints    lists  new  array  list  next  transactional  id  hint  state  get  if  transactional  id  hints  size    throw  new    illegal  state  exception    there  should  be  at  most  one  next  transactional  id  hint  written  by  the  first  subtask  else  if  transactional  id  hints  size    next  transactional  id  hint  new    flink  kafka  producer    next  transactional  id  hint      this  means  that  this  is  either    the  first  execution  of  this  application    previous  execution  has  failed  before  first  checkpoint  completed  in  case  of    we  have  to  abort  all  previous  transactions  abort  transactions  transactional  ids  generator  generate  ids  to  abort  else  next  transactional  id  hint  transactional  id  hints  get    super  initialize  state  context    override  protected    optional    flink  kafka  producer    kafka  transaction  context  initialize  user  context  if  semantic    flink  kafka  producer    semantic  exactly  once  return    optional  empty    set    string  transactional  ids  generate  new  transactional  ids  reset  available  transactional  ids  pool  transactional  ids  return    optional  of  new    flink  kafka  producer    kafka  transaction  context  transactional  ids  private    set    string  generate  new  transactional  ids  check  state  next  transactional  id  hint  null  next  transactional  id  hint  must  be  present  for  exactly  once    set    string  transactional  ids  transactional  ids  generator  generate  ids  to  use  next  transactional  id  hint  next  free  transactional  id  log  info    generated  new  transactional  ids  transactional  ids  return  transactional  ids    override  protected  void  finish  recovering  context    collection    flink  kafka  producer    kafka  transaction  state  handled  transactions  clean  up  user  context  handled  transactions  reset  available  transactional  ids  pool  get  user  context  get  transactional  ids  log  info    recovered  transactional  ids  get  user  context  get  transactional  ids  protected    flink  kafka  internal  producer  byte  byte  create  producer  return  new    flink  kafka  internal  producer  this  producer  config    after  initialization  make  sure  that  all  previous  transactions  from  the  current  user  context  have  been  completed  param  handled  transactions  transactions  which  were  already  committed  or  aborted  and  do  not  need  further  handling  private  void  clean  up  user  context    collection    flink  kafka  producer    kafka  transaction  state  handled  transactions  if  get  user  context  is  present  return    hash  set    string  abort  transactions  new    hash  set  get  user  context  get  transactional  ids  handled  transactions  for  each  kafka  transaction  state  abort  transactions  remove  kafka  transaction  state  transactional  id  abort  transactions  abort  transactions  private  void  reset  available  transactional  ids  pool    collection    string  transactional  ids  available  transactional  ids  clear  available  transactional  ids  add  all  transactional  ids    utilities  private  void  abort  transactions  final    set    string  transactional  ids  final    class  loader  class  loader    thread  current  thread  get  context  class  loader  transactional  ids  parallel  stream  for  each  transactional  id    the  parallel  stream  executes  the  consumer  in  a  separated  thread  pool    because  the  consumer  e  g    kafka  uses  the  context  classloader  to  construct  some  class  we  should  set  the  correct  classloader  for  it  try    temporary  class  loader  context  ignored    temporary  class  loader  context  of  class  loader  don  t  mess  with  the  original  configuration  or  any  other  properties  of  the  original  object  create  an  internal  kafka  producer  on  our  own  and  do  not  rely  on  init  transactional  producer  final    properties  my  config  new    properties  my  config  put  all  producer  config  init  transactional  producer  config  my  config  transactional  id    flink  kafka  internal  producer  byte  byte  kafka  producer  null  try  kafka  producer  new    flink  kafka  internal  producer  my  config  it  suffices  to  call  init  transactions  this  will  abort  any  lingering  transactions  kafka  producer  init  transactions  finally  if  kafka  producer  null  kafka  producer  close    duration  of  seconds    int  get  transaction  coordinator  id  final    flink  kafka  producer    kafka  transaction  state  current  transaction  current  transaction  if  current  transaction  null  current  transaction  producer  null  throw  new    illegal  argument  exception  return  current  transaction  producer  get  transaction  coordinator  id    for  each  checkpoint  we  create  new  link    flink  kafka  internal  producer  so  that  new  transactions  will  not  clash  with  transactions  created  during  previous  checkpoints  code  producer  init  transactions  assures  that  we  obtain  new  producer  id  and  epoch  counters  private    flink  kafka  internal  producer  byte  byte  create  transactional  producer  throws    flink  kafka  exception    string  transactional  id  available  transactional  ids  poll  if  transactional  id  null  throw  new    flink  kafka  exception    flink  kafka  error  code  producers  pool  empty    too  many  ongoing  snapshots    increase  kafka  producers  pool  size  or  decrease  number  of  concurrent  checkpoints    flink  kafka  internal  producer  byte  byte  producer  init  transactional  producer  transactional  id  true  producer  init  transactions  return  producer  private  void  recycle  transactional  producer    flink  kafka  internal  producer  byte  byte  producer  available  transactional  ids  add  producer  get  transactional  id  producer  flush  producer  close    duration  of  seconds    private    flink  kafka  internal  producer  byte  byte  init  transactional  producer    string  transactional  id  boolean  register  metrics  init  transactional  producer  config  producer  config  transactional  id  return  init  producer  register  metrics  private  static  void  init  transactional  producer  config    properties  producer  config    string  transactional  id  producer  config  put    producer  config  transactional  id  config  transactional  id  private    flink  kafka  internal  producer  byte  byte  init  non  transactional  producer  boolean  register  metrics  producer  config  remove    producer  config  transactional  id  config  return  init  producer  register  metrics  private    flink  kafka  internal  producer  byte  byte  init  producer  boolean  register  metrics    flink  kafka  internal  producer  byte  byte  producer  create  producer    runtime  context  ctx  get  runtime  context  if  flink  kafka  partitioner  null  flink  kafka  partitioner  open  ctx  get  index  of  this  subtask  ctx  get  number  of  parallel  subtasks  if  kafka  schema  instanceof    kafka  context  aware    kafka  context  aware  in  context  aware  schema    kafka  context  aware  in  kafka  schema  context  aware  schema  set  parallel  instance  id  ctx  get  index  of  this  subtask  context  aware  schema  set  num  parallel  instances  ctx  get  number  of  parallel  subtasks  log  info    starting    flink  kafka  internal  producer  to  produce  into  default  topic  ctx  get  index  of  this  subtask    ctx  get  number  of  parallel  subtasks  default  topic  id  register    kafka  metrics  to    flink  accumulators  if  register  metrics    boolean  parse  boolean  producer  config  get  property  key  disable  metrics  false    map    metric  name  extends    metric  metrics  producer  metrics  if  metrics  null    map  r  s    kafka  implementation  returns  null  here  log  info    producer  implementation  does  not  support  metrics  else  final    metric  group  kafka  metric  group  get  runtime  context  get  metric  group  add  group    kafka  producer  for    map    entry    metric  name  extends    metric  entry  metrics  entry  set    string  name  entry  get  key  name    metric  metric  entry  get  value    kafka  metric  mutable  wrapper  wrapper  previously  created  metrics  get  name  if  wrapper  null  wrapper  set  kafka  metric  metric  else  todo  somehow  merge  metrics  from  all  active  producers  wrapper  new    kafka  metric  mutable  wrapper  metric  previously  created  metrics  put  name  wrapper  kafka  metric  group  gauge  name  wrapper  return  producer  protected  void  check  erroneous  throws    flink  kafka  exception    exception  e  async  exception  if  e  null  prevent  double  throwing  async  exception  null  throw  new    flink  kafka  exception    flink  kafka  error  code  external  error    failed  to  send  data  to    kafka  e  get  message  e  private  void  read  object  java  io    object  input  stream  in  throws    i  o  exception    class  not  found  exception  in  default  read  object  private  void  migrate  next  transactional  id  hind  state    function  initialization  context  context  throws    exception    list  state    next  transactional  id  hint  old  next  transactional  id  hint  state  context  get  operator  state  store  get  union  list  state  next  transactional  id  hint  descriptor  next  transactional  id  hint  state  context  get  operator  state  store  get  union  list  state  next  transactional  id  hint  descriptor    v2    array  list    next  transactional  id  hint  old  transactional  id  hints    lists  new  array  list  old  next  transactional  id  hint  state  get  if  old  transactional  id  hints  is  empty  next  transactional  id  hint  state  add  all  old  transactional  id  hints  clear  old  state  old  next  transactional  id  hint  state  clear  private  static    properties  get  properties  from  broker  list    string  broker  list    string  elements  broker  list  split  validate  the  broker  addresses  for    string  broker  elements    net  utils  get  correct  hostname  port  broker    properties  props  new    properties  props  set  property    producer  config  bootstrap  servers  config  broker  list  return  props  protected  static  int  get  partitions  by  topic    string  topic    producer  byte  byte  producer  the  fetched  list  is  immutable  so  we  re  creating  a  mutable  copy  in  order  to  sort  it    list    partition  info  partitions  list  new    array  list  producer  partitions  for  topic  sort  the  partitions  by  partition  id  to  make  sure  the  fetched  partition  list  is  the  same  across  subtasks    collections  sort  partitions  list  new    comparator    partition  info    override  public  int  compare    partition  info  o1    partition  info  o2  return    integer  compare  o1  partition  o2  partition  int  partitions  new  int  partitions  list  size  for  int  i    i  partitions  length  i  partitions  i  partitions  list  get  i  partition  return  partitions    state  for  handling  transactions    visible  for  testing    internal  public  static  class    kafka  transaction  state  private  final  transient    flink  kafka  internal  producer  byte  byte  producer    nullable  final    string  transactional  id  final  long  producer  id  final  short  epoch    visible  for  testing  public    kafka  transaction  state    string  transactional  id    flink  kafka  internal  producer  byte  byte  producer  this  transactional  id  producer  get  producer  id  producer  get  epoch  producer    visible  for  testing  public    kafka  transaction  state    flink  kafka  internal  producer  byte  byte  producer  this  null    short    producer    visible  for  testing  public    kafka  transaction  state    nullable    string  transactional  id  long  producer  id  short  epoch    flink  kafka  internal  producer  byte  byte  producer  this  transactional  id  transactional  id  this  producer  id  producer  id  this  epoch  epoch  this  producer  producer  boolean  is  transactional  return  transactional  id  null  public    flink  kafka  internal  producer  byte  byte  get  producer  return  producer    override  public    string  to  string  return    string  format  s  transactional  id  s  producer  id  s  epoch  s  this  get  class  get  simple  name  transactional  id  producer  id  epoch    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    flink  kafka  producer    kafka  transaction  state  that    flink  kafka  producer    kafka  transaction  state  o  if  producer  id  that  producer  id  return  false  if  epoch  that  epoch  return  false  return  transactional  id  null  transactional  id  equals  that  transactional  id  that  transactional  id  null    override  public  int  hash  code  int  result  transactional  id  null  transactional  id  hash  code    result    result  int  producer  id  producer  id    result    result  int  epoch  return  result    context  associated  to  this  instance  of  the  link    flink  kafka  producer    user  for  keeping  track  of  the  transactional  ids    visible  for  testing    internal  public  static  class    kafka  transaction  context  final    set    string  transactional  ids    visible  for  testing  public    kafka  transaction  context    set    string  transactional  ids  check  not  null  transactional  ids  this  transactional  ids  transactional  ids    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    flink  kafka  producer    kafka  transaction  context  that    flink  kafka  producer    kafka  transaction  context  o  return  transactional  ids  equals  that  transactional  ids    override  public  int  hash  code  return  transactional  ids  hash  code  link  org  apache  flink  api  common  typeutils    type  serializer  for  link    flink  kafka  producer    kafka  transaction  state    visible  for  testing    internal  public  static  class    transaction  state  serializer  extends    type  serializer  singleton    flink  kafka  producer    kafka  transaction  state  private  static  final  long  serial  version  u  i  d  1  l    override  public  boolean  is  immutable  type  return  true    override  public    flink  kafka  producer    kafka  transaction  state  create  instance  return  null    override  public    flink  kafka  producer    kafka  transaction  state  copy    flink  kafka  producer    kafka  transaction  state  from  return  from    override  public    flink  kafka  producer    kafka  transaction  state  copy    flink  kafka  producer    kafka  transaction  state  from    flink  kafka  producer    kafka  transaction  state  reuse  return  from    override  public  int  get  length  return      override  public  void  serialize    flink  kafka  producer    kafka  transaction  state  record    data  output  view  target  throws    i  o  exception  if  record  transactional  id  null  target  write  boolean  false  else  target  write  boolean  true  target  write  u  t  f  record  transactional  id  target  write  long  record  producer  id  target  write  short  record  epoch    override  public    flink  kafka  producer    kafka  transaction  state  deserialize    data  input  view  source  throws    i  o  exception    string  transactional  id  null  if  source  read  boolean  transactional  id  source  read  u  t  f  long  producer  id  source  read  long  short  epoch  source  read  short  return  new    flink  kafka  producer    kafka  transaction  state  transactional  id  producer  id  epoch  null    override  public    flink  kafka  producer    kafka  transaction  state  deserialize    flink  kafka  producer    kafka  transaction  state  reuse    data  input  view  source  throws    i  o  exception  return  deserialize  source    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  boolean  has  transactional  id  source  read  boolean  target  write  boolean  has  transactional  id  if  has  transactional  id  target  write  u  t  f  source  read  u  t  f  target  write  long  source  read  long  target  write  short  source  read  short    override  public    type  serializer  snapshot    flink  kafka  producer    kafka  transaction  state  snapshot  configuration  return  new    transaction  state  serializer  snapshot    serializer  configuration  snapshot  for  compatibility  and  format  evolution    suppress  warnings    weaker  access  public  static  final  class    transaction  state  serializer  snapshot  extends    simple  type  serializer  snapshot    flink  kafka  producer    kafka  transaction  state  public    transaction  state  serializer  snapshot  super    transaction  state  serializer  new  link  org  apache  flink  api  common  typeutils    type  serializer  for  link    flink  kafka  producer    kafka  transaction  context    visible  for  testing    internal  public  static  class    context  state  serializer  extends    type  serializer  singleton    flink  kafka  producer    kafka  transaction  context  private  static  final  long  serial  version  u  i  d  1  l    override  public  boolean  is  immutable  type  return  true    override  public    flink  kafka  producer    kafka  transaction  context  create  instance  return  null    override  public    flink  kafka  producer    kafka  transaction  context  copy    flink  kafka  producer    kafka  transaction  context  from  return  from    override  public    flink  kafka  producer    kafka  transaction  context  copy    flink  kafka  producer    kafka  transaction  context  from    flink  kafka  producer    kafka  transaction  context  reuse  return  from    override  public  int  get  length  return      override  public  void  serialize    flink  kafka  producer    kafka  transaction  context  record    data  output  view  target  throws    i  o  exception  int  num  ids  record  transactional  ids  size  target  write  int  num  ids  for    string  id  record  transactional  ids  target  write  u  t  f  id    override  public    flink  kafka  producer    kafka  transaction  context  deserialize    data  input  view  source  throws    i  o  exception  int  num  ids  source  read  int    set    string  ids  new    hash  set  num  ids  for  int  i    i  num  ids  i  ids  add  source  read  u  t  f  return  new    flink  kafka  producer    kafka  transaction  context  ids    override  public    flink  kafka  producer    kafka  transaction  context  deserialize    flink  kafka  producer    kafka  transaction  context  reuse    data  input  view  source  throws    i  o  exception  return  deserialize  source    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  int  num  ids  source  read  int  target  write  int  num  ids  for  int  i    i  num  ids  i  target  write  u  t  f  source  read  u  t  f    override  public    type  serializer  snapshot    kafka  transaction  context  snapshot  configuration  return  new    context  state  serializer  snapshot    serializer  configuration  snapshot  for  compatibility  and  format  evolution    suppress  warnings    weaker  access  public  static  final  class    context  state  serializer  snapshot  extends    simple  type  serializer  snapshot    kafka  transaction  context  public    context  state  serializer  snapshot  super    context  state  serializer  new    keep  information  required  to  deduce  next  safe  to  use  transactional  id  public  static  class    next  transactional  id  hint  public  int  last  parallelism    public  long  next  free  transactional  id    public    next  transactional  id  hint  this      public    next  transactional  id  hint  int  parallelism  long  next  free  transactional  id  this  last  parallelism  parallelism  this  next  free  transactional  id  next  free  transactional  id    override  public    string  to  string  return    next  transactional  id  hint  last  parallelism  last  parallelism  next  free  transactional  id  next  free  transactional  id    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    next  transactional  id  hint  that    next  transactional  id  hint  o  if  last  parallelism  that  last  parallelism  return  false  return  next  free  transactional  id  that  next  free  transactional  id    override  public  int  hash  code  int  result  last  parallelism  result    result  int  next  free  transactional  id  next  free  transactional  id    return  result  link  org  apache  flink  api  common  typeutils    type  serializer  for  link    flink  kafka  producer    next  transactional  id  hint    visible  for  testing    internal  public  static  class    next  transactional  id  hint  serializer  extends    type  serializer  singleton    next  transactional  id  hint  private  static  final  long  serial  version  u  i  d  1  l    override  public  boolean  is  immutable  type  return  true    override  public    next  transactional  id  hint  create  instance  return  new    next  transactional  id  hint    override  public    next  transactional  id  hint  copy    next  transactional  id  hint  from  return  from    override  public    next  transactional  id  hint  copy    next  transactional  id  hint  from    next  transactional  id  hint  reuse  return  from    override  public  int  get  length  return    long  bytes    integer  bytes    override  public  void  serialize    next  transactional  id  hint  record    data  output  view  target  throws    i  o  exception  target  write  long  record  next  free  transactional  id  target  write  int  record  last  parallelism    override  public    next  transactional  id  hint  deserialize    data  input  view  source  throws    i  o  exception  long  next  free  transactional  id  source  read  long  int  last  parallelism  source  read  int  return  new    next  transactional  id  hint  last  parallelism  next  free  transactional  id    override  public    next  transactional  id  hint  deserialize    next  transactional  id  hint  reuse    data  input  view  source  throws    i  o  exception  return  deserialize  source    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  target  write  long  source  read  long  target  write  int  source  read  int    override  public    type  serializer  snapshot    next  transactional  id  hint  snapshot  configuration  return  new    next  transactional  id  hint  serializer  snapshot    serializer  configuration  snapshot  for  compatibility  and  format  evolution    suppress  warnings    weaker  access  public  static  final  class    next  transactional  id  hint  serializer  snapshot  extends    simple  type  serializer  snapshot    next  transactional  id  hint  public    next  transactional  id  hint  serializer  snapshot  super    next  transactional  id  hint  serializer  new  
public  evolving  public  class    flink  kafka  internal  producer  k  v  implements    producer  k  v  private  static  final    logger  log    logger  factory  get  logger    flink  kafka  internal  producer  class  protected  final    kafka  producer  k  v  kafka  producer    this  lock  and  closed  flag  are  introduced  to  workaround  kafka      because  the  bug  is  only  fixed  in    kafka  2.3    we  need  this  workaround  before    kafka  dependency  is  bumped  to  2.3    to  avoid  deadlock  between  a  transaction  committing  aborting  thread  and  a  producer  closing  thread  todo  remove  the  workaround  after    kafka  dependency  is  bumped  to  2.3    private  final    object  producer  closing  lock  private  volatile  boolean  closed    nullable  protected  final    string  transactional  id  public    flink  kafka  internal  producer    properties  properties  transactional  id  properties  get  property    producer  config  transactional  id  config  kafka  producer  new    kafka  producer  properties  producer  closing  lock  new    object  closed  false    simple  proxy  method  calls    override  public  void  init  transactions  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  init  transactions    override  public  void  begin  transaction  throws    producer  fenced  exception  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  begin  transaction    override  public  void  commit  transaction  throws    producer  fenced  exception  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  commit  transaction    override  public  void  abort  transaction  throws    producer  fenced  exception  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  abort  transaction    override  public  void  send  offsets  to  transaction    map    topic  partition    offset  and  metadata  offsets    string  consumer  group  id  throws    producer  fenced  exception  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  send  offsets  to  transaction  offsets  consumer  group  id    override  public    future    record  metadata  send    producer  record  k  v  record  return  kafka  producer  send  record    override  public    future    record  metadata  send    producer  record  k  v  record    callback  callback  return  kafka  producer  send  record  callback    override  public    list    partition  info  partitions  for    string  topic  synchronized  producer  closing  lock  ensure  not  closed  return  kafka  producer  partitions  for  topic    override  public    map    metric  name  extends    metric  metrics  return  kafka  producer  metrics    override  public  void  close  throw  new    unsupported  operation  exception    close  without  timeout  is  now  allowed  because  it  can  leave  lingering    kafka  threads    override  public  void  close  long  timeout    time  unit  unit  synchronized  producer  closing  lock  kafka  producer  close  timeout  unit  if  log  is  debug  enabled  log  debug    closed  internal    kafka  producer    stacktrace    system  identity  hash  code  this    joiner  on  n  join    thread  current  thread  get  stack  trace  closed  true    override  public  void  close    duration  duration  synchronized  producer  closing  lock  kafka  producer  close  duration  if  log  is  debug  enabled  log  debug    closed  internal    kafka  producer    stacktrace    system  identity  hash  code  this    joiner  on  n  join    thread  current  thread  get  stack  trace  closed  true    new  methods  or  methods  with  changed  behaviour    override  public  void  flush  kafka  producer  flush  if  transactional  id  null  synchronized  producer  closing  lock  ensure  not  closed  flush  new  partitions    instead  of  obtaining  producer  id  and  epoch  from  the  transaction  coordinator  re  use  previously  obtained  ones  so  that  we  can  resume  transaction  after  a  restart    implementation  of  this  method  is  based  on  link    kafka  producer  init  transactions  https  github  com  apache  kafka  commit  5d2422258  cb975a137a42a4e08f03573c49a387e  diff  f4ef1afd8792cd2a2e9069cd7ddea630  public  void  resume  transaction  long  producer  id  short  epoch  synchronized  producer  closing  lock  ensure  not  closed    preconditions  check  state  producer  id    epoch      incorrect  values  for  producer  id  s  and  epoch  s  producer  id  epoch  log  info    attempting  to  resume  transaction  with  producer  id  and  epoch  transactional  id  producer  id  epoch    object  transaction  manager  get  field  kafka  producer  transaction  manager  synchronized  transaction  manager    object  topic  partition  bookkeeper  get  field  transaction  manager  topic  partition  bookkeeper  invoke  transaction  manager  transition  to  get  enum  org  apache  kafka  clients  producer  internals    transaction  manager    state  initializing  invoke  topic  partition  bookkeeper  reset    object  producer  id  and  epoch  get  field  transaction  manager  producer  id  and  epoch  set  field  producer  id  and  epoch  producer  id  producer  id  set  field  producer  id  and  epoch  epoch  epoch  invoke  transaction  manager  transition  to  get  enum  org  apache  kafka  clients  producer  internals    transaction  manager    state  ready  invoke  transaction  manager  transition  to  get  enum  org  apache  kafka  clients  producer  internals    transaction  manager    state  in  transaction  set  field  transaction  manager  transaction  started  true  public    string  get  transactional  id  return  transactional  id  public  long  get  producer  id    object  transaction  manager  get  field  kafka  producer  transaction  manager    object  producer  id  and  epoch  get  field  transaction  manager  producer  id  and  epoch  return  long  get  field  producer  id  and  epoch  producer  id  public  short  get  epoch    object  transaction  manager  get  field  kafka  producer  transaction  manager    object  producer  id  and  epoch  get  field  transaction  manager  producer  id  and  epoch  return  short  get  field  producer  id  and  epoch  epoch    visible  for  testing  public  int  get  transaction  coordinator  id    object  transaction  manager  get  field  kafka  producer  transaction  manager    node  node    node  invoke  transaction  manager  coordinator    find  coordinator  request    coordinator  type  transaction  return  node  id  private  void  ensure  not  closed  if  closed  throw  new    illegal  state  exception    string  format    the  producer  s  has  already  been  closed    system  identity  hash  code  this    besides  committing  link  org  apache  kafka  clients  producer    kafka  producer  commit  transaction  is  also  adding  new  partitions  to  the  transaction  flush  new  partitions  method  is  moving  this  logic  to  pre  commit  flush  to  make  resume  transaction  simpler    otherwise  resume  transaction  would  require  to  restore  state  of  the  not  yet  added  in  flight  partitions  private  void  flush  new  partitions  log  info    flushing  new  partitions    transactional  request  result  result  enqueue  new  partitions    object  sender  get  field  kafka  producer  sender  invoke  sender  wakeup  result  await    enqueues  new  transactions  at  the  transaction  manager  and  returns  a  link    transactional  request  result  that  allows  waiting  on  them  p    if  there  are  no  new  transactions  we  return  a  link    transactional  request  result  that  is  already  done  private    transactional  request  result  enqueue  new  partitions    object  transaction  manager  get  field  kafka  producer  transaction  manager  synchronized  transaction  manager    object  new  partitions  in  transaction  get  field  transaction  manager  new  partitions  in  transaction    object  new  partitions  in  transaction  is  empty  invoke  new  partitions  in  transaction  is  empty    transactional  request  result  result  if  new  partitions  in  transaction  is  empty  instanceof    boolean    boolean  new  partitions  in  transaction  is  empty    object  txn  request  handler  invoke  transaction  manager  add  partitions  to  transaction  handler  invoke  transaction  manager  enqueue  request  new    class  txn  request  handler  get  class  get  superclass  new    object  txn  request  handler  result    transactional  request  result  get  field  txn  request  handler  txn  request  handler  get  class  get  superclass  result  else  we  don  t  have  an  operation  but  this  operation  string  is  also  used  in  add  partitions  to  transaction  handler  result  new    transactional  request  result    add  partitions  to  txn  result  done  return  result  protected  static    enum  get  enum    string  enum  full  name    string  x  enum  full  name  split  if  x  length      string  enum  class  name  x      string  enum  name  x    try    class    enum  cl    class    enum    class  for  name  enum  class  name  return    enum  value  of  cl  enum  name  catch    class  not  found  exception  e  throw  new    runtime  exception    incompatible    kafka  producer  version  e  return  null  protected  static    object  invoke    object  object    string  method  name    object  args    class  arg  types  new    class  args  length  for  int  i    i  args  length  i  arg  types  i  args  i  get  class  return  invoke  object  method  name  arg  types  args  private  static    object  invoke    object  object    string  method  name    class  arg  types    object  args  try    method  method  object  get  class  get  declared  method  method  name  arg  types  method  set  accessible  true  return  method  invoke  object  args  catch    no  such  method  exception    invocation  target  exception    illegal  access  exception  e  throw  new    runtime  exception    incompatible    kafka  producer  version  e    gets  and  returns  the  field  code  field  name  from  the  given    object  code  object  using  reflection  protected  static    object  get  field    object  object    string  field  name  return  get  field  object  object  get  class  field  name    gets  and  returns  the  field  code  field  name  from  the  given    object  code  object  using  reflection  private  static    object  get  field    object  object    class  clazz    string  field  name  try    field  field  clazz  get  declared  field  field  name  field  set  accessible  true  return  field  get  object  catch    no  such  field  exception    illegal  access  exception  e  throw  new    runtime  exception    incompatible    kafka  producer  version  e    sets  the  field  code  field  name  on  the  given    object  code  object  to  code  value  using  reflection  protected  static  void  set  field    object  object    string  field  name    object  value  try    field  field  object  get  class  get  declared  field  field  name  field  set  accessible  true  field  set  object  value  catch    no  such  field  exception    illegal  access  exception  e  throw  new    runtime  exception    incompatible    kafka  producer  version  e  
public  evolving  public  class    flink  kafka  consumer010  t  extends    flink  kafka  consumer  base  t  private  static  final  long  serial  version  u  i  d    l  private  static  final    logger  log    logger  factory  get  logger    flink  kafka  consumer010  class    configuration  key  to  change  the  polling  timeout  public  static  final    string  key  poll  timeout  flink  poll  timeout    from    kafka  s    javadoc    the  time  in  milliseconds  spent  waiting  in  poll  if  data  is  not  available    if    returns  immediately  with  any  records  that  are  available  now  public  static  final  long  default  poll  timeout    l    user  supplied  properties  for    kafka  protected  final    properties  properties    from    kafka  s    javadoc    the  time  in  milliseconds  spent  waiting  in  poll  if  data  is  not  available    if    returns  immediately  with  any  records  that  are  available  now  protected  final  long  poll  timeout    rate  limiter  to  throttle  bytes  read  from    kafka    the  rate  limiter  is  set  via  link  set  rate  limiter    flink  connector  rate  limiter  private    flink  connector  rate  limiter  rate  limiter    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.10  x  param  topic    the  name  of  the  topic  that  should  be  consumed  param  value  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  used  to  configure  the    kafka  consumer  client  and  the    zoo  keeper  client  public    flink  kafka  consumer010    string  topic    deserialization  schema  t  value  deserializer    properties  props  this    collections  singleton  list  topic  value  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.10  x  p    this  constructor  allows  passing  a  see    kafka  deserialization  schema  for  reading  key  value  pairs  offsets  and  topic  names  from    kafka  param  topic    the  name  of  the  topic  that  should  be  consumed  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  used  to  configure  the    kafka  consumer  client  and  the    zoo  keeper  client  public    flink  kafka  consumer010    string  topic    kafka  deserialization  schema  t  deserializer    properties  props  this    collections  singleton  list  topic  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.10  x  p    this  constructor  allows  passing  multiple  topics  to  the  consumer  param  topics    the    kafka  topics  to  read  from  param  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  that  are  used  to  configure  both  the  fetcher  and  the  offset  handler  public    flink  kafka  consumer010    list    string  topics    deserialization  schema  t  deserializer    properties  props  this  topics  new    kafka  deserialization  schema  wrapper  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.10  x  p    this  constructor  allows  passing  multiple  topics  and  a  key  value  deserialization  schema  param  topics    the    kafka  topics  to  read  from  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  that  are  used  to  configure  both  the  fetcher  and  the  offset  handler  public    flink  kafka  consumer010    list    string  topics    kafka  deserialization  schema  t  deserializer    properties  props  this  topics  null  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.10  x    use  this  constructor  to  subscribe  to  multiple  topics  based  on  a  regular  expression  pattern  p    if  partition  discovery  is  enabled  by  setting  a  non  negative  value  for  link    flink  kafka  consumer010  key  partition  discovery  interval  millis  in  the  properties  topics  with  names  matching  the  pattern  will  also  be  subscribed  to  as  they  are  created  on  the  fly  param  subscription  pattern    the  regular  expression  for  a  pattern  of  topic  names  to  subscribe  to  param  value  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  used  to  configure  the    kafka  consumer  client  and  the    zoo  keeper  client    public  evolving  public    flink  kafka  consumer010    pattern  subscription  pattern    deserialization  schema  t  value  deserializer    properties  props  this  subscription  pattern  new    kafka  deserialization  schema  wrapper  value  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.10  x    use  this  constructor  to  subscribe  to  multiple  topics  based  on  a  regular  expression  pattern  p    if  partition  discovery  is  enabled  by  setting  a  non  negative  value  for  link    flink  kafka  consumer010  key  partition  discovery  interval  millis  in  the  properties  topics  with  names  matching  the  pattern  will  also  be  subscribed  to  as  they  are  created  on  the  fly  p    this  constructor  allows  passing  a  see    kafka  deserialization  schema  for  reading  key  value  pairs  offsets  and  topic  names  from    kafka  param  subscription  pattern    the  regular  expression  for  a  pattern  of  topic  names  to  subscribe  to  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  used  to  configure  the    kafka  consumer  client  and  the    zoo  keeper  client    public  evolving  public    flink  kafka  consumer010    pattern  subscription  pattern    kafka  deserialization  schema  t  deserializer    properties  props  this  null  subscription  pattern  deserializer  props  private    flink  kafka  consumer010    nullable    list    string  topics    nullable    pattern  subscription  pattern    kafka  deserialization  schema  t  deserializer    properties  props  super  topics  subscription  pattern  deserializer  get  long  check  not  null  props  props  key  partition  discovery  interval  millis  partition  discovery  disabled  get  boolean  props  key  disable  metrics  false  this  properties  props  set  deserializer  this  properties  configure  the  polling  timeout  try  if  properties  contains  key  key  poll  timeout  this  poll  timeout    long  parse  long  properties  get  property  key  poll  timeout  else  this  poll  timeout  default  poll  timeout  catch    exception  e  throw  new    illegal  argument  exception    cannot  parse  poll  timeout  for  key  poll  timeout  e    override  protected    abstract  fetcher  t  create  fetcher    source  context  t  source  context    map    kafka  topic  partition    long  assigned  partitions  with  initial  offsets    serialized  value    watermark  strategy  t  watermark  strategy    streaming  runtime  context  runtime  context    offset  commit  mode  offset  commit  mode    metric  group  consumer  metric  group  boolean  use  metrics  throws    exception  make  sure  that  auto  commit  is  disabled  when  our  offset  commit  mode  is  on  checkpoints  this  overwrites  whatever  setting  the  user  configured  in  the  properties  adjust  auto  commit  config  properties  offset  commit  mode    if  a  rate  limiter  is  set  then  call  rate  limiter  open  with  the  runtime  context  if  rate  limiter  null  rate  limiter  open  runtime  context  return  new    kafka010  fetcher  source  context  assigned  partitions  with  initial  offsets  watermark  strategy  runtime  context  get  processing  time  service  runtime  context  get  execution  config  get  auto  watermark  interval  runtime  context  get  user  code  class  loader  runtime  context  get  task  name  with  subtasks  deserializer  properties  poll  timeout  runtime  context  get  metric  group  consumer  metric  group  use  metrics  rate  limiter    override  protected    abstract  partition  discoverer  create  partition  discoverer    kafka  topics  descriptor  topics  descriptor  int  index  of  this  subtask  int  num  parallel  subtasks  return  new    kafka010  partition  discoverer  topics  descriptor  index  of  this  subtask  num  parallel  subtasks  properties    override  protected  boolean  get  is  auto  commit  enabled  return  get  boolean  properties    consumer  config  enable  auto  commit  config  true    properties  util  get  long  properties    consumer  config  auto  commit  interval  ms  config        override  protected    map    kafka  topic  partition    long  fetch  offsets  with  timestamp    collection    kafka  topic  partition  partitions  long  timestamp    map    topic  partition    long  partition  offsets  request  new    hash  map  partitions  size  for    kafka  topic  partition  partition  partitions  partition  offsets  request  put  new    topic  partition  partition  get  topic  partition  get  partition  timestamp  final    map    kafka  topic  partition    long  result  new    hash  map  partitions  size  use  a  short  lived  consumer  to  fetch  the  offsets  this  is  ok  because  this  is  a  one  time  operation  that  happens  only  on  startup  try    kafka  consumer  consumer  new    kafka  consumer  properties  for    map    entry    topic  partition    offset  and  timestamp  partition  to  offset  consumer  offsets  for  times  partition  offsets  request  entry  set  result  put  new    kafka  topic  partition  partition  to  offset  get  key  topic  partition  to  offset  get  key  partition  partition  to  offset  get  value  null  null  partition  to  offset  get  value  offset  return  result    utilities    makes  sure  that  the    byte  array  deserializer  is  registered  in  the    kafka  properties  param  props    the    kafka  properties  to  register  the  serializer  in  private  static  void  set  deserializer    properties  props  final    string  de  ser  name    byte  array  deserializer  class  get  name    object  key  de  ser  props  get    consumer  config  key  deserializer  class  config    object  val  de  ser  props  get    consumer  config  value  deserializer  class  config  if  key  de  ser  null  key  de  ser  equals  de  ser  name  log  warn    ignoring  configured  key    de  serializer    consumer  config  key  deserializer  class  config  if  val  de  ser  null  val  de  ser  equals  de  ser  name  log  warn    ignoring  configured  value    de  serializer    consumer  config  value  deserializer  class  config  props  put    consumer  config  key  deserializer  class  config  de  ser  name  props  put    consumer  config  value  deserializer  class  config  de  ser  name    set  a  rate  limiter  to  ratelimit  bytes  read  from    kafka  param  kafka  rate  limiter  public  void  set  rate  limiter    flink  connector  rate  limiter  kafka  rate  limiter  this  rate  limiter  kafka  rate  limiter  public    flink  connector  rate  limiter  get  rate  limiter  return  rate  limiter  
public  evolving  public  class    flink  kafka  producer010  t  extends    flink  kafka  producer  base  t  private  static  final  long  serial  version  u  i  d  1  l    flag  controlling  whether  we  are  writing  the    flink  record  s  timestamp  into    kafka  private  boolean  write  timestamp  to  kafka  false    regular  constructors    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer010    string    serialization  schema    properties    flink  kafka  partitioner  instead  param  broker  list    comma  separated  addresses  of  the  brokers  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  key  less  serialization  schema  public    flink  kafka  producer010    string  broker  list    string  topic  id    serialization  schema  t  serialization  schema  this  topic  id  new    keyed  serialization  schema  wrapper  serialization  schema  get  properties  from  broker  list  broker  list  new    flink  fixed  partitioner  t    creates  a    flink  kafka  producer  for  a  given  topic  the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer010    string    serialization  schema    properties    flink  kafka  partitioner  instead  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  key  less  serialization  schema  param  producer  config    properties  with  the  producer  configuration  public    flink  kafka  producer010    string  topic  id    serialization  schema  t  serialization  schema    properties  producer  config  this  topic  id  new    keyed  serialization  schema  wrapper  serialization  schema  producer  config  new    flink  fixed  partitioner  t    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  key  less  link    serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    since  a  key  less  link    serialization  schema  is  used  all  records  sent  to    kafka  will  not  have  an  attached  key    therefore  if  a  partitioner  is  also  not  provided  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  topic  id    the  topic  to  write  data  to  param  serialization  schema  a  key  less  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  set  to  code  null  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  public    flink  kafka  producer010    string  topic  id    serialization  schema  t  serialization  schema    properties  producer  config    nullable    flink  kafka  partitioner  t  custom  partitioner  this  topic  id  new    keyed  serialization  schema  wrapper  serialization  schema  producer  config  custom  partitioner    key    value  serialization  schema  constructors    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer010    string    keyed  serialization  schema    properties    flink  kafka  partitioner  instead  param  broker  list    comma  separated  addresses  of  the  brokers  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  public    flink  kafka  producer010    string  broker  list    string  topic  id    keyed  serialization  schema  t  serialization  schema  this  topic  id  serialization  schema  get  properties  from  broker  list  broker  list  new    flink  fixed  partitioner  t    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer010    string    keyed  serialization  schema    properties    flink  kafka  partitioner  instead  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  param  producer  config    properties  with  the  producer  configuration  public    flink  kafka  producer010    string  topic  id    keyed  serialization  schema  t  serialization  schema    properties  producer  config  this  topic  id  serialization  schema  producer  config  new    flink  fixed  partitioner  t    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  keyed  link    keyed  serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    if  a  partitioner  is  not  provided  written  records  will  be  partitioned  by  the  attached  key  of  each  record  as  determined  by  link    keyed  serialization  schema  serialize  key    object    if  written  records  do  not  have  a  key  i  e  link    keyed  serialization  schema  serialize  key    object  returns  code  null  they  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  topic  id    the  topic  to  write  data  to  param  serialization  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  set  to  code  null  records  will  be  partitioned  by  the  key  of  each  record  determined  by  link    keyed  serialization  schema  serialize  key    object    if  the  keys  are  code  null  then  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  public    flink  kafka  producer010    string  topic  id    keyed  serialization  schema  t  serialization  schema    properties  producer  config    nullable    flink  kafka  partitioner  t  custom  partitioner  super  topic  id  serialization  schema  producer  config  custom  partitioner    user  configuration    if  set  to  true    flink  will  write  the  event  time  timestamp  attached  to  each  record  into    kafka    timestamps  must  be  positive  for    kafka  to  accept  them  param  write  timestamp  to  kafka    flag  indicating  if    flink  s  internal  timestamps  are  written  to    kafka  public  void  set  write  timestamp  to  kafka  boolean  write  timestamp  to  kafka  this  write  timestamp  to  kafka  write  timestamp  to  kafka    deprecated  constructors  factory  methods    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    this  constructor  allows  writing  timestamps  to    kafka  it  follow  approach  b  see  above  param  in  stream    the  stream  to  write  to    kafka  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  param  producer  config    properties  with  the  producer  configuration  deprecated    use  link    flink  kafka  producer010    string    keyed  serialization  schema    properties  and  call  link  set  write  timestamp  to  kafka  boolean    deprecated  public  static  t    flink  kafka  producer010  configuration  t  write  to  kafka  with  timestamps    data  stream  t  in  stream    string  topic  id    keyed  serialization  schema  t  serialization  schema    properties  producer  config  return  write  to  kafka  with  timestamps  in  stream  topic  id  serialization  schema  producer  config  new    flink  fixed  partitioner  t    creates  a    flink  kafka  producer  for  a  given  topic  the  sink  produces  a    data  stream  to  the  topic  p    this  constructor  allows  writing  timestamps  to    kafka  it  follow  approach  b  see  above  param  in  stream    the  stream  to  write  to    kafka  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  keyless  serialization  schema  param  producer  config    properties  with  the  producer  configuration  deprecated    use  link    flink  kafka  producer010    string    keyed  serialization  schema    properties  and  call  link  set  write  timestamp  to  kafka  boolean    deprecated  public  static  t    flink  kafka  producer010  configuration  t  write  to  kafka  with  timestamps    data  stream  t  in  stream    string  topic  id    serialization  schema  t  serialization  schema    properties  producer  config  return  write  to  kafka  with  timestamps  in  stream  topic  id  new    keyed  serialization  schema  wrapper  serialization  schema  producer  config  new    flink  fixed  partitioner  t    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    this  constructor  allows  writing  timestamps  to    kafka  it  follow  approach  b  see  above  param  in  stream    the  stream  to  write  to    kafka  param  topic  id    the  name  of  the  target  topic  param  serialization  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions  deprecated    use  link    flink  kafka  producer010    string    keyed  serialization  schema    properties    flink  kafka  partitioner  and  call  link  set  write  timestamp  to  kafka  boolean    deprecated  public  static  t    flink  kafka  producer010  configuration  t  write  to  kafka  with  timestamps    data  stream  t  in  stream    string  topic  id    keyed  serialization  schema  t  serialization  schema    properties  producer  config    flink  kafka  partitioner  t  custom  partitioner    flink  kafka  producer010  t  kafka  producer  new    flink  kafka  producer010  topic  id  serialization  schema  producer  config  custom  partitioner    data  stream  sink  t  stream  sink  in  stream  add  sink  kafka  producer  return  new    flink  kafka  producer010  configuration  stream  sink  in  stream  kafka  producer    generic  element  processing    override  public  void  invoke  t  value    context  context  throws    exception  check  erroneous  byte  serialized  key  schema  serialize  key  value  byte  serialized  value  schema  serialize  value  value    string  target  topic  schema  get  target  topic  value  if  target  topic  null  target  topic  default  topic  id    long  timestamp  null  if  this  write  timestamp  to  kafka  timestamp  context  timestamp    producer  record  byte  byte  record  int  partitions  topic  partitions  map  get  target  topic  if  null  partitions  partitions  get  partitions  by  topic  target  topic  producer  topic  partitions  map  put  target  topic  partitions  if  flink  kafka  partitioner  null  record  new    producer  record  target  topic  null  timestamp  serialized  key  serialized  value  else  record  new    producer  record  target  topic  flink  kafka  partitioner  partition  value  serialized  key  serialized  value  target  topic  partitions  timestamp  serialized  key  serialized  value  if  flush  on  checkpoint  synchronized  pending  records  lock  pending  records  producer  send  record  callback    override  protected  void  flush  if  this  producer  null  producer  flush    configuration  object  returned  by  the  write  to  kafka  with  timestamps  call  p    this  is  only  kept  because  it  s  part  of  the  public  api    it  is  not  necessary  anymore  now  that  the  link    sink  function  interface  provides  timestamps  p  p    to  enable  the  settings  this  fake  sink  must  override  all  the  public  methods  in  link    data  stream  sink  p  deprecated    this  class  is  deprecated  since  the  factory  methods  code  write  to  kafka  with  timestamps  for  the  producer  are  also  deprecated    deprecated  public  static  class    flink  kafka  producer010  configuration  t  extends    data  stream  sink  t  private  final    flink  kafka  producer010  producer  private  final    sink  transformation  t  transformation  private    flink  kafka  producer010  configuration    data  stream  sink  t  original  sink    data  stream  t  input  stream    flink  kafka  producer010  t  producer  noinspection  unchecked  super  input  stream  original  sink  get  transformation  get  operator  this  transformation  original  sink  get  transformation  this  producer  producer    defines  whether  the  producer  should  fail  on  errors  or  only  log  them    if  this  is  set  to  true  then  exceptions  will  be  only  logged  if  set  to  false  exceptions  will  be  eventually  thrown  and  cause  the  streaming  program  to  fail  and  enter  recovery  param  log  failures  only    the  flag  to  indicate  logging  only  on  exceptions  public  void  set  log  failures  only  boolean  log  failures  only  producer  set  log  failures  only  log  failures  only    if  set  to  true  the    flink  producer  will  wait  for  all  outstanding  messages  in  the    kafka  buffers  to  be  acknowledged  by  the    kafka  producer  on  a  checkpoint    this  way  the  producer  can  guarantee  that  messages  in  the    kafka  buffers  are  part  of  the  checkpoint  param  flush    flag  indicating  the  flushing  mode  true  flush  on  checkpoint  public  void  set  flush  on  checkpoint  boolean  flush  producer  set  flush  on  checkpoint  flush    if  set  to  true    flink  will  write  the  event  time  timestamp  attached  to  each  record  into    kafka    timestamps  must  be  positive  for    kafka  to  accept  them  param  write  timestamp  to  kafka    flag  indicating  if    flink  s  internal  timestamps  are  written  to    kafka  public  void  set  write  timestamp  to  kafka  boolean  write  timestamp  to  kafka  producer  write  timestamp  to  kafka  write  timestamp  to  kafka    override  methods  to  use  the  transformation  in  this  class    override  public    sink  transformation  t  get  transformation  return  transformation    override  public    data  stream  sink  t  name    string  name  transformation  set  name  name  return  this    override  public    data  stream  sink  t  uid    string  uid  transformation  set  uid  uid  return  this    override  public    data  stream  sink  t  set  uid  hash    string  uid  hash  transformation  set  uid  hash  uid  hash  return  this    override  public    data  stream  sink  t  set  parallelism  int  parallelism  transformation  set  parallelism  parallelism  return  this    override  public    data  stream  sink  t  disable  chaining  this  transformation  set  chaining  strategy    chaining  strategy  never  return  this    override  public    data  stream  sink  t  slot  sharing  group    string  slot  sharing  group  transformation  set  slot  sharing  group  slot  sharing  group  return  this  
public  evolving  public  enum    flink  kafka011  error  code  producers  pool  empty  external  error  
public  evolving  public  class    flink  kafka011  exception  extends    flink  exception  private  static  final  long  serial  version  u  i  d    l  private  final    flink  kafka011  error  code  error  code  public    flink  kafka011  exception    flink  kafka011  error  code  error  code    string  message  super  message  this  error  code  error  code  public    flink  kafka011  exception    flink  kafka011  error  code  error  code    string  message    throwable  cause  super  message  cause  this  error  code  error  code  public    flink  kafka011  error  code  get  error  code  return  error  code  
public  evolving  public  class    flink  kafka  consumer011  t  extends    flink  kafka  consumer010  t  private  static  final  long  serial  version  u  i  d    l    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.11  x  param  topic    the  name  of  the  topic  that  should  be  consumed  param  value  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  used  to  configure  the    kafka  consumer  client  and  the    zoo  keeper  client  public    flink  kafka  consumer011    string  topic    deserialization  schema  t  value  deserializer    properties  props  this    collections  singleton  list  topic  value  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.11  x  p    this  constructor  allows  passing  a  see    kafka  deserialization  schema  for  reading  key  value  pairs  offsets  and  topic  names  from    kafka  param  topic    the  name  of  the  topic  that  should  be  consumed  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  used  to  configure  the    kafka  consumer  client  and  the    zoo  keeper  client  public    flink  kafka  consumer011    string  topic    kafka  deserialization  schema  t  deserializer    properties  props  this    collections  singleton  list  topic  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.11  x  p    this  constructor  allows  passing  multiple  topics  to  the  consumer  param  topics    the    kafka  topics  to  read  from  param  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  that  are  used  to  configure  both  the  fetcher  and  the  offset  handler  public    flink  kafka  consumer011    list    string  topics    deserialization  schema  t  deserializer    properties  props  this  topics  new    kafka  deserialization  schema  wrapper  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.11  x  p    this  constructor  allows  passing  multiple  topics  and  a  key  value  deserialization  schema  param  topics    the    kafka  topics  to  read  from  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  that  are  used  to  configure  both  the  fetcher  and  the  offset  handler  public    flink  kafka  consumer011    list    string  topics    kafka  deserialization  schema  t  deserializer    properties  props  super  topics  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.11  x    use  this  constructor  to  subscribe  to  multiple  topics  based  on  a  regular  expression  pattern  p    if  partition  discovery  is  enabled  by  setting  a  non  negative  value  for  link    flink  kafka  consumer011  key  partition  discovery  interval  millis  in  the  properties  topics  with  names  matching  the  pattern  will  also  be  subscribed  to  as  they  are  created  on  the  fly  param  subscription  pattern    the  regular  expression  for  a  pattern  of  topic  names  to  subscribe  to  param  value  deserializer    the  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  used  to  configure  the    kafka  consumer  client  and  the    zoo  keeper  client    public  evolving  public    flink  kafka  consumer011    pattern  subscription  pattern    deserialization  schema  t  value  deserializer    properties  props  this  subscription  pattern  new    kafka  deserialization  schema  wrapper  value  deserializer  props    creates  a  new    kafka  streaming  source  consumer  for    kafka  0.11  x    use  this  constructor  to  subscribe  to  multiple  topics  based  on  a  regular  expression  pattern  p    if  partition  discovery  is  enabled  by  setting  a  non  negative  value  for  link    flink  kafka  consumer011  key  partition  discovery  interval  millis  in  the  properties  topics  with  names  matching  the  pattern  will  also  be  subscribed  to  as  they  are  created  on  the  fly  p    this  constructor  allows  passing  a  see    kafka  deserialization  schema  for  reading  key  value  pairs  offsets  and  topic  names  from    kafka  param  subscription  pattern    the  regular  expression  for  a  pattern  of  topic  names  to  subscribe  to  param  deserializer    the  keyed  de  serializer  used  to  convert  between    kafka  s  byte  messages  and    flink  s  objects  param  props    the  properties  used  to  configure  the    kafka  consumer  client  and  the    zoo  keeper  client    public  evolving  public    flink  kafka  consumer011    pattern  subscription  pattern    kafka  deserialization  schema  t  deserializer    properties  props  super  subscription  pattern  deserializer  props  
public  evolving  public  class    flink  kafka  producer011  in  extends    two  phase  commit  sink  function  in    flink  kafka  producer011    kafka  transaction  state    flink  kafka  producer011    kafka  transaction  context    semantics  that  can  be  chosen  li  link  exactly  once  li  li  link  at  least  once  li  li  link  none  li  public  enum    semantic    semantic  exactly  once  the    flink  producer  will  write  all  messages  in  a    kafka  transaction  that  will  be  committed  to  the    kafka  on  a  checkpoint  p    in  this  mode  link    flink  kafka  producer011  sets  up  a  pool  of  link    flink  kafka  producer    between  each  checkpoint  there  is  created  new    kafka  transaction  which  is  being  committed  on  link    flink  kafka  producer011  notify  checkpoint  complete  long    if  checkpoint  complete  notifications  are  running  late  link    flink  kafka  producer011  can  run  out  of  link    flink  kafka  producer  s  in  the  pool    in  that  case  any  subsequent  link    flink  kafka  producer011  snapshot  state    function  snapshot  context  requests  will  fail  and  link    flink  kafka  producer011  will  keep  using  the  link    flink  kafka  producer  from  previous  checkpoint    to  decrease  chances  of  failing  checkpoints  there  are  three  options  li  decrease  number  of  max  concurrent  checkpoints  li  li  make  checkpoints  more  reliable  so  that  they  complete  faster  li  li  increase  delay  between  checkpoints  li  li  increase  size  of  link    flink  kafka  producer  s  pool  li  exactly  once    semantic  at  least  once  the    flink  producer  will  wait  for  all  outstanding  messages  in  the    kafka  buffers  to  be  acknowledged  by  the    kafka  producer  on  a  checkpoint  at  least  once    semantic  none  means  that  nothing  will  be  guaranteed    messages  can  be  lost  and  or  duplicated  in  case  of  failure  none  private  static  final    logger  log    logger  factory  get  logger    flink  kafka  producer011  class  private  static  final  long  serial  version  u  i  d  1  l    this  coefficient  determines  what  is  the  safe  scale  down  factor  p    if  the    flink  application  previously  failed  before  first  checkpoint  completed  or  we  are  starting  new  batch  of  link    flink  kafka  producer011  from  scratch  without  clean  shutdown  of  the  previous  one  link    flink  kafka  producer011  doesn  t  know  what  was  the  set  of  previously  used    kafka  s  transactional  id  s    in  that  case  it  will  try  to  play  safe  and  abort  all  of  the  possible  transactional  ids  from  the  range  of  code    get  number  of  parallel  subtasks  kafka  producers  pool  size  safe  scale  down  factor  p    the  range  of  available  to  use  transactional  ids  is  code    get  number  of  parallel  subtasks  kafka  producers  pool  size  p    this  means  that  if  we  decrease  code  get  number  of  parallel  subtasks  by  a  factor  larger  than  code  safe  scale  down  factor  we  can  have  a  left  some  lingering  transaction  public  static  final  int  safe  scale  down  factor      default  number  of    kafka  producers  in  the  pool    see  link    semantic  exactly  once  public  static  final  int  default  kafka  producers  pool  size      default  value  for  kafka  transaction  timeout  public  static  final    time  default  kafka  transaction  timeout    time  hours      configuration  key  for  disabling  the  metrics  reporting  public  static  final    string  key  disable  metrics  flink  disable  metrics    descriptor  of  the  transactional    i  ds  list    note    this  state  is  serialized  by    kryo    serializer  and  it  has  compatibility  problem  that  will  be  removed  later    please  use  next  transactional  id  hint  descriptor    v2    deprecated  private  static  final    list  state  descriptor    next  transactional  id  hint  next  transactional  id  hint  descriptor  new    list  state  descriptor  next  transactional  id  hint    type  information  of    next  transactional  id  hint  class  private  static  final    list  state  descriptor    next  transactional  id  hint  next  transactional  id  hint  descriptor    v2  new    list  state  descriptor  next  transactional  id  hint  v2  new    next  transactional  id  hint  serializer    state  for  next  transactional  id  hint  private  transient    list  state    next  transactional  id  hint  next  transactional  id  hint  state    generator  for    transactional    i  ds  private  transient    transactional  ids  generator  transactional  ids  generator    hint  for  picking  next  transactional  id  private  transient    next  transactional  id  hint  next  transactional  id  hint    user  defined  properties  for  the    producer  private  final    properties  producer  config    the  name  of  the  default  topic  this  producer  is  writing  data  to  private  final    string  default  topic  id    serializable    serialization  schema  for  turning  objects  used  with    flink  into  byte  for    kafka  private  final    keyed  serialization  schema  in  schema    user  provided  partitioner  for  assigning  an  object  to  a    kafka  partition  for  each  topic  private  final    flink  kafka  partitioner  in  flink  kafka  partitioner    partitions  of  each  topic  private  final    map    string  int  topic  partitions  map    max  number  of  producers  in  the  pool    if  all  producers  are  in  use  snapshoting  state  will  throw  an  exception  private  final  int  kafka  producers  pool  size    pool  of  available  transactional  ids  private  final    blocking  deque    string  available  transactional  ids  new    linked  blocking  deque    flag  controlling  whether  we  are  writing  the    flink  record  s  timestamp  into    kafka  private  boolean  write  timestamp  to  kafka  false    flag  indicating  whether  to  accept  failures  and  log  them  or  to  fail  on  failures  private  boolean  log  failures  only    semantic  chosen  for  this  instance  private    semantic  semantic    runtime  fields    the  callback  than  handles  error  propagation  or  logging  callbacks    nullable  private  transient    callback  callback    errors  encountered  in  the  async  producer  are  stored  here    nullable  private  transient  volatile    exception  async  exception    number  of  unacknowledged  records  private  final    atomic  long  pending  records  new    atomic  long    cache  of  metrics  to  replace  already  registered  metrics  instead  of  overwriting  existing  ones  private  final    map    string    kafka  metric  mutable  wrapper  previously  created  metrics  new    hash  map    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  param  broker  list    comma  separated  addresses  of  the  brokers  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  keyless  serialization  schema  public    flink  kafka  producer011    string  broker  list    string  topic  id    serialization  schema  in  serialization  schema  this  topic  id  new    keyed  serialization  schema  wrapper  serialization  schema  get  properties  from  broker  list  broker  list    optional  of  new    flink  fixed  partitioner  in    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer011    string    serialization  schema    properties    optional  instead  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  key  less  serialization  schema  param  producer  config    properties  with  the  producer  configuration  public    flink  kafka  producer011    string  topic  id    serialization  schema  in  serialization  schema    properties  producer  config  this  topic  id  new    keyed  serialization  schema  wrapper  serialization  schema  producer  config    optional  of  new    flink  fixed  partitioner  in    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  key  less  link    serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    since  a  key  less  link    serialization  schema  is  used  all  records  sent  to    kafka  will  not  have  an  attached  key    therefore  if  a  partitioner  is  also  not  provided  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  topic  id    the  topic  to  write  data  to  param  serialization  schema  a  key  less  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  a  partitioner  is  not  provided  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  public    flink  kafka  producer011    string  topic  id    serialization  schema  in  serialization  schema    properties  producer  config    optional    flink  kafka  partitioner  in  custom  partitioner  this  topic  id  new    keyed  serialization  schema  wrapper  serialization  schema  producer  config  custom  partitioner    key    value  serialization  schema  constructors    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer011    string    keyed  serialization  schema    properties    optional  instead  param  broker  list    comma  separated  addresses  of  the  brokers  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  public    flink  kafka  producer011    string  broker  list    string  topic  id    keyed  serialization  schema  in  serialization  schema  this  topic  id  serialization  schema  get  properties  from  broker  list  broker  list    optional  of  new    flink  fixed  partitioner  in    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer011    string    keyed  serialization  schema    properties    optional  instead  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  param  producer  config    properties  with  the  producer  configuration  public    flink  kafka  producer011    string  topic  id    keyed  serialization  schema  in  serialization  schema    properties  producer  config  this  topic  id  serialization  schema  producer  config    optional  of  new    flink  fixed  partitioner  in    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  a    data  stream  to  the  topic  p    using  this  constructor  the  default  link    flink  fixed  partitioner  will  be  used  as  the  partitioner    this  default  partitioner  maps  each  sink  subtask  to  a  single    kafka  partition  i  e  all  records  received  by  a  sink  subtask  will  end  up  in  the  same    kafka  partition  p    to  use  a  custom  partitioner  please  use  link    flink  kafka  producer011    string    keyed  serialization  schema    properties    optional    semantic  int  instead  param  topic  id  id  of  the    kafka  topic  param  serialization  schema    user  defined  serialization  schema  supporting  key  value  messages  param  producer  config    properties  with  the  producer  configuration  param  semantic    defines  semantic  that  will  be  used  by  this  producer  see  link    semantic  public    flink  kafka  producer011    string  topic  id    keyed  serialization  schema  in  serialization  schema    properties  producer  config    semantic  semantic  this  topic  id  serialization  schema  producer  config    optional  of  new    flink  fixed  partitioner  in  semantic  default  kafka  producers  pool  size    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  keyed  link    keyed  serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    if  a  partitioner  is  not  provided  written  records  will  be  partitioned  by  the  attached  key  of  each  record  as  determined  by  link    keyed  serialization  schema  serialize  key    object    if  written  records  do  not  have  a  key  i  e  link    keyed  serialization  schema  serialize  key    object  returns  code  null  they  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  default  topic  id    the  default  topic  to  write  data  to  param  serialization  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  a  partitioner  is  not  provided  records  will  be  partitioned  by  the  key  of  each  record  determined  by  link    keyed  serialization  schema  serialize  key    object    if  the  keys  are  code  null  then  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  public    flink  kafka  producer011    string  default  topic  id    keyed  serialization  schema  in  serialization  schema    properties  producer  config    optional    flink  kafka  partitioner  in  custom  partitioner  this  default  topic  id  serialization  schema  producer  config  custom  partitioner    semantic  at  least  once  default  kafka  producers  pool  size    creates  a    flink  kafka  producer  for  a  given  topic    the  sink  produces  its  input  to  the  topic    it  accepts  a  keyed  link    keyed  serialization  schema  and  possibly  a  custom  link    flink  kafka  partitioner  p    if  a  partitioner  is  not  provided  written  records  will  be  partitioned  by  the  attached  key  of  each  record  as  determined  by  link    keyed  serialization  schema  serialize  key    object    if  written  records  do  not  have  a  key  i  e  link    keyed  serialization  schema  serialize  key    object  returns  code  null  they  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  default  topic  id    the  default  topic  to  write  data  to  param  serialization  schema  a  serializable  serialization  schema  for  turning  user  objects  into  a  kafka  consumable  byte  supporting  key  value  messages  param  producer  config    configuration  properties  for  the    kafka  producer  bootstrap  servers  is  the  only  required  argument  param  custom  partitioner  a  serializable  partitioner  for  assigning  messages  to    kafka  partitions    if  a  partitioner  is  not  provided  records  will  be  partitioned  by  the  key  of  each  record  determined  by  link    keyed  serialization  schema  serialize  key    object    if  the  keys  are  code  null  then  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  param  semantic    defines  semantic  that  will  be  used  by  this  producer  see  link    semantic  param  kafka  producers  pool  size    overwrite  default    kafka  producers  pool  size  see  link    semantic  exactly  once  public    flink  kafka  producer011    string  default  topic  id    keyed  serialization  schema  in  serialization  schema    properties  producer  config    optional    flink  kafka  partitioner  in  custom  partitioner    semantic  semantic  int  kafka  producers  pool  size  super  new    transaction  state  serializer  new    context  state  serializer  this  default  topic  id  check  not  null  default  topic  id  default  topic  id  is  null  this  schema  check  not  null  serialization  schema  serialization  schema  is  null  this  producer  config  check  not  null  producer  config  producer  config  is  null  this  flink  kafka  partitioner  check  not  null  custom  partitioner  custom  partitioner  is  null  or  else  null  this  semantic  check  not  null  semantic  semantic  is  null  this  kafka  producers  pool  size  kafka  producers  pool  size  check  state  kafka  producers  pool  size    kafka  producers  pool  size  must  be  non  empty    closure  cleaner  clean  this  flink  kafka  partitioner    execution  config    closure  cleaner  level  recursive  true    closure  cleaner  ensure  serializable  serialization  schema  set  the  producer  configuration  properties  for  kafka  record  key  value  serializers  if  producer  config  contains  key    producer  config  key  serializer  class  config  this  producer  config  put    producer  config  key  serializer  class  config    byte  array  serializer  class  get  name  else  log  warn    overwriting  the  is  not  recommended    producer  config  key  serializer  class  config  if  producer  config  contains  key    producer  config  value  serializer  class  config  this  producer  config  put    producer  config  value  serializer  class  config    byte  array  serializer  class  get  name  else  log  warn    overwriting  the  is  not  recommended    producer  config  value  serializer  class  config  eagerly  ensure  that  bootstrap  servers  are  set  if  this  producer  config  contains  key    producer  config  bootstrap  servers  config  throw  new    illegal  argument  exception    producer  config  bootstrap  servers  config  must  be  supplied  in  the  producer  config  properties  if  producer  config  contains  key    producer  config  transaction  timeout  config  long  timeout  default  kafka  transaction  timeout  to  milliseconds  check  state  timeout    integer  max  value  timeout    timeout  does  not  fit  into    bit  integer  this  producer  config  put    producer  config  transaction  timeout  config  int  timeout  log  warn    property  not  specified    setting  it  to    producer  config  transaction  timeout  config  default  kafka  transaction  timeout    enable  transaction  timeout  warnings  to  avoid  silent  data  loss    see  kafka    affects  versions  0.11  0.0  and  0.11  0.1    the    kafka  producer  may  not  throw  an  exception  if  the  transaction  failed  to  commit  if  semantic    semantic  exactly  once  final    object  object  this  producer  config  get    producer  config  transaction  timeout  config  final  long  transaction  timeout  if  object  instanceof    string    string  utils  is  numeric    string  object  transaction  timeout    long  parse  long    string  object  else  if  object  instanceof    number  transaction  timeout    number  object  long  value  else  throw  new    illegal  argument  exception    producer  config  transaction  timeout  config  must  be  numeric  was  object  super  set  transaction  timeout  transaction  timeout  super  enable  transaction  timeout  warnings  0.8  this  topic  partitions  map  new    hash  map    properties    if  set  to  true    flink  will  write  the  event  time  timestamp  attached  to  each  record  into    kafka    timestamps  must  be  positive  for    kafka  to  accept  them  param  write  timestamp  to  kafka    flag  indicating  if    flink  s  internal  timestamps  are  written  to    kafka  public  void  set  write  timestamp  to  kafka  boolean  write  timestamp  to  kafka  this  write  timestamp  to  kafka  write  timestamp  to  kafka    defines  whether  the  producer  should  fail  on  errors  or  only  log  them    if  this  is  set  to  true  then  exceptions  will  be  only  logged  if  set  to  false  exceptions  will  be  eventually  thrown  and  cause  the  streaming  program  to  fail  and  enter  recovery  param  log  failures  only    the  flag  to  indicate  logging  only  on  exceptions  public  void  set  log  failures  only  boolean  log  failures  only  this  log  failures  only  log  failures  only    disables  the  propagation  of  exceptions  thrown  when  committing  presumably  timed  out    kafka  transactions  during  recovery  of  the  job    if  a    kafka  transaction  is  timed  out  a  commit  will  never  be  successful    hence  use  this  feature  to  avoid  recovery  loops  of  the    job    exceptions  will  still  be  logged  to  inform  the  user  that  data  loss  might  have  occurred  p    note  that  we  use  link    system  current  time  millis  to  track  the  age  of  a  transaction    moreover  only  exceptions  thrown  during  the  recovery  are  caught  i  e  the  producer  will  attempt  at  least  one  commit  of  the  transaction  before  giving  up  p    override  public    flink  kafka  producer011  in  ignore  failures  after  transaction  timeout  super  ignore  failures  after  transaction  timeout  return  this    utilities    initializes  the  connection  to    kafka    override  public  void  open    configuration  configuration  throws    exception  if  log  failures  only  callback  new    callback    override  public  void  on  completion    record  metadata  metadata    exception  e  if  e  null  log  error    error  while  sending  record  to    kafka  e  get  message  e  acknowledge  message  else  callback  new    callback    override  public  void  on  completion    record  metadata  metadata    exception  exception  if  exception  null  async  exception  null  async  exception  exception  acknowledge  message  if  schema  instanceof    keyed  serialization  schema  wrapper    keyed  serialization  schema  wrapper  in  schema  get  serialization  schema  open  get  runtime  context  get  metric  group  add  group  user  super  open  configuration    override  public  void  invoke    kafka  transaction  state  transaction  in  next    context  context  throws    flink  kafka011  exception  check  erroneous  byte  serialized  key  schema  serialize  key  next  byte  serialized  value  schema  serialize  value  next    string  target  topic  schema  get  target  topic  next  if  target  topic  null  target  topic  default  topic  id    long  timestamp  null  if  this  write  timestamp  to  kafka  timestamp  context  timestamp    producer  record  byte  byte  record  int  partitions  topic  partitions  map  get  target  topic  if  null  partitions  partitions  get  partitions  by  topic  target  topic  transaction  producer  topic  partitions  map  put  target  topic  partitions  if  flink  kafka  partitioner  null  record  new    producer  record  target  topic  flink  kafka  partitioner  partition  next  serialized  key  serialized  value  target  topic  partitions  timestamp  serialized  key  serialized  value  else  record  new    producer  record  target  topic  null  timestamp  serialized  key  serialized  value  pending  records  increment  and  get  transaction  producer  send  record  callback    override  public  void  close  throws    flink  kafka011  exception    first  close  the  producer  for  current  transaction  try  final    kafka  transaction  state  current  transaction  current  transaction  if  current  transaction  null  to  avoid  exceptions  on  aborting  transactions  with  some  pending  records  flush  current  transaction  normal  abort  for  at  least  once  and  none  do  not  clean  up  resources  because  of  producer  reusing  thus  we  need  to  close  it  manually  switch  semantic  case  exactly  once  break  case  at  least  once  case  none  current  transaction  producer  flush  current  transaction  producer  close      time  unit  seconds  break  super  close  catch    exception  e  async  exception    exception  utils  first  or  suppressed  e  async  exception  finally    we  may  have  to  close  producer  of  the  current  transaction  in  case  some  exception  was  thrown  before  the  normal  close  routine  finishes  if  current  transaction  null    i  o  utils  close  quietly  current  transaction  producer    make  sure  all  the  producers  for  pending  transactions  are  closed  pending  transactions  for  each  transaction    i  o  utils  close  quietly  transaction  get  value  producer  make  sure  we  propagate  pending  errors  check  erroneous    logic  for  handling  checkpoint  flushing    override  protected    kafka  transaction  state  begin  transaction  throws    flink  kafka011  exception  switch  semantic  case  exactly  once    flink  kafka  producer  byte  byte  producer  create  transactional  producer  producer  begin  transaction  return  new    kafka  transaction  state  producer  get  transactional  id  producer  case  at  least  once  case  none    do  not  create  new  producer  on  each  begin  transaction  if  it  is  not  necessary  final    kafka  transaction  state  current  transaction  current  transaction  if  current  transaction  null  current  transaction  producer  null  return  new    kafka  transaction  state  current  transaction  producer  return  new    kafka  transaction  state  init  non  transactional  producer  true  default  throw  new    unsupported  operation  exception    not  implemented  semantic    override  protected  void  pre  commit    kafka  transaction  state  transaction  throws    flink  kafka011  exception  switch  semantic  case  exactly  once  case  at  least  once  flush  transaction  break  case  none  break  default  throw  new    unsupported  operation  exception    not  implemented  semantic  check  erroneous    override  protected  void  commit    kafka  transaction  state  transaction  if  transaction  is  transactional  try  transaction  producer  commit  transaction  finally  recycle  transactional  producer  transaction  producer    override  protected  void  recover  and  commit    kafka  transaction  state  transaction  if  transaction  is  transactional  try    flink  kafka  producer  byte  byte  producer  init  transactional  producer  transaction  transactional  id  false  producer  resume  transaction  transaction  producer  id  transaction  epoch  producer  commit  transaction  catch    invalid  txn  state  exception    producer  fenced  exception  ex    that  means  we  have  committed  this  transaction  before  log  warn    encountered  error  while  recovering  transaction    presumably  this  transaction  has  been  already  committed  before  ex  transaction    override  protected  void  abort    kafka  transaction  state  transaction  if  transaction  is  transactional  transaction  producer  abort  transaction  recycle  transactional  producer  transaction  producer    override  protected  void  recover  and  abort    kafka  transaction  state  transaction  if  transaction  is  transactional  try    flink  kafka  producer  byte  byte  producer  init  transactional  producer  transaction  transactional  id  false  producer  init  transactions  private  void  acknowledge  message  pending  records  decrement  and  get    flush  pending  records  param  transaction  private  void  flush    kafka  transaction  state  transaction  throws    flink  kafka011  exception  if  transaction  producer  null  transaction  producer  flush  long  pending  records  count  pending  records  get  if  pending  records  count    throw  new    illegal  state  exception    pending  record  count  must  be  zero  at  this  point  pending  records  count  if  the  flushed  requests  has  errors  we  should  propagate  it  also  and  fail  the  checkpoint  check  erroneous    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception  super  snapshot  state  context  next  transactional  id  hint  state  clear    to  avoid  duplication  only  first  subtask  keeps  track  of  next  transactional  id  hint    otherwise  all  of  the  subtasks  would  write  exactly  same  information  if  get  runtime  context  get  index  of  this  subtask    semantic    semantic  exactly  once  check  state  next  transactional  id  hint  null  next  transactional  id  hint  must  be  set  for  exactly  once  long  next  free  transactional  id  next  transactional  id  hint  next  free  transactional  id    if  we  scaled  up  some  unknown  subtask  must  have  created  new  transactional  ids  from  scratch    in  that  case  we  adjust  next  free  transactional  id  by  the  range  of  transactional  ids  that  could  be  used  for  this  scaling  up  if  get  runtime  context  get  number  of  parallel  subtasks  next  transactional  id  hint  last  parallelism  next  free  transactional  id  get  runtime  context  get  number  of  parallel  subtasks  kafka  producers  pool  size  next  transactional  id  hint  state  add  new    next  transactional  id  hint  get  runtime  context  get  number  of  parallel  subtasks  next  free  transactional  id    override  public  void  initialize  state    function  initialization  context  context  throws    exception  if  semantic    semantic  none    streaming  runtime  context  this  get  runtime  context  is  checkpointing  enabled  log  warn    using  semantic  but  checkpointing  is  not  enabled    switching  to  semantic  semantic    semantic  none  semantic    semantic  none  migrate  next  transactional  id  hind  state  context  transactional  ids  generator  new    transactional  ids  generator  get  runtime  context  get  task  name    streaming  runtime  context  get  runtime  context  get  operator  unique  i  d  get  runtime  context  get  index  of  this  subtask  get  runtime  context  get  number  of  parallel  subtasks  kafka  producers  pool  size  safe  scale  down  factor  if  semantic    semantic  exactly  once  next  transactional  id  hint  null  else    array  list    next  transactional  id  hint  transactional  id  hints    lists  new  array  list  next  transactional  id  hint  state  get  if  transactional  id  hints  size    throw  new    illegal  state  exception    there  should  be  at  most  one  next  transactional  id  hint  written  by  the  first  subtask  else  if  transactional  id  hints  size    next  transactional  id  hint  new    next  transactional  id  hint      this  means  that  this  is  either    the  first  execution  of  this  application    previous  execution  has  failed  before  first  checkpoint  completed  in  case  of    we  have  to  abort  all  previous  transactions  abort  transactions  transactional  ids  generator  generate  ids  to  abort  else  next  transactional  id  hint  transactional  id  hints  get    super  initialize  state  context    override  protected    optional    kafka  transaction  context  initialize  user  context  if  semantic    semantic  exactly  once  return    optional  empty    set    string  transactional  ids  generate  new  transactional  ids  reset  available  transactional  ids  pool  transactional  ids  return    optional  of  new    kafka  transaction  context  transactional  ids  private    set    string  generate  new  transactional  ids  check  state  next  transactional  id  hint  null  next  transactional  id  hint  must  be  present  for  exactly  once    set    string  transactional  ids  transactional  ids  generator  generate  ids  to  use  next  transactional  id  hint  next  free  transactional  id  log  info    generated  new  transactional  ids  transactional  ids  return  transactional  ids    override  protected  void  finish  recovering  context    collection    flink  kafka  producer011    kafka  transaction  state  handled  transactions  clean  up  user  context  handled  transactions  reset  available  transactional  ids  pool  get  user  context  get  transactional  ids  log  info    recovered  transactional  ids  get  user  context  get  transactional  ids    after  initialization  make  sure  that  all  previous  transactions  from  the  current  user  context  have  been  completed  param  handled  transactions  transactions  which  were  already  committed  or  aborted  and  do  not  need  further  handling  private  void  clean  up  user  context    collection    kafka  transaction  state  handled  transactions  if  get  user  context  is  present  return    hash  set    string  abort  transactions  new    hash  set  get  user  context  get  transactional  ids  handled  transactions  for  each  kafka  transaction  state  abort  transactions  remove  kafka  transaction  state  transactional  id  abort  transactions  abort  transactions  private  void  reset  available  transactional  ids  pool    collection    string  transactional  ids  available  transactional  ids  clear  available  transactional  ids  add  all  transactional  ids    utilities  private  void  abort  transactions    set    string  transactional  ids  transactional  ids  parallel  stream  for  each  transactional  id  don  t  mess  with  the  original  configuration  or  any  other  properties  of  the  original  object  create  an  internal  kafka  producer  on  our  own  and  do  not  rely  on  init  transactional  producer  final    properties  my  config  new    properties  my  config  put  all  producer  config  init  transactional  producer  config  my  config  transactional  id  try    flink  kafka  producer  byte  byte  kafka  producer  new    flink  kafka  producer  my  config  it  suffices  to  call  init  transactions  this  will  abort  any  lingering  transactions  kafka  producer  init  transactions  int  get  transaction  coordinator  id  final    kafka  transaction  state  current  transaction  current  transaction  if  current  transaction  null  current  transaction  producer  null  throw  new    illegal  argument  exception  return  current  transaction  producer  get  transaction  coordinator  id    for  each  checkpoint  we  create  new  link    flink  kafka  producer  so  that  new  transactions  will  not  clash  with  transactions  created  during  previous  checkpoints  code  producer  init  transactions  assures  that  we  obtain  new  producer  id  and  epoch  counters  private    flink  kafka  producer  byte  byte  create  transactional  producer  throws    flink  kafka011  exception    string  transactional  id  available  transactional  ids  poll  if  transactional  id  null  throw  new    flink  kafka011  exception    flink  kafka011  error  code  producers  pool  empty    too  many  ongoing  snapshots    increase  kafka  producers  pool  size  or  decrease  number  of  concurrent  checkpoints    flink  kafka  producer  byte  byte  producer  init  transactional  producer  transactional  id  true  producer  init  transactions  return  producer  private  void  recycle  transactional  producer    flink  kafka  producer  byte  byte  producer  available  transactional  ids  add  producer  get  transactional  id  producer  flush  producer  close      time  unit  seconds  private    flink  kafka  producer  byte  byte  init  transactional  producer    string  transactional  id  boolean  register  metrics  init  transactional  producer  config  producer  config  transactional  id  return  init  producer  register  metrics  private  static  void  init  transactional  producer  config    properties  producer  config    string  transactional  id  producer  config  put    producer  config  transactional  id  config  transactional  id  private    flink  kafka  producer  byte  byte  init  non  transactional  producer  boolean  register  metrics  producer  config  remove    producer  config  transactional  id  config  return  init  producer  register  metrics  private    flink  kafka  producer  byte  byte  init  producer  boolean  register  metrics    flink  kafka  producer  byte  byte  producer  new    flink  kafka  producer  this  producer  config    runtime  context  ctx  get  runtime  context  if  flink  kafka  partitioner  null  flink  kafka  partitioner  open  ctx  get  index  of  this  subtask  ctx  get  number  of  parallel  subtasks  log  info    starting    flink  kafka  producer  to  produce  into  default  topic  ctx  get  index  of  this  subtask    ctx  get  number  of  parallel  subtasks  default  topic  id  register    kafka  metrics  to    flink  accumulators  if  register  metrics    boolean  parse  boolean  producer  config  get  property  key  disable  metrics  false    map    metric  name  extends    metric  metrics  producer  metrics  if  metrics  null    map  r  s    kafka  implementation  returns  null  here  log  info    producer  implementation  does  not  support  metrics  else  final    metric  group  kafka  metric  group  get  runtime  context  get  metric  group  add  group    kafka  producer  for    map    entry    metric  name  extends    metric  entry  metrics  entry  set    string  name  entry  get  key  name    metric  metric  entry  get  value    kafka  metric  mutable  wrapper  wrapper  previously  created  metrics  get  name  if  wrapper  null  wrapper  set  kafka  metric  metric  else  todo  somehow  merge  metrics  from  all  active  producers  wrapper  new    kafka  metric  mutable  wrapper  metric  previously  created  metrics  put  name  wrapper  kafka  metric  group  gauge  name  wrapper  return  producer  private  void  check  erroneous  throws    flink  kafka011  exception    exception  e  async  exception  if  e  null  prevent  double  throwing  async  exception  null  throw  new    flink  kafka011  exception    flink  kafka011  error  code  external  error    failed  to  send  data  to    kafka  e  get  message  e  private  void  read  object  java  io    object  input  stream  in  throws    i  o  exception    class  not  found  exception  in  default  read  object  private  void  migrate  next  transactional  id  hind  state    function  initialization  context  context  throws    exception    list  state    next  transactional  id  hint  old  next  transactional  id  hint  state  context  get  operator  state  store  get  union  list  state  next  transactional  id  hint  descriptor  next  transactional  id  hint  state  context  get  operator  state  store  get  union  list  state  next  transactional  id  hint  descriptor    v2    array  list    next  transactional  id  hint  old  transactional  id  hints    lists  new  array  list  old  next  transactional  id  hint  state  get  if  old  transactional  id  hints  is  empty  next  transactional  id  hint  state  add  all  old  transactional  id  hints  clear  old  state  old  next  transactional  id  hint  state  clear  private  static    properties  get  properties  from  broker  list    string  broker  list    string  elements  broker  list  split  validate  the  broker  addresses  for    string  broker  elements    net  utils  get  correct  hostname  port  broker    properties  props  new    properties  props  set  property    producer  config  bootstrap  servers  config  broker  list  return  props  private  static  int  get  partitions  by  topic    string  topic    producer  byte  byte  producer  the  fetched  list  is  immutable  so  we  re  creating  a  mutable  copy  in  order  to  sort  it    list    partition  info  partitions  list  new    array  list  producer  partitions  for  topic  sort  the  partitions  by  partition  id  to  make  sure  the  fetched  partition  list  is  the  same  across  subtasks    collections  sort  partitions  list  new    comparator    partition  info    override  public  int  compare    partition  info  o1    partition  info  o2  return    integer  compare  o1  partition  o2  partition  int  partitions  new  int  partitions  list  size  for  int  i    i  partitions  length  i  partitions  i  partitions  list  get  i  partition  return  partitions    state  for  handling  transactions    visible  for  testing    internal  public  static  class    kafka  transaction  state  private  final  transient    flink  kafka  producer  byte  byte  producer    nullable  final    string  transactional  id  final  long  producer  id  final  short  epoch    visible  for  testing    internal  public    kafka  transaction  state    string  transactional  id    flink  kafka  producer  byte  byte  producer  this  transactional  id  producer  get  producer  id  producer  get  epoch  producer    visible  for  testing    internal  public    kafka  transaction  state    flink  kafka  producer  byte  byte  producer  this  null    short    producer    visible  for  testing    internal  public    kafka  transaction  state    nullable    string  transactional  id  long  producer  id  short  epoch    flink  kafka  producer  byte  byte  producer  this  transactional  id  transactional  id  this  producer  id  producer  id  this  epoch  epoch  this  producer  producer  boolean  is  transactional  return  transactional  id  null    override  public    string  to  string  return    string  format  s  transactional  id  s  producer  id  s  epoch  s  this  get  class  get  simple  name  transactional  id  producer  id  epoch    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    kafka  transaction  state  that    kafka  transaction  state  o  if  producer  id  that  producer  id  return  false  if  epoch  that  epoch  return  false  return  transactional  id  null  transactional  id  equals  that  transactional  id  that  transactional  id  null    override  public  int  hash  code  int  result  transactional  id  null  transactional  id  hash  code    result    result  int  producer  id  producer  id    result    result  int  epoch  return  result    context  associated  to  this  instance  of  the  link    flink  kafka  producer011    user  for  keeping  track  of  the  transactional  ids    visible  for  testing    internal  public  static  class    kafka  transaction  context  final    set    string  transactional  ids    visible  for  testing    internal  public    kafka  transaction  context    set    string  transactional  ids  check  not  null  transactional  ids  this  transactional  ids  transactional  ids    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    kafka  transaction  context  that    kafka  transaction  context  o  return  transactional  ids  equals  that  transactional  ids    override  public  int  hash  code  return  transactional  ids  hash  code  link  org  apache  flink  api  common  typeutils    type  serializer  for  link    kafka  transaction  state    visible  for  testing    internal  public  static  class    transaction  state  serializer  extends    type  serializer  singleton    kafka  transaction  state  private  static  final  long  serial  version  u  i  d  1  l    override  public  boolean  is  immutable  type  return  true    override  public    kafka  transaction  state  create  instance  return  null    override  public    kafka  transaction  state  copy    kafka  transaction  state  from  return  from    override  public    kafka  transaction  state  copy    kafka  transaction  state  from    kafka  transaction  state  reuse  return  from    override  public  int  get  length  return      override  public  void  serialize    kafka  transaction  state  record    data  output  view  target  throws    i  o  exception  if  record  transactional  id  null  target  write  boolean  false  else  target  write  boolean  true  target  write  u  t  f  record  transactional  id  target  write  long  record  producer  id  target  write  short  record  epoch    override  public    kafka  transaction  state  deserialize    data  input  view  source  throws    i  o  exception    string  transactional  id  null  if  source  read  boolean  transactional  id  source  read  u  t  f  long  producer  id  source  read  long  short  epoch  source  read  short  return  new    kafka  transaction  state  transactional  id  producer  id  epoch  null    override  public    kafka  transaction  state  deserialize    kafka  transaction  state  reuse    data  input  view  source  throws    i  o  exception  return  deserialize  source    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  boolean  has  transactional  id  source  read  boolean  target  write  boolean  has  transactional  id  if  has  transactional  id  target  write  u  t  f  source  read  u  t  f  target  write  long  source  read  long  target  write  short  source  read  short    override  public    type  serializer  snapshot    kafka  transaction  state  snapshot  configuration  return  new    transaction  state  serializer  snapshot    serializer  configuration  snapshot  for  compatibility  and  format  evolution    suppress  warnings    weaker  access  public  static  final  class    transaction  state  serializer  snapshot  extends    simple  type  serializer  snapshot    kafka  transaction  state  public    transaction  state  serializer  snapshot  super    transaction  state  serializer  new  link  org  apache  flink  api  common  typeutils    type  serializer  for  link    kafka  transaction  context    visible  for  testing    internal  public  static  class    context  state  serializer  extends    type  serializer  singleton    kafka  transaction  context  private  static  final  long  serial  version  u  i  d  1  l    override  public  boolean  is  immutable  type  return  true    override  public    kafka  transaction  context  create  instance  return  null    override  public    kafka  transaction  context  copy    kafka  transaction  context  from  return  from    override  public    kafka  transaction  context  copy    kafka  transaction  context  from    kafka  transaction  context  reuse  return  from    override  public  int  get  length  return      override  public  void  serialize    kafka  transaction  context  record    data  output  view  target  throws    i  o  exception  int  num  ids  record  transactional  ids  size  target  write  int  num  ids  for    string  id  record  transactional  ids  target  write  u  t  f  id    override  public    kafka  transaction  context  deserialize    data  input  view  source  throws    i  o  exception  int  num  ids  source  read  int    set    string  ids  new    hash  set  num  ids  for  int  i    i  num  ids  i  ids  add  source  read  u  t  f  return  new    kafka  transaction  context  ids    override  public    kafka  transaction  context  deserialize    kafka  transaction  context  reuse    data  input  view  source  throws    i  o  exception  return  deserialize  source    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  int  num  ids  source  read  int  target  write  int  num  ids  for  int  i    i  num  ids  i  target  write  u  t  f  source  read  u  t  f    override  public    type  serializer  snapshot    kafka  transaction  context  snapshot  configuration  return  new    context  state  serializer  snapshot    serializer  configuration  snapshot  for  compatibility  and  format  evolution    suppress  warnings    weaker  access  public  static  final  class    context  state  serializer  snapshot  extends    simple  type  serializer  snapshot    kafka  transaction  context  public    context  state  serializer  snapshot  super    context  state  serializer  new    keep  information  required  to  deduce  next  safe  to  use  transactional  id  public  static  class    next  transactional  id  hint  public  int  last  parallelism    public  long  next  free  transactional  id    public    next  transactional  id  hint  this      public    next  transactional  id  hint  int  parallelism  long  next  free  transactional  id  this  last  parallelism  parallelism  this  next  free  transactional  id  next  free  transactional  id    override  public    string  to  string  return    next  transactional  id  hint  last  parallelism  last  parallelism  next  free  transactional  id  next  free  transactional  id    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    next  transactional  id  hint  that    next  transactional  id  hint  o  if  last  parallelism  that  last  parallelism  return  false  return  next  free  transactional  id  that  next  free  transactional  id    override  public  int  hash  code  int  result  last  parallelism  result    result  int  next  free  transactional  id  next  free  transactional  id    return  result  link  org  apache  flink  api  common  typeutils    type  serializer  for  link    next  transactional  id  hint    visible  for  testing    internal  public  static  class    next  transactional  id  hint  serializer  extends    type  serializer  singleton    next  transactional  id  hint  private  static  final  long  serial  version  u  i  d  1  l    override  public  boolean  is  immutable  type  return  true    override  public    next  transactional  id  hint  create  instance  return  new    next  transactional  id  hint    override  public    next  transactional  id  hint  copy    next  transactional  id  hint  from  return  from    override  public    next  transactional  id  hint  copy    next  transactional  id  hint  from    next  transactional  id  hint  reuse  return  from    override  public  int  get  length  return    long  bytes    integer  bytes    override  public  void  serialize    next  transactional  id  hint  record    data  output  view  target  throws    i  o  exception  target  write  long  record  next  free  transactional  id  target  write  int  record  last  parallelism    override  public    next  transactional  id  hint  deserialize    data  input  view  source  throws    i  o  exception  long  next  free  transactional  id  source  read  long  int  last  parallelism  source  read  int  return  new    next  transactional  id  hint  last  parallelism  next  free  transactional  id    override  public    next  transactional  id  hint  deserialize    next  transactional  id  hint  reuse    data  input  view  source  throws    i  o  exception  return  deserialize  source    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  target  write  long  source  read  long  target  write  int  source  read  int    override  public    type  serializer  snapshot    next  transactional  id  hint  snapshot  configuration  return  new    next  transactional  id  hint  serializer  snapshot    serializer  configuration  snapshot  for  compatibility  and  format  evolution    suppress  warnings    weaker  access  public  static  final  class    next  transactional  id  hint  serializer  snapshot  extends    simple  type  serializer  snapshot    next  transactional  id  hint  public    next  transactional  id  hint  serializer  snapshot  super    next  transactional  id  hint  serializer  new  
public  evolving  public  class    flink  kafka  producer  k  v  implements    producer  k  v  private  static  final    logger  log    logger  factory  get  logger    flink  kafka  producer  class  private  final    kafka  producer  k  v  kafka  producer    this  lock  and  closed  flag  are  introduced  to  workaround  kafka      because  the  bug  is  only  fixed  in    kafka  2.3    we  need  this  workaround  in  0.11  producer  to  avoid  deadlock  between  a  transaction  committing  aborting  thread  and  a  producer  closing  thread  private  final    object  producer  closing  lock  private  volatile  boolean  closed    nullable  private  final    string  transactional  id  public    flink  kafka  producer    properties  properties  transactional  id  properties  get  property    producer  config  transactional  id  config  kafka  producer  new    kafka  producer  properties  producer  closing  lock  new    object  closed  false    simple  proxy  method  calls    override  public  void  init  transactions  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  init  transactions    override  public  void  begin  transaction  throws    producer  fenced  exception  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  begin  transaction    override  public  void  commit  transaction  throws    producer  fenced  exception  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  commit  transaction    override  public  void  abort  transaction  throws    producer  fenced  exception  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  abort  transaction    override  public  void  send  offsets  to  transaction    map    topic  partition    offset  and  metadata  offsets    string  consumer  group  id  throws    producer  fenced  exception  synchronized  producer  closing  lock  ensure  not  closed  kafka  producer  send  offsets  to  transaction  offsets  consumer  group  id    override  public    future    record  metadata  send    producer  record  k  v  record  return  kafka  producer  send  record    override  public    future    record  metadata  send    producer  record  k  v  record    callback  callback  return  kafka  producer  send  record  callback    override  public    list    partition  info  partitions  for    string  topic  synchronized  producer  closing  lock  ensure  not  closed  return  kafka  producer  partitions  for  topic    override  public    map    metric  name  extends    metric  metrics  return  kafka  producer  metrics    override  public  void  close  synchronized  producer  closing  lock  kafka  producer  close  if  log  is  debug  enabled  log  debug    closed  internal    kafka  producer    stacktrace    system  identity  hash  code  this    joiner  on  n  join    thread  current  thread  get  stack  trace  closed  true    override  public  void  close  long  timeout    time  unit  unit  synchronized  producer  closing  lock  kafka  producer  close  timeout  unit  if  log  is  debug  enabled  log  debug    closed  internal    kafka  producer    stacktrace    system  identity  hash  code  this    joiner  on  n  join    thread  current  thread  get  stack  trace  closed  true    new  methods  or  methods  with  changed  behaviour    override  public  void  flush  kafka  producer  flush  if  transactional  id  null  synchronized  producer  closing  lock  ensure  not  closed  flush  new  partitions    instead  of  obtaining  producer  id  and  epoch  from  the  transaction  coordinator  re  use  previously  obtained  ones  so  that  we  can  resume  transaction  after  a  restart    implementation  of  this  method  is  based  on  link  org  apache  kafka  clients  producer    kafka  producer  init  transactions  public  void  resume  transaction  long  producer  id  short  epoch  synchronized  producer  closing  lock  ensure  not  closed    preconditions  check  state  producer  id    epoch      incorrect  values  for  producer  id  s  and  epoch  s  producer  id  epoch  log  info    attempting  to  resume  transaction  with  producer  id  and  epoch  transactional  id  producer  id  epoch    object  transaction  manager  get  value  kafka  producer  transaction  manager  synchronized  transaction  manager    object  sequence  numbers  get  value  transaction  manager  sequence  numbers  invoke  transaction  manager  transition  to  get  enum  org  apache  kafka  clients  producer  internals    transaction  manager    state  initializing  invoke  sequence  numbers  clear    object  producer  id  and  epoch  get  value  transaction  manager  producer  id  and  epoch  set  value  producer  id  and  epoch  producer  id  producer  id  set  value  producer  id  and  epoch  epoch  epoch  invoke  transaction  manager  transition  to  get  enum  org  apache  kafka  clients  producer  internals    transaction  manager    state  ready  invoke  transaction  manager  transition  to  get  enum  org  apache  kafka  clients  producer  internals    transaction  manager    state  in  transaction  set  value  transaction  manager  transaction  started  true    nullable  public    string  get  transactional  id  return  transactional  id  public  long  get  producer  id    object  transaction  manager  get  value  kafka  producer  transaction  manager    object  producer  id  and  epoch  get  value  transaction  manager  producer  id  and  epoch  return  long  get  value  producer  id  and  epoch  producer  id  public  short  get  epoch    object  transaction  manager  get  value  kafka  producer  transaction  manager    object  producer  id  and  epoch  get  value  transaction  manager  producer  id  and  epoch  return  short  get  value  producer  id  and  epoch  epoch    visible  for  testing  public  int  get  transaction  coordinator  id    object  transaction  manager  get  value  kafka  producer  transaction  manager    node  node    node  invoke  transaction  manager  coordinator    find  coordinator  request    coordinator  type  transaction  return  node  id  private  void  ensure  not  closed  if  closed  throw  new    illegal  state  exception    string  format    the  producer  s  has  already  been  closed    system  identity  hash  code  this    besides  committing  link  org  apache  kafka  clients  producer    kafka  producer  commit  transaction  is  also  adding  new  partitions  to  the  transaction  flush  new  partitions  method  is  moving  this  logic  to  pre  commit  flush  to  make  resume  transaction  simpler    otherwise  resume  transaction  would  require  to  restore  state  of  the  not  yet  added  in  flight  partitions  private  void  flush  new  partitions  log  info    flushing  new  partitions    transactional  request  result  result  enqueue  new  partitions    object  sender  get  value  kafka  producer  sender  invoke  sender  wakeup  result  await  private    transactional  request  result  enqueue  new  partitions    object  transaction  manager  get  value  kafka  producer  transaction  manager  synchronized  transaction  manager    object  new  partitions  in  transaction  get  value  transaction  manager  new  partitions  in  transaction    object  new  partitions  in  transaction  is  empty  invoke  new  partitions  in  transaction  is  empty    transactional  request  result  result  if  new  partitions  in  transaction  is  empty  instanceof    boolean    boolean  new  partitions  in  transaction  is  empty    object  txn  request  handler  invoke  transaction  manager  add  partitions  to  transaction  handler  invoke  transaction  manager  enqueue  request  new    class  txn  request  handler  get  class  get  superclass  new    object  txn  request  handler  result    transactional  request  result  get  value  txn  request  handler  txn  request  handler  get  class  get  superclass  result  else  result  new    transactional  request  result  result  done  return  result  private  static    enum  get  enum    string  enum  full  name    string  x  enum  full  name  split  if  x  length      string  enum  class  name  x      string  enum  name  x    try    class    enum  cl    class    enum    class  for  name  enum  class  name  return    enum  value  of  cl  enum  name  catch    class  not  found  exception  e  throw  new    runtime  exception    incompatible    kafka  producer  version  e  return  null  private  static    object  invoke    object  object    string  method  name    object  args    class  arg  types  new    class  args  length  for  int  i    i  args  length  i  arg  types  i  args  i  get  class  return  invoke  object  method  name  arg  types  args  private  static    object  invoke    object  object    string  method  name    class  arg  types    object  args  try    method  method  object  get  class  get  declared  method  method  name  arg  types  method  set  accessible  true  return  method  invoke  object  args  catch    no  such  method  exception    invocation  target  exception    illegal  access  exception  e  throw  new    runtime  exception    incompatible    kafka  producer  version  e  private  static    object  get  value    object  object    string  field  name  return  get  value  object  object  get  class  field  name  private  static    object  get  value    object  object    class  clazz    string  field  name  try    field  field  clazz  get  declared  field  field  name  field  set  accessible  true  return  field  get  object  catch    no  such  field  exception    illegal  access  exception  e  throw  new    runtime  exception    incompatible    kafka  producer  version  e  private  static  void  set  value    object  object    string  field  name    object  value  try    field  field  object  get  class  get  declared  field  field  name  field  set  accessible  true  field  set  object  value  catch    no  such  field  exception    illegal  access  exception  e  throw  new    runtime  exception    incompatible    kafka  producer  version  e  
public  evolving  public  final  class    kafka  topic  partition  implements    serializable  this  serial  version  uid  must  not  change  because  it  would  break  reading  old  serialized  instances  from  savepoints  private  static  final  long  serial  version  u  i  d    l  private  final    string  topic  private  final  int  partition  private  final  int  cached  hash  public    kafka  topic  partition    string  topic  int  partition  this  topic  require  non  null  topic  this  partition  partition  this  cached  hash    topic  hash  code  partition  public    string  get  topic  return  topic  public  int  get  partition  return  partition    override  public    string  to  string  return    kafka  topic  partition  topic  topic  partition  partition    override  public  boolean  equals    object  o  if  this  o  return  true  else  if  o  instanceof    kafka  topic  partition    kafka  topic  partition  that    kafka  topic  partition  o  return  this  partition  that  partition  this  topic  equals  that  topic  else  return  false    override  public  int  hash  code  return  cached  hash    utilities  public  static    string  to  string    map    kafka  topic  partition    long  map    string  builder  sb  new    string  builder  for    map    entry    kafka  topic  partition    long  p  map  entry  set    kafka  topic  partition  ktp  p  get  key  sb  append  ktp  get  topic  append  append  ktp  get  partition  append  append  p  get  value  append  return  sb  to  string  public  static    string  to  string    list    kafka  topic  partition  partitions    string  builder  sb  new    string  builder  for    kafka  topic  partition  p  partitions  sb  append  p  get  topic  append  append  p  get  partition  append  return  sb  to  string  public  static    list    kafka  topic  partition  drop  leader  data    list    kafka  topic  partition  leader  partition  infos    list    kafka  topic  partition  ret  new    array  list  partition  infos  size  for    kafka  topic  partition  leader  ktpl  partition  infos  ret  add  ktpl  get  topic  partition  return  ret  a  link  java  util    comparator  for  link    kafka  topic  partition  s  public  static  class    comparator  implements  java  util    comparator    kafka  topic  partition    override  public  int  compare    kafka  topic  partition  p1    kafka  topic  partition  p2  if  p1  get  topic  equals  p2  get  topic  return  p1  get  topic  compare  to  p2  get  topic  else  return    integer  compare  p1  get  partition  p2  get  partition  
public  evolving  public  interface    kafka  context  aware  t    sets  the  number  of  the  parallel  subtask  that  the    kafka    producer  is  running  on    the  numbering  starts  from    and  goes  up  to  parallelism    parallelism  as  returned  by  link  set  num  parallel  instances  int  default  void  set  parallel  instance  id  int  parallel  instance  id    sets  the  parallelism  with  which  the  parallel  task  of  the    kafka    producer  runs  default  void  set  num  parallel  instances  int  num  parallel  instances    sets  the  available  partitions  for  the  topic  returned  from  link  get  target  topic    object  default  void  set  partitions  int  partitions    returns  the  topic  that  the  presented  element  should  be  sent  to    this  is  not  used  for  setting  the  topic  this  is  done  via  the  link  org  apache  kafka  clients  producer    producer  record  that  is  returned  from  link    kafka  serialization  schema  serialize    object    long  it  is  only  used  for  getting  the  available  partitions  that  are  presented  to  link  set  partitions  int    string  get  target  topic  t  element  
public  evolving  public  interface    kafka  deserialization  schema  t  extends    serializable    result  type  queryable  t    initialization  method  for  the  schema    it  is  called  before  the  actual  working  methods  link  deserialize  and  thus  suitable  for  one  time  setup  work  p    the  provided  link    deserialization  schema    initialization  context  can  be  used  to  access  additional  features  such  as  e  g  registering  user  metrics  param  context    contextual  information  that  can  be  used  during  initialization  default  void  open    deserialization  schema    initialization  context  context  throws    exception    method  to  decide  whether  the  element  signals  the  end  of  the  stream    if  true  is  returned  the  element  won  t  be  emitted  param  next  element    the  element  to  test  for  the  end  of  stream  signal  return    true  if  the  element  signals  end  of  stream  false  otherwise  boolean  is  end  of  stream  t  next  element    deserializes  the    kafka  record  param  record    kafka  record  to  be  deserialized  return    the  deserialized  message  as  an  object  null  if  the  message  cannot  be  deserialized  t  deserialize    consumer  record  byte  byte  record  throws    exception    deserializes  the    kafka  record  p    can  output  multiple  records  through  the  link    collector    note  that  number  and  size  of  the  produced  records  should  be  relatively  small    depending  on  the  source  implementation  records  can  be  buffered  in  memory  or  collecting  records  might  delay  emitting  checkpoint  barrier  param  message    the  message  as  a  byte  array  param  out    the  collector  to  put  the  resulting  messages  default  void  deserialize    consumer  record  byte  byte  message    collector  t  out  throws    exception  t  deserialized  deserialize  message  if  deserialized  null  out  collect  deserialized  
public  evolving  public  interface    kafka  serialization  schema  t  extends    serializable    initialization  method  for  the  schema    it  is  called  before  the  actual  working  methods  link  serialize    object    long  and  thus  suitable  for  one  time  setup  work  p    the  provided  link    serialization  schema    initialization  context  can  be  used  to  access  additional  features  such  as  e  g  registering  user  metrics  param  context    contextual  information  that  can  be  used  during  initialization  default  void  open    serialization  schema    initialization  context  context  throws    exception    serializes  given  element  and  returns  it  as  a  link    producer  record  param  element  element  to  be  serialized  param  timestamp  timestamp  can  be  null  return    kafka  link    producer  record    producer  record  byte  byte  serialize  t  element    nullable    long  timestamp  
public  evolving  public  class    flink  fixed  partitioner  t  extends    flink  kafka  partitioner  t  private  static  final  long  serial  version  u  i  d    l  private  int  parallel  instance  id    override  public  void  open  int  parallel  instance  id  int  parallel  instances    preconditions  check  argument  parallel  instance  id      id  of  this  subtask  cannot  be  negative    preconditions  check  argument  parallel  instances      number  of  subtasks  must  be  larger  than    this  parallel  instance  id  parallel  instance  id    override  public  int  partition  t  record  byte  key  byte  value    string  target  topic  int  partitions    preconditions  check  argument  partitions  null  partitions  length      partitions  of  the  target  topic  is  empty  return  partitions  parallel  instance  id  partitions  length    override  public  boolean  equals    object  o  return  this  o  o  instanceof    flink  fixed  partitioner    override  public  int  hash  code  return    flink  fixed  partitioner  class  hash  code  
public  evolving  public  abstract  class    flink  kafka  partitioner  t  implements    serializable  private  static  final  long  serial  version  u  i  d    l    initializer  for  the  partitioner    this  is  called  once  on  each  parallel  sink  instance  of  the    flink    kafka  producer    this  method  should  be  overridden  if  necessary  param  parallel  instance  id    indexed  id  of  the  parallel  sink  instance  in    flink  param  parallel  instances  the  total  number  of  parallel  instances  public  void  open  int  parallel  instance  id  int  parallel  instances  overwrite  this  method  if  needed    determine  the  id  of  the  partition  that  the  record  should  be  written  to  param  record  the  record  value  param  key  serialized  key  of  the  record  param  value  serialized  value  of  the  record  param  target  topic  target  topic  for  the  record  param  partitions  found  partitions  for  the  target  topic  return  the  id  of  the  target  partition  public  abstract  int  partition  t  record  byte  key  byte  value    string  target  topic  int  partitions  
public  evolving  public  class    j  s  o  n  key  value  deserialization  schema  implements    kafka  deserialization  schema    object  node  private  static  final  long  serial  version  u  i  d    l  private  final  boolean  include  metadata  private    object  mapper  mapper  public    j  s  o  n  key  value  deserialization  schema  boolean  include  metadata  this  include  metadata  include  metadata    override  public    object  node  deserialize    consumer  record  byte  byte  record  throws    exception  if  mapper  null  mapper  new    object  mapper    object  node  node  mapper  create  object  node  if  record  key  null  node  set  key  mapper  read  value  record  key    json  node  class  if  record  value  null  node  set  value  mapper  read  value  record  value    json  node  class  if  include  metadata  node  put  object  metadata  put  offset  record  offset  put  topic  record  topic  put  partition  record  partition  return  node    override  public  boolean  is  end  of  stream    object  node  next  element  return  false    override  public    type  information    object  node  get  produced  type  return  get  for  class    object  node  class  
public  evolving  public  class    type  information  key  value  serialization  schema  k  v  implements    kafka  deserialization  schema    tuple2  k  v    keyed  serialization  schema    tuple2  k  v  private  static  final  long  serial  version  u  i  d    l    the  serializer  for  the  key  private  final    type  serializer  k  key  serializer    the  serializer  for  the  value  private  final    type  serializer  v  value  serializer  reusable  input  deserialization  buffer  private  final    data  input  deserializer  input  deserializer  reusable  output  serialization  buffer  for  the  key  private  transient    data  output  serializer  key  output  serializer  reusable  output  serialization  buffer  for  the  value  private  transient    data  output  serializer  value  output  serializer    the  type  information  to  be  returned  by  link  get  produced  type    it  is  transient  because  it  is  not  serializable    note  that  this  means  that  the  type  information  is  not  available  at  runtime  but  only  prior  to  the  first  serialization  deserialization  private  final  transient    type  information    tuple2  k  v  type  info    creates  a  new  de  serialization  schema  for  the  given  types  param  key  type  info    the  type  information  for  the  key  type  de  serialized  by  this  schema  param  value  type  info    the  type  information  for  the  value  type  de  serialized  by  this  schema  param  ec    the  execution  config  which  is  used  to  parametrize  the  type  serializers  public    type  information  key  value  serialization  schema    type  information  k  key  type  info    type  information  v  value  type  info    execution  config  ec  this  type  info  new    tuple  type  info  key  type  info  value  type  info  this  key  serializer  key  type  info  create  serializer  ec  this  value  serializer  value  type  info  create  serializer  ec  this  input  deserializer  new    data  input  deserializer    creates  a  new  de  serialization  schema  for  the  given  types    this  constructor  accepts  the  types  as  classes  and  internally  constructs  the  type  information  from  the  classes  p    if  the  types  are  parametrized  and  cannot  be  fully  defined  via  classes  use  the  constructor  that  accepts  link    type  information  instead  param  key  class    the  class  of  the  key  de  serialized  by  this  schema  param  value  class    the  class  of  the  value  de  serialized  by  this  schema  param  config    the  execution  config  which  is  used  to  parametrize  the  type  serializers  public    type  information  key  value  serialization  schema    class  k  key  class    class  v  value  class    execution  config  config  this    type  extractor  create  type  info  key  class    type  extractor  create  type  info  value  class  config    override  public    tuple2  k  v  deserialize    consumer  record  byte  byte  record  throws    exception  k  key  null  v  value  null  if  record  key  null  input  deserializer  set  buffer  record  key  key  key  serializer  deserialize  input  deserializer  if  record  value  null  input  deserializer  set  buffer  record  value  value  value  serializer  deserialize  input  deserializer  return  new    tuple2  key  value    this  schema  never  considers  an  element  to  signal  end  of  stream  so  this  method  returns  always  false  param  next  element    the  element  to  test  for  the  end  of  stream  signal  return    returns  false    override  public  boolean  is  end  of  stream    tuple2  k  v  next  element  return  false    override  public  byte  serialize  key    tuple2  k  v  element  if  element  f0  null  return  null  else  key  is  not  null  serialize  it  if  key  output  serializer  null  key  output  serializer  new    data  output  serializer    try  key  serializer  serialize  element  f0  key  output  serializer  catch    i  o  exception  e  throw  new    runtime  exception    unable  to  serialize  record  e  check  if  key  byte  array  size  changed  byte  res  key  output  serializer  get  byte  array  if  res  length  key  output  serializer  length  byte  n  new  byte  key  output  serializer  length    system  arraycopy  res    n    key  output  serializer  length  res  n  key  output  serializer  clear  return  res    override  public  byte  serialize  value    tuple2  k  v  element  if  the  value  is  null  its  serialized  value  is  null  as  well  if  element  f1  null  return  null  if  value  output  serializer  null  value  output  serializer  new    data  output  serializer    try  value  serializer  serialize  element  f1  value  output  serializer  catch    i  o  exception  e  throw  new    runtime  exception    unable  to  serialize  record  e  byte  res  value  output  serializer  get  byte  array  if  res  length  value  output  serializer  length  byte  n  new  byte  value  output  serializer  length    system  arraycopy  res    n    value  output  serializer  length  res  n  value  output  serializer  clear  return  res    override  public    string  get  target  topic    tuple2  k  v  element  return  null  we  are  never  overriding  the  topic    override  public    type  information    tuple2  k  v  get  produced  type  if  type  info  null  return  type  info  else  throw  new    illegal  state  exception    the  type  information  is  not  available  after  this  class  has  been  serialized  and  distributed  
public  evolving  public  class    kafka  extends    connector  descriptor  private    string  version  private    string  topic  private    startup  mode  startup  mode  private    map    integer    long  specific  offsets  private  long  start  timestamp  millis  private    map    string    string  kafka  properties  private    string  sink  partitioner  type  private    class  extends    flink  kafka  partitioner  sink  partitioner  class    connector  descriptor  for  the    apache    kafka  message  queue  public    kafka  super  connector  type  value  kafka    true    sets  the    kafka  version  to  be  used  param  version    kafka  version  e  g  0.8  0.11  etc  public    kafka  version    string  version    preconditions  check  not  null  version  this  version  version  return  this    sets  the  topic  from  which  the  table  is  read  param  topic    the  topic  from  which  the  table  is  read  public    kafka  topic    string  topic    preconditions  check  not  null  topic  this  topic  topic  return  this    sets  the  configuration  properties  for  the    kafka  consumer    resets  previously  set  properties  param  properties    the  configuration  properties  for  the    kafka  consumer  public    kafka  properties    properties  properties    preconditions  check  not  null  properties  if  this  kafka  properties  null  this  kafka  properties  new    hash  map  this  kafka  properties  clear  properties  for  each  k  v  this  kafka  properties  put    string  k    string  v  return  this    adds  a  configuration  properties  for  the    kafka  consumer  param  key  property  key  for  the    kafka  consumer  param  value  property  value  for  the    kafka  consumer  public    kafka  property    string  key    string  value    preconditions  check  not  null  key    preconditions  check  not  null  value  if  this  kafka  properties  null  this  kafka  properties  new    hash  map  kafka  properties  put  key  value  return  this    configures  to  start  reading  from  the  earliest  offset  for  all  partitions  see    flink  kafka  consumer  base  set  start  from  earliest  public    kafka  start  from  earliest  this  startup  mode    startup  mode  earliest  this  specific  offsets  null  return  this    configures  to  start  reading  from  the  latest  offset  for  all  partitions  see    flink  kafka  consumer  base  set  start  from  latest  public    kafka  start  from  latest  this  startup  mode    startup  mode  latest  this  specific  offsets  null  return  this    configures  to  start  reading  from  any  committed  group  offsets  found  in    zookeeper    kafka  brokers  see    flink  kafka  consumer  base  set  start  from  group  offsets  public    kafka  start  from  group  offsets  this  startup  mode    startup  mode  group  offsets  this  specific  offsets  null  return  this    configures  to  start  reading  partitions  from  specific  offsets  set  independently  for  each  partition    resets  previously  set  offsets  param  specific  offsets  the  specified  offsets  for  partitions  see    flink  kafka  consumer  base  set  start  from  specific  offsets    map  public    kafka  start  from  specific  offsets    map    integer    long  specific  offsets  this  startup  mode    startup  mode  specific  offsets  this  specific  offsets    preconditions  check  not  null  specific  offsets  return  this    configures  to  start  reading  partitions  from  specific  offsets  and  specifies  the  given  offset  for  the  given  partition  param  partition  partition  index  param  specific  offset  partition  offset  to  start  reading  from  see    flink  kafka  consumer  base  set  start  from  specific  offsets    map  public    kafka  start  from  specific  offset  int  partition  long  specific  offset  this  startup  mode    startup  mode  specific  offsets  if  this  specific  offsets  null  this  specific  offsets  new    hash  map  this  specific  offsets  put  partition  specific  offset  return  this    configures  to  start  reading  from  partition  offsets  of  the  specified  timestamp  param  start  timestamp  millis  timestamp  to  start  reading  from  see    flink  kafka  consumer  base  set  start  from  timestamp  long  public    kafka  start  from  timestamp  long  start  timestamp  millis  this  startup  mode    startup  mode  timestamp  this  specific  offsets  null  this  start  timestamp  millis  start  timestamp  millis  return  this    configures  how  to  partition  records  from    flink  s  partitions  into    kafka  s  partitions  p    this  strategy  ensures  that  each    flink  partition  ends  up  in  one    kafka  partition  p    note    one    kafka  partition  can  contain  multiple    flink  partitions    examples  p    more    flink  partitions  than    kafka  partitions    some  or  all    kafka  partitions  contain  the  output  of  more  than  one  flink  partition  pre    flink    sinks    kafka    partitions    gt          pre  p    fewer    flink  partitions  than    kafka  partitions  pre    flink    sinks    kafka    partitions    gt      gt          pre  see  org  apache  flink  streaming  connectors  kafka  partitioner    flink  fixed  partitioner  public    kafka  sink  partitioner  fixed  sink  partitioner  type  connector  sink  partitioner  value  fixed  sink  partitioner  class  null  return  this    configures  how  to  partition  records  from    flink  s  partitions  into    kafka  s  partitions  p    this  strategy  ensures  that  records  will  be  distributed  to    kafka  partitions  in  a  round  robin  fashion  p    note    this  strategy  is  useful  to  avoid  an  unbalanced  partitioning    however  it  will  cause  a  lot  of  network  connections  between  all  the    flink  instances  and  all  the    kafka  brokers  public    kafka  sink  partitioner  round  robin  sink  partitioner  type  connector  sink  partitioner  value  round  robin  sink  partitioner  class  null  return  this    configures  how  to  partition  records  from    flink  s  partitions  into    kafka  s  partitions  p    this  strategy  allows  for  a  custom  partitioner  by  providing  an  implementation  of  link    flink  kafka  partitioner  public    kafka  sink  partitioner  custom    class  extends    flink  kafka  partitioner  partitioner  class  sink  partitioner  type  connector  sink  partitioner  value  custom  sink  partitioner  class    preconditions  check  not  null  partitioner  class  return  this    override  protected    map    string    string  to  connector  properties  final    descriptor  properties  properties  new    descriptor  properties  if  version  null  properties  put  string  connector  version  version  if  topic  null  properties  put  string  connector  topic  topic  if  startup  mode  null  properties  put  string  connector  startup  mode    kafka  validator  normalize  startup  mode  startup  mode  if  specific  offsets  null  final    string  builder  string  builder  new    string  builder  int  i    for    map    entry    integer    long  specific  offset  specific  offsets  entry  set  if  i    string  builder  append  string  builder  append  connector  specific  offsets  partition  append  append  specific  offset  get  key  append  append  connector  specific  offsets  offset  append  append  specific  offset  get  value  i  properties  put  string  connector  specific  offsets  string  builder  to  string  if  start  timestamp  millis    properties  put  string  connector  startup  timestamp  millis    string  value  of  start  timestamp  millis  if  kafka  properties  null  this  kafka  properties  for  each  key  value  properties  put  string  connector  properties  key  value  if  sink  partitioner  type  null  properties  put  string  connector  sink  partitioner  sink  partitioner  type  if  sink  partitioner  class  null  properties  put  class  connector  sink  partitioner  class  sink  partitioner  class  return  properties  as  map  
public  evolving  public  class    a  w  s  config  constants    possible  configuration  values  for  the  type  of  credential  provider  to  use  when  accessing  aws    kinesis    internally  a  corresponding  implementation  of  link    a  w  s  credentials  provider  will  be  used  public  enum    credential  provider    look  for  the  environment  variables  aws  access  key  id  and  aws  secret  access  key  to  create  aws  credentials  env  var    look  for    java  system  properties  aws  access  key  id  and  aws  secret  key  to  create  aws  credentials  sys  prop    use  a  aws  credentials  profile  file  to  create  the  aws  credentials  profile    simply  create  aws  credentials  by  supplying  the  aws  access  key  id  and  aws  secret  key  in  the  configuration  properties  basic    create  aws  credentials  by  assuming  a  role    the  credentials  for  assuming  the  role  must  be  supplied  assume  role    use  aws    web  identity  token  in  order  to  assume  a  role  a  token  file  and  role  details  can  be  supplied  as  configuration  or  environment  variables  web  identity  token  a  credentials  provider  chain  will  be  used  that  searches  for  credentials  in  this  order  env  vars  sys  props  web  identity  token  profile  in  the  aws  instance  metadata  auto    the  aws  region  of  the    kinesis  streams  to  be  pulled  us  east    is  used  if  not  set  public  static  final    string  aws  region  aws  region    the  credential  provider  type  to  use  when  aws  credentials  are  required  basic  is  used  if  not  set  public  static  final    string  aws  credentials  provider  aws  credentials  provider    the  aws  access  key  id  to  use  when  setting  credentials  provider  type  to  basic  public  static  final    string  aws  access  key  id  access  key  id  aws  credentials  provider    the  aws  secret  key  to  use  when  setting  credentials  provider  type  to  basic  public  static  final    string  aws  secret  access  key  secret  key  aws  credentials  provider    optional  configuration  for  profile  path  if  credential  provider  type  is  set  to  be  profile  public  static  final    string  aws  profile  path  profile  path  aws  credentials  provider    optional  configuration  for  profile  name  if  credential  provider  type  is  set  to  be  profile  public  static  final    string  aws  profile  name  profile  name  aws  credentials  provider    the  role  arn  to  use  when  credential  provider  type  is  set  to  assume  role  or  web  identity  token  public  static  final    string  aws  role  arn  role  arn  aws  credentials  provider    the  role  session  name  to  use  when  credential  provider  type  is  set  to  assume  role  or  web  identity  token  public  static  final    string  aws  role  session  name  role  session  name  aws  credentials  provider    the  external  id  to  use  when  credential  provider  type  is  set  to  assume  role  public  static  final    string  aws  role  external  id  external  id  aws  credentials  provider    the  absolute  path  to  the  web  identity  token  file  that  should  be  used  if  provider  type  is  set  to  web  identity  token  public  static  final    string  aws  web  identity  token  file  web  identity  token  file  aws  credentials  provider    the  credentials  provider  that  provides  credentials  for  assuming  the  role  when  credential  provider  type  is  set  to  assume  role    roles  can  be  nested  so  aws  role  credentials  provider  can  again  be  set  to  assume  role  public  static  final    string  aws  role  credentials  provider  role  credentials  provider  aws  credentials  provider    the  aws  endpoint  for    kinesis  derived  from  the  aws  region  setting  if  not  set  public  static  final    string  aws  endpoint  aws  endpoint  public  static    string  access  key  id    string  prefix  return  prefix  basic  accesskeyid  public  static    string  secret  key    string  prefix  return  prefix  basic  secretkey  public  static    string  profile  path    string  prefix  return  prefix  profile  path  public  static    string  profile  name    string  prefix  return  prefix  profile  name  public  static    string  role  arn    string  prefix  return  prefix  role  arn  public  static    string  role  session  name    string  prefix  return  prefix  role  session  name  public  static    string  external  id    string  prefix  return  prefix  role  external  id  public  static    string  role  credentials  provider    string  prefix  return  prefix  role  provider  public  static    string  web  identity  token  file    string  prefix  return  prefix  web  identity  token  file  
public  evolving  public  class    consumer  config  constants  extends    a  w  s  config  constants    the  initial  position  to  start  reading  shards  from    this  will  affect  the  link    shard  iterator  type  used  when  the  consumer  tasks  retrieve  the  first  shard  iterator  for  each    kinesis  shard  public  enum    initial  position    start  reading  from  the  earliest  possible  record  in  the  stream  excluding  expired  data  records  trim  horizon    sentinel  sequence  number  sentinel  earliest  sequence  num    start  reading  from  the  latest  incoming  record  latest    sentinel  sequence  number  sentinel  latest  sequence  num    start  reading  from  the  record  at  the  specified  timestamp  at  timestamp    sentinel  sequence  number  sentinel  at  timestamp  sequence  num  private    sentinel  sequence  number  sentinel  sequence  number    initial  position    sentinel  sequence  number  sentinel  sequence  number  this  sentinel  sequence  number  sentinel  sequence  number  public    sentinel  sequence  number  to  sentinel  sequence  number  return  this  sentinel  sequence  number    the  initial  position  to  start  reading    kinesis  streams  from  latest  is  used  if  not  set  public  static  final    string  stream  initial  position  flink  stream  initpos    the  initial  timestamp  to  start  reading    kinesis  stream  from  when  at  timestamp  is  set  for  stream  initial  position  public  static  final    string  stream  initial  timestamp  flink  stream  initpos  timestamp    the  date  format  of  initial  timestamp  to  start  reading    kinesis  stream  from  when  at  timestamp  is  set  for  stream  initial  position  public  static  final    string  stream  timestamp  date  format  flink  stream  initpos  timestamp  format    the  base  backoff  time  between  each  describe  stream  attempt  for  consuming  from    dynamo  d  b  streams  public  static  final    string  stream  describe  backoff  base  flink  stream  describe  backoff  base    the  maximum  backoff  time  between  each  describe  stream  attempt  for  consuming  from    dynamo  d  b  streams  public  static  final    string  stream  describe  backoff  max  flink  stream  describe  backoff  max    the  power  constant  for  exponential  backoff  between  each  describe  stream  attempt  for  consuming  from    dynamo  d  b  streams  public  static  final    string  stream  describe  backoff  exponential  constant  flink  stream  describe  backoff  expconst    the  maximum  number  of  list  shards  attempts  if  we  get  a  recoverable  exception  public  static  final    string  list  shards  retries  flink  list  shards  maxretries    the  base  backoff  time  between  each  list  shards  attempt  public  static  final    string  list  shards  backoff  base  flink  list  shards  backoff  base    the  maximum  backoff  time  between  each  list  shards  attempt  public  static  final    string  list  shards  backoff  max  flink  list  shards  backoff  max    the  power  constant  for  exponential  backoff  between  each  list  shards  attempt  public  static  final    string  list  shards  backoff  exponential  constant  flink  list  shards  backoff  expconst    the  maximum  number  of  records  to  try  to  get  each  time  we  fetch  records  from  a  aws    kinesis  shard  public  static  final    string  shard  getrecords  max  flink  shard  getrecords  maxrecordcount    the  maximum  number  of  get  records  attempts  if  we  get  a  recoverable  exception  public  static  final    string  shard  getrecords  retries  flink  shard  getrecords  maxretries    the  base  backoff  time  between  get  records  attempts  if  we  get  a    provisioned  throughput  exceeded  exception  public  static  final    string  shard  getrecords  backoff  base  flink  shard  getrecords  backoff  base    the  maximum  backoff  time  between  get  records  attempts  if  we  get  a    provisioned  throughput  exceeded  exception  public  static  final    string  shard  getrecords  backoff  max  flink  shard  getrecords  backoff  max    the  power  constant  for  exponential  backoff  between  each  get  records  attempt  public  static  final    string  shard  getrecords  backoff  exponential  constant  flink  shard  getrecords  backoff  expconst    the  interval  between  each  get  records  request  to  a  aws    kinesis  shard  in  milliseconds  public  static  final    string  shard  getrecords  interval  millis  flink  shard  getrecords  intervalmillis    the  maximum  number  of  get  shard  iterator  attempts  if  we  get    provisioned  throughput  exceeded  exception  public  static  final    string  shard  getiterator  retries  flink  shard  getiterator  maxretries    the  base  backoff  time  between  get  shard  iterator  attempts  if  we  get  a    provisioned  throughput  exceeded  exception  public  static  final    string  shard  getiterator  backoff  base  flink  shard  getiterator  backoff  base    the  maximum  backoff  time  between  get  shard  iterator  attempts  if  we  get  a    provisioned  throughput  exceeded  exception  public  static  final    string  shard  getiterator  backoff  max  flink  shard  getiterator  backoff  max    the  power  constant  for  exponential  backoff  between  each  get  shard  iterator  attempt  public  static  final    string  shard  getiterator  backoff  exponential  constant  flink  shard  getiterator  backoff  expconst    the  interval  between  each  attempt  to  discover  new  shards  public  static  final    string  shard  discovery  interval  millis  flink  shard  discovery  intervalmillis    the  config  to  turn  on  adaptive  reads  from  a  shard  public  static  final    string  shard  use  adaptive  reads  flink  shard  adaptivereads    the  interval  after  which  to  consider  a  shard  idle  for  purposes  of  watermark  generation  public  static  final    string  shard  idle  interval  millis  flink  shard  idle  interval    the  interval  for  periodically  synchronizing  the  shared  watermark  state  public  static  final    string  watermark  sync  millis  flink  watermark  sync  interval    the  maximum  delta  allowed  for  the  reader  to  advance  ahead  of  the  shared  global  watermark  public  static  final    string  watermark  lookahead  millis  flink  watermark  lookahead  millis    the  maximum  number  of  records  that  will  be  buffered  before  suspending  consumption  of  a  shard  public  static  final    string  watermark  sync  queue  capacity  flink  watermark  sync  queue  capacity    default  values  for  consumer  configuration  public  static  final    string  default  stream  initial  position    initial  position  latest  to  string  public  static  final    string  default  stream  timestamp  date  format  yyyy  mm  dd  t  hh  mm  ss  sssxxx  public  static  final  long  default  stream  describe  backoff  base    l  public  static  final  long  default  stream  describe  backoff  max    l  public  static  final  double  default  stream  describe  backoff  exponential  constant  1.5  public  static  final  long  default  list  shards  backoff  base    l  public  static  final  long  default  list  shards  backoff  max    l  public  static  final  double  default  list  shards  backoff  exponential  constant  1.5  public  static  final  int  default  list  shards  retries    public  static  final  int  default  shard  getrecords  max    public  static  final  int  default  shard  getrecords  retries    public  static  final  long  default  shard  getrecords  backoff  base    l  public  static  final  long  default  shard  getrecords  backoff  max    l  public  static  final  double  default  shard  getrecords  backoff  exponential  constant  1.5  public  static  final  long  default  shard  getrecords  interval  millis    l  public  static  final  int  default  shard  getiterator  retries    public  static  final  long  default  shard  getiterator  backoff  base    l  public  static  final  long  default  shard  getiterator  backoff  max    l  public  static  final  double  default  shard  getiterator  backoff  exponential  constant  1.5  public  static  final  long  default  shard  discovery  interval  millis    l  public  static  final  boolean  default  shard  use  adaptive  reads  false  public  static  final  long  default  shard  idle  interval  millis    public  static  final  long  default  watermark  sync  millis        to  avoid  shard  iterator  expires  in  link    shard  consumer  s  the  value  for  the  configured  get  records  interval  can  not  exceed    minutes  which  is  the  expire  time  for  retrieved  iterators  public  static  final  long  max  shard  getrecords  interval  millis    l  
public  evolving  public  class    flink  kinesis  consumer  t  extends    rich  parallel  source  function  t  implements    result  type  queryable  t    checkpointed  function  private  static  final  long  serial  version  u  i  d    l  private  static  final    logger  log    logger  factory  get  logger    flink  kinesis  consumer  class    consumer  properties    the  names  of  the    kinesis  streams  that  we  will  be  consuming  from  private  final    list    string  streams    properties  to  parametrize  settings  such  as  aws  service  region  initial  position  in  stream  shard  list  retrieval  behaviours  etc  private  final    properties  config  props    user  supplied  deserialization  schema  to  convert    kinesis  byte  messages  to    flink  objects  private  final    kinesis  deserialization  schema  t  deserializer    the  function  that  determines  which  subtask  a  shard  should  be  assigned  to  private    kinesis  shard  assigner  shard  assigner    kinesis  data  fetcher  default  shard  assigner  private    assigner  with  periodic  watermarks  t  periodic  watermark  assigner  private    watermark  tracker  watermark  tracker    runtime  state    per  task  fetcher  for    kinesis  data  records  where  each  fetcher  pulls  data  from  one  or  more    kinesis  shards  private  transient    kinesis  data  fetcher  t  fetcher    the  sequence  numbers  to  restore  to  upon  restore  from  failure  private  transient    hash  map    stream  shard  metadata    equivalence  wrapper    sequence  number  sequence  nums  to  restore  private  volatile  boolean  running  true    state  for    checkpoint    state  name  to  access  shard  sequence  number  states  cannot  be  changed  private  static  final    string  sequence  nums  state  store  name    kinesis    stream    shard    state  private  transient    list  state    tuple2    stream  shard  metadata    sequence  number  sequence  nums  state  for  checkpoint    constructors    creates  a  new    flink    kinesis    consumer  p    the  aws  credentials  to  be  used  aws  region  of  the    kinesis  streams  initial  position  to  start  streaming  from  are  configured  with  a  link    properties  instance  p  param  stream    the  single  aws    kinesis  stream  to  read  from  param  deserializer    the  deserializer  used  to  convert  raw  bytes  of    kinesis  records  to    java  objects  without  key  param  config  props    the  properties  used  to  configure  aws  credentials  aws  region  and  initial  starting  position  public    flink  kinesis  consumer    string  stream    deserialization  schema  t  deserializer    properties  config  props  this  stream  new    kinesis  deserialization  schema  wrapper  deserializer  config  props    creates  a  new    flink    kinesis    consumer  p    the  aws  credentials  to  be  used  aws  region  of  the    kinesis  streams  initial  position  to  start  streaming  from  are  configured  with  a  link    properties  instance  p  param  stream    the  single  aws    kinesis  stream  to  read  from  param  deserializer    the  keyed  deserializer  used  to  convert  raw  bytes  of    kinesis  records  to    java  objects  param  config  props    the  properties  used  to  configure  aws  credentials  aws  region  and  initial  starting  position  public    flink  kinesis  consumer    string  stream    kinesis  deserialization  schema  t  deserializer    properties  config  props  this    collections  singleton  list  stream  deserializer  config  props    creates  a  new    flink    kinesis    consumer  p    the  aws  credentials  to  be  used  aws  region  of  the    kinesis  streams  initial  position  to  start  streaming  from  are  configured  with  a  link    properties  instance  p  param  streams    the  aws    kinesis  streams  to  read  from  param  deserializer    the  keyed  deserializer  used  to  convert  raw  bytes  of    kinesis  records  to    java  objects  param  config  props    the  properties  used  to  configure  aws  credentials  aws  region  and  initial  starting  position  public    flink  kinesis  consumer    list    string  streams    kinesis  deserialization  schema  t  deserializer    properties  config  props  check  not  null  streams  streams  can  not  be  null  check  argument  streams  size    must  be  consuming  at  least    stream  check  argument  streams  contains  stream  names  cannot  be  empty    strings  this  streams  streams  this  config  props  check  not  null  config  props  config  props  can  not  be  null  check  the  configuration  properties  for  any  conflicting  settings    kinesis  config  util  validate  consumer  configuration  this  config  props  check  not  null  deserializer  deserializer  can  not  be  null  check  argument    instantiation  util  is  serializable  deserializer    the  provided  deserialization  schema  is  not  serializable  deserializer  get  class  get  name    please  check  that  it  does  not  contain  references  to  non  serializable  instances  this  deserializer  deserializer  if  log  is  info  enabled    string  builder  sb  new    string  builder  for    string  stream  streams  sb  append  stream  append  log  info    flink    kinesis    consumer  is  going  to  read  the  following  streams  sb  to  string  public    kinesis  shard  assigner  get  shard  assigner  return  shard  assigner    provide  a  custom  assigner  to  influence  how  shards  are  distributed  over  subtasks  param  shard  assigner  shard  assigner  public  void  set  shard  assigner    kinesis  shard  assigner  shard  assigner  this  shard  assigner  check  not  null  shard  assigner  function  can  not  be  null    closure  cleaner  clean  shard  assigner    execution  config    closure  cleaner  level  recursive  true  public    assigner  with  periodic  watermarks  t  get  periodic  watermark  assigner  return  periodic  watermark  assigner    set  the  assigner  that  will  extract  the  timestamp  from  link  t  and  calculate  the  watermark  param  periodic  watermark  assigner  periodic  watermark  assigner  public  void  set  periodic  watermark  assigner    assigner  with  periodic  watermarks  t  periodic  watermark  assigner  this  periodic  watermark  assigner  periodic  watermark  assigner    closure  cleaner  clean  this  periodic  watermark  assigner    execution  config    closure  cleaner  level  recursive  true  public    watermark  tracker  get  watermark  tracker  return  this  watermark  tracker    set  the  global  watermark  tracker    when  set  it  will  be  used  by  the  fetcher  to  align  the  shard  consumers  by  event  time  param  watermark  tracker  public  void  set  watermark  tracker    watermark  tracker  watermark  tracker  this  watermark  tracker  watermark  tracker    closure  cleaner  clean  this  watermark  tracker    execution  config    closure  cleaner  level  recursive  true    source  life  cycle    override  public  void  run    source  context  t  source  context  throws    exception  all  subtasks  will  run  a  fetcher  regardless  of  whether  or  not  the  subtask  will  initially  have  shards  to  subscribe  to  fetchers  will  continuously  poll  for  changes  in  the  shard  list  so  all  subtasks  can  potentially  have  new  shards  to  subscribe  to  later  on    kinesis  data  fetcher  t  fetcher  create  fetcher  streams  source  context  get  runtime  context  config  props  deserializer  initial  discovery    list    stream  shard  handle  all  shards  fetcher  discover  new  shards  to  subscribe  for    stream  shard  handle  shard  all  shards    stream  shard  metadata    equivalence  wrapper  kinesis  stream  shard  new    stream  shard  metadata    equivalence  wrapper    kinesis  data  fetcher  convert  to  stream  shard  metadata  shard  if  sequence  nums  to  restore  null  if  sequence  nums  to  restore  contains  key  kinesis  stream  shard  if  the  shard  was  already  seen  and  is  contained  in  the  state  just  use  the  sequence  number  stored  in  the  state  fetcher  register  new  subscribed  shard  state  new    kinesis  stream  shard  state  kinesis  stream  shard  get  shard  metadata  shard  sequence  nums  to  restore  get  kinesis  stream  shard  if  log  is  info  enabled  log  info    subtask  is  seeding  the  fetcher  with  restored  shard  starting  state  set  to  the  restored  sequence  number  get  runtime  context  get  index  of  this  subtask  shard  to  string  sequence  nums  to  restore  get  kinesis  stream  shard  else  the  shard  wasn  t  discovered  in  the  previous  run  therefore  should  be  consumed  from  the  beginning  fetcher  register  new  subscribed  shard  state  new    kinesis  stream  shard  state  kinesis  stream  shard  get  shard  metadata  shard    sentinel  sequence  number  sentinel  earliest  sequence  num  get  if  log  is  info  enabled  log  info    subtask  is  seeding  the  fetcher  with  new  discovered  shard  starting  state  set  to  the  sentinel  earliest  sequence  num  get  runtime  context  get  index  of  this  subtask  shard  to  string  else  we  re  starting  fresh  use  the  configured  start  position  as  initial  state    sentinel  sequence  number  starting  seq  num    initial  position  value  of  config  props  get  property    consumer  config  constants  stream  initial  position    consumer  config  constants  default  stream  initial  position  to  sentinel  sequence  number  fetcher  register  new  subscribed  shard  state  new    kinesis  stream  shard  state  kinesis  stream  shard  get  shard  metadata  shard  starting  seq  num  get  if  log  is  info  enabled  log  info    subtask  will  be  seeded  with  initial  shard  starting  state  set  as  sequence  number  get  runtime  context  get  index  of  this  subtask  shard  to  string  starting  seq  num  get  check  that  we  are  running  before  starting  the  fetcher  if  running  return  expose  the  fetcher  from  this  point  so  that  state  snapshots  can  be  taken  from  the  fetcher  s  state  holders  this  fetcher  fetcher  start  the  fetcher  loop    the  fetcher  will  stop  running  only  when  cancel  or  close  is  called  or  an  error  is  thrown  by  threads  created  by  the  fetcher  fetcher  run  fetcher  check  that  the  fetcher  has  terminated  before  fully  closing  fetcher  await  termination  source  context  close    override  public  void  cancel  running  false    kinesis  data  fetcher  fetcher  this  fetcher  this  fetcher  null  this  method  might  be  called  before  the  subtask  actually  starts  running  so  we  must  check  if  the  fetcher  is  actually  created  if  fetcher  null  try  interrupt  the  fetcher  of  any  work  fetcher  shutdown  fetcher  fetcher  await  termination  catch    exception  e  log  warn    error  while  closing    kinesis  data  fetcher  e    override  public  void  close  throws    exception  cancel  super  close    override  public    type  information  t  get  produced  type  return  deserializer  get  produced  type    state    snapshot    restore    override  public  void  initialize  state    function  initialization  context  context  throws    exception    type  information    tuple2    stream  shard  metadata    sequence  number  shards  state  type  info  new    tuple  type  info    type  information  of    stream  shard  metadata  class    type  information  of    sequence  number  class  sequence  nums  state  for  checkpoint  context  get  operator  state  store  get  union  list  state  new    list  state  descriptor  sequence  nums  state  store  name  shards  state  type  info  if  context  is  restored  if  sequence  nums  to  restore  null  sequence  nums  to  restore  new    hash  map  for    tuple2    stream  shard  metadata    sequence  number  kinesis  sequence  number  sequence  nums  state  for  checkpoint  get  sequence  nums  to  restore  put  we  wrap  the  restored  metadata  inside  an  equivalence  wrapper  that  checks  only  stream  name  and  shard  id  so  that  if  a  shard  had  been  closed  due  to  a    kinesis  reshard  operation  for  example  since  the  savepoint  and  has  a  different  metadata  than  what  we  last  stored  we  will  still  be  able  to  match  it  in  sequence  nums  to  restore    please  see  flink    for  details  new    stream  shard  metadata    equivalence  wrapper  kinesis  sequence  number  f0  kinesis  sequence  number  f1  log  info    setting  restore  state  in  the    flink  kinesis  consumer    using  the  following  offsets  sequence  nums  to  restore  else  log  info    no  restore  state  for    flink  kinesis  consumer    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception  if  running  log  debug  snapshot  state  called  on  closed  source  returning  null  else  if  log  is  debug  enabled  log  debug    snapshotting  state  sequence  nums  state  for  checkpoint  clear  if  fetcher  null  if  sequence  nums  to  restore  null  for    map    entry    stream  shard  metadata    equivalence  wrapper    sequence  number  entry  sequence  nums  to  restore  entry  set  sequence  nums  to  restore  is  the  restored  global  union  state  should  only  snapshot  shards  that  actually  belong  to  us  int  hash  code  shard  assigner  assign    kinesis  data  fetcher  convert  to  stream  shard  handle  entry  get  key  get  shard  metadata  get  runtime  context  get  number  of  parallel  subtasks  if    kinesis  data  fetcher  is  this  subtask  should  subscribe  to  hash  code  get  runtime  context  get  number  of  parallel  subtasks  get  runtime  context  get  index  of  this  subtask  sequence  nums  state  for  checkpoint  add    tuple2  of  entry  get  key  get  shard  metadata  entry  get  value  else    hash  map    stream  shard  metadata    sequence  number  last  state  snapshot  fetcher  snapshot  state  if  log  is  debug  enabled  log  debug    snapshotted  state  last  processed  sequence  numbers  checkpoint  id  timestamp  last  state  snapshot  context  get  checkpoint  id  context  get  checkpoint  timestamp  for    map    entry    stream  shard  metadata    sequence  number  entry  last  state  snapshot  entry  set  sequence  nums  state  for  checkpoint  add    tuple2  of  entry  get  key  entry  get  value    this  method  is  exposed  for  tests  that  need  to  mock  the    kinesis  data  fetcher  in  the  consumer  protected    kinesis  data  fetcher  t  create  fetcher    list    string  streams    source  function    source  context  t  source  context    runtime  context  runtime  context    properties  config  props    kinesis  deserialization  schema  t  deserialization  schema  return  new    kinesis  data  fetcher  streams  source  context  runtime  context  config  props  deserialization  schema  shard  assigner  periodic  watermark  assigner  watermark  tracker    visible  for  testing    hash  map    stream  shard  metadata    equivalence  wrapper    sequence  number  get  restored  state  return  sequence  nums  to  restore  
public  evolving  public  class    flink  kinesis  producer  out  extends    rich  sink  function  out  implements    checkpointed  function  public  static  final    string  kinesis  producer  metric  group  kinesis  producer  public  static  final    string  metric  backpressure  cycles  backpressure  cycles  public  static  final    string  metric  outstanding  records  count  outstanding  records  count  private  static  final  long  serial  version  u  i  d    l  private  static  final    logger  log    logger  factory  get  logger    flink  kinesis  producer  class    properties  to  parametrize  settings  such  as  aws  service  region  access  key  etc  private  final    properties  config  props    flag  controlling  the  error  behavior  of  the  producer  private  boolean  fail  on  error  false    maximum  length  of  the  internal  record  queue  before  backpressuring  private  int  queue  limit    integer  max  value    name  of  the  default  stream  to  produce  to    can  be  overwritten  by  the  serialization  schema  private    string  default  stream    default  partition  id    can  be  overwritten  by  the  serialization  schema  private    string  default  partition    schema  for  turning  the  out  type  into  a  byte  array  private  final    kinesis  serialization  schema  out  schema    optional  custom  partitioner  private    kinesis  partitioner  out  custom  partitioner  null    runtime  fields    our    kinesis  instance  for  each  parallel    flink  sink  private  transient    kinesis  producer  producer    backpressuring  waits  for  this  latch  triggered  by  record  callback  private  transient    timeout  latch  backpressure  latch    callback  handling  failures  private  transient    future  callback    user  record  result  callback    counts  how  often  we  have  to  wait  for  kpl  because  we  are  above  the  queue  limit  private  transient    counter  backpressure  cycles    field  for  async  exception  private  transient  volatile    throwable  thrown  exception    initialization  and  configuration    create  a  new    flink  kinesis  producer    this  is  a  constructor  supporting    flink  s  see    serialization  schema  param  schema    serialization  schema  for  the  data  type  param  config  props    the  properties  used  to  configure    kinesis  producer  including  aws  credentials  and  aws  region  public    flink  kinesis  producer  final    serialization  schema  out  schema    properties  config  props  create  a  simple  wrapper  for  the  serialization  schema  this  new    kinesis  serialization  schema  out    override  public  void  open    serialization  schema    initialization  context  context  throws    exception  schema  open  context    override  public    byte  buffer  serialize  out  element  wrap  into    byte  buffer  return    byte  buffer  wrap  schema  serialize  element  use  default  stream  and  hash  key    override  public    string  get  target  stream  out  element  return  null  config  props    create  a  new    flink  kinesis  producer    this  is  a  constructor  supporting  see    kinesis  serialization  schema  param  schema    kinesis  serialization  schema  for  the  data  type  param  config  props    the  properties  used  to  configure    kinesis  producer  including  aws  credentials  and  aws  region  public    flink  kinesis  producer    kinesis  serialization  schema  out  schema    properties  config  props  check  not  null  config  props  config  props  can  not  be  null  this  config  props    kinesis  config  util  replace  deprecated  producer  keys  config  props  check  not  null  schema  serialization  schema  cannot  be  null  check  argument    instantiation  util  is  serializable  schema    the  provided  serialization  schema  is  not  serializable  schema  get  class  get  name    please  check  that  it  does  not  contain  references  to  non  serializable  instances  this  schema  schema    if  set  to  true  the  producer  will  immediately  fail  with  an  exception  on  any  error    otherwise  the  errors  are  logged  and  the  producer  goes  on  param  fail  on  error    error  behavior  flag  public  void  set  fail  on  error  boolean  fail  on  error  this  fail  on  error  fail  on  error    the  link    kinesis  producer  holds  an  unbounded  queue  internally    to  avoid  memory  problems  under  high  loads  a  limit  can  be  employed  above  which  the  internal  queue  will  be  flushed  thereby  applying  backpressure  param  queue  limit    the  maximum  length  of  the  internal  queue  before  backpressuring  public  void  set  queue  limit  int  queue  limit  check  argument  queue  limit    queue  limit  must  be  a  positive  number  this  queue  limit  queue  limit    set  a  default  stream  name  param  default  stream    name  of  the  default    kinesis  stream  public  void  set  default  stream    string  default  stream  this  default  stream  default  stream    set  default  partition  id  param  default  partition    name  of  the  default  partition  public  void  set  default  partition    string  default  partition  this  default  partition  default  partition  public  void  set  custom  partitioner    kinesis  partitioner  out  partitioner  check  not  null  partitioner  partitioner  cannot  be  null  check  argument    instantiation  util  is  serializable  partitioner    the  provided  custom  partitioner  is  not  serializable  partitioner  get  class  get  name    please  check  that  it  does  not  contain  references  to  non  serializable  instances  this  custom  partitioner  partitioner    lifecycle  methods    override  public  void  open    configuration  parameters  throws    exception  super  open  parameters  schema  open  get  runtime  context  get  metric  group  add  group  user  check  and  pass  the  configuration  properties    kinesis  producer  configuration  producer  config    kinesis  config  util  get  validated  producer  configuration  config  props  producer  get  kinesis  producer  producer  config  final    metric  group  kinesis  mectric  group  get  runtime  context  get  metric  group  add  group  kinesis  producer  metric  group  this  backpressure  cycles  kinesis  mectric  group  counter  metric  backpressure  cycles  kinesis  mectric  group  gauge  metric  outstanding  records  count  producer  get  outstanding  records  count  backpressure  latch  new    timeout  latch  callback  new    future  callback    user  record  result    override  public  void  on  success    user  record  result  result  backpressure  latch  trigger  if  result  is  successful  if  fail  on  error  only  remember  the  first  thrown  exception  if  thrown  exception  null  thrown  exception  new    runtime  exception    record  was  not  sent  successful  else  log  warn    record  was  not  sent  successful    override  public  void  on  failure    throwable  t  backpressure  latch  trigger  if  fail  on  error  thrown  exception  t  else  log  warn    an  exception  occurred  while  processing  a  record  t  if  this  custom  partitioner  null  this  custom  partitioner  initialize  get  runtime  context  get  index  of  this  subtask  get  runtime  context  get  number  of  parallel  subtasks  log  info    started    kinesis  producer  instance  for  region  producer  config  get  region    override  public  void  invoke  out  value    context  context  throws    exception  if  this  producer  null  throw  new    runtime  exception    kinesis  producer  has  been  closed  check  and  propagate  async  error  boolean  did  wait  for  flush  enforce  queue  limit  if  did  wait  for  flush  check  and  propagate  async  error    string  stream  default  stream    string  partition  default  partition    byte  buffer  serialized  schema  serialize  value  maybe  set  custom  stream    string  custom  stream  schema  get  target  stream  value  if  custom  stream  null  stream  custom  stream    string  explicit  hashkey  null  maybe  set  custom  partition  if  custom  partitioner  null  partition  custom  partitioner  get  partition  id  value  explicit  hashkey  custom  partitioner  get  explicit  hash  key  value  if  stream  null  if  fail  on  error  throw  new    runtime  exception    no  target  stream  set  else  log  warn    no  target  stream  set    skipping  record  return    listenable  future    user  record  result  cb  producer  add  user  record  stream  partition  explicit  hashkey  serialized    futures  add  callback  cb  callback    override  public  void  close  throws    exception  log  info    closing  producer  super  close  if  producer  null  log  info    flushing  outstanding  records  producer  get  outstanding  records  count  try  to  flush  all  outstanding  records  flush  sync  log  info    flushing  done    destroying  producer  instance  producer  destroy  producer  null  make  sure  we  propagate  pending  errors  check  and  propagate  async  error    override  public  void  initialize  state    function  initialization  context  context  throws    exception  nothing  to  do    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception  check  for  asynchronous  errors  and  fail  the  checkpoint  if  necessary  check  and  propagate  async  error  flush  sync  if  producer  get  outstanding  records  count    throw  new    illegal  state  exception    number  of  outstanding  records  must  be  zero  at  this  point  producer  get  outstanding  records  count  if  the  flushed  requests  has  errors  we  should  propagate  it  also  and  fail  the  checkpoint  check  and  propagate  async  error    utilities    creates  a  link    kinesis  producer    exposed  so  that  tests  can  inject  mock  producers  easily    visible  for  testing  protected    kinesis  producer  get  kinesis  producer    kinesis  producer  configuration  producer  config  return  new    kinesis  producer  producer  config    check  if  there  are  any  asynchronous  exceptions    if  so  rethrow  the  exception  private  void  check  and  propagate  async  error  throws    exception  if  thrown  exception  null    string  error  messages  if  thrown  exception  instanceof    user  record  failed  exception    list    attempt  attempts    user  record  failed  exception  thrown  exception  get  result  get  attempts  for    attempt  attempt  attempts  if  attempt  get  error  message  null  error  messages  attempt  get  error  message  n  if  fail  on  error  throw  new    runtime  exception    an  exception  was  thrown  while  processing  a  record  error  messages  thrown  exception  else  log  warn    an  exception  was  thrown  while  processing  a  record  error  messages  thrown  exception  reset  prevent  double  throwing  thrown  exception  null    if  the  internal  queue  of  the  link    kinesis  producer  gets  too  long  flush  some  of  the  records  until  we  are  below  the  limit  again    we  don  t  want  to  flush    all  records  at  this  point  since  that  would  break  record  aggregation  return  boolean  whether  flushing  occurred  or  not  private  boolean  enforce  queue  limit  int  attempt    while  producer  get  outstanding  records  count  queue  limit  backpressure  cycles  inc  if  attempt    log  warn    waiting  for  the  queue  length  to  drop  below  the  limit  takes  unusually  long  still  not  done  after  attempts  attempt  attempt  try  backpressure  latch  await    catch    interrupted  exception  e  log  warn    flushing  was  interrupted  break  return  attempt    a  reimplementation  of  link    kinesis  producer  flush  sync    this  implementation  releases  the  block  on  flushing  if  an  interruption  occurred  private  void  flush  sync  throws    exception  while  producer  get  outstanding  records  count    producer  flush  try    thread  sleep    catch    interrupted  exception  e  log  warn    flushing  was  interrupted  break  
public  evolving  public  abstract  class    kinesis  partitioner  t  implements    serializable  private  static  final  long  serial  version  u  i  d    l    return  a  partition  id  based  on  the  input  param  element    element  to  partition  return  a  string  representing  the  partition  id  public  abstract    string  get  partition  id  t  element    optional  method  for  setting  an  explicit  hash  key  param  element    element  to  get  the  hash  key  for  return  the  hash  key  for  the  element  public    string  get  explicit  hash  key  t  element  return  null    optional  initializer  param  index  of  this  subtask    index  of  this  partitioner  instance  param  number  of  parallel  subtasks    total  number  of  parallel  instances  public  void  initialize  int  index  of  this  subtask  int  number  of  parallel  subtasks  
public  evolving  public  interface    kinesis  shard  assigner  extends    serializable    returns  the  index  of  the  target  subtask  that  a  specific  shard  should  be  assigned  to    for  return  values  outside  the  subtask  range  modulus  operation  will  be  applied  automatically  hence  it  is  also  valid  to  just  return  a  hash  code  p    the  resulting  distribution  of  shards  should  have  the  following  contract  ul  li      uniform  distribution  across  subtasks  li  li      deterministic  calls  for  a  given  shard  always  return  same  index  li  ul  p    the  above  contract  is  crucial  and  cannot  be  broken    consumer  subtasks  rely  on  this  contract  to  filter  out  shards  that  they  should  not  subscribe  to  guaranteeing  that  each  shard  of  a  stream  will  always  be  assigned  to  one  subtask  in  a  uniformly  distributed  manner  param  shard  the  shard  to  determine  param  num  parallel  subtasks  total  number  of  subtasks  return  target  index  if  index  falls  outside  of  the  range  modulus  operation  will  be  applied  int  assign    stream  shard  handle  shard  int  num  parallel  subtasks  
public  evolving  public  interface    kinesis  deserialization  schema  t  extends    serializable    result  type  queryable  t    initialization  method  for  the  schema    it  is  called  before  the  actual  working  methods  link  deserialize  and  thus  suitable  for  one  time  setup  work  p    the  provided  link    deserialization  schema    initialization  context  can  be  used  to  access  additional  features  such  as  e  g  registering  user  metrics  param  context    contextual  information  that  can  be  used  during  initialization  default  void  open    deserialization  schema    initialization  context  context  throws    exception    deserializes  a    kinesis  record  s  bytes    if  the  record  cannot  be  deserialized  code  null  may  be  returned    this  informs  the    flink    kinesis    consumer  to  process  the    kinesis  record  without  producing  any  output  for  it  i  e  effectively  skipping  the  record  param  record  value  the  record  s  value  as  a  byte  array  param  partition  key  the  record  s  partition  key  at  the  time  of  writing  param  seq  num  the  sequence  number  of  this  record  in  the    kinesis  shard  param  approx  arrival  timestamp  the  server  side  timestamp  of  when    kinesis  received  and  stored  the  record  param  stream  the  name  of  the    kinesis  stream  that  this  record  was  sent  to  param  shard  id    the  identifier  of  the  shard  the  record  was  sent  to  return  the  deserialized  message  as  an    java  object  code  null  if  the  message  cannot  be  deserialized  throws    i  o  exception  t  deserialize  byte  record  value    string  partition  key    string  seq  num  long  approx  arrival  timestamp    string  stream    string  shard  id  throws    i  o  exception    method  to  decide  whether  the  element  signals  the  end  of  the  stream    if  true  is  returned  the  element  won  t  be  emitted  param  next  element  the  element  to  test  for  the  end  of  stream  signal  return  true  if  the  element  signals  end  of  stream  false  otherwise  todo  flink    add  support  for  boolean  is  end  of  stream  t  next  element  
public  evolving  public  interface    kinesis  serialization  schema  t  extends    serializable    initialization  method  for  the  schema    it  is  called  before  the  actual  working  methods  link  serialize    object  and  thus  suitable  for  one  time  setup  work  p    the  provided  link    initialization  context  can  be  used  to  access  additional  features  such  as  e  g  registering  user  metrics  param  context    contextual  information  that  can  be  used  during  initialization  default  void  open    initialization  context  context  throws    exception    serialize  the  given  element  into  a    byte  buffer  param  element    the  element  to  serialize  return    serialized  representation  of  the  element    byte  buffer  serialize  t  element    optional  method  to  determine  the  target  stream  based  on  the  element    return  code  null  code  to  use  the  default  stream  param  element    the  element  to  determine  the  target  stream  from  return  target  stream  name    string  get  target  stream  t  element  
public  evolving  public  class    job  manager  watermark  tracker  extends    watermark  tracker  private    global  aggregate  manager  aggregate  manager  private  final    string  aggregate  name  private  final    watermark  aggregate  function  aggregate  function  new    watermark  aggregate  function  private  final  long  log  accumulator  interval  millis  private  long  update  timeout  count  public    job  manager  watermark  tracker    string  aggregate  name  this  aggregate  name    public    job  manager  watermark  tracker    string  aggregate  name  long  log  accumulator  interval  millis  super  this  aggregate  name  aggregate  name  this  log  accumulator  interval  millis  log  accumulator  interval  millis    override  public  long  update  watermark  long  local  watermark    watermark  update  update  new    watermark  update  update  id  get  subtask  id  update  watermark  local  watermark  try  byte  result  bytes  aggregate  manager  update  global  aggregate  aggregate  name    instantiation  util  serialize  object  update  aggregate  function    watermark  result  result    instantiation  util  deserialize  object  result  bytes  this  get  class  get  class  loader  this  update  timeout  count  result  update  timeout  count  return  result  watermark  catch    class  not  found  exception    i  o  exception  ex  throw  new    runtime  exception  ex    override  public  void  open    runtime  context  context  super  open  context  this  aggregate  function  update  timeout  millis  super  get  update  timeout  millis  this  aggregate  function  log  accumulator  interval  millis  log  accumulator  interval  millis    preconditions  check  argument  context  instanceof    streaming  runtime  context    streaming  runtime  context  runtime  context    streaming  runtime  context  context  this  aggregate  manager  runtime  context  get  global  aggregate  manager  public  long  get  update  timeout  count  return  update  timeout  count    watermark  aggregation  input  protected  static  class    watermark  update  implements    serializable  protected  long  watermark    long  min  value  protected    string  id    watermark  aggregation  result  protected  static  class    watermark  result  implements    serializable  protected  long  watermark    long  min  value  protected  long  update  timeout  count      aggregate  function  for  computing  a  combined  watermark  of  parallel  subtasks  private  static  class    watermark  aggregate  function  implements    aggregate  function  byte    map    string    watermark  state  byte  private  long  update  timeout  millis  default  update  timeout  millis  private  long  log  accumulator  interval  millis    todo  wrap  accumulator  static  long  add  count  static  long  last  logged  private  static  final    logger  log    logger  factory  get  logger    watermark  aggregate  function  class    override  public    map    string    watermark  state  create  accumulator  return  new    hash  map    override  public    map    string    watermark  state  add  byte  value  bytes    map    string    watermark  state  accumulator  add  count  final    watermark  update  value  try  value    instantiation  util  deserialize  object  value  bytes  this  get  class  get  class  loader  catch    exception  e  throw  new    runtime  exception  e    watermark  state  ws  accumulator  get  value  id  if  ws  null  accumulator  put  value  id  ws  new    watermark  state  ws  watermark  value  watermark  ws  last  updated    system  current  time  millis  return  accumulator    override  public  byte  get  result    map    string    watermark  state  accumulator  long  update  timeout  count    long  current  time    system  current  time  millis  long  global  watermark    long  max  value  for    map    entry    string    watermark  state  e  accumulator  entry  set    watermark  state  ws  e  get  value  if  ws  last  updated  update  timeout  millis  current  time  ignore  outdated  entry  if  ws  watermark    long  max  value  update  timeout  count  continue  global  watermark    math  min  ws  watermark  global  watermark    watermark  result  result  new    watermark  result  result  watermark  global  watermark    long  max  value    long  min  value  global  watermark  result  update  timeout  count  update  timeout  count  if  log  accumulator  interval  millis    if  current  time  last  logged  log  accumulator  interval  millis  last  logged    system  current  time  millis  log  info    watermark  aggregate  function  added  timeout  map  add  count  update  timeout  count  accumulator  try  return    instantiation  util  serialize  object  result  catch    i  o  exception  e  throw  new    runtime  exception  e    override  public    map    string    watermark  state  merge    map    string    watermark  state  accumulator  a    map    string    watermark  state  accumulator  b  not  required  throw  new    unsupported  operation  exception  
public  evolving  public  abstract  class    watermark  tracker  implements    closeable    serializable  public  static  final  long  default  update  timeout  millis        subtasks  that  have  not  provided  a  watermark  update  within  the  configured  interval  will  be  considered  idle  and  excluded  from  target  watermark  calculation  private  long  update  timeout  millis  default  update  timeout  millis    unique  id  for  the  subtask    using  string  instead  of  subtask  index  so  synchronization  can  spawn  across  multiple  sources  private    string  subtask  id    watermark  state  protected  static  class    watermark  state  protected  long  watermark    long  min  value  protected  long  last  updated  public  long  get  watermark  return  watermark    override  public    string  to  string  return    watermark  state  watermark  watermark  last  updated  last  updated  protected    string  get  subtask  id  return  this  subtask  id  protected  long  get  update  timeout  millis  return  this  update  timeout  millis  public  abstract  long  get  update  timeout  count    subtasks  that  have  not  provided  a  watermark  update  within  the  configured  interval  will  be  considered  idle  and  excluded  from  target  watermark  calculation  param  update  timeout  millis  public  void  set  update  timeout  millis  long  update  timeout  millis  this  update  timeout  millis  update  timeout  millis    set  the  current  watermark  of  the  owning  subtask  and  return  the  global  low  watermark  based  on  the  current  state  snapshot    periodically  called  by  the  enclosing  consumer  instance  which  is  responsible  for  any  timer  management  etc  param  local  watermark  return  public  abstract  long  update  watermark  final  long  local  watermark  protected  long  get  current  time  return    system  current  time  millis  public  void  open    runtime  context  context  if  context  instanceof    streaming  runtime  context  this  subtask  id    streaming  runtime  context  context  get  operator  unique  i  d  context  get  index  of  this  subtask  else  this  subtask  id  context  get  task  name  with  subtasks    override  public  void  close  no  work  to  do  here  
public  evolving  public    r  m  q  sink    r  m  q  connection  config  rmq  connection  config    string  queue  name    serialization  schema  in  schema  this  rmq  connection  config  queue  name  schema  null  null  
public  evolving  public    r  m  q  sink    r  m  q  connection  config  rmq  connection  config    serialization  schema  in  schema    r  m  q  sink  publish  options  in  publish  options  this  rmq  connection  config  null  schema  publish  options  null  
public  evolving  public    r  m  q  sink    r  m  q  connection  config  rmq  connection  config    serialization  schema  in  schema    r  m  q  sink  publish  options  in  publish  options    serializable  return  listener  return  listener  this  rmq  connection  config  null  schema  publish  options  return  listener  
public  evolving  public  interface    r  m  q  sink  publish  options  in  extends  java  io    serializable    compute  the  message  s  routing  key  from  the  data  param  a    the  data  used  by  the  sink  return    the  routing  key  of  the  message  null  will  raise  a    null  pointer  exception    string  compute  routing  key  in  a    compute  the  message  s  properties  from  the  data  param  a    the  data  used  by  the  sink  return    the  message  s  properties  can  be  null    basic  properties  compute  properties  in  a    compute  the  exchange  from  the  data  param  a    the  data  used  by  the  sink  return    the  exchange  to  publish  the  message  to  null  will  raise  a    null  pointer  exception    string  compute  exchange  in  a    compute  the  mandatory  flag  passed  to  method  link  com  rabbitmq  client    channel  basic  publish    string    string  boolean  boolean    basic  properties  byte  a  link    serializable  return  listener  is  mandatory  if  this  flag  can  be  true  param  a    the  data  used  by  the  sink  return    the  mandatory  flag  default  boolean  compute  mandatory  in  a  return  false    compute  the  immediate  flag  passed  to  method  link  com  rabbitmq  client    channel  basic  publish    string    string  boolean  boolean    basic  properties  byte  a  link    serializable  return  listener  is  mandatory  if  this  flag  can  be  true  param  a    the  data  used  by  the  sink  return    the  mandatory  flag  default  boolean  compute  immediate  in  a  return  false  
public  evolving  public  class    hadoop  dummy  progressable  implements    progressable    override  public  void  progress  
public  evolving  public  class    hadoop  dummy  reporter  implements    reporter    override  public  void  progress    override  public  void  set  status    string  status    override  public    counter  get  counter    enum  name  return  null    override  public    counter  get  counter    string  group    string  name  return  null    override  public  void  incr  counter    enum  key  long  amount    override  public  void  incr  counter    string  group    string  counter  long  amount    override  public    input  split  get  input  split  throws    unsupported  operation  exception  return  null    there  should  be  an    override  but  some    c  d  h4  dependency  does  not  contain  this  method  public  float  get  progress  return    
public  evolving  public  class    double  counter  implements    simple  accumulator    double  private  static  final  long  serial  version  u  i  d  1  l  private  double  local  value    public    double  counter  public    double  counter  double  value  this  local  value  value    accumulator    consider  using  link  add  double  instead  for  primitive  double  values    override  public  void  add    double  value  local  value  value    override  public    double  get  local  value  return  local  value    override  public  void  merge    accumulator    double    double  other  this  local  value  other  get  local  value    override  public  void  reset  local  this  local  value      override  public    double  counter  clone    double  counter  result  new    double  counter  result  local  value  local  value  return  result    primitive    specializations  public  void  add  double  value  local  value  value  public  double  get  local  value  primitive  return  this  local  value    utilities    override  public    string  to  string  return    double  counter  this  local  value  
public  evolving  public  class    double  maximum  implements    simple  accumulator    double  private  static  final  long  serial  version  u  i  d  1  l  private  double  max    double  negative  infinity  public    double  maximum  public    double  maximum  double  value  this  max  value    accumulator    consider  using  link  add  double  instead  for  primitive  double  values    override  public  void  add    double  value  this  max    math  max  this  max  value    override  public    double  get  local  value  return  this  max    override  public  void  merge    accumulator    double    double  other  this  max    math  max  this  max  other  get  local  value    override  public  void  reset  local  this  max    double  negative  infinity    override  public    double  maximum  clone    double  maximum  clone  new    double  maximum  clone  max  this  max  return  clone    primitive    specializations  public  void  add  double  value  this  max    math  max  this  max  value  public  double  get  local  value  primitive  return  this  max    utilities    override  public    string  to  string  return    double  maximum  this  max  
public  evolving  public  class    double  minimum  implements    simple  accumulator    double  private  static  final  long  serial  version  u  i  d  1  l  private  double  min    double  positive  infinity  public    double  minimum  public    double  minimum  double  value  this  min  value    accumulator    consider  using  link  add  double  instead  for  primitive  double  values    override  public  void  add    double  value  this  min    math  min  this  min  value    override  public    double  get  local  value  return  this  min    override  public  void  merge    accumulator    double    double  other  this  min    math  min  this  min  other  get  local  value    override  public  void  reset  local  this  min    double  positive  infinity    override  public    double  minimum  clone    double  minimum  clone  new    double  minimum  clone  min  this  min  return  clone    primitive    specializations  public  void  add  double  value  this  min    math  min  this  min  value  public  double  get  local  value  primitive  return  this  min    utilities    override  public    string  to  string  return    double  minimum  this  min  
public  evolving  public  class    int  counter  implements    simple  accumulator    integer  private  static  final  long  serial  version  u  i  d  1  l  private  int  local  value    public    int  counter  public    int  counter  int  value  this  local  value  value    accumulator    consider  using  link  add  int  instead  for  primitive  int  values    override  public  void  add    integer  value  local  value  value    override  public    integer  get  local  value  return  local  value    override  public  void  merge    accumulator    integer    integer  other  this  local  value  other  get  local  value    override  public  void  reset  local  this  local  value      override  public    int  counter  clone    int  counter  result  new    int  counter  result  local  value  local  value  return  result    primitive    specializations  public  void  add  int  value  local  value  value  public  int  get  local  value  primitive  return  this  local  value    utilities    override  public    string  to  string  return    int  counter  this  local  value  
public  evolving  public  class    int  maximum  implements    simple  accumulator    integer  private  static  final  long  serial  version  u  i  d  1  l  private  int  max    integer  min  value  public    int  maximum  public    int  maximum  int  value  this  max  value    accumulator    consider  using  link  add  int  instead  for  primitive  integer  values    override  public  void  add    integer  value  this  max    math  max  this  max  value    override  public    integer  get  local  value  return  this  max    override  public  void  merge    accumulator    integer    integer  other  this  max    math  max  this  max  other  get  local  value    override  public  void  reset  local  this  max    integer  min  value    override  public    int  maximum  clone    int  maximum  clone  new    int  maximum  clone  max  this  max  return  clone    primitive    specializations  public  void  add  int  value  this  max    math  max  this  max  value  public  int  get  local  value  primitive  return  this  max    utilities    override  public    string  to  string  return    int  maximum  this  max  
public  evolving  public  class    int  minimum  implements    simple  accumulator    integer  private  static  final  long  serial  version  u  i  d  1  l  private  int  min    integer  max  value  public    int  minimum  public    int  minimum  int  value  this  min  value    accumulator    consider  using  link  add  int  instead  for  primitive  integer  values    override  public  void  add    integer  value  this  min    math  min  this  min  value    override  public    integer  get  local  value  return  this  min    override  public  void  merge    accumulator    integer    integer  other  this  min    math  min  this  min  other  get  local  value    override  public  void  reset  local  this  min    integer  max  value    override  public    int  minimum  clone    int  minimum  clone  new    int  minimum  clone  min  this  min  return  clone    primitive    specializations  public  void  add  int  value  this  min    math  min  this  min  value  public  int  get  local  value  primitive  return  this  min    utilities    override  public    string  to  string  return    int  minimum  this  min  
public  evolving  public  class    long  counter  implements    simple  accumulator    long  private  static  final  long  serial  version  u  i  d  1  l  private  long  local  value  public    long  counter  public    long  counter  long  value  this  local  value  value    accumulator    consider  using  link  add  long  instead  for  primitive  long  values    override  public  void  add    long  value  this  local  value  value    override  public    long  get  local  value  return  this  local  value    override  public  void  merge    accumulator    long    long  other  this  local  value  other  get  local  value    override  public  void  reset  local  this  local  value      override  public    long  counter  clone    long  counter  result  new    long  counter  result  local  value  local  value  return  result    primitive    specializations  public  void  add  long  value  this  local  value  value  public  long  get  local  value  primitive  return  this  local  value    utilities    override  public    string  to  string  return    long  counter  this  local  value  
public  evolving  public  class    long  maximum  implements    simple  accumulator    long  private  static  final  long  serial  version  u  i  d  1  l  private  long  max    long  min  value  public    long  maximum  public    long  maximum  long  value  this  max  value    accumulator    consider  using  link  add  long  instead  for  primitive  long  values    override  public  void  add    long  value  this  max    math  max  this  max  value    override  public    long  get  local  value  return  this  max    override  public  void  merge    accumulator    long    long  other  this  max    math  max  this  max  other  get  local  value    override  public  void  reset  local  this  max    long  min  value    override  public    long  maximum  clone    long  maximum  clone  new    long  maximum  clone  max  this  max  return  clone    primitive    specializations  public  void  add  long  value  this  max    math  max  this  max  value  public  long  get  local  value  primitive  return  this  max    utilities    override  public    string  to  string  return    long  maximum  this  max  
public  evolving  public  class    long  minimum  implements    simple  accumulator    long  private  static  final  long  serial  version  u  i  d  1  l  private  long  min    long  max  value  public    long  minimum  public    long  minimum  long  value  this  min  value    accumulator    consider  using  link  add  long  instead  for  primitive  long  values    override  public  void  add    long  value  this  min    math  min  this  min  value    override  public    long  get  local  value  return  this  min    override  public  void  merge    accumulator    long    long  other  this  min    math  min  this  min  other  get  local  value    override  public  void  reset  local  this  min    long  max  value    override  public    long  minimum  clone    long  minimum  clone  new    long  minimum  clone  min  this  min  return  clone    primitive    specializations  public  void  add  long  value  this  min    math  min  this  min  value  public  long  get  local  value  primitive  return  this  min    utilities    override  public    string  to  string  return    long  minimum  this  min  
public  evolving  public  class    serialized  list  accumulator  t  implements    accumulator  t    array  list  byte  private  static  final  long  serial  version  u  i  d  1  l  private    array  list  byte  local  value  new    array  list    override  public  void  add  t  value  throw  new    unsupported  operation  exception  public  void  add  t  value    type  serializer  t  serializer  throws    i  o  exception  try    byte  array  output  stream  out  stream  new    byte  array  output  stream    data  output  view  stream  wrapper  out  new    data  output  view  stream  wrapper  out  stream  serializer  serialize  value  out  local  value  add  out  stream  to  byte  array  catch    i  o  exception  e  throw  new    i  o  exception    failed  to  serialize  value  value  e    override  public    array  list  byte  get  local  value  return  local  value    override  public  void  reset  local  local  value  clear    override  public  void  merge    accumulator  t    array  list  byte  other  local  value  add  all  other  get  local  value    override  public    serialized  list  accumulator  t  clone    serialized  list  accumulator  t  new  instance  new    serialized  list  accumulator  t  new  instance  local  value  new    array  list  byte  local  value  return  new  instance    suppress  warnings  unchecked  public  static  t    list  t  deserialize  list    array  list  byte  data    type  serializer  t  serializer  throws    i  o  exception    class  not  found  exception    list  t  result  new    array  list  t  data  size  for  byte  bytes  data    byte  array  input  stream  in  stream  new    byte  array  input  stream  bytes    data  input  view  stream  wrapper  in  new    data  input  view  stream  wrapper  in  stream  t  val  serializer  deserialize  in  result  add  val  return  result    override  public    string  to  string  return    serialized  list  accumulator  local  value  size  elements  
public  evolving  public  interface    aggregator  t  extends    value  extends    serializable    gets  the  aggregator  s  current  aggregate  return    the  aggregator  s  current  aggregate  t  get  aggregate    aggregates  the  given  element    in  the  case  of  a  i  sum  i  aggregator  this  method  adds  the  given  value  to  the  sum  param  element    the  element  to  aggregate  void  aggregate  t  element    resets  the  internal  state  of  the  aggregator    this  must  bring  the  aggregator  into  the  same  state  as  if  it  was  newly  initialized  void  reset  
public  evolving  public  class    aggregator  with  name  t  extends    value  private  final    string  name  private  final    aggregator  t  aggregator    creates  a  new  instance  for  the  given  aggregator  and  name  param  name    the  name  that  the  aggregator  is  registered  under  param  aggregator    the  aggregator  public    aggregator  with  name    string  name    aggregator  t  aggregator  this  name  name  this  aggregator  aggregator    gets  the  name  that  the  aggregator  is  registered  under  return    the  name  that  the  aggregator  is  registered  under  public    string  get  name  return  name    gets  the  aggregator  return    the  aggregator  public    aggregator  t  get  aggregator  return  aggregator  
public  evolving  public  interface    convergence  criterion  t  extends    value  extends    serializable    decide  whether  the  iterative  algorithm  has  converged  boolean  is  converged  int  iteration  t  value  
public  evolving    deprecated  public  enum    code  analysis  mode    code  analysis  does  not  take  place  disable    hints  for  improvement  of  the  program  are  printed  to  the  log  hint    the  program  will  be  automatically  optimized  with  knowledge  from  code  analysis  optimize  
public  evolving  public  interface    data  distribution  extends    i  o  readable  writable    serializable    returns  the  i  th  bucket  s  upper  bound  given  that  the  distribution  is  to  be  split  into  code  total  buckets  buckets  p    assuming  i  n  i  buckets  let  code  b  i  be  the  result  from  calling  code  get  bucket  boundary  i  n  then  the  distribution  will  partition  the  data  domain  in  the  following  fashion  pre  inf  b    b    b    b  n    b  n    b  n    inf  pre  p    note    the  last  bucket  s  upper  bound  is  actually  discarded  by  many  algorithms    the  last  bucket  is  assumed  to  hold  all  values  i  v  i  such  that  code  v  get  bucket  boundary  n    n  where  i  n  i  is  the  number  of  buckets  param  bucket  num    the  number  of  the  bucket  for  which  to  get  the  upper  bound  param  total  num  buckets    the  number  of  buckets  to  split  the  data  into  return  a  record  whose  values  act  as  bucket  boundaries  for  the  specified  bucket    object  get  bucket  boundary  int  bucket  num  int  total  num  buckets    the  number  of  fields  in  the  composite  key    this  determines  how  many  fields  in  the  records  define  the  bucket    the  number  of  fields  must  be  the  size  of  the  array  returned  by  the  function  link  get  bucket  boundary  int  int  return    the  number  of  fields  in  the  composite  key  int  get  number  of  fields    gets  the  type  of  the  key  by  which  the  data  set  is  partitioned  return    the  type  of  the  key  by  which  the  data  set  is  partitioned    type  information  get  key  types  
public  evolving  public  interface    range  boundaries  t  extends    serializable    get  the  range  index  of  record  param  record    the  input  record  return    the  range  index  int  get  range  index  t  record  
public  evolving    functional  interface  public  interface    serializable  timestamp  assigner  t  extends    timestamp  assigner  t    serializable  
public  evolving    functional  interface  public  interface    timestamp  assigner  supplier  t  extends    serializable    instantiates  a  link    timestamp  assigner    timestamp  assigner  t  create  timestamp  assigner    context  context  static  t    timestamp  assigner  supplier  t  of    serializable  timestamp  assigner  t  assigner  return  new    supplier  from  serializable  timestamp  assigner  assigner    additional  information  available  to  link  create  timestamp  assigner    context    this  can  be  access  to  link  org  apache  flink  metrics    metric  group    metric  groups  for  example  interface    context    returns  the  metric  group  for  the  context  in  which  the  created  link    timestamp  assigner  is  used  p    instances  of  this  class  can  be  used  to  register  new  metrics  with    flink  and  to  create  a  nested  hierarchy  based  on  the  group  names    see  link    metric  group  for  more  information  for  the  metrics  system  see    metric  group    metric  group  get  metric  group    we  need  an  actual  class    implementing  this  as  a  lambda  in  link  of    serializable  timestamp  assigner  would  not  allow  the  link    closure  cleaner  to  reach  into  the  link    serializable  timestamp  assigner  class    supplier  from  serializable  timestamp  assigner  t  implements    timestamp  assigner  supplier  t  private  static  final  long  serial  version  u  i  d  1  l  private  final    serializable  timestamp  assigner  t  assigner  public    supplier  from  serializable  timestamp  assigner    serializable  timestamp  assigner  t  assigner  this  assigner  assigner    override  public    timestamp  assigner  t  create  timestamp  assigner    context  context  return  assigner  
public  evolving    functional  interface  public  interface    watermark  generator  supplier  t  extends    serializable    instantiates  a  link    watermark  generator    watermark  generator  t  create  watermark  generator    context  context    additional  information  available  to  link  create  watermark  generator    context    this  can  be  access  to  link  org  apache  flink  metrics    metric  group    metric  groups  for  example  interface    context    returns  the  metric  group  for  the  context  in  which  the  created  link    watermark  generator  is  used  p    instances  of  this  class  can  be  used  to  register  new  metrics  with    flink  and  to  create  a  nested  hierarchy  based  on  the  group  names    see  link    metric  group  for  more  information  for  the  metrics  system  see    metric  group    metric  group  get  metric  group  
public  evolving  public  interface    external  resource  driver    retrieve  the  information  of  the  external  resources  according  to  the  amount  param  amount  of  the  required  external  resources  return  information  set  of  the  required  external  resources  throws    exception  if  there  is  something  wrong  during  retrieving    set  extends    external  resource  info  retrieve  resource  info  long  amount  throws    exception  
public  evolving  public  interface    external  resource  driver  factory    construct  the    external  resource  driver  from  configuration  param  config  configuration  for  this  external  resource  return  the  driver  for  this  external  resource  throws    exception  if  there  is  something  wrong  during  the  creation    external  resource  driver  create  external  resource  driver    configuration  config  throws    exception  
public  evolving  public  interface    external  resource  info    get  the  property  indicated  by  the  specified  key  param  key  of  the  required  property  return  an  code    optional  containing  the  value  associated  to  the  key  or  an  empty  code    optional  if  no  value  has  been  stored  under  the  given  key    optional    string  get  property    string  key    get  all  property  keys  return  collection  of  all  property  keys    collection    string  get  keys  
public  evolving  public  interface    aggregate  function  in  acc  out  extends    function    serializable    creates  a  new  accumulator  starting  a  new  aggregate  p    the  new  accumulator  is  typically  meaningless  unless  a  value  is  added  via  link  add    object    object  p    the  accumulator  is  the  state  of  a  running  aggregation    when  a  program  has  multiple  aggregates  in  progress  such  as  per  key  and  window  the  state  per  key  and  window  is  the  size  of  the  accumulator  return  a  new  accumulator  corresponding  to  an  empty  aggregate  acc  create  accumulator    adds  the  given  input  value  to  the  given  accumulator  returning  the  new  accumulator  value  p    for  efficiency  the  input  accumulator  may  be  modified  and  returned  param  value    the  value  to  add  param  accumulator    the  accumulator  to  add  the  value  to  return    the  accumulator  with  the  updated  state  acc  add  in  value  acc  accumulator    gets  the  result  of  the  aggregation  from  the  accumulator  param  accumulator    the  accumulator  of  the  aggregation  return    the  final  aggregation  result  out  get  result  acc  accumulator    merges  two  accumulators  returning  an  accumulator  with  the  merged  state  p    this  function  may  reuse  any  of  the  given  accumulators  as  the  target  for  the  merge  and  return  that    the  assumption  is  that  the  given  accumulators  will  not  be  used  any  more  after  having  been  passed  to  this  function  param  a    an  accumulator  to  merge  param  b    another  accumulator  to  merge  return    the  accumulator  with  the  merged  state  acc  merge  acc  a  acc  b  
public  evolving  public  abstract  class    rich  aggregate  function  in  acc  out  extends    abstract  rich  function  implements    aggregate  function  in  acc  out  private  static  final  long  serial  version  u  i  d  1  l    override  public  abstract  acc  create  accumulator    override  public  abstract  acc  add  in  value  acc  accumulator    override  public  abstract  out  get  result  acc  accumulator    override  public  abstract  acc  merge  acc  a  acc  b  
public  evolving  public  enum    input  dependency  constraint    schedule  the  task  if  any  input  is  consumable  any    schedule  the  task  if  all  the  inputs  are  consumable  all  
public  evolving  public  interface    checkpointable  input  format  s  extends    input  split  t  extends    serializable    returns  the  split  currently  being  read  along  with  its  current  state    this  will  be  used  to  restore  the  state  of  the  reading  channel  when  recovering  from  a  task  failure    in  the  case  of  a  simple  text  file  the  state  can  correspond  to  the  last  read  offset  in  the  split  return    the  state  of  the  channel  throws    i  o  exception    thrown  if  the  creation  of  the  state  object  failed  t  get  current  state  throws    i  o  exception    restores  the  state  of  a  parallel  instance  reading  from  an  link    input  format    this  is  necessary  when  recovering  from  a  task  failure    when  this  method  is  called  the  input  format  it  guaranteed  to  be  configured  p  b  note  b    the  caller  has  to  make  sure  that  the  provided  split  is  the  one  to  whom  the  state  belongs  param  split    the  split  to  be  opened  param  state    the  state  from  which  to  start  from    this  can  contain  the  offset  but  also  other  data  depending  on  the  input  format  void  reopen  s  split  t  state  throws    i  o  exception  
public  evolving  public  abstract  class    file  path  filter  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l    name  of  an  unfinished    hadoop  file  public  static  final    string  hadoop  copying    copying    returns  code  true  if  the  code  file  path  given  is  to  be  ignored  when  processing  a  directory  e  g  pre  code  public  boolean  filter  paths    path  file  path  return  file  path  get  name  starts  with  file  path  get  name  contains    copying  pre  public  abstract  boolean  filter  path    path  file  path    returns  the  default  filter  which  excludes  the  following  files  ul  li    files  starting  with  quot  quot  li  li    files  starting  with  quot  quot  li  li    files  containing  the  string  quot    copying  quot  li  ul  return    the  singleton  instance  of  the  default  file  path  filter  public  static    file  path  filter  create  default  filter  return    default  filter  instance    the  default  filter    the  default  file  path  filtering  method  and  is  used  if  no  other  such  function  is  provided    this  filter  leaves  out  files  starting  with  and    copying  public  static  class    default  filter  extends    file  path  filter  private  static  final  long  serial  version  u  i  d  1  l  static  final    default  filter  instance  new    default  filter    default  filter    override  public  boolean  filter  path    path  file  path  return  file  path  null  file  path  get  name  starts  with  file  path  get  name  starts  with  file  path  get  name  contains  hadoop  copying  
public  evolving  public  class    parse  exception  extends    runtime  exception  private  static  final  long  serial  version  u  i  d    l  public    parse  exception  super  public    parse  exception    string  message  super  message  public    parse  exception    throwable  cause  super  cause  public    parse  exception    string  message    throwable  cause  super  message  cause  
public  evolving  public  interface    flink  connector  rate  limiter  extends    serializable  a  method  that  can  be  used  to  create  and  configure  a  ratelimiter  based  on  the  runtime  context  param  runtime  context  void  open    runtime  context  runtime  context    sets  the  desired  rate  for  the  rate  limiter  param  rate  void  set  rate  long  rate    acquires  permits  for  the  rate  limiter  void  acquire  int  permits  long  get  rate  void  close  
public  evolving  public  final  class    replicating  input  format  ot  s  extends    input  split  extends    rich  input  format  ot  s  private  static  final  long  serial  version  u  i  d  1  l  private    input  format  ot  s  replicated  i  f  public    replicating  input  format    input  format  ot  s  wrapped  i  f  this  replicated  i  f  wrapped  i  f  public    input  format  ot  s  get  replicated  input  format  return  this  replicated  i  f    override  public  void  configure    configuration  parameters  this  replicated  i  f  configure  parameters    override  public    base  statistics  get  statistics    base  statistics  cached  statistics  throws    i  o  exception  return  this  replicated  i  f  get  statistics  cached  statistics    override  public  s  create  input  splits  int  min  num  splits  throws    i  o  exception  return  this  replicated  i  f  create  input  splits  min  num  splits    override  public    input  split  assigner  get  input  split  assigner  s  input  splits  return  new    replicating  input  split  assigner  input  splits    override  public  void  open  s  split  throws    i  o  exception  this  replicated  i  f  open  split    override  public  boolean  reached  end  throws    i  o  exception  return  this  replicated  i  f  reached  end    override  public  ot  next  record  ot  reuse  throws    i  o  exception  return  this  replicated  i  f  next  record  reuse    override  public  void  close  throws    i  o  exception  this  replicated  i  f  close    override  public  void  set  runtime  context    runtime  context  context  if  this  replicated  i  f  instanceof    rich  input  format    rich  input  format  this  replicated  i  f  set  runtime  context  context    override  public    runtime  context  get  runtime  context  if  this  replicated  i  f  instanceof    rich  input  format  return    rich  input  format  this  replicated  i  f  get  runtime  context  else  throw  new    runtime  exception    the  underlying  input  format  to  this    replicating  input  format  isn  t  context  aware    override  public  void  open  input  format  throws    i  o  exception  if  this  replicated  i  f  instanceof    rich  input  format    rich  input  format  this  replicated  i  f  open  input  format    override  public  void  close  input  format  throws    i  o  exception  if  this  replicated  i  f  instanceof    rich  input  format    rich  input  format  this  replicated  i  f  close  input  format  
public  evolving  public  enum    job  status    job  is  newly  created  no  task  has  started  to  run  created    terminal  state  non  terminal    some  tasks  are  scheduled  or  running  some  may  be  pending  some  may  be  finished  running    terminal  state  non  terminal    the  job  has  failed  and  is  currently  waiting  for  the  cleanup  to  complete  failing    terminal  state  non  terminal    the  job  has  failed  with  a  non  recoverable  task  failure  failed    terminal  state  globally    job  is  being  cancelled  cancelling    terminal  state  non  terminal    job  has  been  cancelled  canceled    terminal  state  globally    all  of  the  job  s  tasks  have  successfully  finished  finished    terminal  state  globally    the  job  is  currently  undergoing  a  reset  and  total  restart  restarting    terminal  state  non  terminal    the  job  has  been  suspended  which  means  that  it  has  been  stopped  but  not  been  removed  from  a  potential  ha  job  store  suspended    terminal  state  locally    the  job  is  currently  reconciling  and  waits  for  task  execution  report  to  recover  state  reconciling    terminal  state  non  terminal  private  enum    terminal  state  non  terminal  locally  globally  private  final    terminal  state  terminal  state    job  status    terminal  state  terminal  state  this  terminal  state  terminal  state    checks  whether  this  state  is  i  globally  terminal  i  a  globally  terminal  job  is  complete  and  cannot  fail  any  more  and  will  not  be  restarted  or  recovered  by  another  standby  master  node  p    when  a  globally  terminal  state  has  been  reached  all  recovery  data  for  the  job  is  dropped  from  the  high  availability  services  return    true  if  this  job  status  is  globally  terminal  false  otherwise  public  boolean  is  globally  terminal  state  return  terminal  state    terminal  state  globally    checks  whether  this  state  is  i  locally  terminal  i    locally  terminal  refers  to  the  state  of  a  job  s  execution  graph  within  an  executing    job  manager    if  the  execution  graph  is  locally  terminal  the    job  manager  will  not  continue  executing  or  recovering  the  job  p    the  only  state  that  is  locally  terminal  but  not  globally  terminal  is  link  suspended  which  is  typically  entered  when  the  executing    job  manager  looses  its  leader  status  return    true  if  this  job  status  is  terminal  false  otherwise  public  boolean  is  terminal  state  return  terminal  state    terminal  state  non  terminal  
public  evolving  public  interface    program  description    returns  a  description  of  the  plan  that  is  generated  by  the  assembler  and  also  of  the  arguments  if  they  are  available    the  description  should  be  simple  text  as  it  may  be  rendered  in  different  environments  console  web  interface    typical  things  that  should  be  included  are  ul  li  expected  input  format  li  li  description  of  output  li  li  description  of  what  is  done  li  li  description  of  arguments  to  customize  plan  if  available  li  ul  return  a  description  of  the  program  and  of  its  arguments  if  available    string  get  description  
public  evolving  public  class    restart  strategies    generates    no  restart  strategy  configuration  return    no  restart  strategy  configuration  public  static    restart  strategy  configuration  no  restart  return  new    no  restart  strategy  configuration  public  static    restart  strategy  configuration  fall  back  restart  return  new    fallback  restart  strategy  configuration    generates  a    fixed  delay  restart  strategy  configuration  param  restart  attempts    number  of  restart  attempts  for  the    fixed  delay  restart  strategy  param  delay  between  attempts    delay  in  between  restart  attempts  for  the    fixed  delay  restart  strategy  return    fixed  delay  restart  strategy  public  static    restart  strategy  configuration  fixed  delay  restart  int  restart  attempts  long  delay  between  attempts  return  fixed  delay  restart  restart  attempts    time  of  delay  between  attempts    time  unit  milliseconds    generates  a    fixed  delay  restart  strategy  configuration  param  restart  attempts    number  of  restart  attempts  for  the    fixed  delay  restart  strategy  param  delay  interval    delay  in  between  restart  attempts  for  the    fixed  delay  restart  strategy  return    fixed  delay  restart  strategy  public  static    restart  strategy  configuration  fixed  delay  restart  int  restart  attempts    time  delay  interval  return  new    fixed  delay  restart  strategy  configuration  restart  attempts  delay  interval    generates  a    failure  rate  restart  strategy  configuration  param  failure  rate    maximum  number  of  restarts  in  given  interval  code  failure  interval  before  failing  a  job  param  failure  interval    time  interval  for  failures  param  delay  interval    delay  in  between  restart  attempts  public  static    failure  rate  restart  strategy  configuration  failure  rate  restart  int  failure  rate    time  failure  interval    time  delay  interval  return  new    failure  rate  restart  strategy  configuration  failure  rate  failure  interval  delay  interval    abstract  configuration  for  restart  strategies  public  abstract  static  class    restart  strategy  configuration  implements    serializable  private  static  final  long  serial  version  u  i  d    l  private    restart  strategy  configuration    returns  a  description  which  is  shown  in  the  web  interface  return    description  of  the  restart  strategy  public  abstract    string  get  description    override  public    string  to  string  return  get  description    configuration  representing  no  restart  strategy  public  static  final  class    no  restart  strategy  configuration  extends    restart  strategy  configuration  private  static  final  long  serial  version  u  i  d    l    override  public    string  get  description  return    restart  deactivated    override  public  boolean  equals    object  o  if  this  o  return  true  return  o  instanceof    no  restart  strategy  configuration    override  public  int  hash  code  return    objects  hash    configuration  representing  a  fixed  delay  restart  strategy  public  static  final  class    fixed  delay  restart  strategy  configuration  extends    restart  strategy  configuration  private  static  final  long  serial  version  u  i  d    l  private  final  int  restart  attempts  private  final    time  delay  between  attempts  interval    fixed  delay  restart  strategy  configuration  int  restart  attempts    time  delay  between  attempts  interval  this  restart  attempts  restart  attempts  this  delay  between  attempts  interval  delay  between  attempts  interval  public  int  get  restart  attempts  return  restart  attempts  public    time  get  delay  between  attempts  interval  return  delay  between  attempts  interval    override  public  int  hash  code  int  result  restart  attempts  result    result  delay  between  attempts  interval  null  delay  between  attempts  interval  hash  code    return  result    override  public  boolean  equals    object  obj  if  obj  instanceof    fixed  delay  restart  strategy  configuration    fixed  delay  restart  strategy  configuration  other    fixed  delay  restart  strategy  configuration  obj  return  restart  attempts  other  restart  attempts  delay  between  attempts  interval  equals  other  delay  between  attempts  interval  else  return  false    override  public    string  get  description  return    string  format    restart  with  fixed  delay  s  d  restart  attempts  delay  between  attempts  interval  restart  attempts    configuration  representing  a  failure  rate  restart  strategy  public  static  final  class    failure  rate  restart  strategy  configuration  extends    restart  strategy  configuration  private  static  final  long  serial  version  u  i  d    l  private  final  int  max  failure  rate  private  final    time  failure  interval  private  final    time  delay  between  attempts  interval  public    failure  rate  restart  strategy  configuration  int  max  failure  rate    time  failure  interval    time  delay  between  attempts  interval  this  max  failure  rate  max  failure  rate  this  failure  interval  failure  interval  this  delay  between  attempts  interval  delay  between  attempts  interval  public  int  get  max  failure  rate  return  max  failure  rate  public    time  get  failure  interval  return  failure  interval  public    time  get  delay  between  attempts  interval  return  delay  between  attempts  interval    override  public    string  get  description  return    string  format    failure  rate  restart  with  maximum  of  d  failures  within  interval  s  and  fixed  delay  s  max  failure  rate  failure  interval  to  string  delay  between  attempts  interval  to  string    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    failure  rate  restart  strategy  configuration  that    failure  rate  restart  strategy  configuration  o  return  max  failure  rate  that  max  failure  rate    objects  equals  failure  interval  that  failure  interval    objects  equals  delay  between  attempts  interval  that  delay  between  attempts  interval    override  public  int  hash  code  return    objects  hash  max  failure  rate  failure  interval  delay  between  attempts  interval    restart  strategy  configuration  that  could  be  used  by  jobs  to  use  cluster  level  restart  strategy    useful  especially  when  one  has  a  custom  implementation  of  restart  strategy  set  via  flink  conf  yaml  public  static  final  class    fallback  restart  strategy  configuration  extends    restart  strategy  configuration  private  static  final  long  serial  version  u  i  d    l    override  public    string  get  description  return    cluster  level  default  restart  strategy    override  public  boolean  equals    object  o  if  this  o  return  true  return  o  instanceof    fallback  restart  strategy  configuration    override  public  int  hash  code  return    objects  hash    reads  a  link    restart  strategy  configuration  from  a  given  link    readable  config  param  configuration  configuration  object  to  retrieve  parameters  from  return  link    optional  empty  when  no  restart  strategy  parameters  provided  public  static    optional    restart  strategy  configuration  from  configuration    readable  config  configuration  return  configuration  get  optional    restart  strategy  options  restart  strategy  map  conf  name  parse  configuration  conf  name  configuration  private  static    restart  strategy  configuration  parse  configuration    string  restartstrategy  kind    readable  config  configuration  switch  restartstrategy  kind  to  lower  case  case  none  case  off  case  disable  return  no  restart  case  fixeddelay  case  fixed  delay  int  attempts  configuration  get    restart  strategy  options  restart  strategy  fixed  delay  attempts    duration  delay  configuration  get    restart  strategy  options  restart  strategy  fixed  delay  delay  return  fixed  delay  restart  attempts  delay  to  millis  case  failurerate  case  failure  rate  int  max  failures  configuration  get    restart  strategy  options  restart  strategy  failure  rate  max  failures  per  interval    duration  failure  rate  interval  configuration  get    restart  strategy  options  restart  strategy  failure  rate  failure  rate  interval    duration  failure  rate  delay  configuration  get    restart  strategy  options  restart  strategy  failure  rate  delay  return  failure  rate  restart  max  failures    time  milliseconds  failure  rate  interval  to  millis    time  milliseconds  failure  rate  delay  to  millis  default  throw  new    illegal  argument  exception    unknown  restart  strategy  restartstrategy  kind  
public  evolving  public  abstract  class    abstract  deserialization  schema  t  implements    deserialization  schema  t  private  static  final  long  serial  version  u  i  d  2  l    the  type  produced  by  this  code    deserialization  schema  private  final    type  information  t  type    creates  a  new    abstract  deserialization  schema  and  tries  to  infer  the  type  returned  by  this    deserialization  schema  p    this  constructor  is  usable  whenever  the    deserialization  schema  concretely  defines  its  type  without  generic  variables  pre  code  public  class    my  deserialization  schema  extends    abstract  deserialization  schema    my  type  public    my  type  deserialize  byte  message  throws    i  o  exception  pre  protected    abstract  deserialization  schema  try  this  type    type  extractor  create  type  info    abstract  deserialization  schema  class  get  class    null  null  catch    invalid  types  exception  e  throw  new    flink  runtime  exception    the  implementation  of    abstract  deserialization  schema  is  using  a  generic  variable    this  is  not  supported  because  due  to    java  s  generic  type  erasure  it  will  not  be  possible  to  determine  the  full  type  at  runtime    for  generic  implementations  please  pass  the    type  information  or  type  class  explicitly  to  the  constructor    creates  an    abstract  deserialization  schema  that  returns  the    type  information  indicated  by  the  given  class    this  constructor  is  only  necessary  when  creating  a  generic  implementation  see  link    abstract  deserialization  schema    generic    use  p    this  constructor  may  fail  if  the  class  is  generic    in  that  case  please  use  the  constructor  that  accepts  a  link    abstract  deserialization  schema    type  hint    type  hint  or  a  link    abstract  deserialization  schema    type  information    type  information  param  type    the  class  of  the  produced  type  protected    abstract  deserialization  schema    class  t  type  check  not  null  type  type  this  type    type  information  of  type    creates  an    abstract  deserialization  schema  that  returns  the    type  information  indicated  by  the  given  type  hint    this  constructor  is  only  necessary  when  creating  a  generic  implementation  see  link    abstract  deserialization  schema    generic    use  param  type  hint    the    type  hint  for  the  produced  type  protected    abstract  deserialization  schema    type  hint  t  type  hint  check  not  null  type  hint  type  hint  this  type  type  hint  get  type  info    creates  an    abstract  deserialization  schema  that  returns  the  given    type  information  for  the  produced  type    this  constructor  is  only  necessary  when  creating  a  generic  implementation  see  link    abstract  deserialization  schema    generic    use  param  type  info    the    type  information  for  the  produced  type  protected    abstract  deserialization  schema    type  information  t  type  info  this  type  check  not  null  type  info  type  info    de  serializes  the  byte  message  param  message    the  message  as  a  byte  array  return    the  de  serialized  message  as  an  object    override  public  abstract  t  deserialize  byte  message  throws    i  o  exception    method  to  decide  whether  the  element  signals  the  end  of  the  stream    if  true  is  returned  the  element  won  t  be  emitted  p    this  default  implementation  returns  always  false  meaning  the  stream  is  interpreted  to  be  unbounded  param  next  element    the  element  to  test  for  the  end  of  stream  signal  return    true  if  the  element  signals  end  of  stream  false  otherwise    override  public  boolean  is  end  of  stream  t  next  element  return  false    gets  the  type  produced  by  this  deserializer    this  is  the  type  that  was  passed  to  the  constructor  or  reflectively  inferred  if  the  default  constructor  was  called    override  public    type  information  t  get  produced  type  return  type  
public  evolving  public  interface    bulk  writer  t    adds  an  element  to  the  encoder    the  encoder  may  temporarily  buffer  the  element  or  immediately  write  it  to  the  stream  p    it  may  be  that  adding  this  element  fills  up  an  internal  buffer  and  causes  the  encoding  and  flushing  of  a  batch  of  internally  buffered  elements  param  element    the  element  to  add  throws    i  o  exception    thrown  if  the  element  cannot  be  added  to  the  encoder  or  if  the  output  stream  throws  an  exception  void  add  element  t  element  throws    i  o  exception    flushes  all  intermediate  buffered  data  to  the  output  stream    it  is  expected  that  flushing  often  may  reduce  the  efficiency  of  the  encoding  throws    i  o  exception    thrown  if  the  encoder  cannot  be  flushed  or  if  the  output  stream  throws  an  exception  void  flush  throws    i  o  exception    finishes  the  writing    this  must  flush  all  internal  buffer  finish  encoding  and  write  footers  p    the  writer  is  not  expected  to  handle  any  more  records  via  link  add  element    object  after  this  method  is  called  p  b    important  b    this  method  must  not  close  the  stream  that  the  writer  writes  to    closing  the  stream  is  expected  to  happen  through  the  invoker  of  this  method  afterwards  throws    i  o  exception    thrown  if  the  finalization  fails  void  finish  throws    i  o  exception  a  factory  that  creates  a  link    bulk  writer  param  t    the  type  of  record  to  write    functional  interface  interface    factory  t  extends    serializable    creates  a  writer  that  writes  to  the  given  stream  param  out    the  output  stream  to  write  the  encoded  data  to  throws    i  o  exception    thrown  if  the  writer  cannot  be  opened  or  if  the  output  stream  throws  an  exception    bulk  writer  t  create    f  s  data  output  stream  out  throws    i  o  exception  
public  evolving  public  interface    encoder  in  extends    serializable    writes  one  element  to  the  bucket  file  param  element  the  element  to  be  written  param  stream  the  stream  to  write  the  element  to  void  encode  in  element    output  stream  stream  throws    i  o  exception  
public  evolving  public  class    simple  string  encoder  in  implements    encoder  in  private  static  final  long  serial  version  u  i  d    l  private    string  charset  name  private  transient    charset  charset    creates  a  new  code    string  writer  that  uses  code  utf    charset  to  convert  strings  to  bytes  public    simple  string  encoder  this  utf      creates  a  new  code    string  writer  that  uses  the  given  charset  to  convert  strings  to  bytes  param  charset  name    name  of  the  charset  to  be  used  must  be  valid  input  for  code    charset  for  name  charset  name  public    simple  string  encoder    string  charset  name  this  charset  name  charset  name    override  public  void  encode  in  element    output  stream  stream  throws    i  o  exception  if  charset  null  charset    charset  for  name  charset  name  stream  write  element  to  string  get  bytes  charset  stream  write  n  
public  evolving  public  class    simple  string  schema  implements    deserialization  schema    string    serialization  schema    string  private  static  final  long  serial  version  u  i  d  1  l    the  charset  to  use  to  convert  between  strings  and  bytes    the  field  is  transient  because  we  serialize  a  different  delegate  object  instead  private  transient    charset  charset    creates  a  new    simple  string  schema  that  uses  utf    as  the  encoding  public    simple  string  schema  this    standard  charsets  utf      creates  a  new    simple  string  schema  that  uses  the  given  charset  to  convert  between  strings  and  bytes  param  charset    the  charset  to  use  to  convert  between  strings  and  bytes  public    simple  string  schema    charset  charset  this  charset  check  not  null  charset    gets  the  charset  used  by  this  schema  for  serialization  return    the  charset  used  by  this  schema  for  serialization  public    charset  get  charset  return  charset    kafka    serialization    override  public    string  deserialize  byte  message  return  new    string  message  charset    override  public  boolean  is  end  of  stream    string  next  element  return  false    override  public  byte  serialize    string  element  return  element  get  bytes  charset    override  public    type  information    string  get  produced  type  return    basic  type  info  string  type  info    java    serialization  private  void  write  object    object  output  stream  out  throws    i  o  exception  out  default  write  object  out  write  u  t  f  charset  name  private  void  read  object  java  io    object  input  stream  in  throws    i  o  exception    class  not  found  exception  in  default  read  object    string  charset  name  in  read  u  t  f  this  charset    charset  for  name  charset  name  
public  evolving  public  interface    aggregating  state  in  out  extends    merging  state  in  out  
public  evolving  public  class    aggregating  state  descriptor  in  acc  out  extends    state  descriptor    aggregating  state  in  out  acc  private  static  final  long  serial  version  u  i  d  1  l    the  aggregation  function  for  the  state  private  final    aggregate  function  in  acc  out  agg  function    creates  a  new  state  descriptor  with  the  given  name  function  and  type  p    if  this  constructor  fails  because  it  is  not  possible  to  describe  the  type  via  a  class  consider  using  the  link    aggregating  state  descriptor    string    aggregate  function    type  information  constructor  param  name    the  unique  name  for  the  state  param  agg  function    the  code    aggregate  function  used  to  aggregate  the  state  param  state  type    the  type  of  the  accumulator    the  accumulator  is  stored  in  the  state  public    aggregating  state  descriptor    string  name    aggregate  function  in  acc  out  agg  function    class  acc  state  type  super  name  state  type  null  this  agg  function  check  not  null  agg  function    creates  a  new  code    reducing  state  descriptor  with  the  given  name  and  default  value  param  name    the  unique  name  for  the  state  param  agg  function    the  code    aggregate  function  used  to  aggregate  the  state  param  state  type    the  type  of  the  accumulator    the  accumulator  is  stored  in  the  state  public    aggregating  state  descriptor    string  name    aggregate  function  in  acc  out  agg  function    type  information  acc  state  type  super  name  state  type  null  this  agg  function  check  not  null  agg  function    creates  a  new  code    value  state  descriptor  with  the  given  name  and  default  value  param  name    the  unique  name  for  the  state  param  agg  function    the  code    aggregate  function  used  to  aggregate  the  state  param  type  serializer    the  serializer  for  the  accumulator    the  accumulator  is  stored  in  the  state  public    aggregating  state  descriptor    string  name    aggregate  function  in  acc  out  agg  function    type  serializer  acc  type  serializer  super  name  type  serializer  null  this  agg  function  check  not  null  agg  function    returns  the  aggregate  function  to  be  used  for  the  state  public    aggregate  function  in  acc  out  get  aggregate  function  return  agg  function    override  public    type  get  type  return    type  aggregating  
public  evolving  public  interface    appending  state  in  out  extends    state    returns  the  current  value  for  the  state    when  the  state  is  not  partitioned  the  returned  value  is  the  same  for  all  inputs  in  a  given  operator  instance    if  state  partitioning  is  applied  the  value  returned  depends  on  the  current  operator  input  as  the  operator  maintains  an  independent  state  for  each  partition  p  b  note  to  implementers  b  if  the  state  is  empty  then  this  method  should  return  code  null  return    the  operator  state  value  corresponding  to  the  current  input  or  code  null  if  the  state  is  empty  throws    exception    thrown  if  the  system  cannot  access  the  state  out  get  throws    exception    updates  the  operator  state  accessible  by  link  get  by  adding  the  given  value  to  the  list  of  values    the  next  time  link  get  is  called  for  the  same  state  partition  the  returned  state  will  represent  the  updated  list  p    if  null  is  passed  in  the  state  value  will  remain  unchanged  param  value    the  new  value  for  the  state  throws    exception    thrown  if  the  system  cannot  access  the  state  void  add  in  value  throws    exception  
public  evolving  public  interface    broadcast  state  k  v  extends    read  only  broadcast  state  k  v    associates  a  new  value  with  the  given  key  param  key    the  key  of  the  mapping  param  value    the  new  value  of  the  mapping  throws    exception    thrown  if  the  system  cannot  access  the  state  void  put  k  key  v  value  throws    exception    copies  all  of  the  mappings  from  the  given  map  into  the  state  param  map    the  mappings  to  be  stored  in  this  state  throws    exception    thrown  if  the  system  cannot  access  the  state  void  put  all    map  k  v  map  throws    exception    deletes  the  mapping  of  the  given  key  param  key    the  key  of  the  mapping  throws    exception    thrown  if  the  system  cannot  access  the  state  void  remove  k  key  throws    exception    iterates  over  all  the  mappings  in  the  state  return    an  iterator  over  all  the  mappings  in  the  state  throws    exception    thrown  if  the  system  cannot  access  the  state    iterator    map    entry  k  v  iterator  throws    exception    returns  all  the  mappings  in  the  state  return    an  iterable  view  of  all  the  key  value  pairs  in  the  state  throws    exception    thrown  if  the  system  cannot  access  the  state    iterable    map    entry  k  v  entries  throws    exception  
public  evolving    deprecated  public  interface    folding  state  t  acc  extends    appending  state  t  acc  
public  evolving    deprecated  public  class    folding  state  descriptor  t  acc  extends    state  descriptor    folding  state  t  acc  acc  private  static  final  long  serial  version  u  i  d  1  l  private  final    fold  function  t  acc  fold  function    creates  a  new  code    folding  state  descriptor  with  the  given  name  type  and  initial  value  p    if  this  constructor  fails  because  it  is  not  possible  to  describe  the  type  via  a  class  consider  using  the  link    folding  state  descriptor    string  acc    fold  function    type  information  constructor  param  name    the  unique  name  for  the  state  param  initial  value    the  initial  value  of  the  fold  param  fold  function    the  code    fold  function  used  to  aggregate  the  state  param  type  class    the  type  of  the  values  in  the  state  public    folding  state  descriptor    string  name  acc  initial  value    fold  function  t  acc  fold  function    class  acc  type  class  super  name  type  class  initial  value  this  fold  function  require  non  null  fold  function  if  fold  function  instanceof    rich  function  throw  new    unsupported  operation  exception    fold  function  of    folding  state  can  not  be  a    rich  function    creates  a  new  code    folding  state  descriptor  with  the  given  name  and  default  value  param  name    the  unique  name  for  the  state  param  initial  value    the  initial  value  of  the  fold  param  fold  function    the  code    fold  function  used  to  aggregate  the  state  param  type  info    the  type  of  the  values  in  the  state  public    folding  state  descriptor    string  name  acc  initial  value    fold  function  t  acc  fold  function    type  information  acc  type  info  super  name  type  info  initial  value  this  fold  function  require  non  null  fold  function  if  fold  function  instanceof    rich  function  throw  new    unsupported  operation  exception    fold  function  of    folding  state  can  not  be  a    rich  function    creates  a  new  code    value  state  descriptor  with  the  given  name  and  default  value  param  name    the  unique  name  for  the  state  param  initial  value    the  initial  value  of  the  fold  param  fold  function    the  code    fold  function  used  to  aggregate  the  state  param  type  serializer    the  type  serializer  of  the  values  in  the  state  public    folding  state  descriptor    string  name  acc  initial  value    fold  function  t  acc  fold  function    type  serializer  acc  type  serializer  super  name  type  serializer  initial  value  this  fold  function  require  non  null  fold  function  if  fold  function  instanceof    rich  function  throw  new    unsupported  operation  exception    fold  function  of    folding  state  can  not  be  a    rich  function    returns  the  fold  function  to  be  used  for  the  folding  state  public    fold  function  t  acc  get  fold  function  return  fold  function    override  public    type  get  type  return    type  folding  
public  evolving  public  interface    keyed  state  store    gets  a  handle  to  the  system  s  key  value  state    the  key  value  state  is  only  accessible  if  the  function  is  executed  on  a    keyed  stream    on  each  access  the  state  exposes  the  value  for  the  key  of  the  element  currently  processed  by  the  function    each  function  may  have  multiple  partitioned  states  addressed  with  different  names  p    because  the  scope  of  each  value  is  the  key  of  the  currently  processed  element  and  the  elements  are  distributed  by  the    flink  runtime  the  system  can  transparently  scale  out  and  redistribute  the  state  and    keyed  stream  p    the  following  code  example  shows  how  to  implement  a  continuous  counter  that  counts  how  many  times  elements  of  a  certain  key  occur  and  emits  an  updated  count  for  that  element  on  each  occurrence  pre  code    data  stream    my  type  stream    keyed  stream    my  type  keyed  stream  stream  key  by  id  keyed  stream  map  new    rich  map  function    my  type    tuple2    my  type    long  private    value  state    long  count  public  void  open    configuration  cfg  state  get  runtime  context  get  state  new    value  state  descriptor    long  count    long  serializer  instance  0  l  public    tuple2    my  type    long  map    my  type  value  long  count  state  value    state  update  value  return  new    tuple2  value  count  pre  param  state  properties    the  descriptor  defining  the  properties  of  the  stats  param  t    the  type  of  value  stored  in  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  of  a    keyed  stream    public  evolving  t    value  state  t  get  state    value  state  descriptor  t  state  properties    gets  a  handle  to  the  system  s  key  value  list  state    this  state  is  similar  to  the  state  accessed  via  link  get  state    value  state  descriptor  but  is  optimized  for  state  that  holds  lists    one  can  adds  elements  to  the  list  or  retrieve  the  list  as  a  whole  p    this  state  is  only  accessible  if  the  function  is  executed  on  a    keyed  stream  pre  code    data  stream    my  type  stream    keyed  stream    my  type  keyed  stream  stream  key  by  id  keyed  stream  map  new    rich  flat  map  function    my  type    list    my  type  private    list  state    my  type  state  public  void  open    configuration  cfg  state  get  runtime  context  get  list  state  new    list  state  descriptor  my  state    my  type  class  public  void  flat  map    my  type  value    collector    my  type  out  if  value  is  divider  for    my  type  t  state  get  out  collect  t  else  state  add  value  pre  param  state  properties    the  descriptor  defining  the  properties  of  the  stats  param  t    the  type  of  value  stored  in  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  os  a    keyed  stream    public  evolving  t    list  state  t  get  list  state    list  state  descriptor  t  state  properties    gets  a  handle  to  the  system  s  key  value  reducing  state    this  state  is  similar  to  the  state  accessed  via  link  get  state    value  state  descriptor  but  is  optimized  for  state  that  aggregates  values  p    this  state  is  only  accessible  if  the  function  is  executed  on  a    keyed  stream  pre  code    data  stream    my  type  stream    keyed  stream    my  type  keyed  stream  stream  key  by  id  keyed  stream  map  new    rich  map  function    my  type    list    my  type  private    reducing  state    long  state  public  void  open    configuration  cfg  state  get  runtime  context  get  reducing  state  new    reducing  state  descriptor  sum  a  b  a  b    long  class  public    tuple2    my  type    long  map    my  type  value  state  add  value  count  return  new    tuple2  value  state  get  pre  param  state  properties    the  descriptor  defining  the  properties  of  the  stats  param  t    the  type  of  value  stored  in  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  of  a    keyed  stream    public  evolving  t    reducing  state  t  get  reducing  state    reducing  state  descriptor  t  state  properties    gets  a  handle  to  the  system  s  key  value  folding  state    this  state  is  similar  to  the  state  accessed  via  link  get  state    value  state  descriptor  but  is  optimized  for  state  that  aggregates  values  with  different  types  p    this  state  is  only  accessible  if  the  function  is  executed  on  a    keyed  stream  pre  code    data  stream    my  type  stream    keyed  stream    my  type  keyed  stream  stream  key  by  id    aggregate  function  aggregate  function  keyed  stream  map  new    rich  map  function    my  type    list    my  type  private    aggregating  state    my  type    long  state  public  void  open    configuration  cfg  state  get  runtime  context  get  aggregating  state  new    aggregating  state  descriptor  sum  aggregate  function    long  class  public    tuple2    my  type    long  map    my  type  value  state  add  value  return  new    tuple2  value  state  get  pre  param  state  properties    the  descriptor  defining  the  properties  of  the  stats  param  in    the  type  of  the  values  that  are  added  to  the  state  param  acc    the  type  of  the  accumulator  intermediate  aggregation  state  param  out    the  type  of  the  values  that  are  returned  from  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  of  a    keyed  stream    public  evolving  in  acc  out    aggregating  state  in  out  get  aggregating  state    aggregating  state  descriptor  in  acc  out  state  properties    gets  a  handle  to  the  system  s  key  value  folding  state    this  state  is  similar  to  the  state  accessed  via  link  get  state    value  state  descriptor  but  is  optimized  for  state  that  aggregates  values  with  different  types  p    this  state  is  only  accessible  if  the  function  is  executed  on  a    keyed  stream  pre  code    data  stream    my  type  stream    keyed  stream    my  type  keyed  stream  stream  key  by  id  keyed  stream  map  new    rich  map  function    my  type    list    my  type  private    folding  state    my  type    long  state  public  void  open    configuration  cfg  state  get  runtime  context  get  reducing  state  new    folding  state  descriptor  sum  0  l  a  b  a  count  b    long  class  public    tuple2    my  type    long  map    my  type  value  state  add  value  return  new    tuple2  value  state  get  pre  param  state  properties    the  descriptor  defining  the  properties  of  the  stats  param  t    the  type  of  value  stored  in  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  of  a    keyed  stream  deprecated  will  be  removed  in  a  future  version  in  favor  of  link    aggregating  state    public  evolving    deprecated  t  acc    folding  state  t  acc  get  folding  state    folding  state  descriptor  t  acc  state  properties    gets  a  handle  to  the  system  s  key  value  map  state    this  state  is  similar  to  the  state  accessed  via  link  get  state    value  state  descriptor  but  is  optimized  for  state  that  is  composed  of  user  defined  key  value  pairs  p    this  state  is  only  accessible  if  the  function  is  executed  on  a    keyed  stream  pre  code    data  stream    my  type  stream    keyed  stream    my  type  keyed  stream  stream  key  by  id  keyed  stream  map  new    rich  map  function    my  type    list    my  type  private    map  state    my  type    long  state  public  void  open    configuration  cfg  state  get  runtime  context  get  map  state  new    map  state  descriptor  sum    my  type  class    long  class  public    tuple2    my  type    long  map    my  type  value  return  new    tuple2  value  state  get  value  pre  param  state  properties    the  descriptor  defining  the  properties  of  the  stats  param  uk    the  type  of  the  user  keys  stored  in  the  state  param  uv    the  type  of  the  user  values  stored  in  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  of  a    keyed  stream    public  evolving  uk  uv    map  state  uk  uv  get  map  state    map  state  descriptor  uk  uv  state  properties  
public  evolving  public  interface    list  state  t  extends    merging  state  t    iterable  t    updates  the  operator  state  accessible  by  link  get  by  updating  existing  values  to  to  the  given  list  of  values    the  next  time  link  get  is  called  for  the  same  state  partition  the  returned  state  will  represent  the  updated  list  p    if  null  or  an  empty  list  is  passed  in  the  state  value  will  be  null  param  values    the  new  values  for  the  state  throws    exception    the  method  may  forward  exception  thrown  internally  by  i  o  or  functions  void  update    list  t  values  throws    exception    updates  the  operator  state  accessible  by  link  get  by  adding  the  given  values  to  existing  list  of  values    the  next  time  link  get  is  called  for  the  same  state  partition  the  returned  state  will  represent  the  updated  list  p    if  null  or  an  empty  list  is  passed  in  the  state  value  remains  unchanged  param  values    the  new  values  to  be  added  to  the  state  throws    exception    the  method  may  forward  exception  thrown  internally  by  i  o  or  functions  void  add  all    list  t  values  throws    exception  
public  evolving  public  class    list  state  descriptor  t  extends    state  descriptor    list  state  t    list  t  private  static  final  long  serial  version  u  i  d  2  l    creates  a  new  code    list  state  descriptor  with  the  given  name  and  list  element  type  p    if  this  constructor  fails  because  it  is  not  possible  to  describe  the  type  via  a  class  consider  using  the  link    list  state  descriptor    string    type  information  constructor  param  name    the  unique  name  for  the  state  param  element  type  class    the  type  of  the  elements  in  the  state  public    list  state  descriptor    string  name    class  t  element  type  class  super  name  new    list  type  info  element  type  class  null    creates  a  new  code    list  state  descriptor  with  the  given  name  and  list  element  type  param  name    the  unique  name  for  the  state  param  element  type  info    the  type  of  the  elements  in  the  state  public    list  state  descriptor    string  name    type  information  t  element  type  info  super  name  new    list  type  info  element  type  info  null    creates  a  new  code    list  state  descriptor  with  the  given  name  and  list  element  type  param  name    the  unique  name  for  the  state  param  type  serializer    the  type  serializer  for  the  list  values  public    list  state  descriptor    string  name    type  serializer  t  type  serializer  super  name  new    list  serializer  type  serializer  null    gets  the  serializer  for  the  elements  contained  in  the  list  return    the  serializer  for  the  elements  in  the  list  public    type  serializer  t  get  element  serializer  call  get  serializer  here  to  get  the  initialization  check  and  proper  error  message  final    type  serializer    list  t  raw  serializer  get  serializer  if  raw  serializer  instanceof    list  serializer  throw  new    illegal  state  exception  return    list  serializer  t  raw  serializer  get  element  serializer    override  public    type  get  type  return    type  list  
public  evolving  public  interface    map  state  uk  uv  extends    state    returns  the  current  value  associated  with  the  given  key  param  key    the  key  of  the  mapping  return    the  value  of  the  mapping  with  the  given  key  throws    exception    thrown  if  the  system  cannot  access  the  state  uv  get  uk  key  throws    exception    associates  a  new  value  with  the  given  key  param  key    the  key  of  the  mapping  param  value    the  new  value  of  the  mapping  throws    exception    thrown  if  the  system  cannot  access  the  state  void  put  uk  key  uv  value  throws    exception    copies  all  of  the  mappings  from  the  given  map  into  the  state  param  map    the  mappings  to  be  stored  in  this  state  throws    exception    thrown  if  the  system  cannot  access  the  state  void  put  all    map  uk  uv  map  throws    exception    deletes  the  mapping  of  the  given  key  param  key    the  key  of  the  mapping  throws    exception    thrown  if  the  system  cannot  access  the  state  void  remove  uk  key  throws    exception    returns  whether  there  exists  the  given  mapping  param  key    the  key  of  the  mapping  return    true  if  there  exists  a  mapping  whose  key  equals  to  the  given  key  throws    exception    thrown  if  the  system  cannot  access  the  state  boolean  contains  uk  key  throws    exception    returns  all  the  mappings  in  the  state  return    an  iterable  view  of  all  the  key  value  pairs  in  the  state  throws    exception    thrown  if  the  system  cannot  access  the  state    iterable    map    entry  uk  uv  entries  throws    exception    returns  all  the  keys  in  the  state  return    an  iterable  view  of  all  the  keys  in  the  state  throws    exception    thrown  if  the  system  cannot  access  the  state    iterable  uk  keys  throws    exception    returns  all  the  values  in  the  state  return    an  iterable  view  of  all  the  values  in  the  state  throws    exception    thrown  if  the  system  cannot  access  the  state    iterable  uv  values  throws    exception    iterates  over  all  the  mappings  in  the  state  return    an  iterator  over  all  the  mappings  in  the  state  throws    exception    thrown  if  the  system  cannot  access  the  state    iterator    map    entry  uk  uv  iterator  throws    exception    returns  true  if  this  state  contains  no  key  value  mappings  otherwise  false  return    true  if  this  state  contains  no  key  value  mappings  otherwise  false  throws    exception    thrown  if  the  system  cannot  access  the  state  boolean  is  empty  throws    exception  
public  evolving  public  class    map  state  descriptor  uk  uv  extends    state  descriptor    map  state  uk  uv    map  uk  uv  private  static  final  long  serial  version  u  i  d  1  l    create  a  new  code    map  state  descriptor  with  the  given  name  and  the  given  type  serializers  param  name    the  name  of  the  code    map  state  descriptor  param  key  serializer    the  type  serializer  for  the  keys  in  the  state  param  value  serializer    the  type  serializer  for  the  values  in  the  state  public    map  state  descriptor    string  name    type  serializer  uk  key  serializer    type  serializer  uv  value  serializer  super  name  new    map  serializer  key  serializer  value  serializer  null    create  a  new  code    map  state  descriptor  with  the  given  name  and  the  given  type  information  param  name    the  name  of  the  code    map  state  descriptor  param  key  type  info    the  type  information  for  the  keys  in  the  state  param  value  type  info    the  type  information  for  the  values  in  the  state  public    map  state  descriptor    string  name    type  information  uk  key  type  info    type  information  uv  value  type  info  super  name  new    map  type  info  key  type  info  value  type  info  null    create  a  new  code    map  state  descriptor  with  the  given  name  and  the  given  type  information  p    if  this  constructor  fails  because  it  is  not  possible  to  describe  the  type  via  a  class  consider  using  the  link    map  state  descriptor    string    type  information    type  information  constructor  param  name    the  name  of  the  code    map  state  descriptor  param  key  class    the  class  of  the  type  of  keys  in  the  state  param  value  class    the  class  of  the  type  of  values  in  the  state  public    map  state  descriptor    string  name    class  uk  key  class    class  uv  value  class  super  name  new    map  type  info  key  class  value  class  null    override  public    type  get  type  return    type  map    gets  the  serializer  for  the  keys  in  the  state  return    the  serializer  for  the  keys  in  the  state  public    type  serializer  uk  get  key  serializer  final    type  serializer    map  uk  uv  raw  serializer  get  serializer  if  raw  serializer  instanceof    map  serializer  throw  new    illegal  state  exception    unexpected  serializer  type  return    map  serializer  uk  uv  raw  serializer  get  key  serializer    gets  the  serializer  for  the  values  in  the  state  return    the  serializer  for  the  values  in  the  state  public    type  serializer  uv  get  value  serializer  final    type  serializer    map  uk  uv  raw  serializer  get  serializer  if  raw  serializer  instanceof    map  serializer  throw  new    illegal  state  exception    unexpected  serializer  type  return    map  serializer  uk  uv  raw  serializer  get  value  serializer  
public  evolving  public  interface    merging  state  in  out  extends    appending  state  in  out  
public  evolving  public  interface    operator  state  store    creates  or  restores  a  link    broadcast  state  broadcast  state    this  type  of  state  can  only  be  created  to  store  the  state  of  a  code    broadcast  stream    each  state  is  registered  under  a  unique  name    the  provided  serializer  is  used  to  de  serialize  the  state  in  case  of  checkpointing  snapshot  restore    the  returned  broadcast  state  has  code  key  value  format  p  b  caution  the  user  has  to  guarantee  that  all  task  instances  store  the  same  elements  in  this  type  of  state  b  p    each  operator  instance  individually  maintains  and  stores  elements  in  the  broadcast  state    the  fact  that  the  incoming  stream  is  a  broadcast  one  guarantees  that  all  instances  see  all  the  elements    upon  recovery  or  re  scaling  the  same  state  is  given  to  each  of  the  instances    to  avoid  hotspots  each  task  reads  its  previous  partition  and  if  there  are  more  tasks  scale  up  then  the  new  instances  read  from  the  old  instances  in  a  round  robin  fashion    this  is  why  each  instance  has  to  guarantee  that  it  stores  the  same  elements  as  the  rest    if  not  upon  recovery  or  rescaling  you  may  have  unpredictable  redistribution  of  the  partitions  thus  unpredictable  results  param  state  descriptor    the  descriptor  for  this  state  providing  a  name  a  serializer  for  the  keys  and  one  for  the  values  param  k    the  type  of  the  keys  in  the  broadcast  state  param  v    the  type  of  the  values  in  the  broadcast  state  return    the    broadcast    state  k  v    broadcast  state  k  v  get  broadcast  state    map  state  descriptor  k  v  state  descriptor  throws    exception    creates  or  restores  a  list  state    each  state  is  registered  under  a  unique  name    the  provided  serializer  is  used  to  de  serialize  the  state  in  case  of  checkpointing  snapshot  restore  p    note  the  semantic  differences  between  an  operator  list  state  and  a  keyed  list  state  see  link    keyed  state  store  get  list  state    list  state  descriptor    under  the  context  of  operator  state  the  list  is  a  collection  of  state  items  that  are  independent  from  each  other  and  eligible  for  redistribution  across  operator  instances  in  case  of  changed  operator  parallelism    in  other  words  these  state  items  are  the  finest  granularity  at  which  non  keyed  state  can  be  redistributed  and  should  not  be  correlated  with  each  other  p    the  redistribution  scheme  of  this  list  state  upon  operator  rescaling  is  a  round  robin  pattern  such  that  the  logical  whole  state  a  concatenation  of  all  the  lists  of  state  elements  previously  managed  by  each  operator  before  the  restore  is  evenly  divided  into  as  many  sublists  as  there  are  parallel  operators  param  state  descriptor    the  descriptor  for  this  state  providing  a  name  and  serializer  param  s    the  generic  type  of  the  state  return  a  list  for  all  state  partitions  s    list  state  s  get  list  state    list  state  descriptor  s  state  descriptor  throws    exception    creates  or  restores  a  list  state    each  state  is  registered  under  a  unique  name    the  provided  serializer  is  used  to  de  serialize  the  state  in  case  of  checkpointing  snapshot  restore  p    note  the  semantic  differences  between  an  operator  list  state  and  a  keyed  list  state  see  link    keyed  state  store  get  list  state    list  state  descriptor    under  the  context  of  operator  state  the  list  is  a  collection  of  state  items  that  are  independent  from  each  other  and  eligible  for  redistribution  across  operator  instances  in  case  of  changed  operator  parallelism    in  other  words  these  state  items  are  the  finest  granularity  at  which  non  keyed  state  can  be  redistributed  and  should  not  be  correlated  with  each  other  p    the  redistribution  scheme  of  this  list  state  upon  operator  rescaling  is  a  broadcast  pattern  such  that  the  logical  whole  state  a  concatenation  of  all  the  lists  of  state  elements  previously  managed  by  each  operator  before  the  restore  is  restored  to  all  parallel  operators  so  that  each  of  them  will  get  the  union  of  all  state  items  before  the  restore  param  state  descriptor    the  descriptor  for  this  state  providing  a  name  and  serializer  param  s    the  generic  type  of  the  state  return  a  list  for  all  state  partitions  s    list  state  s  get  union  list  state    list  state  descriptor  s  state  descriptor  throws    exception    returns  a  set  with  the  names  of  all  currently  registered  states  return  set  of  names  for  all  registered  states    set    string  get  registered  state  names    returns  a  set  with  the  names  of  all  currently  registered  broadcast  states  return  set  of  names  for  all  registered  broadcast  states    set    string  get  registered  broadcast  state  names  
public  evolving  public  interface    read  only  broadcast  state  k  v  extends    state    returns  the  current  value  associated  with  the  given  key  p    the  user  code  must  not  modify  the  value  returned  as  this  can  lead  to  inconsistent  states  param  key    the  key  of  the  mapping  return    the  value  of  the  mapping  with  the  given  key  throws    exception    thrown  if  the  system  cannot  access  the  state  v  get  k  key  throws    exception    returns  whether  there  exists  the  given  mapping  param  key    the  key  of  the  mapping  return    true  if  there  exists  a  mapping  whose  key  equals  to  the  given  key  throws    exception    thrown  if  the  system  cannot  access  the  state  boolean  contains  k  key  throws    exception    returns  an  immutable  link    iterable  over  the  entries  in  the  state  p    the  user  code  must  not  modify  the  entries  of  the  returned  immutable  iterator  as  this  can  lead  to  inconsistent  states    iterable    map    entry  k  v  immutable  entries  throws    exception  
public  evolving  public  interface    reducing  state  t  extends    merging  state  t  t  
public  evolving  public  class    reducing  state  descriptor  t  extends    state  descriptor    reducing  state  t  t  private  static  final  long  serial  version  u  i  d  1  l  private  final    reduce  function  t  reduce  function    creates  a  new  code    reducing  state  descriptor  with  the  given  name  type  and  default  value  p    if  this  constructor  fails  because  it  is  not  possible  to  describe  the  type  via  a  class  consider  using  the  link    reducing  state  descriptor    string    reduce  function    type  information  constructor  param  name    the  unique  name  for  the  state  param  reduce  function    the  code    reduce  function  used  to  aggregate  the  state  param  type  class    the  type  of  the  values  in  the  state  public    reducing  state  descriptor    string  name    reduce  function  t  reduce  function    class  t  type  class  super  name  type  class  null  this  reduce  function  check  not  null  reduce  function  if  reduce  function  instanceof    rich  function  throw  new    unsupported  operation  exception    reduce  function  of    reducing  state  can  not  be  a    rich  function    creates  a  new  code    reducing  state  descriptor  with  the  given  name  and  default  value  param  name    the  unique  name  for  the  state  param  reduce  function    the  code    reduce  function  used  to  aggregate  the  state  param  type  info    the  type  of  the  values  in  the  state  public    reducing  state  descriptor    string  name    reduce  function  t  reduce  function    type  information  t  type  info  super  name  type  info  null  this  reduce  function  check  not  null  reduce  function    creates  a  new  code    value  state  descriptor  with  the  given  name  and  default  value  param  name    the  unique  name  for  the  state  param  reduce  function    the  code    reduce  function  used  to  aggregate  the  state  param  type  serializer    the  type  serializer  of  the  values  in  the  state  public    reducing  state  descriptor    string  name    reduce  function  t  reduce  function    type  serializer  t  type  serializer  super  name  type  serializer  null  this  reduce  function  check  not  null  reduce  function    returns  the  reduce  function  to  be  used  for  the  reducing  state  public    reduce  function  t  get  reduce  function  return  reduce  function    override  public    type  get  type  return    type  reducing  
public  evolving  public  interface    state    removes  the  value  mapped  under  the  current  key  void  clear  
public  evolving  public  abstract  class    state  descriptor  s  extends    state  t  implements    serializable  private  static  final    logger  log    logger  factory  get  logger    state  descriptor  class    an  enumeration  of  the  types  of  supported  states    used  to  identify  the  state  type  when  writing  and  restoring  checkpoints  and  savepoints  important    do  not  change  the  order  of  the  elements  in  this  enum  ordinal  is  used  in  serialization  public  enum    type  deprecated    enum  for  migrating  from  old  checkpoints  savepoint  versions    deprecated  unknown  value  list  reducing  folding  aggregating  map  private  static  final  long  serial  version  u  i  d  1  l    name  that  uniquely  identifies  state  created  from  this    state  descriptor  protected  final    string  name    the  serializer  for  the  type    may  be  eagerly  initialized  in  the  constructor  or  lazily  once  the  link  initialize  serializer  unless  set    execution  config  method  is  called  private  final    atomic  reference    type  serializer  t  serializer  atomic  reference  new    atomic  reference    the  type  information  describing  the  value  type    only  used  to  if  the  serializer  is  created  lazily    nullable  private    type  information  t  type  info    name  for  queries  against  state  created  from  this    state  descriptor    nullable  private    string  queryable  state  name    name  for  queries  against  state  created  from  this    state  descriptor    nonnull  private    state  ttl  config  ttl  config    state  ttl  config  disabled    the  default  value  returned  by  the  state  when  no  other  value  is  bound  to  a  key    nullable  protected  transient  t  default  value    create  a  new  code    state  descriptor  with  the  given  name  and  the  given  type  serializer  param  name    the  name  of  the  code    state  descriptor  param  serializer    the  type  serializer  for  the  values  in  the  state  param  default  value    the  default  value  that  will  be  set  when  requesting  state  without  setting  a  value  before  protected    state  descriptor    string  name    type  serializer  t  serializer    nullable  t  default  value  this  name  check  not  null  name  name  must  not  be  null  this  serializer  atomic  reference  set  check  not  null  serializer  serializer  must  not  be  null  this  default  value  default  value    create  a  new  code    state  descriptor  with  the  given  name  and  the  given  type  information  param  name    the  name  of  the  code    state  descriptor  param  type  info    the  type  information  for  the  values  in  the  state  param  default  value    the  default  value  that  will  be  set  when  requesting  state  without  setting  a  value  before  protected    state  descriptor    string  name    type  information  t  type  info    nullable  t  default  value  this  name  check  not  null  name  name  must  not  be  null  this  type  info  check  not  null  type  info  type  information  must  not  be  null  this  default  value  default  value    create  a  new  code    state  descriptor  with  the  given  name  and  the  given  type  information  p    if  this  constructor  fails  because  it  is  not  possible  to  describe  the  type  via  a  class  consider  using  the  link    state  descriptor    string    type  information    object  constructor  param  name    the  name  of  the  code    state  descriptor  param  type    the  class  of  the  type  of  values  in  the  state  param  default  value    the  default  value  that  will  be  set  when  requesting  state  without  setting  a  value  before  protected    state  descriptor    string  name    class  t  type    nullable  t  default  value  this  name  check  not  null  name  name  must  not  be  null  check  not  null  type  type  class  must  not  be  null  try  this  type  info    type  extractor  create  type  info  type  catch    exception  e  throw  new    runtime  exception    could  not  create  the  type  information  for  type  get  name    the  most  common  reason  is  failure  to  infer  the  generic  type  information  due  to    java  s  type  erasure    in  that  case  please  pass  a    type  hint  instead  of  a  class  to  describe  the  type    for  example  to  describe    tuple2    string    string  as  a  generic  type  use  new    pravega  deserialization  schema  new    type  hint    tuple2    string    string  serializer  e  this  default  value  default  value    returns  the  name  of  this  code    state  descriptor  public    string  get  name  return  name    returns  the  default  value  public  t  get  default  value  if  default  value  null    type  serializer  t  serializer  serializer  atomic  reference  get  if  serializer  null  return  serializer  copy  default  value  else  throw  new    illegal  state  exception    serializer  not  yet  initialized  else  return  null    returns  the  link    type  serializer  that  can  be  used  to  serialize  the  value  in  the  state    note  that  the  serializer  may  initialized  lazily  and  is  only  guaranteed  to  exist  after  calling  link  initialize  serializer  unless  set    execution  config  public    type  serializer  t  get  serializer    type  serializer  t  serializer  serializer  atomic  reference  get  if  serializer  null  return  serializer  duplicate  else  throw  new    illegal  state  exception    serializer  not  yet  initialized    visible  for  testing  final    type  serializer  t  get  original  serializer    type  serializer  t  serializer  serializer  atomic  reference  get  if  serializer  null  return  serializer  else  throw  new    illegal  state  exception    serializer  not  yet  initialized    sets  the  name  for  queries  of  state  created  from  this  descriptor  p    if  a  name  is  set  the  created  state  will  be  published  for  queries  during  runtime    the  name  needs  to  be  unique  per  job    if  there  is  another  state  instance  published  under  the  same  name  the  job  will  fail  during  runtime  param  queryable  state  name    state  name  for  queries  unique  name  per  job  throws    illegal  state  exception    if  queryable  state  name  already  set  public  void  set  queryable    string  queryable  state  name    preconditions  check  argument  ttl  config  get  update  type    state  ttl  config    update  type    disabled    queryable  state  is  currently  not  supported  with  ttl  if  this  queryable  state  name  null  this  queryable  state  name    preconditions  check  not  null  queryable  state  name    registration  name  else  throw  new    illegal  state  exception    queryable  state  name  already  set    returns  the  queryable  state  name  return    queryable  state  name  or  code  null  code  if  not  set    nullable  public    string  get  queryable  state  name  return  queryable  state  name    returns  whether  the  state  created  from  this  descriptor  is  queryable  return  code  true  code  if  state  is  queryable  code  false  code  otherwise  public  boolean  is  queryable  return  queryable  state  name  null    configures  optional  activation  of  state  time  to  live  ttl  p    state  user  value  will  expire  become  unavailable  and  be  cleaned  up  in  storage  depending  on  configured  link    state  ttl  config  param  ttl  config  configuration  of  state  ttl  public  void  enable  time  to  live    state  ttl  config  ttl  config    preconditions  check  not  null  ttl  config    preconditions  check  argument  ttl  config  get  update  type    state  ttl  config    update  type    disabled  queryable  state  name  null    queryable  state  is  currently  not  supported  with  ttl  this  ttl  config  ttl  config    nonnull    internal  public    state  ttl  config  get  ttl  config  return  ttl  config    checks  whether  the  serializer  has  been  initialized    serializer  initialization  is  lazy  to  allow  parametrization  of  serializers  with  an  link    execution  config  via  link  initialize  serializer  unless  set    execution  config  return    true  if  the  serializers  have  been  initialized  false  otherwise  public  boolean  is  serializer  initialized  return  serializer  atomic  reference  get  null    initializes  the  serializer  unless  it  has  been  initialized  before  param  execution  config    the  execution  config  to  use  when  creating  the  serializer  public  void  initialize  serializer  unless  set    execution  config  execution  config  if  serializer  atomic  reference  get  null  check  state  type  info  null  no  serializer  and  no  type  info  try  to  instantiate  and  set  the  serializer    type  serializer  t  serializer  type  info  create  serializer  execution  config  use  cas  to  assure  the  singleton  if  serializer  atomic  reference  compare  and  set  null  serializer  log  debug    someone  else  beat  us  at  initializing  the  serializer    standard    utils    override  public  final  int  hash  code  return  name  hash  code    get  class  hash  code    override  public  final  boolean  equals    object  o  if  o  this  return  true  else  if  o  null  o  get  class  this  get  class  final    state  descriptor  that    state  descriptor  o  return  this  name  equals  that  name  else  return  false    override  public    string  to  string  return  get  class  get  simple  name  name  name  default  value  default  value  serializer  serializer  atomic  reference  get  is  queryable  queryable  state  name  queryable  state  name  public  abstract    type  get  type    serialization  private  void  write  object  final    object  output  stream  out  throws    i  o  exception  write  all  the  non  transient  fields  out  default  write  object  write  the  non  serializable  default  value  field  if  default  value  null  we  don  t  have  a  default  value  out  write  boolean  false  else    type  serializer  t  serializer  serializer  atomic  reference  get  check  not  null  serializer    serializer  not  initialized  we  have  a  default  value  out  write  boolean  true  byte  serialized  default  value  try    byte  array  output  stream  baos  new    byte  array  output  stream    data  output  view  stream  wrapper  out  view  new    data  output  view  stream  wrapper  baos    type  serializer  t  duplicate  serializer  serializer  duplicate  duplicate  serializer  serialize  default  value  out  view  out  view  flush  serialized  default  value  baos  to  byte  array  catch    exception  e  throw  new    i  o  exception    unable  to  serialize  default  value  of  type  default  value  get  class  get  simple  name  e  out  write  int  serialized  default  value  length  out  write  serialized  default  value  private  void  read  object  final    object  input  stream  in  throws    i  o  exception    class  not  found  exception  read  the  non  transient  fields  in  default  read  object  read  the  default  value  field  boolean  has  default  value  in  read  boolean  if  has  default  value    type  serializer  t  serializer  serializer  atomic  reference  get  check  not  null  serializer    serializer  not  initialized  int  size  in  read  int  byte  buffer  new  byte  size  in  read  fully  buffer  try    byte  array  input  stream  bais  new    byte  array  input  stream  buffer    data  input  view  stream  wrapper  in  view  new    data  input  view  stream  wrapper  bais  default  value  serializer  deserialize  in  view  catch    exception  e  throw  new    i  o  exception    unable  to  deserialize  default  value  e  else  default  value  null  
public  evolving  public  class    state  ttl  config  implements    serializable  private  static  final  long  serial  version  u  i  d    l  public  static  final    state  ttl  config  disabled  new  builder    time  milliseconds    long  max  value  set  update  type    update  type    disabled  build    this  option  value  configures  when  to  update  last  access  timestamp  which  prolongs  state  ttl  public  enum    update  type  ttl  is  disabled    state  does  not  expire    disabled    last  access  timestamp  is  initialised  when  state  is  created  and  updated  on  every  write  operation    on  create  and  write    the  same  as  code    on  create  and  write  code  but  also  updated  on  read    on  read  and  write    this  option  configures  whether  expired  user  value  can  be  returned  or  not  public  enum    state  visibility    return  expired  user  value  if  it  is  not  cleaned  up  yet    return  expired  if  not  cleaned  up    never  return  expired  user  value    never  return  expired    this  option  configures  time  scale  to  use  for  ttl  public  enum    ttl  time  characteristic    processing  time  see  also  code  org  apache  flink  streaming  api    time  characteristic    processing  time  code    processing  time  private  final    update  type  update  type  private  final    state  visibility  state  visibility  private  final    ttl  time  characteristic  ttl  time  characteristic  private  final    time  ttl  private  final    cleanup  strategies  cleanup  strategies  private    state  ttl  config    update  type  update  type    state  visibility  state  visibility    ttl  time  characteristic  ttl  time  characteristic    time  ttl    cleanup  strategies  cleanup  strategies  this  update  type  check  not  null  update  type  this  state  visibility  check  not  null  state  visibility  this  ttl  time  characteristic  check  not  null  ttl  time  characteristic  this  ttl  check  not  null  ttl  this  cleanup  strategies  cleanup  strategies  check  argument  ttl  to  milliseconds    ttl  is  expected  to  be  positive    nonnull  public    update  type  get  update  type  return  update  type    nonnull  public    state  visibility  get  state  visibility  return  state  visibility    nonnull  public    time  get  ttl  return  ttl    nonnull  public    ttl  time  characteristic  get  ttl  time  characteristic  return  ttl  time  characteristic  public  boolean  is  enabled  return  update  type    update  type    disabled    nonnull  public    cleanup  strategies  get  cleanup  strategies  return  cleanup  strategies    override  public    string  to  string  return    state  ttl  config  update  type  update  type  state  visibility  state  visibility  ttl  time  characteristic  ttl  time  characteristic  ttl  ttl    nonnull  public  static    builder  new  builder    nonnull    time  ttl  return  new    builder  ttl    builder  for  the  link    state  ttl  config  public  static  class    builder  private    update  type  update  type    on  create  and  write  private    state  visibility  state  visibility    never  return  expired  private    ttl  time  characteristic  ttl  time  characteristic    processing  time  private    time  ttl  private  boolean  is  cleanup  in  background  true  private  final    enum  map    cleanup  strategies    strategies    cleanup  strategies    cleanup  strategy  strategies  new    enum  map    cleanup  strategies    strategies  class  public    builder    nonnull    time  ttl  this  ttl  ttl    sets  the  ttl  update  type  param  update  type    the  ttl  update  type  configures  when  to  update  last  access  timestamp  which  prolongs  state  ttl    nonnull  public    builder  set  update  type    update  type  update  type  this  update  type  update  type  return  this    nonnull  public    builder  update  ttl  on  create  and  write  return  set  update  type    update  type    on  create  and  write    nonnull  public    builder  update  ttl  on  read  and  write  return  set  update  type    update  type    on  read  and  write    sets  the  state  visibility  param  state  visibility    the  state  visibility  configures  whether  expired  user  value  can  be  returned  or  not    nonnull  public    builder  set  state  visibility    nonnull    state  visibility  state  visibility  this  state  visibility  state  visibility  return  this    nonnull  public    builder  return  expired  if  not  cleaned  up  return  set  state  visibility    state  visibility    return  expired  if  not  cleaned  up    nonnull  public    builder  never  return  expired  return  set  state  visibility    state  visibility    never  return  expired    sets  the  time  characteristic  param  ttl  time  characteristic    the  time  characteristic  configures  time  scale  to  use  for  ttl    nonnull  public    builder  set  ttl  time  characteristic    nonnull    ttl  time  characteristic  ttl  time  characteristic  this  ttl  time  characteristic  ttl  time  characteristic  return  this    nonnull  public    builder  use  processing  time  return  set  ttl  time  characteristic    processing  time    cleanup  expired  state  in  full  snapshot  on  checkpoint    nonnull  public    builder  cleanup  full  snapshot  strategies  put    cleanup  strategies    strategies  full  state  scan  snapshot  empty  strategy  return  this    cleanup  expired  state  incrementally  cleanup  local  state  p    upon  every  state  access  this  cleanup  strategy  checks  a  bunch  of  state  keys  for  expiration  and  cleans  up  expired  ones    it  keeps  a  lazy  iterator  through  all  keys  with  relaxed  consistency  if  backend  supports  it    this  way  all  keys  should  be  regularly  checked  and  cleaned  eventually  over  time  if  any  state  is  constantly  being  accessed  p    additionally  to  the  incremental  cleanup  upon  state  access  it  can  also  run  per  every  record    caution  if  there  are  a  lot  of  registered  states  using  this  option  they  all  will  be  iterated  for  every  record  to  check  if  there  is  something  to  cleanup  p    note  if  no  access  happens  to  this  state  or  no  records  are  processed  in  case  of  code  run  cleanup  for  every  record  expired  state  will  persist  p    note    time  spent  for  the  incremental  cleanup  increases  record  processing  latency  p    note    at  the  moment  incremental  cleanup  is  implemented  only  for    heap  state  backend    setting  it  for    rocks  d  b  will  have  no  effect  p    note    if  heap  state  backend  is  used  with  synchronous  snapshotting  the  global  iterator  keeps  a  copy  of  all  keys  while  iterating  because  of  its  specific  implementation  which  does  not  support  concurrent  modifications    enabling  of  this  feature  will  increase  memory  consumption  then    asynchronous  snapshotting  does  not  have  this  problem  param  cleanup  size  max  number  of  keys  pulled  from  queue  for  clean  up  upon  state  touch  for  any  key  param  run  cleanup  for  every  record  run  incremental  cleanup  per  each  processed  record    nonnull  public    builder  cleanup  incrementally    nonnegative  int  cleanup  size  boolean  run  cleanup  for  every  record  strategies  put    cleanup  strategies    strategies  incremental  cleanup  new    incremental  cleanup  strategy  cleanup  size  run  cleanup  for  every  record  return  this    cleanup  expired  state  while    rocksdb  compaction  is  running  p    rocks  d  b  compaction  filter  will  query  current  timestamp  used  to  check  expiration  from    flink  every  time  after  processing  code  query  time  after  num  entries  number  of  state  entries    updating  the  timestamp  more  often  can  improve  cleanup  speed  but  it  decreases  compaction  performance  because  it  uses  jni  call  from  native  code  param  query  time  after  num  entries  number  of  state  entries  to  process  by  compaction  filter  before  updating  current  timestamp    nonnull  public    builder  cleanup  in  rocksdb  compact  filter  long  query  time  after  num  entries  strategies  put    cleanup  strategies    strategies  rocksdb  compaction  filter  new    rocksdb  compact  filter  cleanup  strategy  query  time  after  num  entries  return  this    disable  default  cleanup  of  expired  state  in  background  enabled  by  default  p    if  some  specific  cleanup  is  configured  e  g  link  cleanup  incrementally  int  boolean  or  link  cleanup  in  rocksdb  compact  filter  long  this  setting  does  not  disable  it    nonnull  public    builder  disable  cleanup  in  background  is  cleanup  in  background  false  return  this    sets  the  ttl  time  param  ttl    the  ttl  time    nonnull  public    builder  set  ttl    nonnull    time  ttl  this  ttl  ttl  return  this    nonnull  public    state  ttl  config  build  return  new    state  ttl  config  update  type  state  visibility  ttl  time  characteristic  ttl  new    cleanup  strategies  strategies  is  cleanup  in  background  ttl  cleanup  strategies  p    this  class  configures  when  to  cleanup  expired  state  with  ttl    by  default  state  is  always  cleaned  up  on  explicit  read  access  if  found  expired    currently  cleanup  of  state  full  snapshot  can  be  additionally  activated  public  static  class    cleanup  strategies  implements    serializable  private  static  final  long  serial  version  u  i  d    l  static  final    cleanup  strategy  empty  strategy  new    empty  cleanup  strategy  private  final  boolean  is  cleanup  in  background  private  final    enum  map    strategies    cleanup  strategy  strategies    fixed  strategies  ordinals  in  code  strategies  config  field  enum    strategies  full  state  scan  snapshot  incremental  cleanup  rocksdb  compaction  filter    base  interface  for  cleanup  strategies  configurations  interface    cleanup  strategy  extends    serializable  static  class    empty  cleanup  strategy  implements    cleanup  strategy  private  static  final  long  serial  version  u  i  d    l  private    cleanup  strategies    enum  map    strategies    cleanup  strategy  strategies  boolean  is  cleanup  in  background  this  strategies  strategies  this  is  cleanup  in  background  is  cleanup  in  background  public  boolean  in  full  snapshot  return  strategies  contains  key    strategies  full  state  scan  snapshot  public  boolean  is  cleanup  in  background  return  is  cleanup  in  background    nullable  public    incremental  cleanup  strategy  get  incremental  cleanup  strategy    incremental  cleanup  strategy  default  strategy  is  cleanup  in  background  default  incremental  cleanup  strategy  null  return    incremental  cleanup  strategy  strategies  get  or  default    strategies  incremental  cleanup  default  strategy  public  boolean  in  rocksdb  compact  filter  return  get  rocksdb  compact  filter  cleanup  strategy  null    nullable  public    rocksdb  compact  filter  cleanup  strategy  get  rocksdb  compact  filter  cleanup  strategy    rocksdb  compact  filter  cleanup  strategy  default  strategy  is  cleanup  in  background  default  rocksdb  compact  filter  cleanup  strategy  null  return    rocksdb  compact  filter  cleanup  strategy  strategies  get  or  default    strategies  rocksdb  compaction  filter  default  strategy    configuration  of  cleanup  strategy  while  taking  the  full  snapshot  public  static  class    incremental  cleanup  strategy  implements    cleanup  strategies    cleanup  strategy  private  static  final  long  serial  version  u  i  d    l  static  final    incremental  cleanup  strategy  default  incremental  cleanup  strategy  new    incremental  cleanup  strategy    false    max  number  of  keys  pulled  from  queue  for  clean  up  upon  state  touch  for  any  key  private  final  int  cleanup  size    whether  to  run  incremental  cleanup  per  each  processed  record  private  final  boolean  run  cleanup  for  every  record  private    incremental  cleanup  strategy  int  cleanup  size  boolean  run  cleanup  for  every  record    preconditions  check  argument  cleanup  size      number  of  incrementally  cleaned  up  state  entries  cannot  be  negative  this  cleanup  size  cleanup  size  this  run  cleanup  for  every  record  run  cleanup  for  every  record  public  int  get  cleanup  size  return  cleanup  size  public  boolean  run  cleanup  for  every  record  return  run  cleanup  for  every  record    configuration  of  cleanup  strategy  using  custom  compaction  filter  in    rocks  d  b  public  static  class    rocksdb  compact  filter  cleanup  strategy  implements    cleanup  strategies    cleanup  strategy  private  static  final  long  serial  version  u  i  d    l  static  final    rocksdb  compact  filter  cleanup  strategy  default  rocksdb  compact  filter  cleanup  strategy  new    rocksdb  compact  filter  cleanup  strategy    l    number  of  state  entries  to  process  by  compaction  filter  before  updating  current  timestamp  private  final  long  query  time  after  num  entries  private    rocksdb  compact  filter  cleanup  strategy  long  query  time  after  num  entries  this  query  time  after  num  entries  query  time  after  num  entries  public  long  get  query  time  after  num  entries  return  query  time  after  num  entries  
public  evolving  public  interface    value  state  t  extends    state    returns  the  current  value  for  the  state    when  the  state  is  not  partitioned  the  returned  value  is  the  same  for  all  inputs  in  a  given  operator  instance    if  state  partitioning  is  applied  the  value  returned  depends  on  the  current  operator  input  as  the  operator  maintains  an  independent  state  for  each  partition  p    if  you  didn  t  specify  a  default  value  when  creating  the  link    value  state  descriptor  this  will  return  code  null  when  to  value  was  previously  set  using  link  update    object  return    the  state  value  corresponding  to  the  current  input  throws    i  o  exception    thrown  if  the  system  cannot  access  the  state  t  value  throws    i  o  exception    updates  the  operator  state  accessible  by  link  value  to  the  given  value    the  next  time  link  value  is  called  for  the  same  state  partition  the  returned  state  will  represent  the  updated  value    when  a  partitioned  state  is  updated  with  null  the  state  for  the  current  key  will  be  removed  and  the  default  value  is  returned  on  the  next  access  param  value    the  new  value  for  the  state  throws    i  o  exception    thrown  if  the  system  cannot  access  the  state  void  update  t  value  throws    i  o  exception  
public  evolving  public  class    value  state  descriptor  t  extends    state  descriptor    value  state  t  t  private  static  final  long  serial  version  u  i  d  1  l    creates  a  new  code    value  state  descriptor  with  the  given  name  type  and  default  value  p    if  this  constructor  fails  because  it  is  not  possible  to  describe  the  type  via  a  class  consider  using  the  link    value  state  descriptor    string    type  information    object  constructor  deprecated    use  link    value  state  descriptor    string    class  instead  and  manually  manage  the  default  value  by  checking  whether  the  contents  of  the  state  is  code  null  param  name    the  unique  name  for  the  state  param  type  class    the  type  of  the  values  in  the  state  param  default  value    the  default  value  that  will  be  set  when  requesting  state  without  setting  a  value  before    deprecated  public    value  state  descriptor    string  name    class  t  type  class  t  default  value  super  name  type  class  default  value    creates  a  new  code    value  state  descriptor  with  the  given  name  and  default  value  deprecated    use  link    value  state  descriptor    string    type  information  instead  and  manually  manage  the  default  value  by  checking  whether  the  contents  of  the  state  is  code  null  param  name    the  unique  name  for  the  state  param  type  info    the  type  of  the  values  in  the  state  param  default  value    the  default  value  that  will  be  set  when  requesting  state  without  setting  a  value  before    deprecated  public    value  state  descriptor    string  name    type  information  t  type  info  t  default  value  super  name  type  info  default  value    creates  a  new  code    value  state  descriptor  with  the  given  name  default  value  and  the  specific  serializer  deprecated    use  link    value  state  descriptor    string    type  serializer  instead  and  manually  manage  the  default  value  by  checking  whether  the  contents  of  the  state  is  code  null  param  name    the  unique  name  for  the  state  param  type  serializer    the  type  serializer  of  the  values  in  the  state  param  default  value    the  default  value  that  will  be  set  when  requesting  state  without  setting  a  value  before    deprecated  public    value  state  descriptor    string  name    type  serializer  t  type  serializer  t  default  value  super  name  type  serializer  default  value    creates  a  new  code    value  state  descriptor  with  the  given  name  and  type  p    if  this  constructor  fails  because  it  is  not  possible  to  describe  the  type  via  a  class  consider  using  the  link    value  state  descriptor    string    type  information  constructor  param  name    the  unique  name  for  the  state  param  type  class    the  type  of  the  values  in  the  state  public    value  state  descriptor    string  name    class  t  type  class  super  name  type  class  null    creates  a  new  code    value  state  descriptor  with  the  given  name  and  type  param  name    the  unique  name  for  the  state  param  type  info    the  type  of  the  values  in  the  state  public    value  state  descriptor    string  name    type  information  t  type  info  super  name  type  info  null    creates  a  new  code    value  state  descriptor  with  the  given  name  and  the  specific  serializer  param  name    the  unique  name  for  the  state  param  type  serializer    the  type  serializer  of  the  values  in  the  state  public    value  state  descriptor    string  name    type  serializer  t  type  serializer  super  name  type  serializer  null    override  public    type  get  type  return    type  value  
public  evolving  public  final  class    time  implements    serializable  private  static  final  long  serial  version  u  i  d    l    the  time  unit  for  this  policy  s  time  interval  private  final    time  unit  unit    the  size  of  the  windows  generated  by  this  policy  private  final  long  size    instantiation  only  via  factory  method  private    time  long  size    time  unit  unit  this  unit  check  not  null  unit  time  unit  may  not  be  null  this  size  size    properties    gets  the  time  unit  for  this  policy  s  time  interval  return    the  time  unit  for  this  policy  s  time  interval  public    time  unit  get  unit  return  unit    gets  the  length  of  this  policy  s  time  interval  return    the  length  of  this  policy  s  time  interval  public  long  get  size  return  size    converts  the  time  interval  to  milliseconds  return    the  time  interval  in  milliseconds  public  long  to  milliseconds  return  unit  to  millis  size    override  public    string  to  string  return  to  milliseconds  ms    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    time  time    time  o  return  to  milliseconds  time  to  milliseconds    override  public  int  hash  code  return    objects  hash  to  milliseconds    factory    creates  a  new  link    time  of  the  given  duration  and  link    time  unit  param  size    the  duration  of  time  param  unit    the  unit  of  time  of  the  duration  for  example  code    time  unit  seconds  return    the  time  policy  public  static    time  of  long  size    time  unit  unit  return  new    time  size  unit    creates  a  new  link    time  that  represents  the  given  number  of  milliseconds  public  static    time  milliseconds  long  milliseconds  return  of  milliseconds    time  unit  milliseconds    creates  a  new  link    time  that  represents  the  given  number  of  seconds  public  static    time  seconds  long  seconds  return  of  seconds    time  unit  seconds    creates  a  new  link    time  that  represents  the  given  number  of  minutes  public  static    time  minutes  long  minutes  return  of  minutes    time  unit  minutes    creates  a  new  link    time  that  represents  the  given  number  of  hours  public  static    time  hours  long  hours  return  of  hours    time  unit  hours    creates  a  new  link    time  that  represents  the  given  number  of  days  public  static    time  days  long  days  return  of  days    time  unit  days  
public  evolving  public  class    local  time  type  info  t  extends    temporal  extends    type  information  t  implements    atomic  type  t  private  static  final  long  serial  version  u  i  d  1  l  public  static  final    local  time  type  info    local  date  local  date  new    local  time  type  info    local  date  class    local  date  serializer  instance    local  date  comparator  class  public  static  final    local  time  type  info    local  time  local  time  new    local  time  type  info    local  time  class    local  time  serializer  instance    local  time  comparator  class  public  static  final    local  time  type  info    local  date  time  local  date  time  new    local  time  type  info    local  date  time  class    local  date  time  serializer  instance    local  date  time  comparator  class  private  final    class  t  clazz  private  final    type  serializer  t  serializer  private  final    class  extends    type  comparator  t  comparator  class  protected    local  time  type  info    class  t  clazz    type  serializer  t  serializer    class  extends    type  comparator  t  comparator  class  this  clazz  check  not  null  clazz  this  serializer  check  not  null  serializer  this  comparator  class  check  not  null  comparator  class    override  public  boolean  is  basic  type  return  false    override  public  boolean  is  tuple  type  return  false    override  public  int  get  arity  return      override  public  int  get  total  fields  return      override  public    class  t  get  type  class  return  clazz    override  public  boolean  is  key  type  return  true    override  public    type  serializer  t  create  serializer    execution  config  execution  config  return  serializer    override  public    type  comparator  t  create  comparator  boolean  sort  order  ascending    execution  config  execution  config  return  instantiate  comparator  comparator  class  sort  order  ascending    override  public  int  hash  code  return    objects  hash  clazz  serializer  comparator  class    override  public  boolean  can  equal    object  obj  return  obj  instanceof    local  time  type  info    override  public  boolean  equals    object  obj  if  obj  instanceof    local  time  type  info    suppress  warnings  unchecked    local  time  type  info  t  other    local  time  type  info  t  obj  return  other  can  equal  this  this  clazz  other  clazz  serializer  equals  other  serializer  this  comparator  class  other  comparator  class  else  return  false    override  public    string  to  string  return  clazz  get  simple  name  private  static  x    type  comparator  x  instantiate  comparator    class  extends    type  comparator  x  comparator  class  boolean  ascending  order  try    constructor  extends    type  comparator  x  constructor  comparator  class  get  constructor  boolean  class  return  constructor  new  instance  ascending  order  catch    exception  e  throw  new    runtime  exception    could  not  initialize  comparator  comparator  class  get  name  e  public  static    local  time  type  info  get  info  for    class  type  check  not  null  type  if  type    local  date  class  return  local  date  else  if  type    local  time  class  return  local  time  else  if  type    local  date  time  class  return  local  date  time  return  null  
public  evolving  public  class    sql  time  type  info  t  extends    type  information  t  implements    atomic  type  t  private  static  final  long  serial  version  u  i  d    l    suppress  warnings  rawtypes  unchecked  public  static  final    sql  time  type  info    date  date  new    sql  time  type  info    date  class    sql  date  serializer  instance    class    date  comparator  class    suppress  warnings  rawtypes  unchecked  public  static  final    sql  time  type  info    time  time  new    sql  time  type  info    time  class    sql  time  serializer  instance    class    date  comparator  class    suppress  warnings  rawtypes  unchecked  public  static  final    sql  time  type  info    timestamp  timestamp  new    sql  time  type  info    timestamp  class    sql  timestamp  serializer  instance    class    sql  timestamp  comparator  class  private  final    class  t  clazz  private  final    type  serializer  t  serializer  private  final    class  extends    type  comparator  t  comparator  class  protected    sql  time  type  info    class  t  clazz    type  serializer  t  serializer    class  extends    type  comparator  t  comparator  class  this  clazz  check  not  null  clazz  this  serializer  check  not  null  serializer  this  comparator  class  check  not  null  comparator  class    override  public  boolean  is  basic  type  return  false    override  public  boolean  is  tuple  type  return  false    override  public  int  get  arity  return      override  public  int  get  total  fields  return      override  public    class  t  get  type  class  return  clazz    override  public  boolean  is  key  type  return  true    override  public    type  serializer  t  create  serializer    execution  config  execution  config  return  serializer    override  public    type  comparator  t  create  comparator  boolean  sort  order  ascending    execution  config  execution  config  return  instantiate  comparator  comparator  class  sort  order  ascending    override  public  int  hash  code  return    objects  hash  clazz  serializer  comparator  class    override  public  boolean  can  equal    object  obj  return  obj  instanceof    sql  time  type  info    override  public  boolean  equals    object  obj  if  obj  instanceof    sql  time  type  info    suppress  warnings  unchecked    sql  time  type  info  t  other    sql  time  type  info  t  obj  return  other  can  equal  this  this  clazz  other  clazz  serializer  equals  other  serializer  this  comparator  class  other  comparator  class  else  return  false    override  public    string  to  string  return  clazz  get  simple  name  private  static  x    type  comparator  x  instantiate  comparator    class  extends    type  comparator  x  comparator  class  boolean  ascending  order  try    constructor  extends    type  comparator  x  constructor  comparator  class  get  constructor  boolean  class  return  constructor  new  instance  ascending  order  catch    exception  e  throw  new    runtime  exception    could  not  initialize  comparator  comparator  class  get  name  e    suppress  warnings  unchecked  public  static  x    sql  time  type  info  x  get  info  for    class  x  type  check  not  null  type  if  type    date  class  return    sql  time  type  info  x  date  else  if  type    time  class  return    sql  time  type  info  x  time  else  if  type    timestamp  class  return    sql  time  type  info  x  timestamp  return  null  
public  evolving  public  class    types    returns  type  information  for  link  java  lang    void    does  not  support  a  null  value  public  static  final    type  information    void  void    basic  type  info  void  type  info    returns  type  information  for  link  java  lang    string    supports  a  null  value  public  static  final    type  information    string  string    basic  type  info  string  type  info    returns  type  information  for  both  a  primitive  code  byte  code  and  link  java  lang    byte    does  not  support  a  null  value  public  static  final    type  information    byte  byte    basic  type  info  byte  type  info    returns  type  information  for  both  a  primitive  code  boolean  code  and  link  java  lang    boolean    does  not  support  a  null  value  public  static  final    type  information    boolean  boolean    basic  type  info  boolean  type  info    returns  type  information  for  both  a  primitive  code  short  code  and  link  java  lang    short    does  not  support  a  null  value  public  static  final    type  information    short  short    basic  type  info  short  type  info    returns  type  information  for  both  a  primitive  code  int  code  and  link  java  lang    integer    does  not  support  a  null  value  public  static  final    type  information    integer  int    basic  type  info  int  type  info    returns  type  information  for  both  a  primitive  code  long  code  and  link  java  lang    long    does  not  support  a  null  value  public  static  final    type  information    long  long    basic  type  info  long  type  info    returns  type  information  for  both  a  primitive  code  float  code  and  link  java  lang    float    does  not  support  a  null  value  public  static  final    type  information    float  float    basic  type  info  float  type  info    returns  type  information  for  both  a  primitive  code  double  code  and  link  java  lang    double    does  not  support  a  null  value  public  static  final    type  information    double  double    basic  type  info  double  type  info    returns  type  information  for  both  a  primitive  code  char  code  and  link  java  lang    character    does  not  support  a  null  value  public  static  final    type  information    character  char    basic  type  info  char  type  info    returns  type  information  for  link  java  math    big  decimal    supports  a  null  value  public  static  final    type  information    big  decimal  big  dec    basic  type  info  big  dec  type  info    returns  type  information  for  link  java  math    big  integer    supports  a  null  value  public  static  final    type  information    big  integer  big  int    basic  type  info  big  int  type  info    returns  type  information  for  link  java  sql    date    supports  a  null  value  public  static  final    type  information    date  sql  date    sql  time  type  info  date    returns  type  information  for  link  java  sql    time    supports  a  null  value  public  static  final    type  information    time  sql  time    sql  time  type  info  time    returns  type  information  for  link  java  sql    timestamp    supports  a  null  value  public  static  final    type  information    timestamp  sql  timestamp    sql  time  type  info  timestamp    returns  type  information  for  link  java  time    local  date    supports  a  null  value  public  static  final    type  information    local  date  local  date    local  time  type  info  local  date    returns  type  information  for  link  java  time    local  time    supports  a  null  value  public  static  final    type  information    local  time  local  time    local  time  type  info  local  time    returns  type  information  for  link  java  time    local  date  time    supports  a  null  value  public  static  final    type  information    local  date  time  local  date  time    local  time  type  info  local  date  time    returns  type  infomation  for  link  java  time    instant    supports  a  null  value  public  static  final    type  information    instant  instant    basic  type  info  instant  type  info  checkstyle  off    method  name    returns  type  information  for  link  org  apache  flink  types    row  with  fields  of  the  given  types  a  row  itself  must  not  be  null  p  a  row  is  a  fixed  length  null  aware  composite  type  for  storing  multiple  values  in  a  deterministic  field  order    every  field  can  be  null  regardless  of  the  field  s  type    the  type  of  row  fields  cannot  be  automatically  inferred  therefore  it  is  required  to  provide  type  information  whenever  a  row  is  produced  p    the  schema  of  rows  can  have  up  to  code    integer  max  value  code  fields  however  all  row  instances  must  strictly  adhere  to  the  schema  defined  by  the  type  info  p    this  method  generates  type  information  with  fields  of  the  given  types  the  fields  have  the  default  names  f0  f1  f2  param  types    the  types  of  the  row  fields  e  g    types  string    types  int  public  static    type  information    row  row    type  information  types  return  new    row  type  info  types    returns  type  information  for  link  org  apache  flink  types    row  with  fields  of  the  given  types  and  with  given  names  a  row  must  not  be  null  p  a  row  is  a  fixed  length  null  aware  composite  type  for  storing  multiple  values  in  a  deterministic  field  order    every  field  can  be  null  independent  of  the  field  s  type    the  type  of  row  fields  cannot  be  automatically  inferred  therefore  it  is  required  to  provide  type  information  whenever  a  row  is  used  p    the  schema  of  rows  can  have  up  to  code    integer  max  value  code  fields  however  all  row  instances  must  strictly  adhere  to  the  schema  defined  by  the  type  info  p    example  use  code  row  named  new    string  name  number    types  string    types  int  param  field  names  array  of  field  names  param  types  array  of  field  types  public  static    type  information    row  row  named    string  field  names    type  information  types  return  new    row  type  info  types  field  names    returns  type  information  for  subclasses  of    flink  s  link  org  apache  flink  api  java  tuple    tuple  namely  link  org  apache  flink  api  java  tuple    tuple0  till  link  org  apache  flink  api  java  tuple    tuple25  with  fields  of  the  given  types  a  tuple  must  not  be  null  p  a  tuple  is  a  fixed  length  composite  type  for  storing  multiple  values  in  a  deterministic  field  order    fields  of  a  tuple  are  typed    tuples  are  the  most  efficient  composite  type  a  tuple  does  not  support  null  valued  fields  unless  the  type  of  the  field  supports  nullability  param  types    the  types  of  the  tuple  fields  e  g    types  string    types  int  public  static  t  extends    tuple    type  information  t  tuple    type  information  types  return  new    tuple  type  info  types    returns  type  information  for  typed  subclasses  of    flink  s  link  org  apache  flink  api  java  tuple    tuple    typed  subclassed  are  classes  that  extend  link  org  apache  flink  api  java  tuple    tuple0  till  link  org  apache  flink  api  java  tuple    tuple25  to  provide  types  for  all  fields  and  might  add  additional  getters  and  setters  for  better  readability    additional  member  fields  must  not  be  added  a  tuple  must  not  be  null  p  a  tuple  is  a  fixed  length  composite  type  for  storing  multiple  values  in  a  deterministic  field  order    fields  of  a  tuple  are  typed    tuples  are  the  most  efficient  composite  type  a  tuple  does  not  support  null  valued  fields  unless  the  type  of  the  field  supports  nullability  p    the  generic  types  for  all  fields  of  the  tuple  can  be  defined  in  a  hierarchy  of  subclasses  p    if    flink  s  type  analyzer  is  unable  to  extract  a  tuple  type  information  with  type  information  for  all  fields  an  link  org  apache  flink  api  common  functions    invalid  types  exception  is  thrown  p    example  use  pre  code  class    my  tuple  extends    tuple2    integer    string  public  int  get  id  return  f0  public    string  get  name  return  f1    types  tuple    my  tuple  class  pre  param  tuple  subclass  a  subclass  of  link  org  apache  flink  api  java  tuple    tuple0  till  link  org  apache  flink  api  java  tuple    tuple25  that  defines  all  field  types  and  does  not  add  any  additional  fields  public  static  t  extends    tuple    type  information  t  tuple    class  t  tuple  subclass  final    type  information  t  ti    type  extractor  create  type  info  tuple  subclass  if  ti  instanceof    tuple  type  info  return  ti  throw  new    invalid  types  exception    tuple  type  expected  but  was  ti    returns  type  information  for  a  pojo    plain    old    java    object  p  a  pojo  class  is  public  and  standalone  no  non  static  inner  class    it  has  a  public  no  argument  constructor    all  non  static  non  transient  fields  in  the  class  and  all  superclasses  are  either  public  and  non  final  or  have  a  public  getter  and  a  setter  method  that  follows  the    java  beans  naming  conventions  for  getters  and  setters  p  a  pojo  is  a  fixed  length  and  null  aware  composite  type    every  field  can  be  null  independent  of  the  field  s  type  p    the  generic  types  for  all  fields  of  the  pojo  can  be  defined  in  a  hierarchy  of  subclasses  p    if    flink  s  type  analyzer  is  unable  to  extract  a  valid  pojo  type  information  with  type  information  for  all  fields  an  link  org  apache  flink  api  common  functions    invalid  types  exception  is  thrown    alternatively  you  can  use  link    types  pojo    class    map  to  specify  all  fields  manually  param  pojo  class  pojo  class  to  be  analyzed  by    flink  public  static  t    type  information  t  pojo    class  t  pojo  class  final    type  information  t  ti    type  extractor  create  type  info  pojo  class  if  ti  instanceof    pojo  type  info  return  ti  throw  new    invalid  types  exception  pojo  type  expected  but  was  ti    returns  type  information  for  a  pojo    plain    old    java    object  and  allows  to  specify  all  fields  manually  p  a  pojo  class  is  public  and  standalone  no  non  static  inner  class    it  has  a  public  no  argument  constructor    all  non  static  non  transient  fields  in  the  class  and  all  superclasses  are  either  public  and  non  final  or  have  a  public  getter  and  a  setter  method  that  follows  the    java  beans  naming  conventions  for  getters  and  setters  p  a  pojo  is  a  fixed  length  null  aware  composite  type  with  non  deterministic  field  order    every  field  can  be  null  independent  of  the  field  s  type  p    the  generic  types  for  all  fields  of  the  pojo  can  be  defined  in  a  hierarchy  of  subclasses  p    if    flink  s  type  analyzer  is  unable  to  extract  a  pojo  field  an  link  org  apache  flink  api  common  functions    invalid  types  exception  is  thrown  p  strong    note  strong    in  most  cases  the  type  information  of  fields  can  be  determined  automatically  we  recommend  to  use  link    types  pojo    class  param  pojo  class  pojo  class  param  fields  map  of  fields  that  map  a  name  to  type  information    the  map  key  is  the  name  of  the  field  and  the  value  is  its  type  public  static  t    type  information  t  pojo    class  t  pojo  class    map    string    type  information  fields  final    list    pojo  field  pojo  fields  new    array  list  fields  size  for    map    entry    string    type  information  field  fields  entry  set  final    field  f    type  extractor  get  declared  field  pojo  class  field  get  key  if  f  null  throw  new    invalid  types  exception    field  field  get  key  could  not  be  accessed  pojo  fields  add  new    pojo  field  f  field  get  value  return  new    pojo  type  info  pojo  class  pojo  fields    returns  generic  type  information  for  any    java  object    the  serialization  logic  will  use  the  general  purpose  serializer    kryo  p    generic  types  are  black  boxes  for    flink  but  allow  any  object  and  null  values  in  fields  p    by  default  serialization  of  this  type  is  not  very  efficient    please  read  the  documentation  about  how  to  improve  efficiency  namely  by  pre  registering  classes  param  generic  class  any    java  class  public  static  t    type  information  t  generic    class  t  generic  class  return  new    generic  type  info  generic  class    returns  type  information  for    java  arrays  of  primitive  type  such  as  code  byte  code    the  array  must  not  be  null  param  element  type  element  type  of  the  array  e  g    types  boolean    types  int    types  double  public  static    type  information  primitive  array    type  information  element  type  if  element  type  boolean  return    primitive  array  type  info  boolean  primitive  array  type  info  else  if  element  type  byte  return    primitive  array  type  info  byte  primitive  array  type  info  else  if  element  type  short  return    primitive  array  type  info  short  primitive  array  type  info  else  if  element  type  int  return    primitive  array  type  info  int  primitive  array  type  info  else  if  element  type  long  return    primitive  array  type  info  long  primitive  array  type  info  else  if  element  type  float  return    primitive  array  type  info  float  primitive  array  type  info  else  if  element  type  double  return    primitive  array  type  info  double  primitive  array  type  info  else  if  element  type  char  return    primitive  array  type  info  char  primitive  array  type  info  throw  new    illegal  argument  exception    invalid  element  type  for  a  primitive  array    returns  type  information  for    java  arrays  of  object  types  such  as  code    string  code  code    integer  code    the  array  itself  must  not  be  null    null  values  for  elements  are  supported  param  element  type  element  type  of  the  array    suppress  warnings  unchecked  public  static  e    type  information  e  object  array    type  information  e  element  type  if  element  type    types  string  return    type  information    basic  array  type  info  string  array  type  info  return    object  array  type  info  get  info  for  element  type    returns  type  information  for    flink  value  types  classes  that  implement  link  org  apache  flink  types    value    built  in  value  types  do  not  support  null  values  except  for  link  org  apache  flink  types    string  value  p    value  types  describe  their  serialization  and  deserialization  manually    instead  of  going  through  a  general  purpose  serialization  framework  a  value  type  is  reasonable  when  general  purpose  serialization  would  be  highly  inefficient    the  wrapped  value  can  be  altered  allowing  programmers  to  reuse  objects  and  take  pressure  off  the  garbage  collector  p    flink  provides  built  in  value  types  for  all    java  primitive  types  such  as  link  org  apache  flink  types    boolean  value  link  org  apache  flink  types    int  value  as  well  as  link  org  apache  flink  types    string  value  link  org  apache  flink  types    null  value  link  org  apache  flink  types    list  value  and  link  org  apache  flink  types    map  value  param  value  type  class  that  implements  link  org  apache  flink  types    value  public  static  v  extends    value    type  information  v  value    class  v  value  type  return  new    value  type  info  value  type    returns  type  information  for  a    java  link  java  util    map  a  map  must  not  be  null    null  values  in  keys  are  not  supported    an  entry  s  value  can  be  null  p    by  default  maps  are  untyped  and  treated  as  a  generic  type  in    flink  therefore  it  is  useful  to  pass  type  information  whenever  a  map  is  used  p  strong    note  strong    flink  does  not  preserve  the  concrete  link    map  type    it  converts  a  map  into  link    hash  map  when  copying  or  deserializing  param  key  type  type  information  for  the  map  s  keys  param  value  type  type  information  for  the  map  s  values  public  static  k  v    type  information    map  k  v  map    type  information  k  key  type    type  information  v  value  type  return  new    map  type  info  key  type  value  type    returns  type  information  for  a    java  link  java  util    list  a  list  must  not  be  null    null  values  in  elements  are  not  supported  p    by  default  lists  are  untyped  and  treated  as  a  generic  type  in    flink  therefore  it  is  useful  to  pass  type  information  whenever  a  list  is  used  p  strong    note  strong    flink  does  not  preserve  the  concrete  link    list  type    it  converts  a  list  into  link    array  list  when  copying  or  deserializing  param  element  type  type  information  for  the  list  s  elements  public  static  e    type  information    list  e  list    type  information  e  element  type  return  new    list  type  info  element  type    returns  type  information  for    java  enumerations    null  values  are  not  supported  param  enum  type  enumeration  class  extending  link  java  lang    enum  public  static  e  extends    enum  e    type  information  e  enum    class  e  enum  type  return  new    enum  type  info  enum  type    returns  type  information  for    flink  s  link  org  apache  flink  types    either  type    null  values  are  not  supported  p    either  type  can  be  used  for  a  value  of  two  possible  types  p    example  use  code    types  either    types  void    types  int  code  param  left  type  type  information  of  left  side  link  org  apache  flink  types    either    left  param  right  type  type  information  of  right  side  link  org  apache  flink  types    either    right  public  static  l  r    type  information    either  l  r  either    type  information  l  left  type    type  information  r  right  type  return  new    either  type  info  left  type  right  type  checkstyle  on    method  name  
public  evolving  public  abstract  class    composite  type  serializer  snapshot  t  s  extends    type  serializer  t  implements    type  serializer  snapshot  t    indicates  schema  compatibility  of  the  serializer  configuration  persisted  as  the  outer  snapshot  protected  enum    outer  schema  compatibility  compatible  as  is  compatible  after  migration  incompatible    magic  number  for  integrity  checks  during  deserialization  private  static  final  int  magic  number      current  version  of  the  base  serialization  format  p  note    we  start  from  version      this  version  is  represented  by  the  link  get  current  version  method    previously  this  method  was  used  to  represent  the  outer  snapshot  s  version  now  represented  by  the  link  get  current  outer  snapshot  version  method  p    to  bridge  this  transition  we  set  the  starting  version  of  the  base  format  to  be  at  least  larger  than  the  highest  version  of  previously  defined  values  in  implementing  subclasses  which  was  link  highest  legacy  read  version    this  allows  us  to  identify  legacy  deserialization  paths  which  did  not  contain  versioning  for  the  base  format  simply  by  checking  if  the  read  version  of  the  snapshot  is  smaller  than  or  equal  to  link  highest  legacy  read  version  private  static  final  int  version    private  static  final  int  highest  legacy  read  version    private    nested  serializers  snapshot  delegate  nested  serializers  snapshot  delegate  private  final    class  s  corresponding  serializer  class    constructor  to  be  used  for  read  instantiation  param  corresponding  serializer  class  the  expected  class  of  the  new  serializer    suppress  warnings  unchecked  public    composite  type  serializer  snapshot    class  extends    type  serializer  corresponding  serializer  class  this  corresponding  serializer  class    class  s  check  not  null  corresponding  serializer  class    constructor  to  be  used  for  writing  the  snapshot  param  serializer  instance  an  instance  of  the  originating  serializer  of  this  snapshot    suppress  warnings  unchecked  public    composite  type  serializer  snapshot  s  serializer  instance  check  not  null  serializer  instance  this  nested  serializers  snapshot  delegate  new    nested  serializers  snapshot  delegate  get  nested  serializers  serializer  instance  this  corresponding  serializer  class    class  s  serializer  instance  get  class    override  public  final  int  get  current  version  return  version    override  public  final  void  write  snapshot    data  output  view  out  throws    i  o  exception  internal  write  outer  snapshot  out  nested  serializers  snapshot  delegate  write  nested  serializer  snapshots  out    override  public  final  void  read  snapshot  int  read  version    data  input  view  in    class  loader  user  code  class  loader  throws    i  o  exception  if  read  version  highest  legacy  read  version  internal  read  outer  snapshot  in  user  code  class  loader  else  legacy  internal  read  outer  snapshot  read  version  in  user  code  class  loader  this  nested  serializers  snapshot  delegate    nested  serializers  snapshot  delegate  read  nested  serializer  snapshots  in  user  code  class  loader  public    type  serializer  snapshot  get  nested  serializer  snapshots  return  nested  serializers  snapshot  delegate  get  nested  serializer  snapshots    override  public  final    type  serializer  schema  compatibility  t  resolve  schema  compatibility    type  serializer  t  new  serializer  return  internal  resolve  schema  compatibility  new  serializer  nested  serializers  snapshot  delegate  get  nested  serializer  snapshots    internal    type  serializer  schema  compatibility  t  internal  resolve  schema  compatibility    type  serializer  t  new  serializer    type  serializer  snapshot  snapshots  if  new  serializer  get  class  corresponding  serializer  class  return    type  serializer  schema  compatibility  incompatible  s  casted  new  serializer  corresponding  serializer  class  cast  new  serializer  final    outer  schema  compatibility  outer  schema  compatibility  resolve  outer  schema  compatibility  casted  new  serializer  final    type  serializer  new  nested  serializers  get  nested  serializers  casted  new  serializer  check  that  nested  serializer  arity  remains  identical  if  not  short  circuit  result  if  new  nested  serializers  length  snapshots  length  return    type  serializer  schema  compatibility  incompatible  return  construct  final  schema  compatibility  result  new  nested  serializers  snapshots  outer  schema  compatibility    internal  void  set  nested  serializers  snapshot  delegate    nested  serializers  snapshot  delegate  delegate  this  nested  serializers  snapshot  delegate  check  not  null  delegate    override  public  final    type  serializer  t  restore  serializer    suppress  warnings  unchecked    type  serializer  t  serializer    type  serializer  t  create  outer  serializer  with  nested  serializers  nested  serializers  snapshot  delegate  get  restored  nested  serializers  return  serializer    outer  serializer  access  methods    returns  the  version  of  the  current  outer  snapshot  s  written  binary  format  return  the  version  of  the  current  outer  snapshot  s  written  binary  format  protected  abstract  int  get  current  outer  snapshot  version    gets  the  nested  serializers  from  the  outer  serializer  param  outer  serializer  the  outer  serializer  return  the  nested  serializers  protected  abstract    type  serializer  get  nested  serializers  s  outer  serializer    creates  an  instance  of  the  outer  serializer  with  a  given  array  of  its  nested  serializers  param  nested  serializers  array  of  nested  serializers  to  create  the  outer  serializer  with  return  an  instance  of  the  outer  serializer  protected  abstract  s  create  outer  serializer  with  nested  serializers    type  serializer  nested  serializers    outer  snapshot  methods  need  to  be  overridden  if  outer  snapshot  is  not  empty  or  in  other  words  the  outer  serializer  has  extra  configuration  beyond  its  nested  serializers    writes  the  outer  snapshot  i  e  any  information  beyond  the  nested  serializers  of  the  outer  serializer  p    the  base  implementation  of  this  methods  writes  nothing  i  e  it  assumes  that  the  outer  serializer  only  has  nested  serializers  and  no  extra  information    otherwise  if  the  outer  serializer  contains  some  extra  information  that  needs  to  be  persisted  as  part  of  the  serializer  snapshot  this  must  be  overridden    note  that  this  method  and  the  corresponding  methods  link  read  outer  snapshot  int    data  input  view    class  loader  link  resolve  outer  schema  compatibility    type  serializer  needs  to  be  implemented  param  out  the  link    data  output  view  to  write  the  outer  snapshot  to  protected  void  write  outer  snapshot    data  output  view  out  throws    i  o  exception    reads  the  outer  snapshot  i  e  any  information  beyond  the  nested  serializers  of  the  outer  serializer  p    the  base  implementation  of  this  methods  reads  nothing  i  e  it  assumes  that  the  outer  serializer  only  has  nested  serializers  and  no  extra  information    otherwise  if  the  outer  serializer  contains  some  extra  information  that  has  been  persisted  as  part  of  the  serializer  snapshot  this  must  be  overridden    note  that  this  method  and  the  corresponding  methods  link  write  outer  snapshot    data  output  view  link  resolve  outer  schema  compatibility    type  serializer  needs  to  be  implemented  param  read  outer  snapshot  version  the  read  version  of  the  outer  snapshot  param  in  the  link    data  input  view  to  read  the  outer  snapshot  from  param  user  code  class  loader  the  user  code  class  loader  protected  void  read  outer  snapshot  int  read  outer  snapshot  version    data  input  view  in    class  loader  user  code  class  loader  throws    i  o  exception    checks  whether  the  outer  snapshot  is  compatible  with  a  given  new  serializer  p    the  base  implementation  of  this  method  just  returns  code  true  i  e  it  assumes  that  the  outer  serializer  only  has  nested  serializers  and  no  extra  information  and  therefore  the  result  of  the  check  must  always  be  true    otherwise  if  the  outer  serializer  contains  some  extra  information  that  has  been  persisted  as  part  of  the  serializer  snapshot  this  must  be  overridden    note  that  this  method  and  the  corresponding  methods  link  write  outer  snapshot    data  output  view  link  read  outer  snapshot  int    data  input  view    class  loader  needs  to  be  implemented  param  new  serializer  the  new  serializer  which  contains  the  new  outer  information  to  check  against  return  a  flag  indicating  whether  or  not  the  new  serializer  s  outer  information  is  compatible  with  the  one  written  in  this  snapshot  deprecated  this  method  is  deprecated  and  will  be  removed  in  the  future    please  implement  link  resolve  outer  schema  compatibility    type  serializer  instead    deprecated  protected  boolean  is  outer  snapshot  compatible  s  new  serializer  return  true    checks  the  schema  compatibility  of  the  given  new  serializer  based  on  the  outer  snapshot  p    the  base  implementation  of  this  method  assumes  that  the  outer  serializer  only  has  nested  serializers  and  no  extra  information  and  therefore  the  result  of  the  check  is  link    outer  schema  compatibility  compatible  as  is    otherwise  if  the  outer  serializer  contains  some  extra  information  that  has  been  persisted  as  part  of  the  serializer  snapshot  this  must  be  overridden    note  that  this  method  and  the  corresponding  methods  link  write  outer  snapshot    data  output  view  link  read  outer  snapshot  int    data  input  view    class  loader  needs  to  be  implemented  param  new  serializer  the  new  serializer  which  contains  the  new  outer  information  to  check  against  return  a  link    outer  schema  compatibility  indicating  whether  or  the  new  serializer  s  outer  information  is  compatible  requires  migration  or  incompatible  with  the  one  written  in  this  snapshot  protected    outer  schema  compatibility  resolve  outer  schema  compatibility  s  new  serializer  return  is  outer  snapshot  compatible  new  serializer    outer  schema  compatibility  compatible  as  is    outer  schema  compatibility  incompatible    utilities  private  void  internal  write  outer  snapshot    data  output  view  out  throws    i  o  exception  out  write  int  magic  number  out  write  int  get  current  outer  snapshot  version  write  outer  snapshot  out  private  void  internal  read  outer  snapshot    data  input  view  in    class  loader  user  code  class  loader  throws    i  o  exception  final  int  magic  number  in  read  int  if  magic  number  magic  number  throw  new    i  o  exception    string  format    corrupt  data  magic  number  mismatch    expected  8x  found  8x  magic  number  magic  number  final  int  outer  snapshot  version  in  read  int  read  outer  snapshot  outer  snapshot  version  in  user  code  class  loader  private  void  legacy  internal  read  outer  snapshot  int  legacy  read  version    data  input  view  in    class  loader  user  code  class  loader  throws    i  o  exception  legacy  versions  did  not  contain  the  pre  fixed  magic  numbers  just  read  the  outer  snapshot  read  outer  snapshot  legacy  read  version  in  user  code  class  loader  private    type  serializer  schema  compatibility  t  construct  final  schema  compatibility  result    type  serializer  new  nested  serializers    type  serializer  snapshot  nested  serializer  snapshots    outer  schema  compatibility  outer  schema  compatibility    intermediate  compatibility  result  t  nested  serializers  compatibility  result    composite  type  serializer  util  construct  intermediate  compatibility  result  new  nested  serializers  nested  serializer  snapshots  if  outer  schema  compatibility    outer  schema  compatibility  incompatible  nested  serializers  compatibility  result  is  incompatible  return    type  serializer  schema  compatibility  incompatible  if  outer  schema  compatibility    outer  schema  compatibility  compatible  after  migration  nested  serializers  compatibility  result  is  compatible  after  migration  return    type  serializer  schema  compatibility  compatible  after  migration  if  nested  serializers  compatibility  result  is  compatible  with  reconfigured  serializer    suppress  warnings  unchecked    type  serializer  t  reconfigured  composite  serializer  create  outer  serializer  with  nested  serializers  nested  serializers  compatibility  result  get  nested  serializers  return    type  serializer  schema  compatibility  compatible  with  reconfigured  serializer  reconfigured  composite  serializer  return    type  serializer  schema  compatibility  compatible  as  is  
public  evolving  public  abstract  class    simple  type  serializer  snapshot  t  implements    type  serializer  snapshot  t    this  snapshot  starts  from  version    since    flink  1.7  x  so  that  version    is  reserved  for  implementing  backwards  compatible  code  paths  in  case  we  decide  to  make  this  snapshot  backwards  compatible  with  the  link    parameterless  type  serializer  config  private  static  final  int  current  version      the  class  of  the  serializer  for  this  snapshot    the  field  is  null  if  the  serializer  was  created  for  read  and  has  not  been  read  yet    nonnull  private    supplier  extends    type  serializer  t  serializer  supplier    constructor  to  create  snapshot  from  serializer  writing  the  snapshot  public    simple  type  serializer  snapshot    nonnull    supplier  extends    type  serializer  t  serializer  supplier  this  serializer  supplier  check  not  null  serializer  supplier    serializer    snapshot    methods    override  public  int  get  current  version  return  current  version    override  public    type  serializer  t  restore  serializer  return  serializer  supplier  get    override  public    type  serializer  schema  compatibility  t  resolve  schema  compatibility    type  serializer  t  new  serializer  return  new  serializer  get  class  serializer  supplier  get  get  class    type  serializer  schema  compatibility  compatible  as  is    type  serializer  schema  compatibility  incompatible    override  public  void  write  snapshot    data  output  view  out  throws    i  o  exception    override  public  void  read  snapshot  int  read  version    data  input  view  in    class  loader  class  loader  throws    i  o  exception  switch  read  version  case    break  case    we  don  t  need  the  classname  any  more  read  and  drop  to  maintain  compatibility  in  read  u  t  f  break  default  throw  new    i  o  exception    unrecognized  version  read  version  standard  utilities    override  public  final  boolean  equals    object  obj  return  obj  null  obj  get  class  get  class    override  public  final  int  hash  code  return  get  class  hash  code    override  public    string  to  string  return  get  class  get  name  
public  evolving  public  abstract  class    type  comparator  t  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l    computes  a  hash  value  for  the  given  record    the  hash  value  should  include  all  fields  in  the  record  relevant  to  the  comparison  p    the  hash  code  is  typically  not  used  as  it  is  in  hash  tables  and  for  partitioning  but  it  is  further  scrambled  to  make  sure  that  a  projection  of  the  hash  values  to  a  lower  cardinality  space  is  as  results  in  a  rather  uniform  value  distribution    however  any  collisions  produced  by  this  method  cannot  be  undone    while  it  is  not  important  to  create  hash  codes  that  cover  the  full  spectrum  of  bits  in  the  integer  it  is  important  to  avoid  collisions  when  combining  two  value  as  much  as  possible  param  record    the  record  to  be  hashed  return  a  hash  value  for  the  record  see  java  lang    object  hash  code  public  abstract  int  hash  t  record    sets  the  given  element  as  the  comparison  reference  for  future  calls  to  link  equal  to  reference    object  and  link  compare  to  reference    type  comparator    this  method  must  set  the  given  element  into  this  comparator  instance  s  state    if  the  comparison  happens  on  a  subset  of  the  fields  from  the  record  this  method  may  extract  those  fields  p  a  typical  example  for  checking  the  equality  of  two  elements  is  the  following  pre  code  e  e1  e  e2    type  comparator  e  acc  acc  set  reference  e1  boolean  equal  acc  equal  to  reference  e2  pre    the  rational  behind  this  method  is  that  elements  are  typically  compared  using  certain  features  that  are  extracted  from  them  such  de  serializing  as  a  subset  of  fields    when  setting  the  reference  this  extraction  happens    the  extraction  needs  happen  only  once  per  element  even  though  an  element  is  often  compared  to  multiple  other  elements  such  as  when  finding  equal  elements  in  the  process  of  grouping  the  elements  param  to  compare    the  element  to  set  as  the  comparison  reference  public  abstract  void  set  reference  t  to  compare    checks  whether  the  given  element  is  equal  to  the  element  that  has  been  set  as  the  comparison  reference  in  this  comparator  instance  param  candidate    the  candidate  to  check  return    true  if  the  element  is  equal  to  the  comparison  reference  false  otherwise  see  set  reference    object  public  abstract  boolean  equal  to  reference  t  candidate    this  method  compares  the  element  that  has  been  set  as  reference  in  this  type  accessor  to  the  element  set  as  reference  in  the  given  type  accessor    similar  to  comparing  two  elements  code  e1  and  code  e2  via  a  comparator  this  method  can  be  used  the  following  way  pre  code  e  e1  e  e2    type  comparator  e  acc1    type  comparator  e  acc2  acc1  set  reference  e1  acc2  set  reference  e2  int  comp  acc1  compare  to  reference  acc2  pre    the  rational  behind  this  method  is  that  elements  are  typically  compared  using  certain  features  that  are  extracted  from  them  such  de  serializing  as  a  subset  of  fields    when  setting  the  reference  this  extraction  happens    the  extraction  needs  happen  only  once  per  element  even  though  an  element  is  typically  compared  to  many  other  elements  when  establishing  a  sorted  order    the  actual  comparison  performed  by  this  method  may  be  very  cheap  as  it  happens  on  the  extracted  features  param  referenced  comparator    the  type  accessors  where  the  element  for  comparison  has  been  set  as  reference  return  a  value  smaller  than  zero  if  the  reference  value  of  code  referenced  accessors  is  smaller  than  the  reference  value  of  this  type  accessor  a  value  greater  than  zero  if  it  is  larger  zero  if  both  are  equal  see  set  reference    object  public  abstract  int  compare  to  reference    type  comparator  t  referenced  comparator  a  special  case  method  that  the  runtime  uses  for  special    pact  record  support  public  boolean  supports  compare  against  reference  return  false    compares  two  records  in  object  form    the  return  value  indicates  the  order  of  the  two  in  the  same  way  as  defined  by  link  java  util    comparator  compare    object    object  param  first    the  first  record  param  second    the  second  record  return    an  integer  defining  the  oder  among  the  objects  in  the  same  way  as  link  java  util    comparator  compare    object    object  see  java  util    comparator  compare    object    object  public  abstract  int  compare  t  first  t  second    compares  two  records  in  serialized  form    the  return  value  indicates  the  order  of  the  two  in  the  same  way  as  defined  by  link  java  util    comparator  compare    object    object  p    this  method  may  de  serialize  the  records  or  compare  them  directly  based  on  their  binary  representation  param  first  source    the  input  view  containing  the  first  record  param  second  source    the  input  view  containing  the  second  record  return    an  integer  defining  the  oder  among  the  objects  in  the  same  way  as  link  java  util    comparator  compare    object    object  throws    i  o  exception    thrown  if  any  of  the  input  views  raised  an  exception  when  reading  the  records  see  java  util    comparator  compare    object    object  public  abstract  int  compare  serialized    data  input  view  first  source    data  input  view  second  source  throws    i  o  exception    checks  whether  the  data  type  supports  the  creation  of  a  normalized  key  for  comparison  return    true  if  the  data  type  supports  the  creation  of  a  normalized  key  for  comparison  false  otherwise  public  abstract  boolean  supports  normalized  key    check  whether  this  comparator  supports  to  serialize  the  record  in  a  format  that  replaces  its  keys  by  a  normalized  key  return    true  if  the  comparator  supports  that  specific  form  of  serialization  false  if  not  public  abstract  boolean  supports  serialization  with  key  normalization    gets  the  number  of  bytes  that  the  normalized  key  would  maximally  take  a  value  of  link  java  lang    integer  max  value  is  interpreted  as  infinite  return    the  number  of  bytes  that  the  normalized  key  would  maximally  take  public  abstract  int  get  normalize  key  len    checks  whether  the  given  number  of  bytes  for  a  normalized  is  only  a  prefix  to  determine  the  order  of  elements  of  the  data  type  for  which  this  comparator  provides  the  comparison  methods    for  example  if  the  data  type  is  ordered  with  respect  to  an  integer  value  it  contains  then  this  method  would  return  true  if  the  number  of  key  bytes  is  smaller  than  four  return    true  if  the  given  number  of  bytes  is  only  a  prefix  false  otherwise  public  abstract  boolean  is  normalized  key  prefix  only  int  key  bytes    writes  a  normalized  key  for  the  given  record  into  the  target  byte  array  starting  at  the  specified  position  and  writing  exactly  the  given  number  of  bytes    note  that  the  comparison  of  the  bytes  is  treating  the  bytes  as  unsigned  bytes  code  int  byte  i  bytes  i  0x  f  f  p    if  the  meaningful  part  of  the  normalized  key  takes  less  than  the  given  number  of  bytes  then  it  must  be  padded    padding  is  typically  required  for  variable  length  data  types  such  as  strings    the  padding  uses  a  special  character  either  code    or  code  0xff  depending  on  whether  shorter  values  are  sorted  to  the  beginning  or  the  end  p    this  method  is  similar  to  link  org  apache  flink  types    normalizable  key  copy  normalized  key    memory  segment  int  int    in  the  case  that  multiple  fields  of  a  record  contribute  to  the  normalized  key  it  is  crucial  that  the  fields  align  on  the  byte  field  i  e  that  every  field  always  takes  up  the  exact  same  number  of  bytes  param  record    the  record  for  which  to  create  the  normalized  key  param  target    the  byte  array  into  which  to  write  the  normalized  key  bytes  param  offset    the  offset  in  the  byte  array  where  to  start  writing  the  normalized  key  bytes  param  num  bytes    the  number  of  bytes  to  be  written  exactly  see  org  apache  flink  types    normalizable  key  copy  normalized  key    memory  segment  int  int  public  abstract  void  put  normalized  key  t  record    memory  segment  target  int  offset  int  num  bytes    writes  the  record  in  such  a  fashion  that  all  keys  are  normalizing  and  at  the  beginning  of  the  serialized  data    this  must  only  be  used  when  for  all  the  key  fields  the  full  normalized  key  is  used    the  method  code  supports  serialization  with  key  normalization  allows  to  check  that  param  record    the  record  object  into  which  to  read  the  record  data  param  target    the  stream  to  which  to  write  the  data  see  supports  serialization  with  key  normalization  see  read  with  key  denormalization    object    data  input  view  see  org  apache  flink  types    normalizable  key  copy  normalized  key    memory  segment  int  int  public  abstract  void  write  with  key  normalization  t  record    data  output  view  target  throws    i  o  exception    reads  the  record  back  while  de  normalizing  the  key  fields    this  must  only  be  used  when  for  all  the  key  fields  the  full  normalized  key  is  used  which  is  hinted  by  the  code  supports  serialization  with  key  normalization  method  param  reuse    the  reuse  object  into  which  to  read  the  record  data  param  source    the  stream  from  which  to  read  the  data  see  supports  serialization  with  key  normalization  see  write  with  key  normalization    object    data  output  view  see  org  apache  flink  types    normalizable  key  copy  normalized  key    memory  segment  int  int  public  abstract  t  read  with  key  denormalization  t  reuse    data  input  view  source  throws    i  o  exception    flag  whether  normalized  key  comparisons  should  be  inverted  key  should  be  interpreted  inverted  i  e  descending  return    true  if  all  normalized  key  comparisons  should  invert  the  sign  of  the  comparison  result  false  if  the  normalized  key  should  be  used  as  is  public  abstract  boolean  invert  normalized  key    creates  a  copy  of  this  class    the  copy  must  be  deep  such  that  no  state  set  in  the  copy  affects  this  instance  of  the  comparator  class  return  a  deep  copy  of  this  comparator  instance  public  abstract    type  comparator  t  duplicate    extracts  the  key  fields  from  a  record    this  is  for  use  by  the    pair  comparator  to  provide  interoperability  between  different  record  types    note  that  at  least  one  key  should  be  extracted  param  record    the  record  that  contains  the  key  s  param  target    the  array  to  write  the  key  s  into  param  index    the  offset  of  the  target  array  to  start  writing  into  return  the  number  of  keys  added  to  target  public  abstract  int  extract  keys    object  record    object  target  int  index    get  the  field  comparators    this  is  used  together  with  link  extract  keys    object    object  int  to  provide  interoperability  between  different  record  types    note  that  this  should  return  at  least  one    comparator  and  that  the  number  of    comparators  must  match  the  number  of  extracted  keys  return    an    array  of    comparators  for  the  extracted  keys    suppress  warnings  rawtypes  public  abstract    type  comparator  get  flat  comparators    suppress  warnings  rawtypes  public  int  compare  against  reference    comparable  keys  throw  new    unsupported  operation  exception    workaround  hack  
public  evolving  public  abstract  class    type  serializer  t  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l    general  information  about  the  type  and  the  serializer    gets  whether  the  type  is  an  immutable  type  return    true  if  the  type  is  immutable  public  abstract  boolean  is  immutable  type    creates  a  deep  copy  of  this  serializer  if  it  is  necessary  i  e  if  it  is  stateful    this  can  return  itself  if  the  serializer  is  not  stateful    we  need  this  because    serializers  might  be  used  in  several  threads    stateless  serializers  are  inherently  thread  safe  while  stateful  serializers  might  not  be  thread  safe  public  abstract    type  serializer  t  duplicate    instantiation    cloning    creates  a  new  instance  of  the  data  type  return  a  new  instance  of  the  data  type  public  abstract  t  create  instance    creates  a  deep  copy  of  the  given  element  in  a  new  element  param  from    the  element  reuse  be  copied  return  a  deep  copy  of  the  element  public  abstract  t  copy  t  from    creates  a  copy  from  the  given  element    the  method  makes  an  attempt  to  store  the  copy  in  the  given  reuse  element  if  the  type  is  mutable    this  is  however  not  guaranteed  param  from    the  element  to  be  copied  param  reuse    the  element  to  be  reused    may  or  may  not  be  used  return  a  deep  copy  of  the  element  public  abstract  t  copy  t  from  t  reuse    gets  the  length  of  the  data  type  if  it  is  a  fix  length  data  type  return    the  length  of  the  data  type  or  code    code  for  variable  length  data  types  public  abstract  int  get  length    serializes  the  given  record  to  the  given  target  output  view  param  record    the  record  to  serialize  param  target    the  output  view  to  write  the  serialized  data  to  throws    i  o  exception    thrown  if  the  serialization  encountered  an  i  o  related  error    typically  raised  by  the  output  view  which  may  have  an  underlying  i  o  channel  to  which  it  delegates  public  abstract  void  serialize  t  record    data  output  view  target  throws    i  o  exception    de  serializes  a  record  from  the  given  source  input  view  param  source    the  input  view  from  which  to  read  the  data  return    the  deserialized  element  throws    i  o  exception    thrown  if  the  de  serialization  encountered  an  i  o  related  error    typically  raised  by  the  input  view  which  may  have  an  underlying  i  o  channel  from  which  it  reads  public  abstract  t  deserialize    data  input  view  source  throws    i  o  exception    de  serializes  a  record  from  the  given  source  input  view  into  the  given  reuse  record  instance  if  mutable  param  reuse    the  record  instance  into  which  to  de  serialize  the  data  param  source    the  input  view  from  which  to  read  the  data  return    the  deserialized  element  throws    i  o  exception    thrown  if  the  de  serialization  encountered  an  i  o  related  error    typically  raised  by  the  input  view  which  may  have  an  underlying  i  o  channel  from  which  it  reads  public  abstract  t  deserialize  t  reuse    data  input  view  source  throws    i  o  exception    copies  exactly  one  record  from  the  source  input  view  to  the  target  output  view    whether  this  operation  works  on  binary  data  or  partially  de  serializes  the  record  to  determine  its  length  such  as  for  records  of  variable  length  is  up  to  the  implementer    binary  copies  are  typically  faster  a  copy  of  a  record  containing  two  integer  numbers    bytes  total  is  most  efficiently  implemented  as  code  target  write  source    param  source    the  input  view  from  which  to  read  the  record  param  target    the  target  output  view  to  which  to  write  the  record  throws    i  o  exception    thrown  if  any  of  the  two  views  raises  an  exception  public  abstract  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  public  abstract  boolean  equals    object  obj  public  abstract  int  hash  code    serializer  configuration  snapshot  for  checkpoints  savepoints    snapshots  the  configuration  of  this    type  serializer    this  method  is  only  relevant  if  the  serializer  is  used  to  state  stored  in  checkpoints  savepoints  p    the  snapshot  of  the    type  serializer  is  supposed  to  contain  all  information  that  affects  the  serialization  format  of  the  serializer    the  snapshot  serves  two  purposes    first  to  reproduce  the  serializer  when  the  checkpoint  savepoint  is  restored  and  second  to  check  whether  the  serialization  format  is  compatible  with  the  serializer  used  in  the  restored  program  p  b  important  b    type  serializer  snapshots  changed  after    flink  1.6    serializers  implemented  against    flink  versions  up  to  1.6  should  still  work  but  adjust  to  new  model  to  enable  state  evolution  and  be  future  proof    see  the  class  level  comments  section    upgrading    type  serializers  to  the  new    type  serializer  snapshot  model  for  details  see    type  serializer  snapshot  resolve  schema  compatibility    type  serializer  return  snapshot  of  the  serializer  s  current  configuration  cannot  be  code  null  public  abstract    type  serializer  snapshot  t  snapshot  configuration  
public  evolving    deprecated  public  abstract  class    type  serializer  config  snapshot  t  extends    versioned  i  o  readable  writable  implements    type  serializer  snapshot  t    version    magic  number  for  the  format  that  bridges  between  the  old  and  new  interface  static  final  int  adapter  version  0x7  a53c4f0    the  user  code  class  loader  only  relevant  if  this  configuration  instance  was  deserialized  from  binary  form  private    class  loader  user  code  class  loader    the  originating  serializer  of  this  configuration  snapshot  private    type  serializer  t  serializer    set  the  originating  serializer  of  this  configuration  snapshot    internal  public  final  void  set  prior  serializer    type  serializer  t  serializer  this  serializer    preconditions  check  not  null  serializer    set  the  user  code  class  loader    only  relevant  if  this  configuration  instance  was  deserialized  from  binary  form  p    this  method  is  not  part  of  the  public  user  facing  api  and  cannot  be  overriden  param  user  code  class  loader  user  code  class  loader    internal  public  final  void  set  user  code  class  loader    class  loader  user  code  class  loader  this  user  code  class  loader    preconditions  check  not  null  user  code  class  loader    returns  the  user  code  class  loader    only  relevant  if  this  configuration  instance  was  deserialized  from  binary  form  return  the  user  code  class  loader    internal  public  final    class  loader  get  user  code  class  loader  return  user  code  class  loader    implementation  of  the    type  serializer  snapshot  interface    override  public  final  int  get  current  version  return  adapter  version    override  public  final  void  write  snapshot    data  output  view  out  throws    i  o  exception  check  state  serializer  null  the  prior  serializer  has  not  been  set  on  this  write  the  snapshot  for  a  non  updated  serializer  this  mimics  the  previous  behavior  where  the    type  serializer  was    java  serialized  for  backwards  compatibility    type  serializer  serialization  util  write  serializer  out  serializer  now  delegate  to  the  snapshots  own  writing  code  write  out    override  public  final  void  read  snapshot  int  read  version    data  input  view  in    class  loader  user  code  class  loader  throws    i  o  exception  if  read  version  adapter  version  throw  new    i  o  exception    wrong  unexpected  version  for  the    type  serializer  config  snapshot  read  version  serializer    type  serializer  serialization  util  try  read  serializer  in  user  code  class  loader  true  now  delegate  to  the  snapshots  own  reading  code  set  user  code  class  loader  user  code  class  loader  read  in    creates  a  serializer  using  this  configuration  that  is  capable  of  reading  data  written  by  the  serializer  described  by  this  configuration  return  the  restored  serializer    override  public  final    type  serializer  t  restore  serializer  if  serializer  null  throw  new    illegal  state  exception    trying  to  restore  the  prior  serializer  via    type  serializer  config  snapshot  but  the  prior  serializer  has  not  been  set  else  if  serializer  instanceof    unloadable  dummy  type  serializer    throwable  original  error    unloadable  dummy  type  serializer  serializer  get  original  error  throw  new    illegal  state  exception    could  not    java  deserialize    type  serializer  while  restoring  checkpoint  metadata  for  serializer  snapshot  get  class  get  name    please  update  to  the    type  serializer  snapshot  interface  that  removes    java    serialization  to  avoid  this  problem  in  the  future  original  error  else  return  this  serializer    override  public    type  serializer  schema  compatibility  t  resolve  schema  compatibility    type  serializer  t  new  serializer  if  new  serializer  instanceof    type  serializer  config  snapshot    self  resolving  type  serializer    suppress  warnings  unchecked    self  resolving  type  serializer  t  self  resolving  type  serializer    self  resolving  type  serializer  t  new  serializer  return  self  resolving  type  serializer  resolve  schema  compatibility  via  redirecting  to  new  snapshot  class  this  we  reach  here  if  this  legacy  config  snapshot  did  not  override  resolve  schema  compatibility  to  redirect  the  compatibility  check  to  a  new    type  serializer  snapshot  the  corresponding  new  serializer  does  not  make  use  of  the    self  resolving  type  serializer  to  assist  with  the  redirection  throw  new    unsupported  operation  exception    serializer  snapshot  get  class  get  name  is  still  implementing  the  deprecated    type  serializer  config  snapshot  class  n    please  update  it  to  implement  the    type  serializer  snapshot  interface  to  enable  state  evolution  as  well  as  being  future  proof  n  n    if  possible  you  should  try  to  perform  the  update  in  place  i  e  use  the  same  snapshot  class  under  the  same  name  but  change  it  to  implement    type  serializer  snapshot  instead  n  n    otherwise  if  the  above  isn  t  possible  perhaps  because  the  new  snapshot  is  intended  to  have  completely  n  different  written  contents  or  intended  to  have  a  different  class  name  n  retain  the  old  serializer  snapshot  class  extending    type  serializer  config  snapshot  under  the  same  name  n  and  give  the  updated  serializer  snapshot  class  the  one  extending    type  serializer  snapshot  a  new  name  n    afterwards  override  the    type  serializer  config  snapshot  resolve  schema  compatibility    type  serializer  n  method  on  the  old  snapshot  to  perform  the  compatibility  check  based  on  configuration  written  by  the  old  serializer  snapshot  class    this  interface  assists  with  the  migration  path  to  the  new  serialization  abstraction  p    this  interface  can  be  used  for  cases  where  the  ensure  compatibility  method  cannot  be  removed    implementing  this  interface  by  your  link    type  serializer  would  allow  it  to  redirect  the  compatibility  check  to  the  corresponding  code    type  serializer  snapshot  class  p    please  note  that  if  it  is  possible  to  directly  override  link    type  serializer  config  snapshot  resolve  schema  compatibility  and  preform  the  redirection  logic  there  then  that  is  the  preferred  way    this  interface  is  useful  for  cases  where  there  is  not  enough  information  and  the  new  serializer  should  assist  with  the  redirection    internal  public  interface    self  resolving  type  serializer  e    resolve    schema    compatibility  p    given  an  instance  of  a  code    type  serializer  config  snapshot  this  method  should  redirect  the  compatibility  check  to  the  new  code    type  serializer  snapshot  class  along  with  the  relevant  information  as  present  in  the  given  code  deprecated  config  snapshot  param  deprecated  config  snapshot  the  not  yet  migrated  config  snapshot  class  return  the  compatibility  result  of  the  code  deprecated  config  snapshot  with  code  this  serializer    type  serializer  schema  compatibility  e  resolve  schema  compatibility  via  redirecting  to  new  snapshot  class    type  serializer  config  snapshot  e  deprecated  config  snapshot  
public  evolving  public  class    type  serializer  schema  compatibility  t    enum  for  the  type  of  the  compatibility  enum    type    this  indicates  that  the  new  serializer  continued  to  be  used  as  is  compatible  as  is    this  indicates  that  it  is  possible  to  use  the  new  serializer  after  performing  a  full  scan  migration  over  all  state  by  reading  bytes  with  the  previous  serializer  and  then  writing  it  again  with  the  new  serializer  effectively  converting  the  serialization  schema  to  correspond  to  the  new  serializer  compatible  after  migration    this  indicates  that  a  reconfigured  version  of  the  new  serializer  is  compatible  and  should  be  used  instead  of  the  original  new  serializer  compatible  with  reconfigured  serializer    this  indicates  that  the  new  serializer  is  incompatible  even  with  migration    this  normally  implies  that  the  deserialized    java  class  can  not  be  commonly  recognized  by  the  previous  and  new  serializer  incompatible    the  type  of  the  compatibility  private  final    type  result  type  private  final    type  serializer  t  reconfigured  new  serializer    returns  a  result  that  indicates  that  the  new  serializer  is  compatible  and  no  migration  is  required    the  new  serializer  can  continued  to  be  used  as  is  return  a  result  that  indicates  migration  is  not  required  for  the  new  serializer  public  static  t    type  serializer  schema  compatibility  t  compatible  as  is  return  new    type  serializer  schema  compatibility    type  compatible  as  is  null    returns  a  result  that  indicates  that  the  new  serializer  can  be  used  after  migrating  the  written  bytes  i  e  reading  it  with  the  old  serializer  and  then  writing  it  again  with  the  new  serializer  return  a  result  that  indicates  that  the  new  serializer  can  be  used  after  migrating  the  written  bytes  public  static  t    type  serializer  schema  compatibility  t  compatible  after  migration  return  new    type  serializer  schema  compatibility    type  compatible  after  migration  null    returns  a  result  that  indicates  a  reconfigured  version  of  the  new  serializer  is  compatible  and  should  be  used  instead  of  the  original  new  serializer  param  reconfigured  serializer  the  reconfigured  version  of  the  new  serializer  return  a  result  that  indicates  a  reconfigured  version  of  the  new  serializer  is  compatible  and  should  be  used  instead  of  the  original  new  serializer  public  static  t    type  serializer  schema  compatibility  t  compatible  with  reconfigured  serializer    type  serializer  t  reconfigured  serializer  return  new    type  serializer  schema  compatibility    type  compatible  with  reconfigured  serializer    preconditions  check  not  null  reconfigured  serializer    returns  a  result  that  indicates  there  is  no  possible  way  for  the  new  serializer  to  be  use  able    this  normally  indicates  that  there  is  no  common    java  class  between  what  the  previous  bytes  can  be  deserialized  into  and  what  can  be  written  by  the  new  serializer  p    in  this  case  there  is  no  possible  way  for  the  new  serializer  to  continue  to  be  used  even  with  migration    recovery  of  the    flink  job  will  fail  return  a  result  that  indicates  incompatibility  between  the  new  and  previous  serializer  public  static  t    type  serializer  schema  compatibility  t  incompatible  return  new    type  serializer  schema  compatibility    type  incompatible  null  private    type  serializer  schema  compatibility    type  result  type    nullable    type  serializer  t  reconfigured  new  serializer  this  result  type    preconditions  check  not  null  result  type  this  reconfigured  new  serializer  reconfigured  new  serializer    returns  whether  or  not  the  type  of  the  compatibility  is  link    type  compatible  as  is  return  whether  or  not  the  type  of  the  compatibility  is  link    type  compatible  as  is  public  boolean  is  compatible  as  is  return  result  type    type  compatible  as  is    returns  whether  or  not  the  type  of  the  compatibility  is  link    type  compatible  after  migration  return  whether  or  not  the  type  of  the  compatibility  is  link    type  compatible  after  migration  public  boolean  is  compatible  after  migration  return  result  type    type  compatible  after  migration    returns  whether  or  not  the  type  of  the  compatibility  is  link    type  compatible  with  reconfigured  serializer  return  whether  or  not  the  type  of  the  compatibility  is  link    type  compatible  with  reconfigured  serializer  public  boolean  is  compatible  with  reconfigured  serializer  return  result  type    type  compatible  with  reconfigured  serializer    gets  the  reconfigured  serializer    this  throws  an  exception  if  link  is  compatible  with  reconfigured  serializer  is  code  false  public    type  serializer  t  get  reconfigured  serializer    preconditions  check  state  is  compatible  with  reconfigured  serializer    it  is  only  possible  to  get  a  reconfigured  serializer  if  the  compatibility  type  is  s  but  the  type  is  s    type  compatible  with  reconfigured  serializer  result  type  return  reconfigured  new  serializer    returns  whether  or  not  the  type  of  the  compatibility  is  link    type  incompatible  return  whether  or  not  the  type  of  the  compatibility  is  link    type  incompatible  public  boolean  is  incompatible  return  result  type    type  incompatible    override  public    string  to  string  return    type  serializer  schema  compatibility  result  type  result  type  reconfigured  new  serializer  reconfigured  new  serializer  
public  evolving  public  interface    type  serializer  snapshot  t    returns  the  version  of  the  current  snapshot  s  written  binary  format  return  the  version  of  the  current  snapshot  s  written  binary  format  int  get  current  version    writes  the  serializer  snapshot  to  the  provided  link    data  output  view    the  current  version  of  the  written  serializer  snapshot  s  binary  format  is  specified  by  the  link  get  current  version  method  param  out  the  link    data  output  view  to  write  the  snapshot  to  throws    i  o  exception    thrown  if  the  snapshot  data  could  not  be  written  see  write  versioned  snapshot    data  output  view    type  serializer  snapshot  void  write  snapshot    data  output  view  out  throws    i  o  exception    reads  the  serializer  snapshot  from  the  provided  link    data  input  view    the  version  of  the  binary  format  that  the  serializer  snapshot  was  written  with  is  provided    this  version  can  be  used  to  determine  how  the  serializer  snapshot  should  be  read  param  read  version  version  of  the  serializer  snapshot  s  written  binary  format  param  in  the  link    data  input  view  to  read  the  snapshot  from  param  user  code  class  loader  the  user  code  classloader  throws    i  o  exception    thrown  if  the  snapshot  data  could  be  read  or  parsed  see  read  versioned  snapshot    data  input  view    class  loader  void  read  snapshot  int  read  version    data  input  view  in    class  loader  user  code  class  loader  throws    i  o  exception    recreates  a  serializer  instance  from  this  snapshot    the  returned  serializer  can  be  safely  used  to  read  data  written  by  the  prior  serializer  i  e  the  serializer  that  created  this  snapshot  return  a  serializer  instance  restored  from  this  serializer  snapshot    type  serializer  t  restore  serializer    checks  a  new  serializer  s  compatibility  to  read  data  written  by  the  prior  serializer  p    when  a  checkpoint  savepoint  is  restored  this  method  checks  whether  the  serialization  format  of  the  data  in  the  checkpoint  savepoint  is  compatible  for  the  format  of  the  serializer  used  by  the  program  that  restores  the  checkpoint  savepoint    the  outcome  can  be  that  the  serialization  format  is  compatible  that  the  program  s  serializer  needs  to  reconfigure  itself  meaning  to  incorporate  some  information  from  the    type  serializer  snapshot  to  be  compatible  that  the  format  is  outright  incompatible  or  that  a  migration  needed    in  the  latter  case  the    type  serializer  snapshot  produces  a  serializer  to  deserialize  the  data  and  the  restoring  program  s  serializer  re  serializes  the  data  thus  converting  the  format  during  the  restore  operation  param  new  serializer  the  new  serializer  to  check  return  the  serializer  compatibility  result    type  serializer  schema  compatibility  t  resolve  schema  compatibility    type  serializer  t  new  serializer  read  write  utilities    writes  the  given  snapshot  to  the  out  stream    one  should  always  use  this  method  to  write  snapshots  out  rather  than  directly  calling  link  write  snapshot    data  output  view  p    the  snapshot  written  with  this  method  can  be  read  via  link  read  versioned  snapshot    data  input  view    class  loader  static  void  write  versioned  snapshot    data  output  view  out    type  serializer  snapshot  snapshot  throws    i  o  exception  out  write  u  t  f  snapshot  get  class  get  name  out  write  int  snapshot  get  current  version  snapshot  write  snapshot  out    reads  a  snapshot  from  the  stream  performing  resolving  p    this  method  reads  snapshots  written  by  link  write  versioned  snapshot    data  output  view    type  serializer  snapshot  static  t    type  serializer  snapshot  t  read  versioned  snapshot    data  input  view  in    class  loader  cl  throws    i  o  exception  final    type  serializer  snapshot  t  snapshot    type  serializer  snapshot  serialization  util  read  and  instantiate  snapshot  class  in  cl  int  version  in  read  int  if  version  adapter  version  snapshot  instanceof    type  serializer  config  snapshot  the  snapshot  was  upgraded  directly  in  place  from  a    type  serializer  config  snapshot  read  and  drop  the  previously    java  serialized  serializer  and  get  the  actual  correct  read  version  note  this  implicitly  assumes  that  the  version  was  properly  written  before  the  actual  snapshot  content    type  serializer  serialization  util  try  read  serializer  in  cl  true  version  in  read  int  snapshot  read  snapshot  version  in  cl  return  snapshot  
public  evolving  public  enum    boundedness  a  bounded  stream  is  a  stream  with  finite  records  p    in  the  context  of  sources  a  bounded  stream  expects  the  source  to  put  a  boundary  of  the  records  it  emits    such  boundaries  could  be  number  of  records  number  of  bytes  elapsed  time  and  so  on    such  indication  of  how  to  bound  a  stream  is  typically  passed  to  the  sources  via  configurations    when  the  sources  emit  a  bounded  stream    flink  may  leverage  this  property  to  do  specific  optimizations  in  the  execution  p    unlike  unbounded  streams  the  bounded  streams  are  usually  order  insensitive    that  means  the  source  implementations  may  not  have  to  keep  track  of  the  event  times  or  watermarks    instead  a  higher  throughput  would  be  preferred  bounded  a  continuous  unbounded  stream  is  a  stream  with  infinite  records  p    in  the  context  of  sources  an  infinite  stream  expects  the  source  implementation  to  run  without  an  upfront  indication  to    flink  that  they  will  eventually  stop    the  sources  may  eventually  be  terminated  when  users  cancel  the  jobs  or  some  source  specific  condition  is  met  p  a  continuous  unbounded  stream  may  also  eventually  stop  at  some  point    but  before  that  happens    flink  always  assumes  the  sources  are  going  to  run  forever  continuous  unbounded  
public  evolving  public  final  class    reader  info  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l  private  final  int  subtask  id  private  final    string  location  public    reader  info  int  subtask  id    string  location  this  subtask  id  subtask  id  this  location  location  return  the  id  of  the  subtask  that  runs  the  source  reader  public  int  get  subtask  id  return  subtask  id  return  the  location  of  the  subtask  that  runs  this  source  reader  public    string  get  location  return  location    override  public  int  hash  code  return    objects  hash  subtask  id  location    override  public  boolean  equals    object  obj  if  obj  instanceof    reader  info  return  false    reader  info  other    reader  info  obj  return  subtask  id  other  subtask  id  location  equals  other  location  
public  evolving  public  interface    reader  output  t  extends    source  output  t    emit  a  record  without  a  timestamp  p    use  this  method  if  the  source  system  does  not  have  a  notion  of  records  with  timestamps  p    the  events  later  pass  through  a  link    timestamp  assigner  which  attaches  a  timestamp  to  the  event  based  on  the  event  s  contents    for  example  a  file  source  with  json  records  would  not  have  a  generic  timestamp  from  the  file  reading  and  json  parsing  process  and  thus  use  this  method  to  produce  initially  a  record  without  a  timestamp    the  code    timestamp  assigner  in  the  next  step  would  be  used  to  extract  timestamp  from  a  field  of  the  json  object  param  record  the  record  to  emit    override  void  collect  t  record    emit  a  record  with  a  timestamp  p    use  this  method  if  the  source  system  has  timestamps  attached  to  records    typical  examples  would  be    logs    pub  subs  or    message    queues  like    kafka  or    kinesis  which  store  a  timestamp  with  each  event  p    the  events  typically  still  pass  through  a  link    timestamp  assigner  which  may  decide  to  either  use  this  source  provided  timestamp  or  replace  it  with  a  timestamp  stored  within  the  event  for  example  if  the  event  was  a  json  object  one  could  configure  a  timestamp  assigner  that  extracts  one  of  the  object  s  fields  and  uses  that  as  a  timestamp  param  record  the  record  to  emit  param  timestamp  the  timestamp  of  the  record    override  void  collect  t  record  long  timestamp    emits  the  given  watermark  p    emitting  a  watermark  also  implicitly  marks  the  stream  as  i  active  i  ending  previously  marked  idleness    override  void  emit  watermark    watermark  watermark    marks  this  output  as  idle  meaning  that  downstream  operations  do  not  wait  for  watermarks  from  this  output  p    an  output  becomes  active  again  as  soon  as  the  next  watermark  is  emitted    override  void  mark  idle    creates  a  code    source  output  for  a  specific    source    split    use  these  outputs  if  you  want  to  run  split  local  logic  like  watermark  generation  p    if  a  split  local  output  was  already  created  for  this  split  id  the  method  will  return  that  instance  so  that  only  one  split  local  output  exists  per  split  id  p  b  important  b    after  the  split  has  been  finished  it  is  crucial  to  release  the  created  output  again    otherwise  it  will  continue  to  contribute  to  the  watermark  generation  like  a  perpetually  stalling  source  split  and  may  hold  back  the  watermark  indefinitely  see  release  output  for  split    string    source  output  t  create  output  for  split    string  split  id    releases  the  code    source  output  created  for  the  split  with  the  given  id  see  create  output  for  split    string  void  release  output  for  split    string  split  id  
public  evolving  public  interface    source  t    split  t  extends    source  split    enum  chk  t  extends    serializable    get  the  boundedness  of  this  source  return  the  boundedness  of  this  source    boundedness  get  boundedness    creates  a  new  reader  to  read  data  from  the  spits  it  gets  assigned    the  reader  starts  fresh  and  does  not  have  any  state  to  resume  param  reader  context    the  link    source  reader  context  context  for  the  source  reader  return  a  new    source  reader    source  reader  t    split  t  create  reader    source  reader  context  reader  context    creates  a  new    split  enumerator  for  this  source  starting  a  new  input  param  enum  context    the  link    split  enumerator  context  context  for  the  split  enumerator  return  a  new    split  enumerator    split  enumerator    split  t    enum  chk  t  create  enumerator    split  enumerator  context    split  t  enum  context    restores  an  enumerator  from  a  checkpoint  param  enum  context    the  link    split  enumerator  context  context  for  the  restored  split  enumerator  param  checkpoint    the  checkpoint  to  restore  the    split  enumerator  from  return  a    split  enumerator  restored  from  the  given  checkpoint    split  enumerator    split  t    enum  chk  t  restore  enumerator    split  enumerator  context    split  t  enum  context    enum  chk  t  checkpoint  throws    i  o  exception  serializers  for  the  metadata    creates  a  serializer  for  the  source  splits    splits  are  serialized  when  sending  them  from  enumerator  to  reader  and  when  checkpointing  the  reader  s  current  state  return    the  serializer  for  the  split  type    simple  versioned  serializer    split  t  get  split  serializer    creates  the  serializer  for  the  link    split  enumerator  checkpoint    the  serializer  is  used  for  the  result  of  the  link    split  enumerator  snapshot  state  method  return    the  serializer  for  the    split  enumerator  checkpoint    simple  versioned  serializer    enum  chk  t  get  enumerator  checkpoint  serializer  
public  evolving  public  interface    source  event  extends    serializable  
public  evolving  public  interface    source  output  t  extends    watermark  output    emit  a  record  without  a  timestamp  p    use  this  method  if  the  source  system  does  not  have  a  notion  of  records  with  timestamps  p    the  events  later  pass  through  a  link    timestamp  assigner  which  attaches  a  timestamp  to  the  event  based  on  the  event  s  contents    for  example  a  file  source  with  json  records  would  not  have  a  generic  timestamp  from  the  file  reading  and  json  parsing  process  and  thus  use  this  method  to  produce  initially  a  record  without  a  timestamp    the  code    timestamp  assigner  in  the  next  step  would  be  used  to  extract  timestamp  from  a  field  of  the  json  object  param  record  the  record  to  emit  void  collect  t  record    emit  a  record  with  a  timestamp  p    use  this  method  if  the  source  system  has  timestamps  attached  to  records    typical  examples  would  be    logs    pub  subs  or    message    queues  like    kafka  or    kinesis  which  store  a  timestamp  with  each  event  p    the  events  typically  still  pass  through  a  link    timestamp  assigner  which  may  decide  to  either  use  this  source  provided  timestamp  or  replace  it  with  a  timestamp  stored  within  the  event  for  example  if  the  event  was  a  json  object  one  could  configure  a  timestamp  assigner  that  extracts  one  of  the  object  s  fields  and  uses  that  as  a  timestamp  param  record  the  record  to  emit  param  timestamp  the  timestamp  of  the  record  void  collect  t  record  long  timestamp  
public  evolving  public  interface    source  reader  t    split  t  extends    source  split  extends    auto  closeable    start  the  reader  void  start    poll  the  next  available  record  into  the  link    source  output  p    the  implementation  must  make  sure  this  method  is  non  blocking  p    although  the  implementation  can  emit  multiple  records  into  the  given    source  output  it  is  recommended  not  doing  so    instead  emit  one  record  into  the    source  output  and  return  a  link    input  status  more  available  to  let  the  caller  thread  know  there  are  more  records  available  return    the    input  status  of  the    source  reader  after  the  method  invocation    input  status  poll  next    reader  output  t  output  throws    exception    checkpoint  on  the  state  of  the  source  return  the  state  of  the  source    list    split  t  snapshot  state  return  a  future  that  will  be  completed  once  there  is  a  record  available  to  poll    completable  future    void  is  available    adds  a  list  of  splits  for  this  reader  to  read  param  splits    the  splits  assigned  by  the  split  enumerator  void  add  splits    list    split  t  splits    handle  a  source  event  sent  by  the  link    split  enumerator  param  source  event  the  event  sent  by  the  link    split  enumerator  void  handle  source  events    source  event  source  event  
public  evolving  public  interface    source  reader  context  return    the  metric  group  this  source  belongs  to    metric  group  metric  group    send  a  source  event  to  the  source  coordinator  param  source  event  the  source  event  to  coordinator  void  send  source  event  to  coordinator    source  event  source  event  
public  evolving  public  interface    source  split    get  the  split  id  of  this  source  split  return  id  of  this  source  split    string  split  id  
public  evolving  public  interface    split  enumerator    split  t  extends    source  split    checkpoint  t  extends    auto  closeable    start  the  split  enumerator  p    the  default  behavior  does  nothing  void  start    handles  the  source  event  from  the  source  reader  param  subtask  id  the  subtask  id  of  the  source  reader  who  sent  the  source  event  param  source  event  the  source  event  from  the  source  reader  void  handle  source  event  int  subtask  id    source  event  source  event    add  a  split  back  to  the  split  enumerator    it  will  only  happen  when  a  link    source  reader  fails  and  there  are  splits  assigned  to  it  after  the  last  successful  checkpoint  param  splits    the  split  to  add  back  to  the  enumerator  for  reassignment  param  subtask  id    the  id  of  the  subtask  to  which  the  returned  splits  belong  void  add  splits  back    list    split  t  splits  int  subtask  id    add  a  new  source  reader  with  the  given  subtask  id  param  subtask  id  the  subtask  id  of  the  new  source  reader  void  add  reader  int  subtask  id    checkpoints  the  state  of  this  split  enumerator  return  an  object  containing  the  state  of  the  split  enumerator  throws    exception  when  the  snapshot  cannot  be  taken    checkpoint  t  snapshot  state  throws    exception    called  to  close  the  enumerator  in  case  it  holds  on  to  any  resources  like  threads  or  network  connections    override  void  close  throws    i  o  exception  
public  evolving  public  interface    split  enumerator  context    split  t  extends    source  split    metric  group  metric  group    send  a  source  event  to  a  source  reader    the  source  reader  is  identified  by  its  subtask  id  param  subtask  id  the  subtask  id  of  the  source  reader  to  send  this  event  to  param  event  the  source  event  to  send  void  send  event  to  source  reader  int  subtask  id    source  event  event    get  the  current  parallelism  of  this    source    note  that  due  to  auto  scaling  the  parallelism  may  change  over  time    therefore  the    split  enumerator  should  not  cache  the  return  value  of  this  method  but  always  invoke  this  method  to  get  the  latest  parallelism  return  the  parallelism  of  the    source  int  current  parallelism    get  the  currently  registered  readers    the  mapping  is  from  subtask  id  to  the  reader  info  return  the  currently  registered  readers    map    integer    reader  info  registered  readers    assign  the  splits  param  new  split  assignments  the  new  split  assignments  to  add  void  assign  splits    splits  assignment    split  t  new  split  assignments    invoke  the  callable  and  handover  the  return  value  to  the  handler  which  will  be  executed  by  the  source  coordinator    when  this  method  is  invoked  multiple  times    the  code    coallble  code  s  may  be  executed  in  a  thread  pool  concurrently  p    it  is  important  to  make  sure  that  the  callable  does  not  modify  any  shared  state  especially  the  states  that  will  be  a  part  of  the  link    split  enumerator  snapshot  state    otherwise  the  there  might  be  unexpected  behavior  param  callable  a  callable  to  call  param  handler  a  handler  that  handles  the  return  value  of  or  the  exception  thrown  from  the  callable  t  void  call  async    callable  t  callable    bi  consumer  t    throwable  handler    invoke  the  given  callable  periodically  and  handover  the  return  value  to  the  handler  which  will  be  executed  by  the  source  coordinator    when  this  method  is  invoked  multiple  times    the  code    coallble  code  s  may  be  executed  in  a  thread  pool  concurrently  p    it  is  important  to  make  sure  that  the  callable  does  not  modify  any  shared  state  especially  the  states  that  will  be  a  part  of  the  link    split  enumerator  snapshot  state    otherwise  the  there  might  be  unexpected  behavior  param  callable  the  callable  to  call  param  handler  a  handler  that  handles  the  return  value  of  or  the  exception  thrown  from  the  callable  param  initial  delay  the  initial  delay  of  calling  the  callable  param  period  the  period  between  two  invocations  of  the  callable  t  void  call  async    callable  t  callable    bi  consumer  t    throwable  handler  long  initial  delay  long  period  
public  evolving  public  final  class    splits  assignment    split  t  extends    source  split  private  final    map    integer    list    split  t  assignment  public    splits  assignment    map    integer    list    split  t  assignment  this  assignment  assignment  return  a  mapping  from  subtask  id  to  their  split  assignment  public    map    integer    list    split  t  assignment  return  assignment    override  public    string  to  string  return  assignment  to  string  
public  evolving  public  final  class    list  type  info  t  extends    type  information    list  t  private  static  final  long  serial  version  u  i  d  1  l  private  final    type  information  t  element  type  info  public    list  type  info    class  t  element  type  class  this  element  type  info  of  check  not  null  element  type  class  element  type  class  public    list  type  info    type  information  t  element  type  info  this  element  type  info  check  not  null  element  type  info  element  type  info    list  type  info  specific  properties    gets  the  type  information  for  the  elements  contained  in  the  list  public    type  information  t  get  element  type  info  return  element  type  info    type  information  implementation    override  public  boolean  is  basic  type  return  false    override  public  boolean  is  tuple  type  return  false    override  public  int  get  arity  return      override  public  int  get  total  fields  similar  as  arrays  the  lists  are  opaque  to  the  direct  field  addressing  logic  since  the  list  s  elements  are  not  addressable  we  do  not  expose  them  return      suppress  warnings  unchecked    override  public    class    list  t  get  type  class  return    class    list  t    class    list  class    override  public  boolean  is  key  type  return  false    override  public    type  serializer    list  t  create  serializer    execution  config  config    type  serializer  t  element  type  serializer  element  type  info  create  serializer  config  return  new    list  serializer  element  type  serializer    override  public    string  to  string  return    list  element  type  info    override  public  boolean  equals    object  obj  if  obj  this  return  true  else  if  obj  instanceof    list  type  info  final    list  type  info  other    list  type  info  obj  return  other  can  equal  this  element  type  info  equals  other  element  type  info  else  return  false    override  public  int  hash  code  return    element  type  info  hash  code      override  public  boolean  can  equal    object  obj  return  obj  null  obj  get  class  get  class  
public  evolving  public  class    map  type  info  k  v  extends    type  information    map  k  v    the  type  information  for  the  keys  in  the  map  private  final    type  information  k  key  type  info    the  type  information  for  the  values  in  the  map  private  final    type  information  v  value  type  info  public    map  type  info    type  information  k  key  type  info    type  information  v  value  type  info  this  key  type  info    preconditions  check  not  null  key  type  info    the  key  type  information  cannot  be  null  this  value  type  info    preconditions  check  not  null  value  type  info    the  value  type  information  cannot  be  null  public    map  type  info    class  k  key  class    class  v  value  class  this  key  type  info  of  check  not  null  key  class    the  key  class  cannot  be  null  this  value  type  info  of  check  not  null  value  class    the  value  class  cannot  be  null    map  type  info  specific  properties    gets  the  type  information  for  the  keys  in  the  map  public    type  information  k  get  key  type  info  return  key  type  info    gets  the  type  information  for  the  values  in  the  map  public    type  information  v  get  value  type  info  return  value  type  info    type  information  implementation    override  public  boolean  is  basic  type  return  false    override  public  boolean  is  tuple  type  return  false    override  public  int  get  arity  return      override  public  int  get  total  fields  return      suppress  warnings  unchecked    override  public    class    map  k  v  get  type  class  return    class    map  k  v    class    map  class    override  public  boolean  is  key  type  return  false    override  public    type  serializer    map  k  v  create  serializer    execution  config  config    type  serializer  k  key  type  serializer  key  type  info  create  serializer  config    type  serializer  v  value  type  serializer  value  type  info  create  serializer  config  return  new    map  serializer  key  type  serializer  value  type  serializer    override  public    string  to  string  return    map  key  type  info  value  type  info    override  public  boolean  equals    object  obj  if  obj  this  return  true  else  if  obj  instanceof    map  type  info    suppress  warnings  unchecked    map  type  info  k  v  other    map  type  info  k  v  obj  return  other  can  equal  this  key  type  info  equals  other  key  type  info  value  type  info  equals  other  value  type  info  else  return  false    override  public  int  hash  code  return    key  type  info  hash  code  value  type  info  hash  code    override  public  boolean  can  equal    object  obj  return  obj  null  obj  get  class  get  class  
public  evolving  public  final  class    multiset  type  info  t  extends    map  type  info  t    integer  private  static  final  long  serial  version  u  i  d  1  l  public    multiset  type  info    class  t  element  type  class  super  element  type  class    integer  class  public    multiset  type  info    type  information  t  element  type  info  super  element  type  info    basic  type  info  int  type  info    multiset  type  info  specific  properties    gets  the  type  information  for  the  elements  contained  in  the    multiset  public    type  information  t  get  element  type  info  return  get  key  type  info    override  public    string  to  string  return    multiset  get  key  type  info    override  public  boolean  equals    object  obj  if  obj  this  return  true  else  if  obj  instanceof    multiset  type  info  final    multiset  type  info  other    multiset  type  info  obj  return  other  can  equal  this  get  key  type  info  equals  other  get  key  type  info  else  return  false    override  public  int  hash  code  return    get  key  type  info  hash  code      override  public  boolean  can  equal    object  obj  return  obj  null  obj  get  class  get  class    suppress  warnings  unchecked    public  evolving  public  static  c    multiset  type  info  c  get  info  for    type  information  c  component  info  check  not  null  component  info  return  new    multiset  type  info  component  info  
public  evolving  public  class    row  type  info  extends    tuple  type  info  base    row  private  static  final  long  serial  version  u  i  d    l  private  static  final    string  regex  int  field  0-9  private  static  final    string  regex  str  field  p  l  p  l  p    digit  private  static  final    string  regex  field  regex  str  field  regex  int  field  private  static  final    string  regex  nested  fields  regex  field  private  static  final    string  regex  nested  fields  wildcard  regex  nested  fields    expression  keys  select  all  char    expression  keys  select  all  char  scala  private  static  final    pattern  pattern  nested  fields    pattern  compile  regex  nested  fields  private  static  final    pattern  pattern  nested  fields  wildcard    pattern  compile  regex  nested  fields  wildcard  private  static  final    pattern  pattern  int  field    pattern  compile  regex  int  field  protected  final    string  field  names    temporary  variable  for  directly  passing  orders  to  comparators  private  boolean  comparator  orders  null  public    row  type  info    type  information  types  super    row  class  types  this  field  names  new    string  types  length  for  int  i    i  types  length  i  field  names  i  f  i  public    row  type  info    type  information  types    string  field  names  super    row  class  types  check  not  null  field  names    field  names  should  not  be  null  check  argument  types  length  field  names  length    number  of  field  types  and  names  is  different  check  argument  has  duplicate  field  names  field  names    field  names  are  not  unique  this  field  names    arrays  copy  of  field  names  field  names  length    override  public  void  get  flat  fields    string  field  expression  int  offset    list    flat  field  descriptor  result    matcher  matcher  pattern  nested  fields  wildcard  matcher  field  expression  if  matcher  matches  throw  new    invalid  field  reference  exception    invalid  tuple  field  reference  field  expression    string  field  matcher  group    if  field  equals    expression  keys  select  all  char  field  equals    expression  keys  select  all  char  scala  handle  select  all  int  key  position    for    type  information  f  type  types  if  f  type  instanceof    composite  type    composite  type  c  type    composite  type  f  type  c  type  get  flat  fields    expression  keys  select  all  char  offset  key  position  result  key  position  c  type  get  total  fields    else  result  add  new    flat  field  descriptor  offset  key  position  f  type  key  position  else  field  matcher  group      matcher  int  field  matcher  pattern  int  field  matcher  field  int  field  index  if  int  field  matcher  matches  field  expression  is  an  integer  field  index    integer  value  of  field  else  field  index  this  get  field  index  field  fetch  the  field  type  will  throw  exception  if  the  index  is  illegal    type  information  field  type  this  get  type  at  field  index  compute  the  offset  for  int  i    i  field  index  i  offset  this  get  type  at  i  get  total  fields    string  tail  matcher  group    if  tail  null  expression  hasn  t  nested  field  if  field  type  instanceof    composite  type    composite  type  field  type  get  flat  fields  offset  result  else  result  add  new    flat  field  descriptor  offset  field  type  else  expression  has  nested  field  if  field  type  instanceof    composite  type    composite  type  field  type  get  flat  fields  tail  offset  result  else  throw  new    invalid  field  reference  exception    nested  field  expression  tail  not  possible  on  atomic  type  field  type    override  public  x    type  information  x  get  type  at    string  field  expression    matcher  matcher  pattern  nested  fields  matcher  field  expression  if  matcher  matches  if  field  expression  equals    expression  keys  select  all  char  field  expression  equals    expression  keys  select  all  char  scala  throw  new    invalid  field  reference  exception    wildcard  expressions  are  not  allowed  here  else  throw  new    invalid  field  reference  exception    invalid  format  of    row  field  expression  field  expression    string  field  matcher  group      matcher  int  field  matcher  pattern  int  field  matcher  field  int  field  index  if  int  field  matcher  matches  field  expression  is  an  integer  field  index    integer  value  of  field  else  field  index  this  get  field  index  field  fetch  the  field  type  will  throw  exception  if  the  index  is  illegal    type  information  x  field  type  this  get  type  at  field  index    string  tail  matcher  group    if  tail  null  found  the  type  return  field  type  else  if  field  type  instanceof    composite  type  return    composite  type  field  type  get  type  at  tail  else  throw  new    invalid  field  reference  exception    nested  field  expression  tail  not  possible  on  atomic  type  field  type    override  public    type  comparator    row  create  comparator  int  logical  key  fields  boolean  orders  int  logical  field  offset    execution  config  config  comparator  orders  orders    type  comparator    row  comparator  super  create  comparator  logical  key  fields  orders  logical  field  offset  config  comparator  orders  null  return  comparator    override  protected    type  comparator  builder    row  create  type  comparator  builder  if  comparator  orders  null  throw  new    illegal  state  exception    cannot  create  comparator  builder  without  orders  return  new    row  type  comparator  builder  comparator  orders    override  public    string  get  field  names  return  field  names    override  public  int  get  field  index    string  field  name  for  int  i    i  field  names  length  i  if  field  names  i  equals  field  name  return  i  return      override  public    type  serializer    row  create  serializer    execution  config  config  int  len  get  arity    type  serializer  field  serializers  new    type  serializer  len  for  int  i    i  len  i  field  serializers  i  types  i  create  serializer  config  return  new    row  serializer  field  serializers    override  public  boolean  can  equal    object  obj  return  obj  instanceof    row  type  info    override  public  int  hash  code  return    super  hash  code    the  equals  method  does  only  check  for  field  types    field  names  do  not  matter  during  runtime  so  we  can  consider  rows  with  the  same  field  types  as  equal    use  link    row  type  info  schema  equals    object  for  checking  schema  equivalence    override  public  boolean  equals    object  obj  if  obj  instanceof    row  type  info  final    row  type  info  other    row  type  info  obj  return  other  can  equal  this  super  equals  other  else  return  false    override  public    string  to  string    string  builder  bld  new    string  builder    row  if  types  length    bld  append  append  field  names    append  append  types    for  int  i    i  types  length  i  bld  append  append  field  names  i  append  append  types  i  bld  append  return  bld  to  string    creates  a  serializer  for  the  old  link    row  format  before    flink  1.11  p    the  serialization  format  has  changed  from  1.10  to  1.11  and  added  link    row  get  kind    deprecated  public    type  serializer    row  create  legacy  serializer    execution  config  config  int  len  get  arity    type  serializer  field  serializers  new    type  serializer  len  for  int  i    i  len  i  field  serializers  i  types  i  create  serializer  config  return  new    row  serializer  field  serializers  true    returns  the  field  types  of  the  row    the  order  matches  the  order  of  the  field  names  public    type  information  get  field  types  return  types    tests  whether  an  other  object  describes  the  same  schema  equivalent  row  information  public  boolean  schema  equals    object  obj  return  equals  obj    arrays  equals  field  names    row  type  info  obj  field  names  private  boolean  has  duplicate  field  names    string  field  names    hash  set    string  names  new    hash  set  for    string  field  field  names  if  names  add  field  return  true  return  false  private  class    row  type  comparator  builder  implements    type  comparator  builder    row  private  final    array  list    type  comparator  field  comparators  new    array  list    type  comparator  private  final    array  list    integer  logical  key  fields  new    array  list    integer  private  final  boolean  comparator  orders  public    row  type  comparator  builder  boolean  comparator  orders  this  comparator  orders  comparator  orders    override  public  void  initialize  type  comparator  builder  int  size  field  comparators  ensure  capacity  size  logical  key  fields  ensure  capacity  size    override  public  void  add  comparator  field  int  field  id    type  comparator  comparator  field  comparators  add  comparator  logical  key  fields  add  field  id    override  public    type  comparator    row  create  type  comparator    execution  config  config  check  state  field  comparators  size      no  field  comparators  were  defined  for  the    tuple  type  comparator  builder  check  state  logical  key  fields  size      no  key  fields  were  defined  for  the    tuple  type  comparator  builder  check  state  field  comparators  size  logical  key  fields  size    the  number  of  field  comparators  and  key  fields  is  not  equal  final  int  max  key    collections  max  logical  key  fields  check  state  max  key      the  maximum  key  field  must  be  greater  or  equal  than      type  serializer  field  serializers  new    type  serializer  max  key    for  int  i    i  max  key  i  field  serializers  i  types  i  create  serializer  config  int  key  positions  new  int  logical  key  fields  size  for  int  i    i  key  positions  length  i  key  positions  i  logical  key  fields  get  i    type  comparator  comparators  new    type  comparator  field  comparators  size  for  int  i    i  field  comparators  size  i  comparators  i  field  comparators  get  i  noinspection  unchecked  return  new    row  comparator  get  arity  key  positions  comparators    type  serializer    object  field  serializers  comparator  orders    creates  a  link    row  type  info  with  projected  fields  param  row  type    the  original    row  type  info  whose  fields  are  projected  param  field  mapping    the  field  mapping  of  the  projection  return  a    row  type  info  with  projected  fields  public  static    row  type  info  project  fields    row  type  info  row  type  int  field  mapping    type  information  field  types  new    type  information  field  mapping  length    string  field  names  new    string  field  mapping  length  for  int  i    i  field  mapping  length  i  field  types  i  row  type  get  type  at  field  mapping  i  field  names  i  row  type  get  field  names  field  mapping  i  return  new    row  type  info  field  types  field  names  
public  evolving  public  class    akka  options    flag  whether  to  capture  call  stacks  for  rpc  ask  calls  public  static  final    config  option    boolean  capture  ask  callstack    config  options  key  akka  ask  callstack  boolean  type  default  value  true  with  description    if  true  call  stack  for  asynchronous  asks  are  captured    that  way  when  an  ask  fails  for  example  times  out  you  get  a  proper  exception  describing  to  the  original  method  call  and  call  site    note  that  in  case  of  having  millions  of  concurrent  rpc  calls  this  may  add  to  the  memory  footprint    timeout  for  akka  ask  calls  public  static  final    config  option    string  ask  timeout    config  options  key  akka  ask  timeout  string  type  default  value    s  with  description    timeout  used  for  all  futures  and  blocking    akka  calls    if    flink  fails  due  to  timeouts  then  you  should  try  to  increase  this  value    timeouts  can  be  caused  by  slow  machines  or  a  congested  network    the  timeout  value  requires  a  time  unit  specifier  ms  s  min  h  d    the    akka  tcp  connection  timeout  public  static  final    config  option    string  tcp  timeout    config  options  key  akka  tcp  timeout  string  type  default  value    s  with  description    timeout  for  all  outbound  connections    if  you  should  experience  problems  with  connecting  to  a    task  manager  due  to  a  slow  network  you  should  increase  this  value    timeout  for  the  startup  of  the  actor  system  public  static  final    config  option    string  startup  timeout    config  options  key  akka  startup  timeout  string  type  no  default  value  with  description    timeout  after  which  the  startup  of  a  remote  component  is  considered  being  failed    heartbeat  interval  of  the  transport  failure  detector  public  static  final    config  option    string  transport  heartbeat  interval    config  options  key  akka  transport  heartbeat  interval  string  type  default  value    s  with  description    heartbeat  interval  for    akka  s  transport  failure  detector    since    flink  uses  tcp  the  detector  is  not  necessary    therefore  the  detector  is  disabled  by  setting  the  interval  to  a  very  high  value    in  case  you  should  need  the  transport  failure  detector  set  the  interval  to  some  reasonable  value    the  interval  value  requires  a  time  unit  specifier  ms  s  min  h  d    allowed  heartbeat  pause  for  the  transport  failure  detector  public  static  final    config  option    string  transport  heartbeat  pause    config  options  key  akka  transport  heartbeat  pause  string  type  default  value    s  with  description    acceptable  heartbeat  pause  for    akka  s  transport  failure  detector    since    flink  uses  tcp  the  detector  is  not  necessary    therefore  the  detector  is  disabled  by  setting  the  pause  to  a  very  high  value    in  case  you  should  need  the  transport  failure  detector  set  the  pause  to  some  reasonable  value    the  pause  value  requires  a  time  unit  specifier  ms  s  min  h  d    detection  threshold  of  transport  failure  detector  public  static  final    config  option    double  transport  threshold    config  options  key  akka  transport  threshold  double  type  default  value  300.0  with  description    threshold  for  the  transport  failure  detector    since    flink  uses  tcp  the  detector  is  not  necessary  and  thus  the  threshold  is  set  to  a  high  value    override  ssl  support  for  the    akka  transport  public  static  final    config  option    boolean  ssl  enabled    config  options  key  akka  ssl  enabled  boolean  type  default  value  true  with  description    turns  on  ssl  for    akka  s  remote  communication    this  is  applicable  only  when  the  global  ssl  flag  security  ssl  enabled  is  set  to  true    maximum  framesize  of  akka  messages  public  static  final    config  option    string  framesize    config  options  key  akka  framesize  string  type  default  value    b  with  description    maximum  size  of  messages  which  are  sent  between  the    job  manager  and  the    task  managers    if    flink  fails  because  messages  exceed  this  limit  then  you  should  increase  it    the  message  size  requires  a  size  unit  specifier    maximum  number  of  messages  until  another  actor  is  executed  by  the  same  thread  public  static  final    config  option    integer  dispatcher  throughput    config  options  key  akka  throughput  int  type  default  value    with  description    number  of  messages  that  are  processed  in  a  batch  before  returning  the  thread  to  the  pool    low  values  denote  a  fair  scheduling  whereas  high  values  can  increase  the  performance  at  the  cost  of  unfairness    log  lifecycle  events  public  static  final    config  option    boolean  log  lifecycle  events    config  options  key  akka  log  lifecycle  events  boolean  type  default  value  false  with  description    turns  on  the    akka  s  remote  logging  of  events    set  this  value  to  true  in  case  of  debugging    timeout  for  all  blocking  calls  that  look  up  remote  actors  public  static  final    config  option    string  lookup  timeout    config  options  key  akka  lookup  timeout  string  type  default  value    s  with  description    timeout  used  for  the  lookup  of  the    job  manager    the  timeout  value  has  to  contain  a  time  unit  specifier  ms  s  min  h  d    timeout  for  all  blocking  calls  on  the  client  side  deprecated    use  the  code    client  options  client  timeout  instead    deprecated  public  static  final    config  option    string  client  timeout    config  options  key  akka  client  timeout  string  type  default  value    s  with  description  deprecated    use  the  client  timeout  instead    timeout  for  all  blocking  calls  on  the  client  side    exit  jvm  on  fatal    akka  errors  public  static  final    config  option    boolean  jvm  exit  on  fatal  error    config  options  key  akka  jvm  exit  on  fatal  error  boolean  type  default  value  true  with  description    exit  jvm  on  fatal    akka  errors    milliseconds  a  gate  should  be  closed  for  after  a  remote  connection  was  disconnected  public  static  final    config  option    long  retry  gate  closed  for    config  options  key  akka  retry  gate  closed  for  long  type  default  value  50  l  with  description    milliseconds  a  gate  should  be  closed  for  after  a  remote  connection  was  disconnected    configurations  for  fork  join  executor  public  static  final    config  option    double  fork  join  executor  parallelism  factor    config  options  key  akka  fork  join  executor  parallelism  factor  double  type  default  value  2.0  with  description    description  builder  text    the  parallelism  factor  is  used  to  determine  thread  pool  size  using  the  following  formula  ceil  available  processors  factor    resulting  size  is  then  bounded  by  the  parallelism  min  and  parallelism  max  values  build  public  static  final    config  option    integer  fork  join  executor  parallelism  min    config  options  key  akka  fork  join  executor  parallelism  min  int  type  default  value    with  description    description  builder  text    min  number  of  threads  to  cap  factor  based  parallelism  number  to  build  public  static  final    config  option    integer  fork  join  executor  parallelism  max    config  options  key  akka  fork  join  executor  parallelism  max  int  type  default  value    with  description    description  builder  text    max  number  of  threads  to  cap  factor  based  parallelism  number  to  build    configurations  for  client  socket  work  pool  public  static  final    config  option    integer  client  socket  worker  pool  size  min    config  options  key  akka  client  socket  worker  pool  pool  size  min  int  type  default  value    with  description    description  builder  text    min  number  of  threads  to  cap  factor  based  number  to  build  public  static  final    config  option    integer  client  socket  worker  pool  size  max    config  options  key  akka  client  socket  worker  pool  pool  size  max  int  type  default  value    with  description    description  builder  text    max  number  of  threads  to  cap  factor  based  number  to  build  public  static  final    config  option    double  client  socket  worker  pool  size  factor    config  options  key  akka  client  socket  worker  pool  pool  size  factor  double  type  default  value  1.0  with  description    description  builder  text    the  pool  size  factor  is  used  to  determine  thread  pool  size  using  the  following  formula  ceil  available  processors  factor    resulting  size  is  then  bounded  by  the  pool  size  min  and  pool  size  max  values  build    configurations  for  server  socket  work  pool  public  static  final    config  option    integer  server  socket  worker  pool  size  min    config  options  key  akka  server  socket  worker  pool  pool  size  min  int  type  default  value    with  description    description  builder  text    min  number  of  threads  to  cap  factor  based  number  to  build  public  static  final    config  option    integer  server  socket  worker  pool  size  max    config  options  key  akka  server  socket  worker  pool  pool  size  max  int  type  default  value    with  description    description  builder  text    max  number  of  threads  to  cap  factor  based  number  to  build  public  static  final    config  option    double  server  socket  worker  pool  size  factor    config  options  key  akka  server  socket  worker  pool  pool  size  factor  double  type  default  value  1.0  with  description    description  builder  text    the  pool  size  factor  is  used  to  determine  thread  pool  size  using  the  following  formula  ceil  available  processors  factor    resulting  size  is  then  bounded  by  the  pool  size  min  and  pool  size  max  values  build    deprecated  options    the    akka  death  watch  heartbeat  interval  deprecated    don  t  use  this  option  anymore    it  has  no  effect  on    flink    deprecated  public  static  final    config  option    string  watch  heartbeat  interval    config  options  key  akka  watch  heartbeat  interval  default  value  ask  timeout  default  value  with  description    description  builder  text    heartbeat  interval  for    akka  s    death  watch  mechanism  to  detect  dead    task  managers    if    task  managers  are  wrongly  marked  dead  because  of  lost  or  delayed  heartbeat  messages  then  you  should  decrease  this  value  or  increase  akka  watch  heartbeat  pause  a  thorough  description  of    akka  s    death  watch  can  be  found  s  link  http  doc  akka  io  docs  akka  snapshot  scala  remoting  html  failure  detector  here  build    the  maximum  acceptable    akka  death  watch  heartbeat  pause  deprecated    don  t  use  this  option  anymore    it  has  no  effect  on    flink    deprecated  public  static  final    config  option    string  watch  heartbeat  pause    config  options  key  akka  watch  heartbeat  pause  default  value    s  with  description    description  builder  text    acceptable  heartbeat  pause  for    akka  s    death  watch  mechanism  a  low  value  does  not  allow  an  irregular  heartbeat    if    task  managers  are  wrongly  marked  dead  because  of  lost  or  delayed  heartbeat  messages  then  you  should  increase  this  value  or  decrease  akka  watch  heartbeat  interval    higher  value  increases  the  time  to  detect  a  dead    task  manager  a  thorough  description  of    akka  s    death  watch  can  be  found  s  link  http  doc  akka  io  docs  akka  snapshot  scala  remoting  html  failure  detector  here  build    detection  threshold  for  the  phi  accrual  watch  failure  detector  deprecated    don  t  use  this  option  anymore    it  has  no  effect  on    flink    deprecated  public  static  final    config  option    integer  watch  threshold    config  options  key  akka  watch  threshold  default  value    with  description    description  builder  text    threshold  for  the    death  watch  failure  detector  a  low  value  is  prone  to  false  positives  whereas  a  high  value  increases  the  time  to  detect  a  dead    task  manager  a  thorough  description  of    akka  s    death  watch  can  be  found  s  link  http  doc  akka  io  docs  akka  snapshot  scala  remoting  html  failure  detector  here  build  
public  evolving  public  class    blob  server  options    the  config  parameter  defining  the  storage  directory  to  be  used  by  the  blob  server  public  static  final    config  option    string  storage  directory  key  blob  storage  directory  no  default  value  with  description    the  config  parameter  defining  the  storage  directory  to  be  used  by  the  blob  server    the  config  parameter  defining  number  of  retires  for  failed  blob  fetches  public  static  final    config  option    integer  fetch  retries  key  blob  fetch  retries  default  value    with  description    the  config  parameter  defining  number  of  retires  for  failed  blob  fetches    the  config  parameter  defining  the  maximum  number  of  concurrent  blob  fetches  that  the    job  manager  serves  public  static  final    config  option    integer  fetch  concurrent  key  blob  fetch  num  concurrent  default  value    with  description    the  config  parameter  defining  the  maximum  number  of  concurrent  blob  fetches  that  the    job  manager  serves    the  config  parameter  defining  the  backlog  of  blob  fetches  on  the    job  manager  public  static  final    config  option    integer  fetch  backlog  key  blob  fetch  backlog  default  value    with  description    the  config  parameter  defining  the  backlog  of  blob  fetches  on  the    job  manager    the  config  parameter  defining  the  server  port  of  the  blob  service    the  port  can  either  be  a  port  such  as    a  range  of  ports  50100-50200  or  a  list  of  ranges  and  or  points  50100-50200  50300-50400    p    setting  the  port  to    will  let  the  os  choose  an  available  port  public  static  final    config  option    string  port  key  blob  server  port  default  value    with  description    the  config  parameter  defining  the  server  port  of  the  blob  service    flag  to  override  ssl  support  for  the  blob  service  transport  public  static  final    config  option    boolean  ssl  enabled  key  blob  service  ssl  enabled  default  value  true  with  description    flag  to  override  ssl  support  for  the  blob  service  transport    cleanup  interval  of  the  blob  caches  at  the  task  managers  in  seconds  p    whenever  a  job  is  not  referenced  at  the  cache  anymore  we  set  a  ttl  and  let  the  periodic  cleanup  task  executed  every  cleanup  interval  seconds  remove  its  blob  files  after  this  ttl  has  passed    this  means  that  a  blob  will  be  retained  at  most  tt    cleanup  interval  tt  seconds  after  not  being  referenced  anymore    therefore  a  recovery  still  has  the  chance  to  use  existing  files  rather  than  to  download  them  again  public  static  final    config  option    long  cleanup  interval  key  blob  service  cleanup  interval  default  value      l  once  per  hour  with  deprecated  keys  library  cache  manager  cleanup  interval  with  description    cleanup  interval  of  the  blob  caches  at  the  task  managers  in  seconds    the  minimum  size  for  messages  to  be  offloaded  to  the    blob  server  public  static  final    config  option    integer  offload  minsize  key  blob  offload  minsize  default  value          1  mi  b  by  default  with  description    the  minimum  size  for  messages  to  be  offloaded  to  the    blob  server    the  socket  timeout  in  milliseconds  for  the  blob  client  public  static  final    config  option    integer  so  timeout  key  blob  client  socket  timeout  default  value      with  description    the  socket  timeout  in  milliseconds  for  the  blob  client    the  connection  timeout  in  milliseconds  for  the  blob  client  public  static  final    config  option    integer  connect  timeout  key  blob  client  connect  timeout  default  value    with  description    the  connection  timeout  in  milliseconds  for  the  blob  client  
public  evolving  public  class    cluster  options    documentation    section    documentation    sections  expert  fault  tolerance  public  static  final    config  option    long  initial  registration  timeout    config  options  key  cluster  registration  initial  timeout  default  value    l  with  description    initial  registration  timeout  between  cluster  components  in  milliseconds    documentation    section    documentation    sections  expert  fault  tolerance  public  static  final    config  option    long  max  registration  timeout    config  options  key  cluster  registration  max  timeout  default  value    l  with  description    maximum  registration  timeout  between  cluster  components  in  milliseconds    documentation    section    documentation    sections  expert  fault  tolerance  public  static  final    config  option    long  error  registration  delay    config  options  key  cluster  registration  error  delay  default  value    l  with  description    the  pause  made  after  an  registration  attempt  caused  an  exception  other  than  timeout  in  milliseconds    documentation    section    documentation    sections  expert  fault  tolerance  public  static  final    config  option    long  refused  registration  delay    config  options  key  cluster  registration  refused  registration  delay  default  value    l  with  description    the  pause  made  after  the  registration  attempt  was  refused  in  milliseconds    documentation    section    documentation    sections  expert  fault  tolerance  public  static  final    config  option    long  cluster  services  shutdown  timeout    config  options  key  cluster  services  shutdown  timeout  default  value    l  with  description    the  shutdown  timeout  for  cluster  services  like  executors  in  milliseconds    documentation    section    documentation    sections  expert  fault  tolerance  public  static  final    config  option    integer  cluster  io  executor  pool  size    config  options  key  cluster  io  pool  size  int  type  no  default  value  with  description    the  size  of  the  io  executor  pool  used  by  the  cluster  to  execute  blocking  io  operations    master  as  well  as    task  manager  processes    by  default  it  will  use    the  number  of  cpu  cores  hardware  contexts  that  the  cluster  process  has  access  to    increasing  the  pool  size  allows  to  run  more  io  operations  concurrently    documentation    section    documentation    sections  expert  scheduling  public  static  final    config  option    boolean  evenly  spread  out  slots  strategy    config  options  key  cluster  evenly  spread  out  slots  default  value  false  with  description    description  builder  text    enable  the  slot  spread  out  allocation  strategy    this  strategy  tries  to  spread  out  the  slots  evenly  across  all  available  s  code    task  executors  build  
public  evolving  public  class    config  option  t  private  static  final    fallback  key  empty  new    fallback  key    static  final    description  empty  description    description  builder  text  build    the  current  key  for  that  config  option  private  final    string  key    the  list  of  deprecated  keys  in  the  order  to  be  checked  private  final    fallback  key  fallback  keys    the  default  value  for  this  option  private  final  t  default  value    the  description  for  this  option  private  final    description  description    type  of  the  value  that  this    config  option  describes  ul  li  type  class  atomic  class  e  g  code    integer  class  code    config  option    integer  li  li  type  class  code    map  class  code    config  option    map    string    string  li  li  type  class  atomic  class  and  is  list  true  for  code    config  option    list    integer  li  ul  private  final    class  clazz  private  final  boolean  is  list    class  get  clazz  return  clazz  boolean  is  list  return  is  list    creates  a  new  config  option  with  fallback  keys  param  key    the  current  key  for  that  config  option  param  clazz  describes  type  of  the    config  option  see  description  of  the  clazz  field  param  description    description  for  that  option  param  default  value    the  default  value  for  this  option  param  is  list  tells  if  the    config  option  describes  a  list  option  see  description  of  the  clazz  field  param  fallback  keys    the  list  of  fallback  keys  in  the  order  to  be  checked    config  option    string  key    class  clazz    description  description  t  default  value  boolean  is  list    fallback  key  fallback  keys  this  key  check  not  null  key  this  description  description  this  default  value  default  value  this  fallback  keys  fallback  keys  null  fallback  keys  length    empty  fallback  keys  this  clazz  check  not  null  clazz  this  is  list  is  list    creates  a  new  config  option  using  this  option  s  key  and  default  value  and  adding  the  given  fallback  keys  p    when  obtaining  a  value  from  the  configuration  via  link    configuration  get  value    config  option  the  fallback  keys  will  be  checked  in  the  order  provided  to  this  method    the  first  key  for  which  a  value  is  found  will  be  used  that  value  will  be  returned  param  fallback  keys    the  fallback  keys  in  the  order  in  which  they  should  be  checked  return  a  new  config  options  with  the  given  fallback  keys  public    config  option  t  with  fallback  keys    string  fallback  keys  final    stream    fallback  key  new  fallback  keys    arrays  stream  fallback  keys  map    fallback  key  create  fallback  key  final    stream    fallback  key  current  alternative  keys    arrays  stream  this  fallback  keys  put  fallback  keys  first  so  that  they  are  prioritized  final    fallback  key  merged  alternative  keys    stream  concat  new  fallback  keys  current  alternative  keys  to  array    fallback  key  new  return  new    config  option  key  clazz  description  default  value  is  list  merged  alternative  keys    creates  a  new  config  option  using  this  option  s  key  and  default  value  and  adding  the  given  deprecated  keys  p    when  obtaining  a  value  from  the  configuration  via  link    configuration  get  value    config  option  the  deprecated  keys  will  be  checked  in  the  order  provided  to  this  method    the  first  key  for  which  a  value  is  found  will  be  used  that  value  will  be  returned  param  deprecated  keys    the  deprecated  keys  in  the  order  in  which  they  should  be  checked  return  a  new  config  options  with  the  given  deprecated  keys  public    config  option  t  with  deprecated  keys    string  deprecated  keys  final    stream    fallback  key  new  deprecated  keys    arrays  stream  deprecated  keys  map    fallback  key  create  deprecated  key  final    stream    fallback  key  current  alternative  keys    arrays  stream  this  fallback  keys  put  deprecated  keys  last  so  that  they  are  de  prioritized  final    fallback  key  merged  alternative  keys    stream  concat  current  alternative  keys  new  deprecated  keys  to  array    fallback  key  new  return  new    config  option  key  clazz  description  default  value  is  list  merged  alternative  keys    creates  a  new  config  option  using  this  option  s  key  and  default  value  and  adding  the  given  description    the  given  description  is  used  when  generation  the  configuration  documention  param  description    the  description  for  this  option  return  a  new  config  option  with  given  description  public    config  option  t  with  description  final    string  description  return  with  description    description  builder  text  description  build    creates  a  new  config  option  using  this  option  s  key  and  default  value  and  adding  the  given  description    the  given  description  is  used  when  generation  the  configuration  documention  param  description    the  description  for  this  option  return  a  new  config  option  with  given  description  public    config  option  t  with  description  final    description  description  return  new    config  option  key  clazz  description  default  value  is  list  fallback  keys    gets  the  configuration  key  return    the  configuration  key  public    string  key  return  key    checks  if  this  option  has  a  default  value  return    true  if  it  has  a  default  value  false  if  not  public  boolean  has  default  value  return  default  value  null    returns  the  default  value  or  null  if  there  is  no  default  value  return    the  default  value  or  null  public  t  default  value  return  default  value    checks  whether  this  option  has  deprecated  keys  return    true  if  the  option  has  deprecated  keys  false  if  not  deprecated    replaced  by  link  has  fallback  keys    deprecated  public  boolean  has  deprecated  keys  return  fallback  keys  empty    arrays  stream  fallback  keys  any  match    fallback  key  is  deprecated    gets  the  deprecated  keys  in  the  order  to  be  checked  return    the  option  s  deprecated  keys  deprecated    replaced  by  link  fallback  keys    deprecated  public    iterable    string  deprecated  keys  return  fallback  keys  empty    collections  empty  list    arrays  stream  fallback  keys  filter    fallback  key  is  deprecated  map    fallback  key  get  key  collect    collectors  to  list    checks  whether  this  option  has  fallback  keys  return    true  if  the  option  has  fallback  keys  false  if  not  public  boolean  has  fallback  keys  return  fallback  keys  empty    gets  the  fallback  keys  in  the  order  to  be  checked  return    the  option  s  fallback  keys  public    iterable    fallback  key  fallback  keys  return  fallback  keys  empty    collections  empty  list    arrays  as  list  fallback  keys    returns  the  description  of  this  option  return    the  option  s  description  public    description  description  return  description    override  public  boolean  equals    object  o  if  this  o  return  true  else  if  o  null  o  get  class    config  option  class    config  option  that    config  option  o  return  this  key  equals  that  key    arrays  equals  this  fallback  keys  that  fallback  keys  this  default  value  null  that  default  value  null  that  default  value  null  this  default  value  equals  that  default  value  else  return  false    override  public  int  hash  code  return    key  hash  code      arrays  hash  code  fallback  keys  default  value  null  default  value  hash  code      override  public    string  to  string  return    string  format    key  s  default  s  fallback  keys  s  key  default  value    arrays  to  string  fallback  keys  
public  evolving  public  class    config  options    starts  building  a  new  link    config  option  param  key    the  key  for  the  config  option  return    the  builder  for  the  config  option  with  the  given  key  public  static    option  builder  key    string  key  check  not  null  key  return  new    option  builder  key    the  option  builder  is  used  to  create  a  link    config  option    it  is  instantiated  via  link    config  options  key    string  public  static  final  class    option  builder    workaround  to  reuse  the  link    typed  config  option  builder  for  a  link    map    map  lt    string    string  gt    suppress  warnings  unchecked  private  static  final    class    map    string    string  properties  map  class    class    map    string    string    class    map  class    the  key  for  the  config  option  private  final    string  key    creates  a  new    option  builder  param  key    the  key  for  the  config  option    option  builder    string  key  this  key  key    defines  that  the  value  of  the  option  should  be  of  link    boolean  type  public    typed  config  option  builder    boolean  boolean  type  return  new    typed  config  option  builder  key    boolean  class    defines  that  the  value  of  the  option  should  be  of  link    integer  type  public    typed  config  option  builder    integer  int  type  return  new    typed  config  option  builder  key    integer  class    defines  that  the  value  of  the  option  should  be  of  link    long  type  public    typed  config  option  builder    long  long  type  return  new    typed  config  option  builder  key    long  class    defines  that  the  value  of  the  option  should  be  of  link    float  type  public    typed  config  option  builder    float  float  type  return  new    typed  config  option  builder  key    float  class    defines  that  the  value  of  the  option  should  be  of  link    double  type  public    typed  config  option  builder    double  double  type  return  new    typed  config  option  builder  key    double  class    defines  that  the  value  of  the  option  should  be  of  link    string  type  public    typed  config  option  builder    string  string  type  return  new    typed  config  option  builder  key    string  class    defines  that  the  value  of  the  option  should  be  of  link    duration  type  public    typed  config  option  builder    duration  duration  type  return  new    typed  config  option  builder  key    duration  class    defines  that  the  value  of  the  option  should  be  of  link    memory  size  type  public    typed  config  option  builder    memory  size  memory  type  return  new    typed  config  option  builder  key    memory  size  class    defines  that  the  value  of  the  option  should  be  of  link    enum  type  param  enum  class    concrete  type  of  the  expected  enum  public  t  extends    enum  t    typed  config  option  builder  t  enum  type    class  t  enum  class  return  new    typed  config  option  builder  key  enum  class    defines  that  the  value  of  the  option  should  be  a  set  of  properties  which  can  be  represented  as  code    map    string    string  public    typed  config  option  builder    map    string    string  map  type  return  new    typed  config  option  builder  key  properties  map  class    creates  a    config  option  with  the  given  default  value  p    this  method  does  not  accept  null    for  options  with  no  default  value  choose  one  of  the  code  no  default  value  methods  param  value    the  default  value  for  the  config  option  param  t    the  type  of  the  default  value  return    the  config  option  with  the  default  value  deprecated  define  the  type  explicitly  first  with  one  of  the  int  type  string  type  etc    deprecated  public  t    config  option  t  default  value  t  value  check  not  null  value  return  new    config  option  key  value  get  class    config  option  empty  description  value  false    creates  a  string  valued  option  with  no  default  value    string  valued  options  are  the  only  ones  that  can  have  no  default  value  return    the  created    config  option  deprecated  define  the  type  explicitly  first  with  one  of  the  int  type  string  type  etc    deprecated  public    config  option    string  no  default  value  return  new    config  option  key    string  class    config  option  empty  description  null  false    builder  for  link    config  option  with  a  defined  atomic  type  param  t  atomic  type  of  the  option  public  static  class    typed  config  option  builder  t  private  final    string  key  private  final    class  t  clazz    typed  config  option  builder    string  key    class  t  clazz  this  key  key  this  clazz  clazz    defines  that  the  option  s  type  should  be  a  list  of  previously  defined  atomic  type  public    list  config  option  builder  t  as  list  return  new    list  config  option  builder  key  clazz    creates  a    config  option  with  the  given  default  value  param  value    the  default  value  for  the  config  option  return    the  config  option  with  the  default  value  public    config  option  t  default  value  t  value  return  new    config  option  key  clazz    config  option  empty  description  value  false    creates  a    config  option  without  a  default  value  return    the  config  option  without  a  default  value  public    config  option  t  no  default  value  return  new    config  option  key  clazz    description  builder  text  build  null  false    builder  for  link    config  option  of  list  of  type  link  e  param  e  list  element  type  of  the  option  public  static  class    list  config  option  builder  e  private  final    string  key  private  final    class  e  clazz    list  config  option  builder    string  key    class  e  clazz  this  key  key  this  clazz  clazz    creates  a    config  option  with  the  given  default  value  param  values    the  list  of  default  values  for  the  config  option  return    the  config  option  with  the  default  value    safe  varargs  public  final    config  option    list  e  default  values  e  values  return  new    config  option  key  clazz    config  option  empty  description    arrays  as  list  values  true    creates  a    config  option  without  a  default  value  return    the  config  option  without  a  default  value  public    config  option    list  e  no  default  value  return  new    config  option  key  clazz    config  option  empty  description  null  true    not  intended  to  be  instantiated  private    config  options  
public  evolving    config  groups  groups    config  group  name    environment  key  prefix  env  public  class    core  options    classloading    parameters    defines  the  class  resolution  strategy  when  loading  classes  from  user  code  meaning  whether  to  first  check  the  user  code  jar  code  child  first  or  the  application  classpath  code  parent  first  p    the  default  settings  indicate  to  load  classes  first  from  the  user  code  jar  which  means  that  user  code  jars  can  include  and  load  different  dependencies  than    flink  uses  transitively  p    exceptions  to  the  rules  are  defined  via  link  always  parent  first  loader  patterns    documentation    section    documentation    sections  expert  class  loading  public  static  final    config  option    string  classloader  resolve  order    config  options  key  classloader  resolve  order  default  value  child  first  with  description    defines  the  class  resolution  strategy  when  loading  classes  from  user  code  meaning  whether  to  first  check  the  user  code  jar  child  first  or  the  application  classpath  parent  first    the  default  settings  indicate  to  load  classes  first  from  the  user  code  jar  which  means  that  user  code  jars  can  include  and  load  different  dependencies  than    flink  uses  transitively    the  namespace  patterns  for  classes  that  are  loaded  with  a  preference  from  the  parent  classloader  meaning  the  application  class  path  rather  than  any  user  code  jar  file    this  option  only  has  an  effect  when  link  classloader  resolve  order  is  set  to  code  child  first  p    it  is  important  that  all  classes  whose  objects  move  between    flink  s  runtime  and  any  user  code  including    flink  connectors  that  run  as  part  of  the  user  code  are  covered  by  these  patterns  here    otherwise  it  is  be  possible  that  the    flink  runtime  and  the  user  code  load  two  different  copies  of  a  class  through  the  different  class  loaders    that  leads  to  errors  like  x  cannot  be  cast  to  x  exceptions  where  both  class  names  are  equal  or  x  cannot  be  assigned  to  y  where  x  should  be  a  subclass  of  y  p    the  following  classes  are  loaded  parent  first  to  avoid  any  duplication  ul  li    all  core    java  classes  java  because  they  must  never  be  duplicated  li  li    all  core    scala  classes  scala    currently    scala  is  used  in  the    flink  runtime  and  in  the  user  code  and  some    scala  classes  cross  the  boundary  such  as  the  i    function  x  i  classes    that  may  change  if    scala  eventually  lives  purely  as  part  of  the  user  code  li  li    all    flink  classes  org  apache  flink    note  that  this  means  that  connectors  and  formats  flink  avro  etc  are  loaded  parent  first  as  well  if  they  are  in  the  core  classpath  li  li    java  annotations  and  loggers  defined  by  the  following  list  javax  annotation  org  slf4j  org  apache  log4j  org  apache  logging  org  apache  commons  logging  ch  qos  logback    this  is  done  for  convenience  to  avoid  duplication  of  annotations  and  multiple  log  bindings  li  ul    documentation    section    documentation    sections  expert  class  loading  public  static  final    config  option    string  always  parent  first  loader  patterns    config  options  key  classloader  parent  first  patterns  default  default  value  java  scala  org  apache  flink  com  esotericsoftware  kryo  org  apache  hadoop  javax  annotation  org  slf4j  org  apache  log4j  org  apache  logging  org  apache  commons  logging  ch  qos  logback  org  xml  javax  xml  org  apache  xerces  org  w3c  with  deprecated  keys  classloader  parent  first  patterns  with  description  a  semicolon  separated  list  of  patterns  that  specifies  which  classes  should  always  be  resolved  through  the  parent    class  loader  first  a  pattern  is  a  simple  prefix  that  is  checked  against  the  fully  qualified  class  name    this  setting  should  generally  not  be  modified    to  add  another  pattern  we  recommend  to  use  classloader  parent  first  patterns  additional  instead    documentation    section    documentation    sections  expert  class  loading  public  static  final    config  option    string  always  parent  first  loader  patterns  additional    config  options  key  classloader  parent  first  patterns  additional  default  value  with  description  a  semicolon  separated  list  of  patterns  that  specifies  which  classes  should  always  be  resolved  through  the  parent    class  loader  first  a  pattern  is  a  simple  prefix  that  is  checked  against  the  fully  qualified  class  name    these  patterns  are  appended  to  always  parent  first  loader  patterns  key    documentation    section    documentation    sections  expert  class  loading  public  static  final    config  option    boolean  fail  on  user  class  loading  metaspace  oom    config  options  key  classloader  fail  on  metaspace  oom  error  boolean  type  default  value  true  with  description    fail    flink  jvm  processes  if    out  of  memory  error    metaspace  is  thrown  while  trying  to  load  a  user  code  class  public  static    string  get  parent  first  loader  patterns    configuration  config    string  base  config  get  string  always  parent  first  loader  patterns    string  append  config  get  string  always  parent  first  loader  patterns  additional  return  parse  parent  first  loader  patterns  base  append    plugin  specific  option  of  link  always  parent  first  loader  patterns    plugins  use  this  parent  first  list  instead  of  the  global  version    documentation    exclude  from  documentation    plugin  classloader  list  is  considered  an  implementation  detail    configuration  only  included  in  case  to  mitigate  unintended  side  effects  of  this  young  feature  public  static  final    config  option    string  plugin  always  parent  first  loader  patterns    config  options  key  plugin  classloader  parent  first  patterns  default  string  type  default  value  java  scala  org  apache  flink  javax  annotation  org  slf4j  org  apache  log4j  org  apache  logging  org  apache  commons  logging  ch  qos  logback  with  description  a  semicolon  separated  list  of  patterns  that  specifies  which  classes  should  always  be  resolved  through  the  plugin  parent    class  loader  first  a  pattern  is  a  simple  prefix  that  is  checked  against  the  fully  qualified  class  name    this  setting  should  generally  not  be  modified    to  add  another  pattern  we  recommend  to  use  plugin  classloader  parent  first  patterns  additional  instead    documentation    exclude  from  documentation    plugin  classloader  list  is  considered  an  implementation  detail    configuration  only  included  in  case  to  mitigate  unintended  side  effects  of  this  young  feature  public  static  final    config  option    string  plugin  always  parent  first  loader  patterns  additional    config  options  key  plugin  classloader  parent  first  patterns  additional  string  type  default  value  with  description  a  semicolon  separated  list  of  patterns  that  specifies  which  classes  should  always  be  resolved  through  the  plugin  parent    class  loader  first  a  pattern  is  a  simple  prefix  that  is  checked  against  the  fully  qualified  class  name    these  patterns  are  appended  to  plugin  always  parent  first  loader  patterns  key  public  static    string  get  plugin  parent  first  loader  patterns    configuration  config    string  base  config  get  string  plugin  always  parent  first  loader  patterns    string  append  config  get  string  plugin  always  parent  first  loader  patterns  additional  return  parse  parent  first  loader  patterns  base  append  private  static    string  parse  parent  first  loader  patterns    string  base    string  append    splitter  splitter    splitter  on  omit  empty  strings  return    iterables  to  array    iterables  concat  splitter  split  base  splitter  split  append    string  class  process  parameters  public  static  final    config  option    string  flink  jvm  options    config  options  key  env  java  opts  string  type  default  value  with  description    description  builder  text    java  options  to  start  the  jvm  of  all    flink  processes  with  build  public  static  final    config  option    string  flink  jm  jvm  options    config  options  key  env  java  opts  jobmanager  string  type  default  value  with  description    description  builder  text    java  options  to  start  the  jvm  of  the    job  manager  with  build  public  static  final    config  option    string  flink  tm  jvm  options    config  options  key  env  java  opts  taskmanager  string  type  default  value  with  description    description  builder  text    java  options  to  start  the  jvm  of  the    task  manager  with  build  public  static  final    config  option    string  flink  hs  jvm  options    config  options  key  env  java  opts  historyserver  string  type  default  value  with  description    description  builder  text    java  options  to  start  the  jvm  of  the    history  server  with  build  public  static  final    config  option    string  flink  cli  jvm  options    config  options  key  env  java  opts  client  string  type  default  value  with  description    description  builder  text    java  options  to  start  the  jvm  of  the    flink    client  with  build    this  options  is  here  only  for  documentation  generation  it  is  only  evaluated  in  the  shell  scripts    suppress  warnings  unused  public  static  final    config  option    string  flink  log  dir    config  options  key  env  log  dir  no  default  value  with  description    defines  the  directory  where  the    flink  logs  are  saved    it  has  to  be  an  absolute  path    defaults  to  the  log  directory  under    flink  s  home    this  options  is  here  only  for  documentation  generation  it  is  only  evaluated  in  the  shell  scripts    suppress  warnings  unused  public  static  final    config  option    integer  flink  log  max    config  options  key  env  log  max  default  value    with  description    the  maximum  number  of  old  log  files  to  keep    this  options  is  here  only  for  documentation  generation  it  is  only  evaluated  in  the  shell  scripts    suppress  warnings  unused  public  static  final    config  option    string  flink  ssh  options    config  options  key  env  ssh  opts  no  default  value  with  description    additional  command  line  options  passed  to  ssh  clients  when  starting  or  stopping    job  manager    task  manager  and    zookeeper  services  start  cluster  sh  stop  cluster  sh  start  zookeeper  quorum  sh  stop  zookeeper  quorum  sh    this  options  is  here  only  for  documentation  generation  it  is  only  evaluated  in  the  shell  scripts    suppress  warnings  unused  public  static  final    config  option    string  flink  hadoop  conf  dir    config  options  key  env  hadoop  conf  dir  no  default  value  with  description    path  to  hadoop  configuration  directory    it  is  required  to  read  hdfs  and  or  yarn  configuration    you  can  also  set  it  via  environment  variable    this  options  is  here  only  for  documentation  generation  it  is  only  evaluated  in  the  shell  scripts    suppress  warnings  unused  public  static  final    config  option    string  flink  yarn  conf  dir    config  options  key  env  yarn  conf  dir  no  default  value  with  description    path  to  yarn  configuration  directory    it  is  required  to  run  flink  on  yarn    you  can  also  set  it  via  environment  variable    this  options  is  here  only  for  documentation  generation  it  is  only  evaluated  in  the  shell  scripts    suppress  warnings  unused  public  static  final    config  option    string  flink  hbase  conf  dir    config  options  key  env  hbase  conf  dir  no  default  value  with  description    path  to  hbase  configuration  directory    it  is  required  to  read  hbase  configuration    you  can  also  set  it  via  environment  variable  generic  io    the  config  parameter  defining  the  directories  for  temporary  files  separated  by  or  the  system  s  link  java  io    file  path  separator    documentation    override  default  local  dirs  on    yarn    flink  tmp  dir  on    mesos    system  get  property  java  io  tmpdir  in  standalone    documentation    section    documentation    sections  common  miscellaneous  public  static  final    config  option    string  tmp  dirs  key  io  tmp  dirs  default  value    system  get  property  java  io  tmpdir  with  deprecated  keys  taskmanager  tmp  dirs  with  description    directories  for  temporary  files  separated  by  or  the  system  s  java  io    file  path  separator  program  public  static  final    config  option    integer  default  parallelism    config  options  key  parallelism  default  default  value    with  description    default  parallelism  for  jobs  file  systems    the  default  filesystem  scheme  used  for  paths  that  do  not  declare  a  scheme  explicitly    documentation    section    documentation    sections  common  miscellaneous  public  static  final    config  option    string  default  filesystem  scheme    config  options  key  fs  default  scheme  no  default  value  with  description    the  default  filesystem  scheme  used  for  paths  that  do  not  declare  a  scheme  explicitly    may  contain  an  authority  e  g  host  port  in  case  of  an  hdfs    name  node    documentation    section    documentation    sections  common  miscellaneous  public  static  final    config  option    string  allowed  fallback  filesystems    config  options  key  fs  allowed  fallback  filesystems  string  type  default  value  with  description  a  semicolon  separated  list  of  file  schemes  for  which    hadoop  can  be  used  instead  of  an  appropriate    flink  plugin  example  s3  wasb    specifies  whether  file  output  writers  should  overwrite  existing  files  by  default    documentation    section    documentation    sections  deprecated  file  sinks  public  static  final    config  option    boolean  filesytem  default  override  key  fs  overwrite  files  default  value  false  with  description    specifies  whether  file  output  writers  should  overwrite  existing  files  by  default    set  to  true  to  overwrite  by  default  false  otherwise    specifies  whether  the  file  systems  should  always  create  a  directory  for  the  output  even  with  a  parallelism  of  one    documentation    section    documentation    sections  deprecated  file  sinks  public  static  final    config  option    boolean  filesystem  output  always  create  directory  key  fs  output  always  create  directory  default  value  false  with  description    file  writers  running  with  a  parallelism  larger  than  one  create  a  directory  for  the  output  file  path  and  put  the  different  result  files  one  per  parallel  writer  task  into  that  directory    if  this  option  is  set  to  true  writers  with  a  parallelism  of    will  also  create  a  directory  and  place  a  single  result  file  into  it    if  the  option  is  set  to  false  the  writer  will  directly  create  the  file  directly  at  the  output  path  without  creating  a  containing  directory    the  total  number  of  input  plus  output  connections  that  a  file  system  for  the  given  scheme  may  open    unlimited  be  default  public  static    config  option    integer  file  system  connection  limit    string  scheme  return    config  options  key  fs  scheme  limit  total  default  value      the  total  number  of  input  connections  that  a  file  system  for  the  given  scheme  may  open    unlimited  be  default  public  static    config  option    integer  file  system  connection  limit  in    string  scheme  return    config  options  key  fs  scheme  limit  input  default  value      the  total  number  of  output  connections  that  a  file  system  for  the  given  scheme  may  open    unlimited  be  default  public  static    config  option    integer  file  system  connection  limit  out    string  scheme  return    config  options  key  fs  scheme  limit  output  default  value      if  any  connection  limit  is  configured  this  option  can  be  optionally  set  to  define  after  which  time  in  milliseconds  stream  opening  fails  with  a  timeout  exception  if  no  stream  connection  becomes  available    unlimited  timeout  be  default  public  static    config  option    long  file  system  connection  limit  timeout    string  scheme  return    config  options  key  fs  scheme  limit  timeout  default  value  0  l    if  any  connection  limit  is  configured  this  option  can  be  optionally  set  to  define  after  which  time  in  milliseconds  inactive  streams  are  reclaimed    this  option  can  help  to  prevent  that  inactive  streams  make  up  the  full  pool  of  limited  connections  and  no  further  connections  can  be  established    unlimited  timeout  be  default  public  static    config  option    long  file  system  connection  limit  stream  inactivity  timeout    string  scheme  return    config  options  key  fs  scheme  limit  stream  timeout  default  value  0  l  
public  evolving  public  class    deployment  options  public  static  final    config  option    string  target  key  execution  target  string  type  no  default  value  with  description    the  deployment  target  for  the  execution  e  g  local  for  local  execution  public  static  final    config  option    boolean  attached  key  execution  attached  boolean  type  default  value  false  with  description    specifies  if  the  pipeline  is  submitted  in  attached  or  detached  mode  public  static  final    config  option    boolean  shutdown  if  attached  key  execution  shutdown  on  attached  exit  boolean  type  default  value  false  with  description    if  the  job  is  submitted  in  attached  mode  perform  a  best  effort  cluster  shutdown  when  the  cli  is  terminated  abruptly  e  g  in  response  to  a  user  interrupt  such  as  typing    ctrl  c  public  static  final    config  option    list    string  job  listeners  key  execution  job  listeners  string  type  as  list  no  default  value  with  description    custom    job  listeners  to  be  registered  with  the  execution  environment    the  registered  listeners  cannot  have  constructors  with  arguments  
public  evolving  public  class    execution  options    should  be  moved  to  code    execution  checkpointing  options  along  with  code    execution  config  use  snapshot  compression  which  should  be  put  into  code    checkpoint  config  public  static  final    config  option    boolean  snapshot  compression    config  options  key  execution  checkpointing  snapshot  compression  boolean  type  default  value  false  with  description    tells  if  we  should  use  compression  for  the  state  snapshot  data  or  not  public  static  final    config  option    duration  buffer  timeout    config  options  key  execution  buffer  timeout  duration  type  default  value    duration  of  millis    with  description    description  builder  text    the  maximum  time  frequency  milliseconds  for  the  flushing  of  the  output  buffers    by  default  the  output  buffers  flush  frequently  to  provide  low  latency  and  to  aid  smooth  developer  experience    setting  the  parameter  can  result  in  three  logical  modes  list    text  element  text  a  positive  value  triggers  flushing  periodically  by  that  interval    text  element  text    triggers  flushing  after  every  record  thus  minimizing  latency    text  element  text    ms  triggers  flushing  only  when  the  output  buffer  is  full  thus  maximizing  throughput  build  
public  evolving  public  class    external  resource  options    the  amount  of  the  external  resource  per  task  executor    this  is  used  as  a  suffix  in  an  actual  config  public  static  final    string  external  resource  amount  suffix  amount    the  driver  factory  class  of  the  external  resource  to  use    this  is  used  as  a  suffix  in  an  actual  config  public  static  final    string  external  resource  driver  factory  suffix  driver  factory  class    the  suffix  of  custom  config  options  prefix  for  the  external  resource  public  static  final    string  external  resource  driver  param  suffix  param    the  naming  pattern  of  custom  config  options  for  the  external  resource    this  is  used  as  a  suffix  private  static  final    string  external  resource  driver  param  pattern  suffix  external  resource  driver  param  suffix  param    the  prefix  for  all  external  resources  configs    has  to  be  combined  with  a  resource  name  and  the  configs  mentioned  below  private  static  final    string  external  resource  prefix  external  resource    list  of  the  resource  name  of  all  external  resources  with  delimiter  e  g  gpu  fpga  for  two  external  resource  gpu  and  fpga    the  resource  name  will  be  used  to  splice  related  config  options  for  external  resource    only  the  resource  name  defined  here  will  go  into  effect  in  external  resource  framework  p    example  pre  code  external  resources  gpu  fpga  external  resource  gpu  driver  factory  class  org  apache  flink  externalresource  gpu    g  p  u  driver  factory  external  resource  gpu  amount    external  resource  gpu  param  type  nvidia  external  resource  fpga  driver  factory  class  org  apache  flink  externalresource  fpga    f  p  g  a  driver  factory  external  resource  fpga  amount    pre  public  static  final    config  option    list    string  external  resource  list  key  external  resources  string  type  as  list  default  values  with  description    list  of  the  resource  name  of  all  external  resources  with  delimiter  e  g  gpu  fpga  for  two  external  resource  gpu  and  fpga    the  resource  name  will  be  used  to  splice  related  config  options  for  external  resource    only  the  resource  name  defined  here  will  go  into  effect  by  external  resource  framework    defines  the  factory  class  name  for  the  external  resource  identified  by  gt  resource  name  lt    the  factory  will  be  used  to  instantiate  the  link  org  apache  flink  api  common  externalresource    external  resource  driver  at  the    task  executor  side  p    it  is  intentionally  included  into  user  docs  while  unused    suppress  warnings  unused  public  static  final    config  option    string  external  resource  driver  factory  class  key  generic  key  with  suffix  external  resource  driver  factory  suffix  string  type  no  default  value  with  description    defines  the  factory  class  name  for  the  external  resource  identified  by  resource  name    the  factory  will  be  used  to  instantiated  the    external  resource  driver  at  the    task  executor  side    for  example  org  apache  flink  externalresource  gpu    g  p  u  driver  factory    the  amount  for  the  external  resource  specified  by  gt  resource  name  lt  per    task  executor  p    it  is  intentionally  included  into  user  docs  while  unused    suppress  warnings    weaker  access  public  static  final    config  option    long  external  resource  amount  key  generic  key  with  suffix  external  resource  amount  suffix  long  type  no  default  value  with  description    the  amount  for  the  external  resource  specified  by  resource  name  per    task  executor    the  naming  pattern  of  custom  config  options  for  the  external  resource  specified  by  gt  resource  name  lt    only  the  configurations  that  follow  this  pattern  would  be  passed  into  the  driver  factory  of  that  external  resource  p    it  is  intentionally  included  into  user  docs  while  unused    suppress  warnings  unused  public  static  final    config  option    string  external  resource  driver  param  key  generic  key  with  suffix  external  resource  driver  param  pattern  suffix  string  type  no  default  value  with  description    the  naming  pattern  of  custom  config  options  for  the  external  resource  specified  by  resource  name    only  the  configurations  that  follow  this  pattern  would  be  passed  into  the  driver  factory  of  that  external  resource  public  static    string  generic  key  with  suffix    string  suffix  return  key  with  resource  name  and  suffix  resource  name  suffix    generate  the  config  option  key  with  resource  name  and  suffix  private  static    string  key  with  resource  name  and  suffix    string  resource  name    string  suffix  return    string  format  s  s  s  external  resource  prefix    preconditions  check  not  null  resource  name    preconditions  check  not  null  suffix    generate  the  config  option  key  for  the  amount  of  external  resource  with  resource  name  public  static    string  get  amount  config  option  for  resource    string  resource  name  return  key  with  resource  name  and  suffix  resource  name  external  resource  amount  suffix    generate  the  config  option  key  for  the  configuration  key  of  external  resource  in  the  deploying  system  public  static    string  get  system  config  key  config  option  for  resource    string  resource  name    string  suffix  return  key  with  resource  name  and  suffix  resource  name  suffix    generate  the  config  option  key  for  the  factory  class  name  of  link  org  apache  flink  api  common  externalresource    external  resource  driver  public  static    string  get  external  resource  driver  factory  config  option  for  resource    string  resource  name  return  key  with  resource  name  and  suffix  resource  name  external  resource  driver  factory  suffix    generate  the  suffix  option  key  prefix  for  the  user  defined  params  for  external  resources  public  static    string  get  external  resource  param  config  prefix  for  resource    string  resource  name  return  key  with  resource  name  and  suffix  resource  name  external  resource  driver  param  suffix  
public  evolving  public  class    heartbeat  manager  options    time  interval  for  requesting  heartbeat  from  sender  side    documentation    section    documentation    sections  expert  fault  tolerance  public  static  final    config  option    long  heartbeat  interval  key  heartbeat  interval  default  value    l  with  description    time  interval  for  requesting  heartbeat  from  sender  side    timeout  for  requesting  and  receiving  heartbeat  for  both  sender  and  receiver  sides    documentation    section    documentation    sections  expert  fault  tolerance  public  static  final    config  option    long  heartbeat  timeout  key  heartbeat  timeout  default  value    l  with  description    timeout  for  requesting  and  receiving  heartbeat  for  both  sender  and  receiver  sides    not  intended  to  be  instantiated  private    heartbeat  manager  options  
public  evolving    documentation    section    documentation    sections  expert  zookeeper  high  availability  public  static  final    config  option    string  ha  zookeeper  mesos  workers  path  key  high  availability  zookeeper  path  mesos  workers  default  value  mesos  workers  with  deprecated  keys  recovery  zookeeper  path  mesos  workers  with  description    description  builder  text    the    zoo  keeper  root  path  for  persisting  the    mesos  worker  information  build  
public  evolving  public  class    history  server  options    the  interval  at  which  the    history  server  polls  link    history  server  options  history  server  archive  dirs  for  new  archives  public  static  final    config  option    long  history  server  archive  refresh  interval  key  historyserver  archive  fs  refresh  interval  default  value    l  with  description    interval  in  milliseconds  for  refreshing  the  archived  job  directories    comma  separated  list  of  directories  which  the    history  server  polls  for  new  archives  public  static  final    config  option    string  history  server  archive  dirs  key  historyserver  archive  fs  dir  no  default  value  with  description    comma  separated  list  of  directories  to  fetch  archived  jobs  from    the  history  server  will  monitor  these  directories  for  archived  jobs    you  can  configure  the    job  manager  to  archive  jobs  to  a  directory  via  jobmanager  archive  fs  dir    if  this  option  is  enabled  then  deleted  job  archives  are  also  deleted  from    history  server  public  static  final    config  option    boolean  history  server  cleanup  expired  jobs  key  historyserver  archive  clean  expired  jobs  default  value  false  with  description    string  format    whether    history  server  should  cleanup  jobs  that  are  no  longer  present  s  history  server  archive  dirs  key    the  local  directory  used  by  the    history  server  web  frontend  public  static  final    config  option    string  history  server  web  dir  key  historyserver  web  tmpdir  no  default  value  with  description    this  configuration  parameter  allows  defining  the    flink  web  directory  to  be  used  by  the  history  server  web  interface    the  web  interface  will  copy  its  static  files  into  the  directory    the  address  under  which  the    history  server  web  frontend  is  accessible  public  static  final    config  option    string  history  server  web  address  key  historyserver  web  address  no  default  value  with  description    address  of  the    history  server  s  web  interface    the  port  under  which  the    history  server  web  frontend  is  accessible  public  static  final    config  option    integer  history  server  web  port  key  historyserver  web  port  default  value    with  description    port  of  the    history  servers  s  web  interface    the  refresh  interval  for  the    history  server  web  frontend  in  milliseconds  public  static  final    config  option    long  history  server  web  refresh  interval  key  historyserver  web  refresh  interval  default  value    l  with  description    the  refresh  interval  for  the    history  server  web  frontend  in  milliseconds    enables    disables  ssl  support  for  the    history  server  web  frontend    only  relevant  if  link    security  options  ssl  rest  enabled  is  enabled  public  static  final    config  option    boolean  history  server  web  ssl  enabled  key  historyserver  web  ssl  enabled  default  value  false  with  description    enable    h  t  t  ps  access  to  the    history  server  web  frontend    this  is  applicable  only  when  the  global  ssl  flag  security  ssl  enabled  is  set  to  true  public  static  final    config  option    integer  history  server  retained  jobs  key  historyserver  archive  retained  jobs  int  type  default  value    with  description    description  builder  text    string  format    the  maximum  number  of  jobs  to  retain  in  each  archive  directory  defined  by  s  history  server  archive  dirs  key  text    if  set  to    default  there  is  no  limit  to  the  number  of  archives  text    if  set  to    or  less  than      history  server  will  throw  an  s  code    illegal  configuration  exception  build  private    history  server  options  
public  evolving  public  class    illegal  configuration  exception  extends    runtime  exception  private  static  final  long  serial  version  u  i  d    l    constructs  an  new    illegal  configuration  exception  with  the  given  error  message  param  message    the  error  message  for  the  exception  public    illegal  configuration  exception    string  message  super  message    constructs  an  new    illegal  configuration  exception  with  the  given  error  message  format  and  arguments  param  format    the  error  message  format  for  the  exception  param  arguments    the  arguments  for  the  format  public    illegal  configuration  exception    string  format    object  arguments  super    string  format  format  arguments    constructs  an  new    illegal  configuration  exception  with  the  given  error  message  and  a  given  cause  param  message    the  error  message  for  the  exception  param  cause    the  exception  that  caused  this  exception  public    illegal  configuration  exception    string  message    throwable  cause  super  message  cause  
public  evolving  public  class    job  manager  options  public  static  final    memory  size  min  jvm  heap  size    memory  size  of  mebi  bytes      the  config  parameter  defining  the  network  address  to  connect  to  for  communication  with  the  job  manager  p    this  value  is  only  interpreted  in  setups  where  a  single    job  manager  with  static  name  or  address  exists  simple  standalone  setups  or  container  setups  with  dynamic  service  name  resolution    it  is  not  used  in  many  high  availability  setups  when  a  leader  election  service  like    zoo  keeper  is  used  to  elect  and  discover  the    job  manager  leader  from  potentially  multiple  standby    job  managers    documentation    section    documentation    sections  common  host  port    documentation    sections  all  job  manager  public  static  final    config  option    string  address  key  jobmanager  rpc  address  no  default  value  with  description    the  config  parameter  defining  the  network  address  to  connect  to  for  communication  with  the  job  manager    this  value  is  only  interpreted  in  setups  where  a  single    job  manager  with  static  name  or  address  exists  simple  standalone  setups  or  container  setups  with  dynamic  service  name  resolution    it  is  not  used  in  many  high  availability  setups  when  a  leader  election  service  like    zoo  keeper  is  used  to  elect  and  discover  the    job  manager  leader  from  potentially  multiple  standby    job  managers    the  local  address  of  the  network  interface  that  the  job  manager  binds  to  public  static  final    config  option    string  bind  host  key  jobmanager  bind  host  string  type  no  default  value  with  description    the  local  address  of  the  network  interface  that  the  job  manager  binds  to    if  not  configured  0.0  0.0  will  be  used    the  config  parameter  defining  the  network  port  to  connect  to  for  communication  with  the  job  manager  p    like  link    job  manager  options  address  this  value  is  only  interpreted  in  setups  where  a  single    job  manager  with  static  name  address  and  port  exists  simple  standalone  setups  or  container  setups  with  dynamic  service  name  resolution    this  config  option  is  not  used  in  many  high  availability  setups  when  a  leader  election  service  like    zoo  keeper  is  used  to  elect  and  discover  the    job  manager  leader  from  potentially  multiple  standby    job  managers    documentation    section    documentation    sections  common  host  port    documentation    sections  all  job  manager  public  static  final    config  option    integer  port  key  jobmanager  rpc  port  default  value    with  description    the  config  parameter  defining  the  network  port  to  connect  to  for  communication  with  the  job  manager    like  address  key  this  value  is  only  interpreted  in  setups  where  a  single    job  manager  with  static  name  address  and  port  exists  simple  standalone  setups  or  container  setups  with  dynamic  service  name  resolution    this  config  option  is  not  used  in  many  high  availability  setups  when  a  leader  election  service  like    zoo  keeper  is  used  to  elect  and  discover  the    job  manager  leader  from  potentially  multiple  standby    job  managers    the  local  port  that  the  job  manager  binds  to  public  static  final    config  option    integer  rpc  bind  port  key  jobmanager  rpc  bind  port  int  type  no  default  value  with  description    the  local  rpc  port  that  the    job  manager  binds  to    if  not  configured  the  external  port  configured  by  port  key  will  be  used  jvm  heap  size  for  the    job  manager  with  memory  size  deprecated  use  link  total  flink  memory  for  standalone  setups  and  link  total  process  memory  for  containerized  setups    deprecated    documentation    section    documentation    sections  all  job  manager  public  static  final    config  option    memory  size  job  manager  heap  memory  key  jobmanager  heap  size  memory  type  no  default  value  with  description  jvm  heap  size  for  the    job  manager  jvm  heap  size  in  megabytes  for  the    job  manager  deprecated  use  link  total  flink  memory  for  standalone  setups  and  link  total  process  memory  for  containerized  setups    deprecated  public  static  final    config  option    integer  job  manager  heap  memory  mb  key  jobmanager  heap  mb  int  type  no  default  value  with  description  jvm  heap  size  in  megabytes  for  the    job  manager    total    process    memory  size  for  the    job  manager    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  total  process  memory  key  jobmanager  memory  process  size  memory  type  no  default  value  with  description    total    process    memory  size  for  the    job  manager    this  includes  all  the  memory  that  a    job  manager  jvm  process  consumes  consisting  of    total    flink    memory  jvm    metaspace  and  jvm    overhead    in  containerized  setups  this  should  be  set  to  the  container  memory    see  also  jobmanager  memory  flink  size  for    total    flink    memory  size  configuration    total    flink    memory  size  for  the    job  manager    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  total  flink  memory  key  jobmanager  memory  flink  size  memory  type  no  default  value  with  description    string  format    total    flink    memory  size  for  the    job  manager    this  includes  all  the  memory  that  a    job  manager  consumes  except  for  jvm    metaspace  and  jvm    overhead    it  consists  of  jvm    heap    memory  and    off  heap    memory    see  also  s  for  total  process  memory  size  configuration  total  process  memory  key  jvm    heap    memory  size  for  the    job  manager    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  jvm  heap  memory  key  jobmanager  memory  heap  size  memory  type  no  default  value  with  description  jvm    heap    memory  size  for    job  manager    the  minimum  recommended  jvm    heap  size  is  min  jvm  heap  size  to  human  readable  string    off  heap    memory  size  for  the    job  manager    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  off  heap  memory  key  jobmanager  memory  off  heap  size  memory  type  default  value    memory  size  of  mebi  bytes    with  description    description  builder  text    off  heap    memory  size  for    job  manager    this  option  covers  all  off  heap  memory  usage  including  direct  and  native  memory  allocation    the  jvm  direct  memory  limit  of  the    job  manager  process  xx    max  direct  memory  size  will  be  set  to  this  value  if  the  limit  is  enabled  by  jobmanager  memory  enable  jvm  direct  memory  limit  build    off  heap    memory  size  for  the    job  manager    documentation    section    documentation    sections  common  memory  public  static  final    config  option    boolean  jvm  direct  memory  limit  enabled  key  jobmanager  memory  enable  jvm  direct  memory  limit  boolean  type  default  value  false  with  description    description  builder  text    whether  to  enable  the  jvm  direct  memory  limit  of  the    job  manager  process  xx    max  direct  memory  size    the  limit  will  be  set  to  the  value  of  s  option  text  off  heap  memory  key  build  jvm    metaspace    size  for  the    job  manager    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  jvm  metaspace  key  jobmanager  memory  jvm  metaspace  size  memory  type  default  value    memory  size  of  mebi  bytes    with  description  jvm    metaspace    size  for  the    job  manager  private  static  final    string  jvm  overhead  description    this  is  off  heap  memory  reserved  for  jvm  overhead  such  as  thread  stack  space  compile  cache  etc    this  includes  native  memory  but  not  direct  memory  and  will  not  be  counted  when    flink  calculates  jvm  max  direct  memory  size  parameter    the  size  of  jvm    overhead  is  derived  to  make  up  the  configured  fraction  of  the    total    process    memory    if  the  derived  size  is  less  or  greater  than  the  configured  min  or  max  size  the  min  or  max  size  will  be  used    the  exact  size  of  jvm    overhead  can  be  explicitly  specified  by  setting  the  min  and  max  size  to  the  same  value    min  jvm    overhead  size  for  the    job  manager    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  jvm  overhead  min  key  jobmanager  memory  jvm  overhead  min  memory  type  default  value    memory  size  of  mebi  bytes    with  description    min  jvm    overhead  size  for  the    job  manager  jvm  overhead  description    max  jvm    overhead  size  for  the    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  jvm  overhead  max  key  jobmanager  memory  jvm  overhead  max  memory  type  default  value    memory  size  parse  1g  with  description    max  jvm    overhead  size  for  the    job  manager  jvm  overhead  description    fraction  of    total    process    memory  to  be  reserved  for  jvm    overhead    documentation    section    documentation    sections  common  memory  public  static  final    config  option    float  jvm  overhead  fraction  key  jobmanager  memory  jvm  overhead  fraction  float  type  default  value  0.1  f  with  description    fraction  of    total    process    memory  to  be  reserved  for  jvm    overhead  jvm  overhead  description    the  maximum  number  of  prior  execution  attempts  kept  in  history    documentation    section    documentation    sections  all  job  manager  public  static  final    config  option    integer  max  attempts  history  size  key  jobmanager  execution  attempts  history  size  default  value    with  deprecated  keys  job  manager  max  attempts  history  size  with  description    the  maximum  number  of  prior  execution  attempts  kept  in  history    this  option  specifies  the  failover  strategy  i  e  how  the  job  computation  recovers  from  task  failures    documentation    section    documentation    sections  all  job  manager    documentation    sections  expert  fault  tolerance  public  static  final    config  option    string  execution  failover  strategy  key  jobmanager  execution  failover  strategy  string  type  default  value  region  with  description    description  builder  text    this  option  specifies  how  the  job  computation  recovers  from  task  failures    accepted  values  are  list  text  full    restarts  all  tasks  to  recover  the  job  text  region    restarts  all  tasks  that  could  be  affected  by  the  task  failure    more  details  can  be  found  s  link  dev  task  failure  recovery  html  restart  pipelined  region  failover  strategy  here  build    the  location  where  the    job  manager  stores  the  archives  of  completed  jobs    documentation    section    documentation    sections  all  job  manager  public  static  final    config  option    string  archive  dir  key  jobmanager  archive  fs  dir  no  default  value  with  description    dictionary  for    job  manager  to  store  the  archives  of  completed  jobs    the  job  store  cache  size  in  bytes  which  is  used  to  keep  completed  jobs  in  memory    documentation    section    documentation    sections  all  job  manager  public  static  final    config  option    long  job  store  cache  size  key  jobstore  cache  size  default  value  50  l    l    l  with  description    the  job  store  cache  size  in  bytes  which  is  used  to  keep  completed  jobs  in  memory    the  time  in  seconds  after  which  a  completed  job  expires  and  is  purged  from  the  job  store    documentation    section    documentation    sections  all  job  manager  public  static  final    config  option    long  job  store  expiration  time  key  jobstore  expiration  time  default  value  60  l  60  l  with  description    the  time  in  seconds  after  which  a  completed  job  expires  and  is  purged  from  the  job  store    the  max  number  of  completed  jobs  that  can  be  kept  in  the  job  store    documentation    section    documentation    sections  all  job  manager  public  static  final    config  option    integer  job  store  max  capacity  key  jobstore  max  capacity  default  value    integer  max  value  with  description    the  max  number  of  completed  jobs  that  can  be  kept  in  the  job  store    the  timeout  in  milliseconds  for  requesting  a  slot  from    slot    pool    documentation    section    documentation    sections  expert  scheduling  public  static  final    config  option    long  slot  request  timeout  key  slot  request  timeout  default  value  5  l  60  l    l  with  description    the  timeout  in  milliseconds  for  requesting  a  slot  from    slot    pool    the  timeout  in  milliseconds  for  a  idle  slot  in    slot    pool    documentation    section    documentation    sections  expert  scheduling  public  static  final    config  option    long  slot  idle  timeout  key  slot  idle  timeout  default  matches  heartbeat  timeout  so  that  sticky  allocation  is  not  lost  on  timeouts  for  local  recovery  default  value    heartbeat  manager  options  heartbeat  timeout  default  value  with  description    the  timeout  in  milliseconds  for  a  idle  slot  in    slot    pool    config  parameter  determining  the  scheduler  implementation    documentation    exclude  from  documentation    scheduler  n  g  is  still  in  development  public  static  final    config  option    string  scheduler  key  jobmanager  scheduler  string  type  default  value  ng  with  description    description  builder  text    determines  which  scheduler  implementation  is  used  to  schedule  tasks    accepted  values  are  list  text  ng  new  generation  scheduler  build    config  parameter  controlling  whether  partitions  should  already  be  released  during  the  job  execution    documentation    exclude  from  documentation    user  normally  should  not  be  expected  to  deactivate  this  feature    we  aim  at  removing  this  flag  eventually  public  static  final    config  option    boolean  partition  release  during  job  execution  key  jobmanager  partition  release  during  job  execution  default  value  true  with  description    controls  whether  partitions  should  already  be  released  during  the  job  execution  private    job  manager  options  throw  new    illegal  access  error  
public  evolving  public  class    memory  size  implements  java  io    serializable    comparable    memory  size  private  static  final  long  serial  version  u  i  d  1  l  public  static  final    memory  size  zero  new    memory  size  0  l  public  static  final    memory  size  max  value  new    memory  size    long  max  value  private  static  final    list    memory  unit  ordered  units    arrays  as  list  bytes  kilo  bytes  mega  bytes  giga  bytes  tera  bytes    the  memory  size  in  bytes  private  final  long  bytes    the  memorized  value  returned  by  to  string  private  transient    string  stringified    the  memorized  value  returned  by  to  human  readable  string  private  transient    string  human  readable  str    constructs  a  new    memory  size  param  bytes    the  size  in  bytes    must  be  zero  or  larger  public    memory  size  long  bytes  check  argument  bytes    bytes  must  be    this  bytes  bytes  public  static    memory  size  of  mebi  bytes  long  mebi  bytes  return  new    memory  size  mebi  bytes      gets  the  memory  size  in  bytes  public  long  get  bytes  return  bytes    gets  the  memory  size  in    kibibytes    bytes  public  long  get  kibi  bytes  return  bytes      gets  the  memory  size  in    mebibytes      kibibytes  public  int  get  mebi  bytes  return  int  bytes      gets  the  memory  size  in    gibibytes      mebibytes  public  long  get  gibi  bytes  return  bytes      gets  the  memory  size  in    tebibytes      gibibytes  public  long  get  tebi  bytes  return  bytes      override  public  int  hash  code  return  int  bytes  bytes      override  public  boolean  equals    object  obj  return  obj  this  obj  null  obj  get  class  this  get  class    memory  size  obj  bytes  this  bytes    override  public    string  to  string  if  stringified  null  stringified  format  to  string  return  stringified  private    string  format  to  string    memory  unit  highest  integer  unit    int  stream  range    ordered  units  size  sequential  filter  idx  bytes  ordered  units  get  idx  get  multiplier    boxed  find  first  map  idx  if  idx    return  ordered  units  get    else  return  ordered  units  get  idx    or  else  bytes  return    string  format  d  s  bytes  highest  integer  unit  get  multiplier  highest  integer  unit  get  units    public    string  to  human  readable  string  if  human  readable  str  null  human  readable  str  format  to  human  readable  string  return  human  readable  str  private    string  format  to  human  readable  string    memory  unit  highest  unit    int  stream  range    ordered  units  size  sequential  filter  idx  bytes  ordered  units  get  idx  get  multiplier  boxed  max    comparator  natural  order  map  ordered  units  get  or  else  bytes  if  highest  unit  bytes  return    string  format  d  s  bytes  bytes  get  units    else  double  approximate  1.0  bytes  highest  unit  get  multiplier  return    string  format    locale  root  3f  s  d  bytes  approximate  highest  unit  get  units    bytes    override  public  int  compare  to    memory  size  that  return    long  compare  this  bytes  that  bytes    calculations  public    memory  size  add    memory  size  that  return  new    memory  size    math  add  exact  this  bytes  that  bytes  public    memory  size  subtract    memory  size  that  return  new    memory  size    math  subtract  exact  this  bytes  that  bytes  public    memory  size  multiply  double  multiplier  check  argument  multiplier    multiplier  must  be      big  decimal  product    big  decimal  value  of  this  bytes  multiply    big  decimal  value  of  multiplier  if  product  compare  to    big  decimal  value  of    long  max  value    throw  new    arithmetic  exception  long  overflow  return  new    memory  size  product  long  value  public    memory  size  divide  long  by  check  argument  by    divisor  must  be    return  new    memory  size  bytes  by    parsing    parses  the  given  string  as  as    memory  size  param  text    the  string  to  parse  return    the  parsed    memory  size  throws    illegal  argument  exception    thrown  if  the  expression  cannot  be  parsed  public  static    memory  size  parse    string  text  throws    illegal  argument  exception  return  new    memory  size  parse  bytes  text    parses  the  given  string  with  a  default  unit  param  text    the  string  to  parse  param  default  unit  specify  the  default  unit  return    the  parsed    memory  size  throws    illegal  argument  exception    thrown  if  the  expression  cannot  be  parsed  public  static    memory  size  parse    string  text    memory  unit  default  unit  throws    illegal  argument  exception  if  has  unit  text  return  parse  text  default  unit  get  units    return  parse  text    parses  the  given  string  as  bytes    the  supported  expressions  are  listed  under  link    memory  size  param  text    the  string  to  parse  return    the  parsed  size  in  bytes  throws    illegal  argument  exception    thrown  if  the  expression  cannot  be  parsed  public  static  long  parse  bytes    string  text  throws    illegal  argument  exception  check  not  null  text  text  final    string  trimmed  text  trim  check  argument  trimmed  is  empty  argument  is  an  empty  or  whitespace  only  string  final  int  len  trimmed  length  int  pos    char  current  while  pos  len  current  trimmed  char  at  pos    current    pos  final    string  number  trimmed  substring    pos  final    string  unit  trimmed  substring  pos  trim  to  lower  case    locale  us  if  number  is  empty  throw  new    number  format  exception  text  does  not  start  with  a  number  final  long  value  try  value    long  parse  long  number  this  throws  a    number  format  exception  on  overflow  catch    number  format  exception  e  throw  new    illegal  argument  exception    the  value  number  cannot  be  re  represented  as  64bit  number  numeric  overflow  final  long  multiplier  parse  unit  unit  map    memory  unit  get  multiplier  or  else  1  l  final  long  result  value  multiplier  check  for  overflow  if  result  multiplier  value  throw  new    illegal  argument  exception    the  value  text  cannot  be  re  represented  as  64bit  number  of  bytes  numeric  overflow  return  result  private  static    optional    memory  unit  parse  unit    string  unit  if  matches  any  unit  bytes  return    optional  of  bytes  else  if  matches  any  unit  kilo  bytes  return    optional  of  kilo  bytes  else  if  matches  any  unit  mega  bytes  return    optional  of  mega  bytes  else  if  matches  any  unit  giga  bytes  return    optional  of  giga  bytes  else  if  matches  any  unit  tera  bytes  return    optional  of  tera  bytes  else  if  unit  is  empty  throw  new    illegal  argument  exception    memory  size  unit  unit  does  not  match  any  of  the  recognized  units    memory  unit  get  all  units  return    optional  empty  private  static  boolean  matches  any    string  str    memory  unit  unit  for    string  s  unit  get  units  if  s  equals  str  return  true  return  false    enum  which  defines  memory  unit  mostly  used  to  parse  value  from  configuration  file  p    to  make  larger  values  more  compact  the  common  size  suffixes  are  supported  ul  li  q  or  1b  or  1bytes  bytes  li  1k  or  1kb  or  1kibibytes  interpreted  as  kibibytes    bytes  li  1m  or  1mb  or  1mebibytes  interpreted  as  mebibytes    kibibytes  li  1g  or  1gb  or  1gibibytes  interpreted  as  gibibytes    mebibytes  li  1t  or  1tb  or  1tebibytes  interpreted  as  tebibytes    gibibytes  ul  public  enum    memory  unit  bytes  new    string  b  bytes  1  l  kilo  bytes  new    string  k  kb  kibibytes    l  mega  bytes  new    string  m  mb  mebibytes    l    l  giga  bytes  new    string  g  gb  gibibytes    l    l    l  tera  bytes  new    string  t  tb  tebibytes    l    l    l    l  private  final    string  units  private  final  long  multiplier    memory  unit    string  units  long  multiplier  this  units  units  this  multiplier  multiplier  public    string  get  units  return  units  public  long  get  multiplier  return  multiplier  public  static    string  get  all  units  return  concatenate  units  bytes  get  units  kilo  bytes  get  units  mega  bytes  get  units  giga  bytes  get  units  tera  bytes  get  units  public  static  boolean  has  unit    string  text  check  not  null  text  text  final    string  trimmed  text  trim  check  argument  trimmed  is  empty  argument  is  an  empty  or  whitespace  only  string  final  int  len  trimmed  length  int  pos    char  current  while  pos  len  current  trimmed  char  at  pos    current    pos  final    string  unit  trimmed  substring  pos  trim  to  lower  case    locale  us  return  unit  length    private  static    string  concatenate  units  final    string  all  units  final    string  builder  builder  new    string  builder    for    string  units  all  units  builder  append  for    string  unit  units  builder  append  unit  builder  append  builder  set  length  builder  length    builder  append  builder  set  length  builder  length    return  builder  to  string  
public  evolving  public  class    metric  options    an  optional  list  of  reporter  names    if  configured  only  reporters  whose  name  matches  any  of  the  names  in  the  list  will  be  started    otherwise  all  reporters  that  could  be  found  in  the  configuration  will  be  started  p    example  pre  code  metrics  reporters  foo  bar  metrics  reporter  foo  class  org  apache  flink  metrics  reporter    j  m  x  reporter  metrics  reporter  foo  interval    metrics  reporter  bar  class  org  apache  flink  metrics  graphite    graphite  reporter  metrics  reporter  bar  port    pre  public  static  final    config  option    string  reporters  list  key  metrics  reporters  no  default  value  with  description    an  optional  list  of  reporter  names    if  configured  only  reporters  whose  name  matches  any  of  the  names  in  the  list  will  be  started    otherwise  all  reporters  that  could  be  found  in  the  configuration  will  be  started  public  static  final    config  option    string  reporter  class  key  metrics  reporter  name  class  no  default  value  with  description    the  reporter  class  to  use  for  the  reporter  named  name  public  static  final    config  option    duration  reporter  interval  key  metrics  reporter  name  interval  duration  type  default  value    duration  of  seconds    with  description    the  reporter  interval  to  use  for  the  reporter  named  name  public  static  final    config  option    string  reporter  config  parameter  key  metrics  reporter  name  parameter  no  default  value  with  description    configures  the  parameter  parameter  for  the  reporter  named  name    the  delimiter  used  to  assemble  the  metric  identifier  public  static  final    config  option    string  scope  delimiter  key  metrics  scope  delimiter  default  value  with  description    delimiter  used  to  assemble  the  metric  identifier    the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a    job  manager  public  static  final    config  option    string  scope  naming  jm  key  metrics  scope  jm  default  value  host  jobmanager  with  description    defines  the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a    job  manager    the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a    task  manager  public  static  final    config  option    string  scope  naming  tm  key  metrics  scope  tm  default  value  host  taskmanager  tm  id  with  description    defines  the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a    task  manager    the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a  job  on  a    job  manager  public  static  final    config  option    string  scope  naming  jm  job  key  metrics  scope  jm  job  default  value  host  jobmanager  job  name  with  description    defines  the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a  job  on  a    job  manager    the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a  job  on  a    task  manager  public  static  final    config  option    string  scope  naming  tm  job  key  metrics  scope  tm  job  default  value  host  taskmanager  tm  id  job  name  with  description    defines  the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a  job  on  a    task  manager    the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a  task  public  static  final    config  option    string  scope  naming  task  key  metrics  scope  task  default  value  host  taskmanager  tm  id  job  name  task  name  subtask  index  with  description    defines  the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  a  task    the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  an  operator  public  static  final    config  option    string  scope  naming  operator  key  metrics  scope  operator  default  value  host  taskmanager  tm  id  job  name  operator  name  subtask  index  with  description    defines  the  scope  format  string  that  is  applied  to  all  metrics  scoped  to  an  operator  public  static  final    config  option    long  latency  interval  key  metrics  latency  interval  default  value  0  l  with  description    defines  the  interval  at  which  latency  tracking  marks  are  emitted  from  the  sources    disables  latency  tracking  if  set  to    or  a  negative  value    enabling  this  feature  can  significantly  impact  the  performance  of  the  cluster  public  static  final    config  option    string  latency  source  granularity  key  metrics  latency  granularity  default  value  operator  with  description    description  builder  text    defines  the  granularity  of  latency  metrics    accepted  values  are  list  text  single    track  latency  without  differentiating  between  sources  and  subtasks  text  operator    track  latency  while  differentiating  between  sources  but  not  subtasks  text  subtask    track  latency  while  differentiating  between  sources  and  subtasks  build    the  number  of  measured  latencies  to  maintain  at  each  operator  public  static  final    config  option    integer  latency  history  size  key  metrics  latency  history  size  default  value    with  description    defines  the  number  of  measured  latencies  to  maintain  at  each  operator    whether    flink  should  report  system  resource  metrics  such  as  machine  s  cpu  memory  or  network  usage  public  static  final    config  option    boolean  system  resource  metrics  key  metrics  system  resource  default  value  false  with  description    flag  indicating  whether    flink  should  report  system  resource  metrics  such  as  machine  s  cpu  memory  or  network  usage    interval  between  probing  of  system  resource  metrics  specified  in  milliseconds    has  an  effect  only  when  link  system  resource  metrics  is  enabled  public  static  final    config  option    long  system  resource  metrics  probing  interval  key  metrics  system  resource  probing  interval  default  value    l  with  description    interval  between  probing  of  system  resource  metrics  specified  in  milliseconds    has  an  effect  only  when  system  resource  metrics  key  is  enabled    the  default  network  port  range  for    flink  s  internal  metric  query  service    the  code    means  that    flink  searches  for  a  free  port    documentation    section    documentation    sections  common  host  port  public  static  final    config  option    string  query  service  port  key  metrics  internal  query  service  port  default  value    with  description    the  port  range  used  for    flink  s  internal  metric  query  service    accepts  a  list  of  ports  50100  50101  ranges  50100-50200  or  a  combination  of  both    it  is  recommended  to  set  a  range  of  ports  to  avoid  collisions  when  multiple    flink  components  are  running  on  the  same  machine    per  default    flink  will  pick  a  random  port    the  thread  priority  for    flink  s  internal  metric  query  service    the  code    means  the  min  priority  and  the  code    means  the  max  priority  public  static  final    config  option    integer  query  service  thread  priority  key  metrics  internal  query  service  thread  priority  default  value    with  description    the  thread  priority  used  for    flink  s  internal  metric  query  service    the  thread  is  created  by    akka  s  thread  pool  executor    the  range  of  the  priority  is  from    min  priority  to    max  priority    warning  increasing  this  value  may  bring  the  main    flink  components  down    the  config  parameter  defining  the  update  interval  for  the  metric  fetcher  used  by  the  web  ui  in  milliseconds  public  static  final    config  option    long  metric  fetcher  update  interval  key  metrics  fetcher  update  interval  default  value    l  with  description    update  interval  for  the  metric  fetcher  used  by  the  web  ui  in  milliseconds    decrease  this  value  for  faster  updating  metrics    increase  this  value  if  the  metric  fetcher  causes  too  much  load    setting  this  value  to    disables  the  metric  fetching  completely  private    metric  options  
public  evolving  public  class    netty  shuffle  environment  options    network    general    options    the  default  network  port  the  task  manager  expects  to  receive  transfer  envelopes  on    the  code    means  that  the    task  manager  searches  for  a  free  port    documentation    section    documentation    sections  common  host  port    documentation    sections  all  task  manager  public  static  final    config  option    integer  data  port  key  taskmanager  data  port  default  value    with  description    the  task  manager  s  external  port  used  for  data  exchange  operations    the  local  network  port  that  the  task  manager  listen  at  for  data  exchange  public  static  final    config  option    integer  data  bind  port  key  taskmanager  data  bind  port  int  type  no  default  value  with  description    the  task  manager  s  bind  port  used  for  data  exchange  operations    if  not  configured  data  port  key  will  be  used    config  parameter  to  override  ssl  support  for  taskmanager  s  data  transport    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    boolean  data  ssl  enabled  key  taskmanager  data  ssl  enabled  default  value  true  with  description    enable  ssl  support  for  the  taskmanager  data  transport    this  is  applicable  only  when  the  global  flag  for  internal  ssl    security  options  ssl  internal  enabled  key  is  set  to  true    boolean  flag  indicating  whether  the  shuffle  data  will  be  compressed  for  blocking  shuffle  mode  p    note    data  is  compressed  per  buffer  and  compression  can  incur  extra  cpu  overhead  so  it  is  more  effective  for  io  bounded  scenario  when  data  compression  ratio  is  high    currently  shuffle  data  compression  is  an  experimental  feature  and  the  config  option  can  be  changed  in  the  future    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    boolean  blocking  shuffle  compression  enabled  key  taskmanager  network  blocking  shuffle  compression  enabled  default  value  false  with  description    boolean  flag  indicating  whether  the  shuffle  data  will  be  compressed  for  blocking  shuffle  mode    note  that  data  is  compressed  per  buffer  and  compression  can  incur  extra  cpu  overhead  so  it  is  more  effective  for  io  bounded  scenario  when  data  compression  ratio  is  high    currently  shuffle  data  compression  is  an  experimental  feature  and  the  config  option  can  be  changed  in  the  future    the  codec  to  be  used  when  compressing  shuffle  data    documentation    exclude  from  documentation    currently    l  z4  is  the  only  legal  option  public  static  final    config  option    string  shuffle  compression  codec  key  taskmanager  network  compression  codec  default  value    l  z4  with  description    the  codec  to  be  used  when  compressing  shuffle  data    boolean  flag  to  enable  disable  more  detailed  metrics  about  inbound  outbound  network  queue  lengths    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    boolean  network  detailed  metrics  key  taskmanager  network  detailed  metrics  default  value  false  with  description    boolean  flag  to  enable  disable  more  detailed  metrics  about  inbound  outbound  network  queue  lengths    number  of  buffers  used  in  the  network  stack    this  defines  the  number  of  possible  tasks  and  shuffles  deprecated  use  link    task  manager  options  network  memory  fraction  link    task  manager  options  network  memory  min  and  link    task  manager  options  network  memory  max  instead    deprecated  public  static  final    config  option    integer  network  num  buffers  key  taskmanager  network  number  of  buffers  default  value      fraction  of  jvm  memory  to  use  for  network  buffers  deprecated  use  link    task  manager  options  network  memory  fraction  instead    deprecated  public  static  final    config  option    float  network  buffers  memory  fraction  key  taskmanager  network  memory  fraction  default  value  0.1  f  with  description    fraction  of  jvm  memory  to  use  for  network  buffers    this  determines  how  many  streaming  data  exchange  channels  a    task  manager  can  have  at  the  same  time  and  how  well  buffered  the  channels  are    if  a  job  is  rejected  or  you  get  a  warning  that  the  system  has  not  enough  buffers  available  increase  this  value  or  the  min  max  values  below    also  note  that  taskmanager  network  memory  min  and  taskmanager  network  memory  max  may  override  this  fraction    minimum  memory  size  for  network  buffers  deprecated  use  link    task  manager  options  network  memory  min  instead    deprecated  public  static  final    config  option    string  network  buffers  memory  min  key  taskmanager  network  memory  min  default  value  64mb  with  description    minimum  memory  size  for  network  buffers    maximum  memory  size  for  network  buffers  deprecated  use  link    task  manager  options  network  memory  max  instead    deprecated  public  static  final    config  option    string  network  buffers  memory  max  key  taskmanager  network  memory  max  default  value  1gb  with  description    maximum  memory  size  for  network  buffers    number  of  network  buffers  to  use  for  each  outgoing  incoming  channel  subpartition  input  channel  p    reasoning    buffer  for  in  flight  data  in  the  subpartition    buffer  for  parallel  serialization    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  network  buffers  per  channel  key  taskmanager  network  memory  buffers  per  channel  default  value    with  description    number  of  exclusive  network  buffers  to  use  for  each  outgoing  incoming  channel  subpartition  inputchannel  in  the  credit  based  flow  control  model    it  should  be  configured  at  least    for  good  performance    buffer  is  for  receiving  in  flight  data  in  the  subpartition  and    buffer  is  for  parallel  serialization    number  of  extra  network  buffers  to  use  for  each  outgoing  incoming  gate  result  partition  input  gate    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  network  extra  buffers  per  gate  key  taskmanager  network  memory  floating  buffers  per  gate  default  value    with  description    number  of  extra  network  buffers  to  use  for  each  outgoing  incoming  gate  result  partition  input  gate    in  credit  based  flow  control  mode  this  indicates  how  many  floating  credits  are  shared  among  all  the  input  channels    the  floating  buffers  are  distributed  based  on  backlog  real  time  output  buffers  in  the  subpartition  feedback  and  can  help  relieve  back  pressure  caused  by  unbalanced  data  distribution  among  the  subpartitions    this  value  should  be  increased  in  case  of  higher  round  trip  times  between  nodes  and  or  larger  number  of  machines  in  the  cluster    number  of  max  buffers  can  be  used  for  each  output  subparition    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  network  max  buffers  per  channel  key  taskmanager  network  memory  max  buffers  per  channel  default  value    with  description    number  of  max  buffers  that  can  be  used  for  each  channel    if  a  channel  exceeds  the  number  of  max  buffers  it  will  make  the  task  become  unavailable  cause  the  back  pressure  and  block  the  data  processing    this  might  speed  up  checkpoint  alignment  by  preventing  excessive  growth  of  the  buffered  in  flight  data  in  case  of  data  skew  and  high  number  of  configured  floating  buffers    this  limit  is  not  strictly  guaranteed  and  can  be  ignored  by  things  like  flat  map  operators  records  spanning  multiple  buffers  or  single  timer  producing  large  amount  of  data    the  timeout  for  requesting  exclusive  buffers  for  each  channel    documentation    exclude  from  documentation    this  option  is  purely  implementation  related  and  may  be  removed  as  the  implementation  changes  public  static  final    config  option    long  network  exclusive  buffers  request  timeout  milliseconds  key  taskmanager  network  memory  exclusive  buffers  request  timeout  ms  default  value    l  with  description    the  timeout  for  requesting  exclusive  buffers  for  each  channel    since  the  number  of  maximum  buffers  and  the  number  of  required  buffers  is  not  the  same  for  local  buffer  pools  there  may  be  deadlock  cases  that  the  upstream  tasks  have  occupied  all  the  buffers  and  the  downstream  tasks  are  waiting  for  the  exclusive  buffers    the  timeout  breaks  the  tie  by  failing  the  request  of  exclusive  buffers  and  ask  users  to  increase  the  number  of  total  buffers    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    string  network  blocking  shuffle  type  key  taskmanager  network  blocking  shuffle  type  default  value  file  with  description    the  blocking  shuffle  type  either  mmap  or  file    the  auto  means  selecting  the  property  type  automatically  based  on  system  memory  architecture    bit  for  mmap  and    bit  for  file    note  that  the  memory  usage  of  mmap  is  not  accounted  by  configured  memory  limits  but  some  resource  frameworks  like  yarn  would  track  this  memory  usage  and  kill  the  container  once  memory  exceeding  some  threshold    also  note  that  this  option  is  experimental  and  might  be  changed  future    netty    options    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  num  arenas  key  taskmanager  network  netty  num  arenas  default  value    with  deprecated  keys  taskmanager  net  num  arenas  with  description    the  number  of    netty  arenas    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  num  threads  server  key  taskmanager  network  netty  server  num  threads  default  value    with  deprecated  keys  taskmanager  net  server  num  threads  with  description    the  number  of    netty  server  threads    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  num  threads  client  key  taskmanager  network  netty  client  num  threads  default  value    with  deprecated  keys  taskmanager  net  client  num  threads  with  description    the  number  of    netty  client  threads    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  connect  backlog  key  taskmanager  network  netty  server  backlog  default  value    default      netty  s  default  with  deprecated  keys  taskmanager  net  server  backlog  with  description    the  netty  server  connection  backlog    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  client  connect  timeout  seconds  key  taskmanager  network  netty  client  connect  timeout  sec  default  value    default    s  2min  with  deprecated  keys  taskmanager  net  client  connect  timeout  sec  with  description    the    netty  client  connection  timeout    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  network  retries  key  taskmanager  network  retries  default  value    with  deprecated  keys  taskmanager  network  retries  with  description    the  number  of  retry  attempts  for  network  communication    currently  it  s  only  used  for  establishing  input  output  channel  connections    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  send  receive  buffer  size  key  taskmanager  network  netty  send  receive  buffer  size  default  value    default      netty  s  default  with  deprecated  keys  taskmanager  net  send  receive  buffer  size  with  description    the    netty  send  and  receive  buffer  size    this  defaults  to  the  system  buffer  size  cat  proc  sys  net  ipv4  tcp  rw  mem  and  is      mi  b  in  modern    linux    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    string  transport  type  key  taskmanager  network  netty  transport  default  value  auto  with  deprecated  keys  taskmanager  net  transport  with  description    the    netty  transport  type  either  nio  or  epoll    the  auto  means  selecting  the  property  mode  automatically  based  on  the  platform    note  that  the  epoll  mode  can  get  better  performance  less  gc  and  have  more  advanced  features  which  are  only  available  on  modern    linux    partition    request    options    minimum  backoff  for  partition  requests  of  input  channels    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  network  request  backoff  initial  key  taskmanager  network  request  backoff  initial  default  value    with  deprecated  keys  taskmanager  net  request  backoff  initial  with  description    minimum  backoff  in  milliseconds  for  partition  requests  of  input  channels    maximum  backoff  for  partition  requests  of  input  channels    documentation    section    documentation    sections  all  task  manager  network  public  static  final    config  option    integer  network  request  backoff  max  key  taskmanager  network  request  backoff  max  default  value    with  deprecated  keys  taskmanager  net  request  backoff  max  with  description    maximum  backoff  in  milliseconds  for  partition  requests  of  input  channels    documentation    exclude  from  documentation  dev  use  only  likely  temporary  public  static  final    config  option    boolean  force  partition  release  on  consumption  key  taskmanager  network  partition  force  release  on  consumption  default  value  false    not  intended  to  be  instantiated  private    netty  shuffle  environment  options  
public  evolving  public  class    optimizer  options    the  maximum  number  of  line  samples  taken  by  the  compiler  for  delimited  inputs    the  samples  are  used  to  estimate  the  number  of  records    this  value  can  be  overridden  for  a  specific  input  with  the  input  format  s  parameters  public  static  final    config  option    integer  delimited  format  max  line  samples  key  compiler  delimited  informat  max  line  samples  default  value    with  description    the  maximum  number  of  line  samples  taken  by  the  compiler  for  delimited  inputs    the  samples  are  used  to  estimate  the  number  of  records    this  value  can  be  overridden  for  a  specific  input  with  the  input  format  s  parameters    the  minimum  number  of  line  samples  taken  by  the  compiler  for  delimited  inputs    the  samples  are  used  to  estimate  the  number  of  records    this  value  can  be  overridden  for  a  specific  input  with  the  input  format  s  parameters  public  static  final    config  option    integer  delimited  format  min  line  samples  key  compiler  delimited  informat  min  line  samples  default  value    with  description    the  minimum  number  of  line  samples  taken  by  the  compiler  for  delimited  inputs    the  samples  are  used  to  estimate  the  number  of  records    this  value  can  be  overridden  for  a  specific  input  with  the  input  format  s  parameters    the  maximal  length  of  a  line  sample  that  the  compiler  takes  for  delimited  inputs    if  the  length  of  a  single  sample  exceeds  this  value  possible  because  of  misconfiguration  of  the  parser  the  sampling  aborts    this  value  can  be  overridden  for  a  specific  input  with  the  input  format  s  parameters  public  static  final    config  option    integer  delimited  format  max  sample  len  key  compiler  delimited  informat  max  sample  len  default  value    with  description    the  maximal  length  of  a  line  sample  that  the  compiler  takes  for  delimited  inputs    if  the  length  of  a  single  sample  exceeds  this  value  possible  because  of  misconfiguration  of  the  parser  the  sampling  aborts    this  value  can  be  overridden  for  a  specific  input  with  the  input  format  s  parameters  
public  evolving  public  class    pipeline  options  a  list  of  jar  files  that  contain  the  user  defined  function  udf  classes  and  all  classes  used  from  within  the    u  d  fs  public  static  final    config  option    list    string  jars  key  pipeline  jars  string  type  as  list  no  default  value  with  description  a  semicolon  separated  list  of  the  jars  to  package  with  the  job  jars  to  be  sent  to  the  cluster    these  have  to  be  valid  paths  a  list  of    u  r  ls  that  are  added  to  the  classpath  of  each  user  code  classloader  of  the  program    paths  must  specify  a  protocol  e  g  file  and  be  accessible  on  all  nodes  public  static  final    config  option    list    string  classpaths  key  pipeline  classpaths  string  type  as  list  no  default  value  with  description  a  semicolon  separated  list  of  the  classpaths  to  package  with  the  job  jars  to  be  sent  to  the  cluster    these  have  to  be  valid    u  r  ls  public  static  final    config  option    boolean  auto  generate  uids  key  pipeline  auto  generate  uids  boolean  type  default  value  true  with  description    description  builder  text    when  auto  generated    u  i  ds  are  disabled  users  are  forced  to  manually  specify    u  i  ds  on    data  stream  applications  linebreak  linebreak  text    it  is  highly  recommended  that  users  specify    u  i  ds  before  deploying  to  production  since  they  are  used  to  match  state  in  savepoints  to  operators  in  a  job    because  auto  generated  id  s  are  likely  to  change  when  modifying  a  job  specifying  custom    i  ds  allow  an  application  to  evolve  over  time  without  discarding  state  build  public  static  final    config  option    boolean  auto  type  registration  key  pipeline  auto  type  registration  boolean  type  default  value  true  with  description    controls  whether    flink  is  automatically  registering  all  types  in  the  user  programs  with    kryo  public  static  final    config  option    duration  auto  watermark  interval  key  pipeline  auto  watermark  interval  duration  type  default  value    duration  zero  with  description    the  interval  of  the  automatic  watermark  emission    watermarks  are  used  throughout  the  streaming  system  to  keep  track  of  the  progress  of  time    they  are  used  for  example  for  time  based  windowing  public  static  final    config  option    closure  cleaner  level  closure  cleaner  level  key  pipeline  closure  cleaner  level  enum  type    closure  cleaner  level  class  default  value    closure  cleaner  level  recursive  with  description    description  builder  text    configures  the  mode  in  which  the  closure  cleaner  works  list  text  s  disables  the  closure  cleaner  completely  code    closure  cleaner  level  none  to  string  text  s  cleans  only  the  top  level  class  without  recursing  into  fields  code    closure  cleaner  level  top  level  to  string  text  s  cleans  all  the  fields  recursively  code    closure  cleaner  level  recursive  to  string  build  public  static  final    config  option    boolean  force  avro  key  pipeline  force  avro  boolean  type  default  value  false  with  description    description  builder  text    forces    flink  to  use  the    apache    avro  serializer  for    p  o  j  os  linebreak  linebreak  text    important    make  sure  to  include  the  s  module  code  flink  avro  build  public  static  final    config  option    boolean  force  kryo  key  pipeline  force  kryo  boolean  type  default  value  false  with  description    if  enabled  forces    type  extractor  to  use    kryo  serializer  for  pojos  even  though  we  could  analyze  as  pojo    in  some  cases  this  might  be  preferable    for  example  when  using  interfaces  with  subclasses  that  cannot  be  analyzed  as  pojo  public  static  final    config  option    boolean  generic  types  key  pipeline  generic  types  boolean  type  default  value  true  with  description    description  builder  text    if  the  use  of  generic  types  is  disabled    flink  will  throw  an  s  whenever  it  encounters  a  data  type  that  would  go  through    kryo  for  serialization  code    unsupported  operation  exception  linebreak  linebreak  text    disabling  generic  types  can  be  helpful  to  eagerly  find  and  eliminate  the  use  of  types  that  would  go  through    kryo  serialization  during  runtime    rather  than  checking  types  individually  using  this  option  will  throw  exceptions  eagerly  in  the  places  where  generic  types  are  used  linebreak  linebreak  text    we  recommend  to  use  this  option  only  during  development  and  pre  production  phases  not  during  actual  production  use    the  application  program  and  or  the  input  data  may  be  such  that  new  previously  unseen  types  occur  at  some  point    in  that  case  setting  this  option  would  cause  the  program  to  fail  build  public  static  final    config  option    map    string    string  global  job  parameters  key  pipeline  global  job  parameters  map  type  no  default  value  with  description    register  a  custom  serializable  user  configuration  object    the  configuration  can  be  accessed  in  operators  public  static  final    config  option    integer  max  parallelism  key  pipeline  max  parallelism  int  type  default  value    with  description    the  program  wide  maximum  parallelism  used  for  operators  which  haven  t  specified  a  maximum  parallelism    the  maximum  parallelism  specifies  the  upper  limit  for  dynamic  scaling  and  the  number  of  key  groups  used  for  partitioned  state  public  static  final    config  option    boolean  object  reuse  key  pipeline  object  reuse  boolean  type  default  value  false  with  description    when  enabled  objects  that    flink  internally  uses  for  deserialization  and  passing  data  to  user  code  functions  will  be  reused    keep  in  mind  that  this  can  lead  to  bugs  when  the  user  code  function  of  an  operation  is  not  aware  of  this  behaviour  public  static  final    config  option    list    string  kryo  default  serializers  key  pipeline  default  kryo  serializers  string  type  as  list  no  default  value  with  description    description  builder  text    semicolon  separated  list  of  pairs  of  class  names  and    kryo  serializers  class  names  to  be  used  as    kryo  default  serializers  linebreak  linebreak  text    example  linebreak  add    text  element  code  class  org  example    example  class  serializer  org  example    example  serializer1  class  org  example    example  class2  serializer  org  example    example  serializer2  build  public  static  final    config  option    list    string  kryo  registered  classes  key  pipeline  registered  kryo  types  string  type  as  list  no  default  value  with  description    description  builder  text    semicolon  separated  list  of  types  to  be  registered  with  the  serialization  stack    if  the  type  is  eventually  serialized  as  a  pojo  then  the  type  is  registered  with  the  pojo  serializer    if  the  type  ends  up  being  serialized  with    kryo  then  it  will  be  registered  at    kryo  to  make  sure  that  only  tags  are  written  build  public  static  final    config  option    list    string  pojo  registered  classes  key  pipeline  registered  pojo  types  string  type  as  list  no  default  value  with  description    description  builder  text    semicolon  separated  list  of  types  to  be  registered  with  the  serialization  stack    if  the  type  is  eventually  serialized  as  a  pojo  then  the  type  is  registered  with  the  pojo  serializer    if  the  type  ends  up  being  serialized  with    kryo  then  it  will  be  registered  at    kryo  to  make  sure  that  only  tags  are  written  build  public  static  final    config  option    boolean  operator  chaining  key  pipeline  operator  chaining  boolean  type  default  value  true  with  description    operator  chaining  allows  non  shuffle  operations  to  be  co  located  in  the  same  thread  fully  avoiding  serialization  and  de  serialization  public  static  final    config  option    list    string  cached  files  key  pipeline  cached  files  string  type  as  list  no  default  value  with  description    description  builder  text    files  to  be  registered  at  the  distributed  cache  under  the  given  name    the  files  will  be  accessible  from  any  user  defined  function  in  the  distributed  runtime  under  a  local  path    files  may  be  local  files  which  will  be  distributed  via    blob  server  or  files  in  a  distributed  file  system    the  runtime  will  copy  the  files  temporarily  to  a  local  cache  if  needed  linebreak  linebreak  text    example  linebreak  add    text  element  code  name  file1  path  file  tmp  file1  name  file2  path  hdfs  tmp  file2  build  
public  evolving  public  class    queryable  state  options    server    options    the  config  parameter  defining  the  server  port  range  of  the  queryable  state  proxy  p  a  proxy  runs  on  each    task    manager  so  many  proxies  may  run  on  the  same  machine  p    given  this  and  to  avoid  port  clashes  the  user  can  specify  a  port  range  and  the  proxy  will  bind  to  the  first  free  port  in  that  range  p    the  specified  range  can  be  ol  li  a  port    li  a  range  of  ports  50100-50200  or  li  a  list  of  ranges  and  ports  50100-50200  50300-50400    ol  p  b    the  default  port  is    b  public  static  final    config  option    string  proxy  port  range  key  queryable  state  proxy  ports  default  value    with  description    the  port  range  of  the  queryable  state  proxy    the  specified  range  can  be  a  single  port    a  range  of  ports  50100-50200  or  a  list  of  ranges  and  ports  50100-50200  50300-50400    with  deprecated  keys  query  proxy  ports    number  of  network  event  loop  threads  for  the  client  proxy    slots  public  static  final    config  option    integer  proxy  network  threads  key  queryable  state  proxy  network  threads  default  value    with  description    number  of  network    netty  s  event  loop    threads  for  queryable  state  proxy  with  deprecated  keys  query  proxy  network  threads    number  of  async  query  threads  for  the  client  proxy    slots  public  static  final    config  option    integer  proxy  async  query  threads  key  queryable  state  proxy  query  threads  default  value    with  description    number  of  query    threads  for  queryable  state  proxy    uses  the  number  of  slots  if  set  to    with  deprecated  keys  query  proxy  query  threads    the  config  parameter  defining  the  server  port  range  of  the  queryable  state  server  p  a  state  server  runs  on  each    task    manager  so  many  server  may  run  on  the  same  machine  p    given  this  and  to  avoid  port  clashes  the  user  can  specify  a  port  range  and  the  server  will  bind  to  the  first  free  port  in  that  range  p    the  specified  range  can  be  ol  li  a  port    li  a  range  of  ports  50100-50200  or  li  a  list  of  ranges  and  ports  50100-50200  50300-50400    ol  p  b    the  default  port  is    b  public  static  final    config  option    string  server  port  range  key  queryable  state  server  ports  default  value    with  description    the  port  range  of  the  queryable  state  server    the  specified  range  can  be  a  single  port    a  range  of  ports  50100-50200  or  a  list  of  ranges  and  ports  50100-50200  50300-50400    with  deprecated  keys  query  server  ports    number  of  network  event  loop  threads  for  the    kv  state  server    slots  public  static  final    config  option    integer  server  network  threads  key  queryable  state  server  network  threads  default  value    with  description    number  of  network    netty  s  event  loop    threads  for  queryable  state  server  with  deprecated  keys  query  server  network  threads    number  of  async  query  threads  for  the    kv  state  server  handler    slots  public  static  final    config  option    integer  server  async  query  threads  key  queryable  state  server  query  threads  default  value    with  description    number  of  query    threads  for  queryable  state  server    uses  the  number  of  slots  if  set  to    with  deprecated  keys  query  server  query  threads    option  whether  the  queryable  state  proxy  and  server  should  be  enabled  where  possible  and  configurable  p    queryable  state  proxy  and  server  are  still  more  experimental  features  hence  disabled  unless  they  are  enable  in  user  configuration  public  static  final    config  option    boolean  enable  queryable  state  proxy  server  key  queryable  state  enable  default  value  false  with  description    option  whether  the  queryable  state  proxy  and  server  should  be  enabled  where  possible  and  configurable    client    options    number  of  network  event  loop  threads  for  the    kv  state  client      use  number  of  available  cores  public  static  final    config  option    integer  client  network  threads  key  queryable  state  client  network  threads  default  value    with  description    number  of  network    netty  s  event  loop    threads  for  queryable  state  client  with  deprecated  keys  query  client  network  threads    not  intended  to  be  instantiated  private    queryable  state  options  
public  evolving  public  interface    readable  config    reads  a  value  using  the  metada  included  in  link    config  option    returns  the  link    config  option  default  value  if  value  key  not  present  in  the  configuration  param  option  metadata  of  the  option  to  read  param  t  type  of  the  value  to  read  return  read  value  or  link    config  option  default  value  if  not  found  see  get  optional    config  option  t  t  get    config  option  t  option    reads  a  value  using  the  metada  included  in  link    config  option    in  contrast  to  link  get    config  option  returns  link    optional  empty  if  value  not  present  param  option  metadata  of  the  option  to  read  param  t  type  of  the  value  to  read  return  read  value  or  link    optional  empty  if  not  found  see  get    config  option  t    optional  t  get  optional    config  option  t  option  
public  evolving  public  class    resource  manager  options    timeout  for  jobs  which  don  t  have  a  job  manager  as  leader  assigned  public  static  final    config  option    string  job  timeout    config  options  key  resourcemanager  job  timeout  default  value    minutes  with  description    timeout  for  jobs  which  don  t  have  a  job  manager  as  leader  assigned    this  option  is  not  used  any  more    deprecated  public  static  final    config  option    integer  local  number  resource  manager    config  options  key  local  number  resourcemanager  default  value    with  description    the  number  of  resource  managers  start    defines  the  network  port  to  connect  to  for  communication  with  the  resource  manager    by  default  the  port  of  the    job  manager  because  the  same    actor  system  is  used    its  not  possible  to  use  this  configuration  key  to  define  port  ranges  public  static  final    config  option    integer  ipc  port    config  options  key  resourcemanager  rpc  port  default  value    with  description    defines  the  network  port  to  connect  to  for  communication  with  the  resource  manager    by  default  the  port  of  the    job  manager  because  the  same    actor  system  is  used    its  not  possible  to  use  this  configuration  key  to  define  port  ranges    documentation    section    documentation    sections  expert  scheduling  public  static  final    config  option    integer  max  slot  num    config  options  key  slotmanager  number  of  slots  max  int  type  default  value    integer  max  value  with  description    defines  the  maximum  number  of  slots  that  the    flink  cluster  allocates    this  configuration  option  is  meant  for  limiting  the  resource  consumption  for  batch  workloads    it  is  not  recommended  to  configure  this  option  for  streaming  workloads  which  may  fail  if  there  are  not  enough  slots    note  that  this  configuration  option  does  not  take  effect  for  standalone  clusters  where  how  many  slots  are  allocated  is  not  controlled  by    flink    the  timeout  for  a  slot  request  to  be  discarded  in  milliseconds  deprecated    use  link    job  manager  options  slot  request  timeout    deprecated  public  static  final    config  option    long  slot  request  timeout    config  options  key  slotmanager  request  timeout  default  value  1  l  with  description    the  timeout  for  a  slot  request  to  be  discarded    time  in  milliseconds  of  the  start  up  period  of  a  standalone  cluster    during  this  time  resource  manager  of  the  standalone  cluster  expects  new  task  executors  to  be  registered  and  will  not  fail  slot  requests  that  can  not  be  satisfied  by  any  current  registered  slots    after  this  time  it  will  fail  pending  and  new  coming  requests  immediately  that  can  not  be  satisfied  by  registered  slots    if  not  set  link  slot  request  timeout  will  be  used  by  default  public  static  final    config  option    long  standalone  cluster  startup  period  time    config  options  key  resourcemanager  standalone  start  up  time  default  value  1  l  with  description    time  in  milliseconds  of  the  start  up  period  of  a  standalone  cluster    during  this  time  resource  manager  of  the  standalone  cluster  expects  new  task  executors  to  be  registered  and  will  not  fail  slot  requests  that  can  not  be  satisfied  by  any  current  registered  slots    after  this  time  it  will  fail  pending  and  new  coming  requests  immediately  that  can  not  be  satisfied  by  registered  slots    if  not  set  slotmanager  request  timeout  will  be  used  by  default    the  timeout  for  an  idle  task  manager  to  be  released  in  milliseconds  deprecated    use  link  task  manager  timeout    deprecated  public  static  final    config  option    long  slot  manager  task  manager  timeout    config  options  key  slotmanager  taskmanager  timeout  default  value    l  with  description    the  timeout  for  an  idle  task  manager  to  be  released    the  timeout  for  an  idle  task  manager  to  be  released  in  milliseconds  public  static  final    config  option    long  task  manager  timeout    config  options  key  resourcemanager  taskmanager  timeout  default  value    l  with  deprecated  keys  slot  manager  task  manager  timeout  key  with  description    description  builder  text    the  timeout  for  an  idle  task  manager  to  be  released  build    release  task  executor  only  when  each  produced  result  partition  is  either  consumed  or  failed  p    currently  produced  result  partition  is  released  when  it  fails  or  consumer  sends  close  request  to  confirm  successful  end  of  consumption  and  to  close  the  communication  channel  deprecated    the  default  value  should  be  reasonable  enough  in  all  cases  this  option  is  to  fallback  to  older  behaviour  which  will  be  removed  or  refactored  in  future    deprecated  public  static  final    config  option    boolean  task  manager  release  when  result  consumed    config  options  key  resourcemanager  taskmanager  release  wait  result  consumed  default  value  true  with  description    description  builder  text    release  task  executor  only  when  each  produced  result  partition  is  either  consumed  or  failed    true  is  default    false  means  that  idle  task  executor  release  is  not  blocked  by  receiver  confirming  consumption  of  result  partition  and  can  happen  right  away  after  resourcemanager  taskmanager  timeout  has  elapsed    setting  this  option  to  false  can  speed  up  task  executor  release  but  can  lead  to  unexpected  failures  if  end  of  consumption  is  slower  than  resourcemanager  taskmanager  timeout  build    prefix  for  passing  custom  environment  variables  to    flink  s  master  process    for  example  for  passing  ld  library  path  as  an  env  variable  to  the    app  master  set  containerized  master  env  ld  library  path  usr  lib  native  in  the  flink  conf  yaml  public  static  final    string  containerized  master  env  prefix  containerized  master  env    similar  to  the  see  containerized  master  env  prefix  this  configuration  prefix  allows  setting  custom  environment  variables  for  the  workers    task  managers  public  static  final    string  containerized  task  manager  env  prefix  containerized  taskmanager  env    not  intended  to  be  instantiated  private    resource  manager  options  
public  evolving    config  groups  groups    config  group  name    fixed  delay  restart  strategy  key  prefix  restart  strategy  fixed  delay    config  group  name    failure  rate  restart  strategy  key  prefix  restart  strategy  failure  rate  public  class    restart  strategy  options  public  static  final    config  option    string  restart  strategy    config  options  key  restart  strategy  no  default  value  with  description    description  builder  text    defines  the  restart  strategy  to  use  in  case  of  job  failures  linebreak  text    accepted  values  are  list  text  s  s  s    no  restart  strategy  code  none  code  off  code  disable  text  s  s    fixed  delay  restart  strategy    more  details  can  be  found  s  code  fixeddelay  code  fixed  delay  link  dev  task  failure  recovery  html  fixed  delay  restart  strategy  here  text  s  s    failure  rate  restart  strategy    more  details  can  be  found  s  code  failurerate  code  failure  rate  link  dev  task  failure  recovery  html  failure  rate  restart  strategy  here  text    if  checkpointing  is  disabled  the  default  value  is  s    if  checkpointing  is  enabled  the  default  value  is  s  with  s  restart  attempts  and  s  delay  code  none  code  fixed  delay  code    integer  max  value  code    s  build  public  static  final    config  option    integer  restart  strategy  fixed  delay  attempts    config  options  key  restart  strategy  fixed  delay  attempts  int  type  default  value    with  description    description  builder  text    the  number  of  times  that    flink  retries  the  execution  before  the  job  is  declared  as  failed  if  s  has  been  set  to  s  code  restart  strategy  key  code  fixed  delay  build  public  static  final    config  option    duration  restart  strategy  fixed  delay  delay    config  options  key  restart  strategy  fixed  delay  delay  duration  type  default  value    duration  of  seconds    with  description    description  builder  text    delay  between  two  consecutive  restart  attempts  if  s  has  been  set  to  s    delaying  the  retries  can  be  helpful  when  the  program  interacts  with  external  systems  where  for  example  connections  or  pending  transactions  should  reach  a  timeout  before  re  execution  is  attempted    it  can  be  specified  using  notation    min    s  code  restart  strategy  key  code  fixed  delay  build  public  static  final    config  option    integer  restart  strategy  failure  rate  max  failures  per  interval    config  options  key  restart  strategy  failure  rate  max  failures  per  interval  default  value    with  description    description  builder  text    maximum  number  of  restarts  in  given  time  interval  before  failing  a  job  if  s  has  been  set  to  s  code  restart  strategy  key  code  failure  rate  build  public  static  final    config  option    duration  restart  strategy  failure  rate  failure  rate  interval    config  options  key  restart  strategy  failure  rate  failure  rate  interval  duration  type  default  value    duration  of  minutes    with  description    description  builder  text    time  interval  for  measuring  failure  rate  if  s  has  been  set  to  s    it  can  be  specified  using  notation    min    s  code  restart  strategy  key  code  failure  rate  build  public  static  final    config  option    duration  restart  strategy  failure  rate  delay    config  options  key  restart  strategy  failure  rate  delay  duration  type  default  value    duration  of  seconds    with  description    description  builder  text    delay  between  two  consecutive  restart  attempts  if  s  has  been  set  to  s    it  can  be  specified  using  notation    min    s  code  restart  strategy  key  code  failure  rate  build  private    restart  strategy  options  throw  new    unsupported  operation  exception    this  class  should  never  be  instantiated  
public  evolving    config  groups  groups    config  group  name    task  manager  memory  key  prefix  taskmanager  memory  public  class    task  manager  options    general    task  manager    options  jvm  heap  size  for  the    task  managers  with  memory  size  deprecated  use  link  total  flink  memory  for  standalone  setups  and  link  total  process  memory  for  containerized  setups    deprecated  public  static  final    config  option    memory  size  task  manager  heap  memory  key  taskmanager  heap  size  memory  type  no  default  value  with  description  jvm  heap  size  for  the    task  managers  which  are  the  parallel  workers  of  the  system    on  yarn  setups  this  value  is  automatically  configured  to  the  size  of  the    task  manager  s  yarn  container  minus  a  certain  tolerance  value  jvm  heap  size  in  megabytes  for  the    task  managers  deprecated  use  link  total  flink  memory  for  standalone  setups  and  link  total  process  memory  for  containerized  setups    deprecated  public  static  final    config  option    integer  task  manager  heap  memory  mb  key  taskmanager  heap  mb  int  type  no  default  value  with  description  jvm  heap  size  in  megabytes  for  the    task  managers  which  are  the  parallel  workers  of  the  system    on  yarn  setups  this  value  is  automatically  configured  to  the  size  of  the    task  manager  s  yarn  container  minus  a  certain  tolerance  value    whether  to  kill  the    task  manager  when  the  task  thread  throws  an    out  of  memory  error    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    boolean  kill  on  out  of  memory  key  taskmanager  jvm  exit  on  oom  boolean  type  default  value  false  with  description    whether  to  kill  the    task  manager  when  the  task  thread  throws  an    out  of  memory  error    whether  the  quarantine  monitor  for  task  managers  shall  be  started    the  quarantine  monitor  shuts  down  the  actor  system  if  it  detects  that  it  has  quarantined  another  actor  system  or  if  it  has  been  quarantined  by  another  actor  system    deprecated  public  static  final    config  option    boolean  exit  on  fatal  akka  error  key  taskmanager  exit  on  fatal  akka  error  boolean  type  default  value  false  with  description    whether  the  quarantine  monitor  for  task  managers  shall  be  started    the  quarantine  monitor  shuts  down  the  actor  system  if  it  detects  that  it  has  quarantined  another  actor  system  or  if  it  has  been  quarantined  by  another  actor  system    the  external  address  of  the  network  interface  where  the    task  manager  is  exposed    overrides  link  host  bind  policy  automatic  address  binding    documentation    section    documentation    sections  common  host  port    documentation    sections  all  task  manager  public  static  final    config  option    string  host  key  taskmanager  host  string  type  no  default  value  with  description    the  external  address  of  the  network  interface  where  the    task  manager  is  exposed    because  different    task  managers  need  different  values  for  this  option  usually  it  is  specified  in  an  additional  non  shared    task  manager  specific  config  file    the  local  address  of  the  network  interface  that  the  task  manager  binds  to  public  static  final    config  option    string  bind  host  key  taskmanager  bind  host  string  type  no  default  value  with  description    the  local  address  of  the  network  interface  that  the  task  manager  binds  to    if  not  configured  0.0  0.0  will  be  used    the  default  network  port  range  the  task  manager  expects  incoming  ipc  connections    the  code    means  that  the    task  manager  searches  for  a  free  port    documentation    section    documentation    sections  common  host  port    documentation    sections  all  task  manager  public  static  final    config  option    string  rpc  port  key  taskmanager  rpc  port  string  type  default  value    with  description    the  external  rpc  port  where  the    task  manager  is  exposed    accepts  a  list  of  ports  50100  50101  ranges  50100-50200  or  a  combination  of  both    it  is  recommended  to  set  a  range  of  ports  to  avoid  collisions  when  multiple    task  managers  are  running  on  the  same  machine    the  local  port  that  the  task  manager  binds  to  public  static  final    config  option    integer  rpc  bind  port  key  taskmanager  rpc  bind  port  int  type  no  default  value  with  description    the  local  rpc  port  that  the    task  manager  binds  to    if  not  configured  the  external  port  configured  by  rpc  port  key  will  be  used    the  initial  registration  backoff  between  two  consecutive  registration  attempts    the  backoff  is  doubled  for  each  new  registration  attempt  until  it  reaches  the  maximum  registration  backoff  deprecated  use  link    cluster  options  initial  registration  timeout  instead    deprecated  public  static  final    config  option    duration  initial  registration  backoff  key  taskmanager  registration  initial  backoff  duration  type  default  value    time  utils  parse  duration    ms  with  deprecated  keys  taskmanager  initial  registration  pause  with  description    the  initial  registration  backoff  between  two  consecutive  registration  attempts    the  backoff  is  doubled  for  each  new  registration  attempt  until  it  reaches  the  maximum  registration  backoff    the  maximum  registration  backoff  between  two  consecutive  registration  attempts  deprecated  use  link    cluster  options  max  registration  timeout  instead    deprecated  public  static  final    config  option    duration  registration  max  backoff  key  taskmanager  registration  max  backoff  duration  type  default  value    time  utils  parse  duration    s  with  deprecated  keys  taskmanager  max  registration  pause  with  description    the  maximum  registration  backoff  between  two  consecutive  registration  attempts    the  max  registration  backoff  requires  a  time  unit  specifier  ms  s  min  h  d    the  backoff  after  a  registration  has  been  refused  by  the  job  manager  before  retrying  to  connect  deprecated  use  link    cluster  options  refused  registration  delay  instead    deprecated  public  static  final    config  option    duration  refused  registration  backoff  key  taskmanager  registration  refused  backoff  duration  type  default  value    time  utils  parse  duration    s  with  deprecated  keys  taskmanager  refused  registration  pause  with  description    the  backoff  after  a  registration  has  been  refused  by  the  job  manager  before  retrying  to  connect    defines  the  timeout  it  can  take  for  the    task  manager  registration    if  the  duration  is  exceeded  without  a  successful  registration  then  the    task  manager  terminates    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    duration  registration  timeout  key  taskmanager  registration  timeout  duration  type  default  value    time  utils  parse  duration    min  with  deprecated  keys  taskmanager  max  registration  duration  with  description    defines  the  timeout  for  the    task  manager  registration    if  the  duration  is  exceeded  without  a  successful  registration  then  the    task  manager  terminates    the  config  parameter  defining  the  number  of  task  slots  of  a  task  manager    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    integer  num  task  slots  key  taskmanager  number  of  task  slots  int  type  default  value    with  description    the  number  of  parallel  operator  or  user  function  instances  that  a  single    task  manager  can  run    if  this  value  is  larger  than    a  single    task  manager  takes  multiple  instances  of  a  function  or  operator    that  way  the    task  manager  can  utilize  multiple  cpu  cores  but  at  the  same  time  the  available  memory  is  divided  between  the  different  operator  or  function  instances    this  value  is  typically  proportional  to  the  number  of  physical  cpu  cores  that  the    task  manager  s  machine  has  e  g  equal  to  the  number  of  cores  or  half  the  number  of  cores    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    boolean  debug  memory  log  key  taskmanager  debug  memory  log  boolean  type  default  value  false  with  deprecated  keys  taskmanager  debug  memory  start  log  thread  with  description    flag  indicating  whether  to  start  a  thread  which  repeatedly  logs  the  memory  usage  of  the  jvm    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    long  debug  memory  usage  log  interval  ms  key  taskmanager  debug  memory  log  interval  long  type  default  value    l  with  deprecated  keys  taskmanager  debug  memory  log  interval  ms  with  description    the  interval  in  ms  for  the  log  thread  to  log  the  current  memory  usage    managed    memory    options    size  of  memory  buffers  used  by  the  network  stack  and  the  memory  manager    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    memory  size  memory  segment  size  key  taskmanager  memory  segment  size  memory  type  default  value    memory  size  parse  32kb  with  description    size  of  memory  buffers  used  by  the  network  stack  and  the  memory  manager    the  config  parameter  for  automatically  defining  the    task  manager  s  binding  address  if  link  host  configuration  option  is  not  set    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    string  host  bind  policy  key  taskmanager  network  bind  policy  string  type  default  value  ip  with  description    description  builder  text    the  automatic  address  binding  policy  used  by  the    task  manager  if  host  key  is  not  set    the  value  should  be  one  of  the  following  n  list  text  name  uses  hostname  as  binding  address  text  ip  uses  host  s  ip  address  as  binding  address  build    the    task  manager  s    resource  i  d    if  not  configured  the    resource  i  d  will  be  generated  with  the    rpc  address    rpc  port  and  a    character  random  string    notice  that  this  option  is  not  valid  in    yarn    mesos  and    native    kubernetes  mode    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    string  task  manager  resource  id  key  taskmanager  resource  id  string  type  no  default  value  with  description    the    task  manager  s    resource  i  d    if  not  configured  the    resource  i  d  will  be  generated  with  the    rpc  address    rpc  port  and  a    character  random  string    notice  that  this  option  is  not  valid  in    yarn    mesos  and    native    kubernetes  mode    resource    options    this  config  option  describes  number  of  cpu  cores  of  task  executors    in  case  of    yarn    mesos    kubernetes  it  is  used  to  launch  a  container  for  the  task  executor  p  do  not  use  this  config  option    this  config  option  is  currently  only  used  internally  for  passing  cpu  cores  into  task  executors  for  dynamic  fine  grained  slot  resource  management    the  feature  is  not  completed  at  the  moment  and  the  config  option  is  experimental  and  might  be  changed  removed  in  the  future    thus  we  do  not  expose  this  config  option  to  users  p    for  configuring  the  cpu  cores  of  container  on    yarn    mesos    kubernetes  please  use  link    yarn  config  options  vcores  link    mesos  task  manager  parameters  mesos  rm  tasks  cpus  and  link    kubernetes  config  options  task  manager  cpu    documentation    exclude  from  documentation  public  static  final    config  option    double  cpu  cores  key  taskmanager  cpu  cores  double  type  no  default  value  with  description  cpu  cores  for  the    task  executors    in  case  of    yarn  setups  this  value  will  be  rounded  to  the  closest  positive  integer    if  not  explicitly  configured  legacy  config  options  yarn  containers  vcores  mesos  resourcemanager  tasks  cpus  and  kubernetes  taskmanager  cpu  will  be  used  for    yarn    mesos    kubernetes  setups  and  num  task  slots  key  will  be  used  for  standalone  setups  approximate  number  of  slots    total    process    memory  size  for  the    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  total  process  memory  key  taskmanager  memory  process  size  memory  type  no  default  value  with  description    total    process    memory  size  for  the    task  executors    this  includes  all  the  memory  that  a    task  executor  consumes  consisting  of    total    flink    memory  jvm    metaspace  and  jvm    overhead    on  containerized  setups  this  should  be  set  to  the  container  memory    see  also  taskmanager  memory  flink  size  for  total    flink  memory  size  configuration    total    flink    memory  size  for  the    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  total  flink  memory  key  taskmanager  memory  flink  size  memory  type  no  default  value  with  description    string  format    total    flink    memory  size  for  the    task  executors    this  includes  all  the  memory  that  a    task  executor  consumes  except  for  jvm    metaspace  and  jvm    overhead    it  consists  of    framework    heap    memory    task    heap    memory    task    off    heap    memory    managed    memory  and    network    memory    see  also  s  for  total  process  memory  size  configuration  total  process  memory  key    framework    heap    memory  size  for    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  framework  heap  memory  key  taskmanager  memory  framework  heap  size  memory  type  default  value    memory  size  parse    m  with  description    framework    heap    memory  size  for    task  executors    this  is  the  size  of  jvm  heap  memory  reserved  for    task  executor  framework  which  will  not  be  allocated  to  task  slots    framework    off    heap    memory  size  for    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  framework  off  heap  memory  key  taskmanager  memory  framework  off  heap  size  memory  type  default  value    memory  size  parse    m  with  description    framework    off    heap    memory  size  for    task  executors    this  is  the  size  of  off  heap  memory  jvm  direct  memory  and  native  memory  reserved  for    task  executor  framework  which  will  not  be  allocated  to  task  slots    the  configured  value  will  be  fully  counted  when    flink  calculates  the  jvm  max  direct  memory  size  parameter    task    heap    memory  size  for    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  task  heap  memory  key  taskmanager  memory  task  heap  size  memory  type  no  default  value  with  description    task    heap    memory  size  for    task  executors    this  is  the  size  of  jvm  heap  memory  reserved  for  tasks    if  not  specified  it  will  be  derived  as    total    flink    memory  minus    framework    heap    memory    task    off    heap    memory    managed    memory  and    network    memory    task    off    heap    memory  size  for    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  task  off  heap  memory  key  taskmanager  memory  task  off  heap  size  memory  type  default  value    memory  size  zero  with  description    task    off    heap    memory  size  for    task  executors    this  is  the  size  of  off  heap  memory  jvm  direct  memory  and  native  memory  reserved  for  tasks    the  configured  value  will  be  fully  counted  when    flink  calculates  the  jvm  max  direct  memory  size  parameter    managed    memory  size  for    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  managed  memory  size  key  taskmanager  memory  managed  size  memory  type  no  default  value  with  deprecated  keys  taskmanager  memory  size  with  description    managed    memory  size  for    task  executors    this  is  the  size  of  off  heap  memory  managed  by  the  memory  manager  reserved  for  sorting  hash  tables  caching  of  intermediate  results  and    rocks  d  b  state  backend    memory  consumers  can  either  allocate  memory  from  the  memory  manager  in  the  form  of    memory  segments  or  reserve  bytes  from  the  memory  manager  and  keep  their  memory  usage  within  that  boundary    if  unspecified  it  will  be  derived  to  make  up  the  configured  fraction  of  the    total    flink    memory    fraction  of    total    flink    memory  to  be  used  as    managed    memory  if  link  managed  memory  size  is  not  specified    documentation    section    documentation    sections  common  memory  public  static  final    config  option    float  managed  memory  fraction  key  taskmanager  memory  managed  fraction  float  type  default  value  0.4  f  with  description    fraction  of    total    flink    memory  to  be  used  as    managed    memory  if    managed    memory  size  is  not  explicitly  specified    min    network    memory  size  for    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  network  memory  min  key  taskmanager  memory  network  min  memory  type  default  value    memory  size  parse  64m  with  deprecated  keys    netty  shuffle  environment  options  network  buffers  memory  min  key  with  description    min    network    memory  size  for    task  executors    network    memory  is  off  heap  memory  reserved  for    shuffle  environment  e  g  network  buffers    network    memory  size  is  derived  to  make  up  the  configured  fraction  of  the    total    flink    memory    if  the  derived  size  is  less  greater  than  the  configured  min  max  size  the  min  max  size  will  be  used    the  exact  size  of    network    memory  can  be  explicitly  specified  by  setting  the  min  max  to  the  same  value    max    network    memory  size  for    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  network  memory  max  key  taskmanager  memory  network  max  memory  type  default  value    memory  size  parse  1g  with  deprecated  keys    netty  shuffle  environment  options  network  buffers  memory  max  key  with  description    max    network    memory  size  for    task  executors    network    memory  is  off  heap  memory  reserved  for    shuffle  environment  e  g  network  buffers    network    memory  size  is  derived  to  make  up  the  configured  fraction  of  the    total    flink    memory    if  the  derived  size  is  less  greater  than  the  configured  min  max  size  the  min  max  size  will  be  used    the  exact  size  of    network    memory  can  be  explicitly  specified  by  setting  the  min  max  to  the  same  value    fraction  of    total    flink    memory  to  be  used  as    network    memory    documentation    section    documentation    sections  common  memory  public  static  final    config  option    float  network  memory  fraction  key  taskmanager  memory  network  fraction  float  type  default  value  0.1  f  with  deprecated  keys    netty  shuffle  environment  options  network  buffers  memory  fraction  key  with  description    fraction  of    total    flink    memory  to  be  used  as    network    memory    network    memory  is  off  heap  memory  reserved  for    shuffle  environment  e  g  network  buffers    network    memory  size  is  derived  to  make  up  the  configured  fraction  of  the    total    flink    memory    if  the  derived  size  is  less  greater  than  the  configured  min  max  size  the  min  max  size  will  be  used    the  exact  size  of    network    memory  can  be  explicitly  specified  by  setting  the  min  max  size  to  the  same  value  jvm    metaspace    size  for  the    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  jvm  metaspace  key  taskmanager  memory  jvm  metaspace  size  memory  type  default  value    memory  size  parse    m  with  description  jvm    metaspace    size  for  the    task  executors    min  jvm    overhead  size  for  the    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  jvm  overhead  min  key  taskmanager  memory  jvm  overhead  min  memory  type  default  value    memory  size  parse    m  with  description    min  jvm    overhead  size  for  the    task  executors    this  is  off  heap  memory  reserved  for  jvm  overhead  such  as  thread  stack  space  compile  cache  etc    this  includes  native  memory  but  not  direct  memory  and  will  not  be  counted  when    flink  calculates  jvm  max  direct  memory  size  parameter    the  size  of  jvm    overhead  is  derived  to  make  up  the  configured  fraction  of  the    total    process    memory    if  the  derived  size  is  less  greater  than  the  configured  min  max  size  the  min  max  size  will  be  used    the  exact  size  of  jvm    overhead  can  be  explicitly  specified  by  setting  the  min  max  size  to  the  same  value    max  jvm    overhead  size  for  the    task  executors    documentation    section    documentation    sections  common  memory  public  static  final    config  option    memory  size  jvm  overhead  max  key  taskmanager  memory  jvm  overhead  max  memory  type  default  value    memory  size  parse  1g  with  description    max  jvm    overhead  size  for  the    task  executors    this  is  off  heap  memory  reserved  for  jvm  overhead  such  as  thread  stack  space  compile  cache  etc    this  includes  native  memory  but  not  direct  memory  and  will  not  be  counted  when    flink  calculates  jvm  max  direct  memory  size  parameter    the  size  of  jvm    overhead  is  derived  to  make  up  the  configured  fraction  of  the    total    process    memory    if  the  derived  size  is  less  greater  than  the  configured  min  max  size  the  min  max  size  will  be  used    the  exact  size  of  jvm    overhead  can  be  explicitly  specified  by  setting  the  min  max  size  to  the  same  value    fraction  of    total    process    memory  to  be  reserved  for  jvm    overhead    documentation    section    documentation    sections  common  memory  public  static  final    config  option    float  jvm  overhead  fraction  key  taskmanager  memory  jvm  overhead  fraction  float  type  default  value  0.1  f  with  description    fraction  of    total    process    memory  to  be  reserved  for  jvm    overhead    this  is  off  heap  memory  reserved  for  jvm  overhead  such  as  thread  stack  space  compile  cache  etc    this  includes  native  memory  but  not  direct  memory  and  will  not  be  counted  when    flink  calculates  jvm  max  direct  memory  size  parameter    the  size  of  jvm    overhead  is  derived  to  make  up  the  configured  fraction  of  the    total    process    memory    if  the  derived  size  is  less  greater  than  the  configured  min  max  size  the  min  max  size  will  be  used    the  exact  size  of  jvm    overhead  can  be  explicitly  specified  by  setting  the  min  max  size  to  the  same  value    task    options    time  interval  in  milliseconds  between  two  successive  task  cancellation  attempts    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    long  task  cancellation  interval  key  task  cancellation  interval  long  type  default  value    l  with  deprecated  keys  task  cancellation  interval  with  description    time  interval  between  two  successive  task  cancellation  attempts  in  milliseconds    timeout  in  milliseconds  after  which  a  task  cancellation  times  out  and  leads  to  a  fatal    task  manager  error  a  value  of  code    code  deactivates  the  watch  dog    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    long  task  cancellation  timeout  key  task  cancellation  timeout  long  type  default  value    l  with  description    timeout  in  milliseconds  after  which  a  task  cancellation  times  out  and  leads  to  a  fatal    task  manager  error  a  value  of    deactivates  the  watch  dog    this  configures  how  long  we  wait  for  the  timers  in  milliseconds  to  finish  all  pending  timer  threads  when  the  stream  task  is  cancelled    documentation    section    documentation    sections  all  task  manager  public  static  final    config  option    long  task  cancellation  timeout  timers    config  options  key  task  cancellation  timers  timeout  long  type  default  value    l  with  deprecated  keys  timerservice  exceptional  shutdown  timeout  with  description    time  we  wait  for  the  timers  in  milliseconds  to  finish  all  pending  timer  threads  when  the  stream  task  is  cancelled    not  intended  to  be  instantiated  private    task  manager  options  
public  evolving  public  class    web  options    config  parameter  defining  the  runtime  monitor  web  frontend  server  address    deprecated  public  static  final    config  option    string  address  key  web  address  no  default  value  with  deprecated  keys  jobmanager  web  address  with  description    address  for  runtime  monitor  web  frontend  server    the  port  for  the  runtime  monitor  web  frontend  server  deprecated    use  link    rest  options  port  instead    deprecated  public  static  final    config  option    integer  port  key  web  port  default  value    with  deprecated  keys  jobmanager  web  port    the  config  parameter  defining  the    access    control    allow    origin  header  for  all  responses  from  the  web  frontend  public  static  final    config  option    string  access  control  allow  origin  key  web  access  control  allow  origin  default  value  with  deprecated  keys  jobmanager  web  access  control  allow  origin  with  description    access    control    allow    origin  header  for  all  responses  from  the  web  frontend    the  config  parameter  defining  the  refresh  interval  for  the  web  frontend  in  milliseconds  public  static  final    config  option    long  refresh  interval  key  web  refresh  interval  default  value    l  with  deprecated  keys  jobmanager  web  refresh  interval  with  description    refresh  interval  for  the  web  frontend  in  milliseconds    config  parameter  to  override  ssl  support  for  the    job  manager    web  ui    deprecated  public  static  final    config  option    boolean  ssl  enabled  key  web  ssl  enabled  default  value  true  with  deprecated  keys  jobmanager  web  ssl  enabled  with  description    flag  indicating  whether  to  override  ssl  support  for  the    job  manager    web  ui    the  config  parameter  defining  the  flink  web  directory  to  be  used  by  the  webmonitor    documentation    override  default    system  get  property  java  io  tmpdir  public  static  final    config  option    string  tmp  dir  key  web  tmpdir  default  value    system  get  property  java  io  tmpdir  with  deprecated  keys  jobmanager  web  tmpdir  with  description    flink  web  directory  which  is  used  by  the  webmonitor    the  config  parameter  defining  the  directory  for  uploading  the  job  jars    if  not  specified  a  dynamic  directory  will  be  used  under  the  directory  specified  by  job  manager  web  tmpdir  key  public  static  final    config  option    string  upload  dir  key  web  upload  dir  no  default  value  with  deprecated  keys  jobmanager  web  upload  dir  with  description    directory  for  uploading  the  job  jars    if  not  specified  a  dynamic  directory  will  be  used  under  the  directory  specified  by  job  manager  web  tmpdir  key    the  config  parameter  defining  the  number  of  archived  jobs  for  the    job  manager  public  static  final    config  option    integer  archive  count  key  web  history  default  value    with  deprecated  keys  jobmanager  web  history  with  description    number  of  archived  jobs  for  the    job  manager    the  log  file  location  may  be  in  log  for  standalone  but  under  log  directory  when  using  yarn  public  static  final    config  option    string  log  path  key  web  log  path  no  default  value  with  deprecated  keys  jobmanager  web  log  path  with  description    path  to  the  log  file  may  be  in  log  for  standalone  but  under  log  directory  when  using  yarn    config  parameter  indicating  whether  jobs  can  be  uploaded  and  run  from  the  web  frontend  public  static  final    config  option    boolean  submit  enable  key  web  submit  enable  default  value  true  with  deprecated  keys  jobmanager  web  submit  enable  with  description    flag  indicating  whether  jobs  can  be  uploaded  and  run  from  the  web  frontend    config  parameter  defining  the  number  of  checkpoints  to  remember  for  recent  history  public  static  final    config  option    integer  checkpoints  history  size  key  web  checkpoints  history  default  value    with  deprecated  keys  jobmanager  web  checkpoints  history  with  description    number  of  checkpoints  to  remember  for  recent  history    time  in  milliseconds  after  which  cached  stats  are  cleaned  up  if  not  accessed  public  static  final    config  option    integer  backpressure  cleanup  interval  key  web  backpressure  cleanup  interval  default  value        with  deprecated  keys  jobmanager  web  backpressure  cleanup  interval  with  description    time  in  milliseconds  after  which  cached  stats  are  cleaned  up  if  not  accessed    time  in  milliseconds  after  which  available  stats  are  deprecated  and  need  to  be  refreshed  by  resampling  public  static  final    config  option    integer  backpressure  refresh  interval  key  web  backpressure  refresh  interval  default  value      with  deprecated  keys  jobmanager  web  backpressure  refresh  interval  with  description    time  in  milliseconds  after  which  available  stats  are  deprecated  and  need  to  be  refreshed  by  resampling    number  of  samples  to  take  to  determine  back  pressure  public  static  final    config  option    integer  backpressure  num  samples  key  web  backpressure  num  samples  default  value    with  deprecated  keys  jobmanager  web  backpressure  num  samples  with  description    number  of  samples  to  take  to  determine  back  pressure    delay  between  samples  to  determine  back  pressure  in  milliseconds  public  static  final    config  option    integer  backpressure  delay  key  web  backpressure  delay  between  samples  default  value    with  deprecated  keys  jobmanager  web  backpressure  delay  between  samples  with  description    delay  between  samples  to  determine  back  pressure  in  milliseconds    timeout  for  asynchronous  operations  by  the  web  monitor  in  milliseconds  public  static  final    config  option    long  timeout  key  web  timeout  default  value  10  l  60  l    l  with  description    timeout  for  asynchronous  operations  by  the  web  monitor  in  milliseconds    not  meant  to  be  instantiated  private    web  options  
public  evolving  public  interface    writable  config    stores  a  given  value  using  the  metadata  included  in  the  link    config  option    the  value  should  be  readable  back  through  link    readable  config  param  option  metadata  information  param  value  value  to  be  stored  param  t  type  of  the  value  to  be  stored  return  instance  of  this  configuration  for  fluent  api  t    writable  config  set    config  option  t  option  t  value  
public  evolving  public  interface    job  client    returns  the  link    job  i  d  that  uniquely  identifies  the  job  this  client  is  scoped  to    job  i  d  get  job  i  d    requests  the  link    job  status  of  the  associated  job    completable  future    job  status  get  job  status    cancels  the  associated  job    completable  future    void  cancel    stops  the  associated  job  on    flink  cluster  p    stopping  works  only  for  streaming  programs    be  aware  that  the  job  might  continue  to  run  for  a  while  after  sending  the  stop  command  because  after  sources  stopped  to  emit  data  all  operators  need  to  finish  processing  param  advance  to  end  of  event  time  flag  indicating  if  the  source  should  inject  a  code  max  watermark  in  the  pipeline  param  savepoint  directory  directory  the  savepoint  should  be  written  to  return  a  link    completable  future  containing  the  path  where  the  savepoint  is  located    completable  future    string  stop  with  savepoint  boolean  advance  to  end  of  event  time    nullable    string  savepoint  directory    triggers  a  savepoint  for  the  associated  job    the  savepoint  will  be  written  to  the  given  savepoint  directory  or  link  org  apache  flink  configuration    checkpointing  options  savepoint  directory  if  it  is  null  param  savepoint  directory  directory  the  savepoint  should  be  written  to  return  a  link    completable  future  containing  the  path  where  the  savepoint  is  located    completable  future    string  trigger  savepoint    nullable    string  savepoint  directory    requests  the  accumulators  of  the  associated  job    accumulators  can  be  requested  while  it  is  running  or  after  it  has  finished    the  class  loader  is  used  to  deserialize  the  incoming  accumulator  results    completable  future    map    string    object  get  accumulators    class  loader  class  loader    returns  the  link    job  execution  result  result  of  the  job  execution  of  the  submitted  job  param  user  classloader  the  classloader  used  to  de  serialize  the  accumulators  of  the  job    completable  future    job  execution  result  get  job  execution  result  final    class  loader  user  classloader  
public  evolving  public  interface    job  listener    callback  on  job  submission    this  is  called  when  code  execute  or  code  execute  async  is  called  p    exactly  one  of  the  passed  parameters  is  null  respectively  for  failure  or  success  param  job  client  a  link    job  client  for  the  submitted    flink  job  param  throwable  the  cause  if  submission  failed  void  on  job  submitted    nullable    job  client  job  client    nullable    throwable  throwable    callback  on  job  execution  finished  successfully  or  unsuccessfully    it  is  only  called  back  when  you  call  code  execute  instead  of  code  execute  async  methods  of  execution  environments  p    exactly  one  of  the  passed  parameters  is  null  respectively  for  failure  or  success  void  on  job  executed    nullable    job  execution  result  job  execution  result    nullable    throwable  throwable  
public  evolving  public  interface    entropy  injecting  file  system    gets  the  marker  string  that  represents  the  substring  of  a  path  to  be  replaced  by  the  entropy  characters    string  get  entropy  injection  key    creates  a  string  with  random  entropy  to  be  injected  into  a  path    string  generate  entropy  
public  evolving  public  class    entropy  injector    handles  entropy  injection  across  regular  and  entropy  aware  file  systems  p    if  the  given  file  system  is  entropy  aware  a  implements  link    entropy  injecting  file  system  then  this  method  replaces  the  entropy  marker  in  the  path  with  random  characters    the  entropy  marker  is  defined  by  link    entropy  injecting  file  system  get  entropy  injection  key  p    if  the  given  file  system  does  not  implement  code    entropy  injecting  file  system  then  this  method  delegates  to  link    file  system  create    path    write  mode  and  returns  the  same  path  in  the  resulting  code    output  stream  and  path  public  static    output  stream  and  path  create  entropy  aware    file  system  fs    path  path    write  mode  write  mode  throws    i  o  exception  check  and  possibly  inject  entropy  into  the  path  final    entropy  injecting  file  system  efs  get  entropy  fs  fs  final    path  processed  path  efs  null  path  resolve  entropy  path  efs  true  create  the  stream  on  the  original  file  system  to  let  the  safety  net  take  its  effect  final    f  s  data  output  stream  out  fs  create  processed  path  write  mode  return  new    output  stream  and  path  out  processed  path    removes  the  entropy  marker  string  from  the  path  if  the  given  file  system  is  an  entropy  injecting  file  system  implements  link    entropy  injecting  file  system  and  the  entropy  marker  key  is  present    otherwise  this  returns  the  path  as  is  param  path    the  path  to  filter  return    the  path  without  the  marker  string  public  static    path  remove  entropy  marker  if  present    file  system  fs    path  path  final    entropy  injecting  file  system  efs  get  entropy  fs  fs  if  efs  null  return  path  else  try  return  resolve  entropy  path  efs  false  catch    i  o  exception  e  this  should  never  happen  because  the  path  was  valid  before  and  we  only  remove  characters  rethrow  to  silence  the  compiler  throw  new    flink  runtime  exception  e  get  message  e  public  static  boolean  is  entropy  injecting    file  system  fs  return  get  entropy  fs  fs  null    nullable  private  static    entropy  injecting  file  system  get  entropy  fs    file  system  fs  if  fs  instanceof    entropy  injecting  file  system  return    entropy  injecting  file  system  fs  else  if  fs  instanceof    safety  net  wrapper  file  system    file  system  delegate    safety  net  wrapper  file  system  fs  get  wrapped  delegate  if  delegate  instanceof    entropy  injecting  file  system  return    entropy  injecting  file  system  delegate  else  if  delegate  instanceof    plugin  file  system  factory    class  loader  fixing  file  system    file  system  inner  fs    plugin  file  system  factory    class  loader  fixing  file  system  delegate  get  inner  if  inner  fs  instanceof    entropy  injecting  file  system  return    entropy  injecting  file  system  inner  fs  return  null  else  return  null  else  return  null    visible  for  testing  static    path  resolve  entropy    path  path    entropy  injecting  file  system  efs  boolean  inject  entropy  throws    i  o  exception  final    string  entropy  injection  key  efs  get  entropy  injection  key  if  entropy  injection  key  null  return  path  else  final  uri  original  uri  path  to  uri  final    string  checkpoint  path  original  uri  get  path  final  int  index  of  key  checkpoint  path  index  of  entropy  injection  key  if  index  of  key    return  path  else  final    string  builder  buffer  new    string  builder  checkpoint  path  length  buffer  append  checkpoint  path    index  of  key  if  inject  entropy  buffer  append  efs  generate  entropy  buffer  append  checkpoint  path  index  of  key  entropy  injection  key  length  checkpoint  path  length  final    string  rewritten  path  buffer  to  string  try  return  new    path  new  uri  original  uri  get  scheme  original  uri  get  authority  rewritten  path  original  uri  get  query  original  uri  get  fragment  normalize  catch    u  r  i  syntax  exception  e  this  could  only  happen  if  the  injected  entropy  string  contains  invalid  characters  throw  new    i  o  exception  uri  format  error  while  processing  path  for  entropy  injection  e    this  class  is  not  meant  to  be  instantiated  private    entropy  injector  
public  evolving  public  interface    file  system  factory  extends    plugin    gets  the  scheme  of  the  file  system  created  by  this  factory    string  get  scheme    creates  a  new  file  system  for  the  given  file  system  uri    the  uri  describes  the  type  of  file  system  via  its  scheme  and  optionally  the  authority  for  example  the  host  of  the  file  system  param  fs  uri    the  uri  that  describes  the  file  system  return  a  new  instance  of  the  specified  file  system  throws    i  o  exception    thrown  if  the  file  system  could  not  be  instantiated    file  system  create  uri  fs  uri  throws    i  o  exception  
public  evolving  public  enum    file  system  kind    an  actual  file  system  with  files  and  directories  file  system    an    object  store    files  correspond  to  objects    there  are  not  really  directories  but  a  directory  like  structure  may  be  mimicked  by  hierarchical  naming  of  files  object  store  
public  evolving  public  class    local  file  system  factory  implements    file  system  factory    override  public    string  get  scheme  return    local  file  system  get  local  fs  u  r  i  get  scheme    override  public    file  system  create  uri  fs  uri  return    local  file  system  get  shared  instance  
public  evolving  public  abstract  class    recoverable  fs  data  output  stream  extends    f  s  data  output  stream    ensures  all  data  so  far  is  persistent  similar  to  link  sync  and  returns  a  handle  to  recover  the  stream  at  the  current  position  public  abstract    resume  recoverable  persist  throws    i  o  exception    closes  the  stream  ensuring  persistence  of  all  data  similar  to  link  sync    this  returns  a    committer  that  can  be  used  to  publish  make  visible  the  file  that  the  stream  was  writing  to  public  abstract    committer  close  for  commit  throws    i  o  exception    closes  this  stream    closing  the  steam  releases  the  local  resources  that  the  stream  uses  but  does  not  result  in  durability  of  previously  written  data    this  method  should  be  interpreted  as  a  close  in  order  to  dispose  or  close  on  failure  p    in  order  to  persist  all  previously  written  data  one  needs  to  call  the  link  close  for  commit  method  and  call  link    committer  commit  on  the  retured  committer  object  throws    i  o  exception    thrown  if  an  error  occurred  during  closing    override  public  abstract  void  close  throws    i  o  exception  a  committer  can  publish  the  file  of  a  stream  that  was  closed    the    committer  can  be  recovered  via  a  link    commit  recoverable  public  interface    committer    commits  the  file  making  it  visible    the  file  will  contain  the  exact  data  as  when  the  committer  was  created  throws    i  o  exception    thrown  if  committing  fails  void  commit  throws    i  o  exception    commits  the  file  making  it  visible    the  file  will  contain  the  exact  data  as  when  the  committer  was  created  p    this  method  tolerates  situations  where  the  file  was  already  committed  and  will  not  raise  an  exception  in  that  case    this  is  important  for  idempotent  commit  retries  as  they  need  to  happen  after  recovery  throws    i  o  exception    thrown  if  committing  fails  void  commit  after  recovery  throws    i  o  exception    gets  a  recoverable  object  to  recover  the  committer    the  recovered  committer  will  commit  the  file  with  the  exact  same  data  as  this  committer  would  commit  it    commit  recoverable  get  recoverable  
public  evolving  public  interface    recoverable  writer    opens  a  new  recoverable  stream  to  write  to  the  given  path    whether  existing  files  will  be  overwritten  is  implementation  specific  and  should  not  be  relied  upon  param  path    the  path  of  the  file  object  to  write  to  return  a  new    recoverable  fs  data  output  stream  writing  a  new  file  object  throws    i  o  exception    thrown  if  the  stream  could  not  be  opened  initialized    recoverable  fs  data  output  stream  open    path  path  throws    i  o  exception    resumes  a  recoverable  stream  consistently  at  the  point  indicated  by  the  given    resume  recoverable    future  writes  to  the  stream  will  continue  append  the  file  as  of  that  point  p    this  method  is  optional  and  whether  it  is  supported  is  indicated  through  the  link  supports  resume  method  param  resumable    the  opaque  handle  with  the  recovery  information  return  a  recoverable  stream  writing  to  the  file  object  as  it  was  at  the  point  when  the    resume  recoverable  was  created  throws    i  o  exception    thrown  if  resuming  fails  throws    unsupported  operation  exception    thrown  if  this  optional  method  is  not  supported    recoverable  fs  data  output  stream  recover    resume  recoverable  resumable  throws    i  o  exception    marks  if  the  writer  requires  to  do  any  additional  cleanup  freeing  of  resources  occupied  as  part  of  a  link    resume  recoverable  e  g  temporarily  files  created  or  objects  uploaded  to  external  systems  p    in  case  cleanup  is  required  then  link  cleanup  recoverable  state    resume  recoverable  should  be  called  return  code  true  if  cleanup  is  required  code  false  otherwise  boolean  requires  cleanup  of  recoverable  state    frees  up  any  resources  that  were  previously  occupied  in  order  to  be  able  to  recover  from  a  potential  failure    these  can  be  temporary  files  that  were  written  to  the  filesystem  or  objects  that  were  uploaded  to    s3  p  b  note  b    this  operation  should  not  throw  an  exception  but  return  false  if  the  cleanup  did  not  happen  for  any  reason  param  resumable    the  link    resume  recoverable  whose  state  we  want  to  clean  up  return  code  true  if  the  resources  were  successfully  freed  code  false  otherwise  e  g  the  file  to  be  deleted  was  not  there  for  any  reason  already  deleted  or  never  created  boolean  cleanup  recoverable  state    resume  recoverable  resumable  throws    i  o  exception    recovers  a  recoverable  stream  consistently  at  the  point  indicated  by  the  given    commit  recoverable  for  finalizing  and  committing    this  will  publish  the  target  file  with  exactly  the  data  that  was  written  up  to  the  point  then  the    commit  recoverable  was  created  param  resumable    the  opaque  handle  with  the  recovery  information  return  a  committer  that  publishes  the  target  file  throws    i  o  exception    thrown  if  recovery  fails    recoverable  fs  data  output  stream    committer  recover  for  commit    commit  recoverable  resumable  throws    i  o  exception    the  serializer  for  the    commit  recoverable  types  created  in  this  writer    this  serializer  should  be  used  to  store  the    commit  recoverable  in  checkpoint  state  or  other  forms  of  persistent  state    simple  versioned  serializer    commit  recoverable  get  commit  recoverable  serializer    the  serializer  for  the    resume  recoverable  types  created  in  this  writer    this  serializer  should  be  used  to  store  the    resume  recoverable  in  checkpoint  state  or  other  forms  of  persistent  state    simple  versioned  serializer    resume  recoverable  get  resume  recoverable  serializer    checks  whether  the  writer  and  its  streams  support  resuming  appending  to  files  after  recovery  via  the  link  recover    resume  recoverable  method  p    if  true  then  this  writer  supports  the  link  recover    resume  recoverable  method    if  false  then  that  method  may  not  be  supported  and  streams  can  only  be  recovered  via  link  recover  for  commit    commit  recoverable  boolean  supports  resume  a  handle  to  an  in  progress  stream  with  a  defined  and  persistent  amount  of  data    the  handle  can  be  used  to  recover  the  stream  as  of  exactly  that  point  and  publish  the  result  file  interface    commit  recoverable  a  handle  to  an  in  progress  stream  with  a  defined  and  persistent  amount  of  data    the  handle  can  be  used  to  recover  the  stream  exactly  as  of  that  point  and  either  publish  the  result  file  or  keep  appending  data  to  the  stream  interface    resume  recoverable  extends    commit  recoverable  
public  evolving  public  interface    input  split  assigner    returns  the  next  input  split  that  shall  be  consumed    the  consumer  s  host  is  passed  as  a  parameter  to  allow  localized  assignments  param  host    the  host  address  of  split  requesting  task  param  task  id    the  id  of  the  split  requesting  task  return  the  next  input  split  to  be  consumed  or  code  null  code  if  no  more  splits  remain    input  split  get  next  input  split    string  host  int  task  id    return  the  splits  to  assigner  if  the  task  failed  to  process  it  param  splits    the  list  of  input  splits  to  be  returned  param  task  id    the  id  of  the  task  that  failed  to  process  the  input  splits  void  return  input  split    list    input  split  splits  int  task  id  
public  evolving  public  enum    input  status    indicator  that  more  data  is  available  and  the  input  can  be  called  immediately  again  to  produce  more  data  more  available    indicator  that  no  data  is  currently  available  but  more  data  will  be  available  in  the  future  again  nothing  available    indicator  that  the  input  has  reached  the  end  of  data  end  of  input  
public  evolving  public  class    simple  versioned  serialization    serializes  the  version  and  datum  into  a  stream  p    data  serialized  via  this  method  can  be  deserialized  via  link  read  version  and  de  serialize    simple  versioned  serializer    data  input  view  p    the  first  four  bytes  will  be  occupied  by  the  version  as  returned  by  link    simple  versioned  serializer  get  version    the  remaining  bytes  will  be  the  serialized  datum  as  produced  by  link    simple  versioned  serializer  serialize    object  plus  its  length    the  resulting  array  will  hence  be  eight  bytes  larger  than  the  serialized  datum  param  serializer    the  serializer  to  serialize  the  datum  with  param  datum    the  datum  to  serialize  param  out    the  stream  to  serialize  to  public  static  t  void  write  version  and  serialize    simple  versioned  serializer  t  serializer  t  datum    data  output  view  out  throws    i  o  exception  check  not  null  serializer  serializer  check  not  null  datum  datum  check  not  null  out  out  final  byte  data  serializer  serialize  datum  out  write  int  serializer  get  version  out  write  int  data  length  out  write  data    deserializes  the  version  and  datum  from  a  stream  p    this  method  deserializes  data  serialized  via  link  write  version  and  serialize    simple  versioned  serializer    object    data  output  view  p    the  first  four  bytes  will  be  interpreted  as  the  version    the  next  four  bytes  will  be  interpreted  as  the  length  of  the  datum  bytes  then  length  many  bytes  will  be  read    finally  the  datum  is  deserialized  via  the  link    simple  versioned  serializer  deserialize  int  byte  method  param  serializer    the  serializer  to  serialize  the  datum  with  param  in    the  stream  to  deserialize  from  public  static  t  t  read  version  and  de  serialize    simple  versioned  serializer  t  serializer    data  input  view  in  throws    i  o  exception  check  not  null  serializer  serializer  check  not  null  in  in  final  int  version  in  read  int  final  int  length  in  read  int  final  byte  data  new  byte  length  in  read  fully  data  return  serializer  deserialize  version  data    serializes  the  version  and  datum  into  a  byte  array    the  first  four  bytes  will  be  occupied  by  the  version  as  returned  by  link    simple  versioned  serializer  get  version  written  in  i  big  endian  i  encoding    the  remaining  bytes  will  be  the  serialized  datum  as  produced  by  link    simple  versioned  serializer  serialize    object    the  resulting  array  will  hence  be  four  bytes  larger  than  the  serialized  datum  p    data  serialized  via  this  method  can  be  deserialized  via  link  read  version  and  de  serialize    simple  versioned  serializer  byte  param  serializer    the  serializer  to  serialize  the  datum  with  param  datum    the  datum  to  serialize  return  a  byte  array  containing  the  serialized  version  and  serialized  datum  throws    i  o  exception    exceptions  from  the  link    simple  versioned  serializer  serialize    object  method  are  forwarded  public  static  t  byte  write  version  and  serialize    simple  versioned  serializer  t  serializer  t  datum  throws    i  o  exception  check  not  null  serializer  serializer  check  not  null  datum  datum  final  byte  data  serializer  serialize  datum  final  byte  version  and  data  new  byte  data  length    final  int  version  serializer  get  version  version  and  data    byte  version    version  and  data    byte  version    version  and  data    byte  version    version  and  data    byte  version  final  int  length  data  length  version  and  data    byte  length    version  and  data    byte  length    version  and  data    byte  length    version  and  data    byte  length  move  the  data  to  the  array    system  arraycopy  data    version  and  data    data  length  return  version  and  data    deserializes  the  version  and  datum  from  a  byte  array    the  first  four  bytes  will  be  read  as  the  version  in  i  big  endian  i  encoding    the  remaining  bytes  will  be  passed  to  the  serializer  for  deserialization  via  link    simple  versioned  serializer  deserialize  int  byte  param  serializer    the  serializer  to  deserialize  the  datum  with  param  bytes    the  bytes  to  deserialize  from  return    the  deserialized  datum  throws    i  o  exception    exceptions  from  the  link    simple  versioned  serializer  deserialize  int  byte  method  are  forwarded  public  static  t  t  read  version  and  de  serialize    simple  versioned  serializer  t  serializer  byte  bytes  throws    i  o  exception  check  not  null  serializer  serializer  check  not  null  bytes  bytes  check  argument  bytes  length    byte  array  below  minimum  length    bytes  final  byte  data  only    arrays  copy  of  range  bytes    bytes  length  final  int  version  bytes    0xff    bytes    0xff    bytes    0xff    bytes    0xff  final  int  length  bytes    0xff    bytes    0xff    bytes    0xff    bytes    0xff  if  length  data  only  length  return  serializer  deserialize  version  data  only  else  throw  new    i  o  exception    corrupt  data  conflicting  lengths    length  fields  length  data  data  only  length    utility  class  not  meant  to  be  instantiated  private    simple  versioned  serialization  
public  evolving  public  interface    versioned    returns  the  version  number  of  the  object    versions  numbers  can  be  used  to  differentiate  evolving  classes  int  get  version  
public  evolving  public  class    version  mismatch  exception  extends    i  o  exception  private  static  final  long  serial  version  u  i  d    l  public    version  mismatch  exception  public    version  mismatch  exception    string  message  super  message  public    version  mismatch  exception    string  message    throwable  cause  super  message  cause  public    version  mismatch  exception    throwable  cause  super  cause  
public  evolving  public  class    data  input  view  stream  wrapper  extends    data  input  stream  implements    data  input  view  public    data  input  view  stream  wrapper    input  stream  in  super  in    override  public  void  skip  bytes  to  read  int  num  bytes  throws    i  o  exception  if  skip  bytes  num  bytes  num  bytes  throw  new    e  o  f  exception    could  not  skip  num  bytes  bytes  
public  evolving  public  class    data  output  view  stream  wrapper  extends    data  output  stream  implements    data  output  view  private  byte  temp  buffer  public    data  output  view  stream  wrapper    output  stream  out  super  out    override  public  void  skip  bytes  to  write  int  num  bytes  throws    i  o  exception  if  temp  buffer  null  temp  buffer  new  byte    while  num  bytes    int  to  write    math  min  num  bytes  temp  buffer  length  write  temp  buffer    to  write  num  bytes  to  write    override  public  void  write    data  input  view  source  int  num  bytes  throws    i  o  exception  if  temp  buffer  null  temp  buffer  new  byte    while  num  bytes    int  to  copy    math  min  num  bytes  temp  buffer  length  source  read  fully  temp  buffer    to  copy  write  temp  buffer    to  copy  num  bytes  to  copy  
public  evolving  public  interface    seekable  data  input  view  extends    data  input  view    sets  the  read  pointer  to  the  given  position  param  position    the  new  read  position  void  set  read  position  long  position  
public  evolving  public  interface    seekable  data  output  view  extends    data  output  view    sets  the  write  pointer  to  the  given  position  param  position    the  new  write  position  void  set  write  position  long  position  
public  evolving  public  interface    plugin    helper  method  to  get  the  class  loader  used  to  load  the  plugin    this  may  be  needed  for  some  plugins  that  use  dynamic  class  loading  afterwards  the  plugin  was  loaded  return  the  class  loader  used  to  load  the  plugin  default    class  loader  get  class  loader  return    preconditions  check  not  null  this  get  class  get  class  loader  s  plugin  with  null  class  loader  this  get  class  get  name    optional  method  for  plugins  to  pick  up  settings  from  the  configuration  param  config    the  configuration  to  apply  to  the  plugin  default  void  configure    configuration  config  
public  evolving  public  class    java  to  value  converter  public  static    value  convert  boxed  java  type    object  boxed  if  boxed  null  return  null  final    class  clazz  boxed  get  class  if  clazz    string  class  return  new    string  value    string  boxed  else  if  clazz    integer  class  return  new    int  value    integer  boxed  else  if  clazz    long  class  return  new    long  value    long  boxed  else  if  clazz    double  class  return  new    double  value    double  boxed  else  if  clazz    float  class  return  new    float  value    float  boxed  else  if  clazz    boolean  class  return  new    boolean  value    boolean  boxed  else  if  clazz    byte  class  return  new    byte  value    byte  boxed  else  if  clazz    short  class  return  new    short  value    short  boxed  else  if  clazz    character  class  return  new    char  value    character  boxed  else  throw  new    illegal  argument  exception    object  is  no  primitive    java  type  public  static    object  convert  value  type    value  value  if  value  null  return  null  if  value  instanceof    string  value  return    string  value  value  get  value  else  if  value  instanceof    int  value  return    int  value  value  get  value  else  if  value  instanceof    long  value  return    long  value  value  get  value  else  if  value  instanceof    double  value  return    double  value  value  get  value  else  if  value  instanceof    float  value  return    float  value  value  get  value  else  if  value  instanceof    boolean  value  return    boolean  value  value  get  value  else  if  value  instanceof    byte  value  return    byte  value  value  get  value  else  if  value  instanceof    short  value  return    short  value  value  get  value  else  if  value  instanceof    char  value  return    char  value  value  get  value  else  throw  new    illegal  argument  exception    object  is  no  primitive    java  type  
public  evolving  public  class    big  dec  parser  extends    field  parser    big  decimal  private  static  final    big  decimal  big  decimal  instance    big  decimal  zero  private    big  decimal  result  private  char  reuse  null    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    big  decimal  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return    try  final  int  length  end  pos  start  pos  if  reuse  null  reuse  length  length  reuse  new  char  length  for  int  j    j  length  j  final  byte  b  bytes  start  pos  j  if  b    b    b  b  b  b  e  b  e  set  error  state    parse  error  state  numeric  value  illegal  character  return    reuse  j  char  bytes  start  pos  j  this  result  new    big  decimal  reuse    length  return  end  pos  limit  limit  end  pos  delimiter  length  catch    number  format  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    big  decimal  create  value  return  big  decimal  instance    override  public    big  decimal  get  last  result  return  this  result    static  utility  to  parse  a  field  of  type    big  decimal  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    big  decimal  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type    big  decimal  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    big  decimal  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  if  length    throw  new    number  format  exception    invalid  input    empty  string  int  i    final  byte  del  byte  byte  delimiter  while  i  length  bytes  start  pos  i  del  byte  i  if  i      character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  start  pos  i    throw  new    number  format  exception    there  is  leading  or  trailing  whitespace  in  the  numeric  field  final  char  chars  new  char  i  for  int  j    j  i  j  final  byte  b  bytes  start  pos  j  if  b    b    b  b  b  b  e  b  e  throw  new    number  format  exception  chars  j  char  bytes  start  pos  j  return  new    big  decimal  chars  
public  evolving  public  class    big  int  parser  extends    field  parser    big  integer  private  static  final    big  integer  big  integer  instance    big  integer  zero  private    big  integer  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    big  integer  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return    if  end  pos  start  pos    character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  end  pos    set  error  state    parse  error  state  numeric  value  illegal  character  return      string  str  new    string  bytes  start  pos  end  pos  start  pos    config  constants  default  charset  try  this  result  new    big  integer  str  return  end  pos  limit  limit  end  pos  delimiter  length  catch    number  format  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    big  integer  create  value  return  big  integer  instance    override  public    big  integer  get  last  result  return  this  result    static  utility  to  parse  a  field  of  type    big  integer  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    big  integer  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type    big  integer  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    big  integer  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  final  int  limited  len  next  string  length  bytes  start  pos  length  delimiter  if  limited  len      character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  start  pos  limited  len    throw  new    number  format  exception    there  is  leading  or  trailing  whitespace  in  the  numeric  field  final    string  str  new    string  bytes  start  pos  limited  len    config  constants  default  charset  return  new    big  integer  str  
public  evolving  public  class    boolean  parser  extends    field  parser    boolean  private  boolean  result    values  for  true  and  false  respectively    must  be  lower  case  private  static  final  byte  true  new  byte  true  get  bytes    config  constants  default  charset    get  bytes    config  constants  default  charset  private  static  final  byte  false  new  byte  false  get  bytes    config  constants  default  charset    get  bytes    config  constants  default  charset    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    boolean  reuse  final  int  i  next  string  end  pos  bytes  start  pos  limit  delimiter  if  i    return    for  byte  a  t  r  u  e  true  if  byte  array  equals  bytes  start  pos  i  start  pos  a  t  r  u  e  result  true  return  i  limit  limit  i  delimiter  length  for  byte  a  f  a  l  s  e  false  if  byte  array  equals  bytes  start  pos  i  start  pos  a  f  a  l  s  e  result  false  return  i  limit  limit  i  delimiter  length  set  error  state    parse  error  state  boolean  invalid  return      override  public    boolean  get  last  result  return  result    override  public    boolean  create  value  return  false    checks  if  a  part  of  a  byte  array  matches  another  byte  array  with  chars  case  insensitive  param  source    the  source  byte  array  param  start    the  offset  into  the  source  byte  array  param  length    the  length  of  the  match  param  other    the  byte  array  which  is  fully  compared  to  the  part  of  the  source  array  return  true  if  other  can  be  found  in  the  specified  part  of  source  false  otherwise  private  static  boolean  byte  array  equals  byte  source  int  start  int  length  byte  other  if  length  other  length  return  false  for  int  i    i  other  length  i  if    character  to  lower  case  source  i  start  other  i  return  false  return  true  
public  evolving  public  class    boolean  value  parser  extends    field  parser    boolean  value  private    boolean  parser  parser  new    boolean  parser  private    boolean  value  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delim    boolean  value  reuse  int  return  value  parser  parse  field  bytes  start  pos  limit  delim  reuse  get  value  set  error  state  parser  get  error  state  reuse  set  value  parser  get  last  result  result  reuse  return  return  value    override  public    boolean  value  get  last  result  return  result    override  public    boolean  value  create  value  return  new    boolean  value  false  
public  evolving  public  class    byte  parser  extends    field  parser    byte  private  byte  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    byte  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  return    int  val    boolean  neg  false  final  int  delim  limit  limit  delimiter  length    if  bytes  start  pos  neg  true  start  pos  check  for  empty  field  with  only  the  sign  if  start  pos  limit  start  pos  delim  limit  delimiter  next  bytes  start  pos  delimiter  set  error  state    parse  error  state  numeric  value  orphan  sign  return    for  int  i  start  pos  i  limit  i  if  i  delim  limit  delimiter  next  bytes  i  delimiter  if  i  start  pos  set  error  state    parse  error  state  empty  column  return    this  result  byte  neg  val  val  return  i  delimiter  length  if  bytes  i    bytes  i    set  error  state    parse  error  state  numeric  value  illegal  character  return    val    val  bytes  i    if  val    byte  max  value  neg  val    byte  min  value  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    this  result  byte  neg  val  val  return  limit    override  public    byte  create  value  return    byte  min  value    override  public    byte  get  last  result  return    byte  value  of  this  result    static  utility  to  parse  a  field  of  type  byte  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    number  format  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  byte  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type  byte  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    number  format  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  byte  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  long  val    boolean  neg  false  if  bytes  start  pos  delimiter  throw  new    number  format  exception    empty  field  if  bytes  start  pos  neg  true  start  pos  length  if  length    bytes  start  pos  delimiter  throw  new    number  format  exception    orphaned  minus  sign  for  length    start  pos  length  if  bytes  start  pos  delimiter  return  byte  neg  val  val  if  bytes  start  pos    bytes  start  pos    throw  new    number  format  exception    invalid  character  val    val  bytes  start  pos    if  val    byte  max  value  neg  val    byte  min  value  throw  new    number  format  exception    value  overflow  underflow  return  byte  neg  val  val  
public  evolving  public  class    byte  value  parser  extends    field  parser    byte  value  private    byte  value  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    byte  value  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  return    int  val    boolean  neg  false  this  result  reusable  final  int  delim  limit  limit  delimiter  length    if  bytes  start  pos  neg  true  start  pos  check  for  empty  field  with  only  the  sign  if  start  pos  limit  start  pos  delim  limit  delimiter  next  bytes  start  pos  delimiter  set  error  state    parse  error  state  numeric  value  orphan  sign  return    for  int  i  start  pos  i  limit  i  if  i  delim  limit  delimiter  next  bytes  i  delimiter  if  i  start  pos  set  error  state    parse  error  state  empty  column  return    reusable  set  value  byte  neg  val  val  return  i  delimiter  length  if  bytes  i    bytes  i    set  error  state    parse  error  state  numeric  value  illegal  character  return    val    val  bytes  i    if  val    byte  max  value  neg  val    byte  min  value  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    reusable  set  value  byte  neg  val  val  return  limit    override  public    byte  value  create  value  return  new    byte  value    override  public    byte  value  get  last  result  return  this  result  
public  evolving  public  class    double  parser  extends    field  parser    double  private  static  final    double  double  instance    double  value  of  0.0  private  double  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    double  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return    if  end  pos  start  pos    character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  end  pos    set  error  state    parse  error  state  numeric  value  illegal  character  return      string  str  new    string  bytes  start  pos  end  pos  start  pos    config  constants  default  charset  try  this  result    double  parse  double  str  return  end  pos  limit  limit  end  pos  delimiter  length  catch    number  format  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    double  create  value  return  double  instance    override  public    double  get  last  result  return    double  value  of  this  result    static  utility  to  parse  a  field  of  type  double  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  double  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type  double  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  double  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  final  int  limited  len  next  string  length  bytes  start  pos  length  delimiter  if  limited  len      character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  start  pos  limited  len    throw  new    number  format  exception    there  is  leading  or  trailing  whitespace  in  the  numeric  field  final    string  str  new    string  bytes  start  pos  limited  len    config  constants  default  charset  return    double  parse  double  str  
public  evolving  public  class    double  value  parser  extends    field  parser    double  value  private    double  value  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    double  value  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return    if  end  pos  start  pos    character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  end  pos    set  error  state    parse  error  state  numeric  value  illegal  character  return      string  str  new    string  bytes  start  pos  end  pos  start  pos    config  constants  default  charset  try  double  value    double  parse  double  str  reusable  set  value  value  this  result  reusable  return  end  pos  limit  limit  end  pos  delimiter  length  catch    number  format  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    double  value  create  value  return  new    double  value    override  public    double  value  get  last  result  return  this  result  
public  evolving  public  abstract  class    field  parser  t    an  enumeration  of  different  types  of  errors  that  may  occur  public  static  enum    parse  error  state    no  error  occurred  none    the  domain  of  the  numeric  type  is  not  large  enough  to  hold  the  parsed  value  numeric  value  overflow  underflow  a  stand  alone  sign  was  encountered  while  parsing  a  numeric  type  numeric  value  orphan  sign    an  illegal  character  was  encountered  while  parsing  a  numeric  type  numeric  value  illegal  character    the  field  was  not  in  a  correct  format  for  the  numeric  type  numeric  value  format  error  a  quoted  string  was  not  terminated  until  the  line  end  unterminated  quoted  string    the  parser  found  characters  between  the  end  of  the  quoted  string  and  the  delimiter  unquoted  chars  after  quoted  string    the  column  is  empty  empty  column    invalid    boolean  value  boolean  invalid  private    charset  charset    standard  charsets  utf    private    parse  error  state  error  state    parse  error  state  none    parses  the  value  of  a  field  from  the  byte  array  taking  care  of  properly  reset  the  state  of  this  parser    the  start  position  within  the  byte  array  and  the  array  s  valid  length  is  given    the  content  of  the  value  is  delimited  by  a  field  delimiter  param  bytes    the  byte  array  that  holds  the  value  param  start  pos    the  index  where  the  field  starts  param  limit    the  limit  unto  which  the  byte  contents  is  valid  for  the  parser    the  limit  is  the  position  one  after  the  last  valid  byte  param  delim    the  field  delimiter  character  param  reuse    an  optional  reusable  field  to  hold  the  value  return    the  index  of  the  next  delimiter  if  the  field  was  parsed  correctly  a  value  less  than    otherwise  public  int  reset  error  state  and  parse  byte  bytes  int  start  pos  int  limit  byte  delim  t  reuse  reset  parser  state  return  parse  field  bytes  start  pos  limit  delim  reuse    each  parser  s  logic  should  be  implemented  inside  this  method  protected  abstract  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delim  t  reuse    reset  the  state  of  the  parser    called  as  the  very  first  method  inside  link    field  parser  reset  error  state  and  parse  byte  int  int  byte    object  by  default  it  just  reset  its  error  state  protected  void  reset  parser  state  this  error  state    parse  error  state  none    gets  the  parsed  field    this  method  returns  the  value  parsed  by  the  last  successful  invocation  of  link  parse  field  byte  int  int  byte    object    it  objects  are  mutable  and  reused  it  will  return  the  object  instance  that  was  passed  the  parse  function  return    the  latest  parsed  field  public  abstract  t  get  last  result    returns  an  instance  of  the  parsed  value  type  return    an  instance  of  the  parsed  value  type  public  abstract  t  create  value    checks  if  the  delimiter  starts  at  the  given  start  position  of  the  byte  array    attention    this  method  assumes  that  enough  characters  follow  the  start  position  for  the  delimiter  check  param  bytes    the  byte  array  that  holds  the  value  param  start  pos    the  index  of  the  byte  array  where  the  check  for  the  delimiter  starts  param  delim    the  delimiter  to  check  for  return  true  if  a  delimiter  starts  at  the  given  start  position  false  otherwise  public  static  final  boolean  delimiter  next  byte  bytes  int  start  pos  byte  delim  for  int  pos    pos  delim  length  pos  check  each  position  if  delim  pos  bytes  start  pos  pos  return  false  return  true    checks  if  the  given  bytes  ends  with  the  delimiter  at  the  given  end  position  param  bytes    the  byte  array  that  holds  the  value  param  end  pos    the  index  of  the  byte  array  where  the  check  for  the  delimiter  ends  param  delim    the  delimiter  to  check  for  return  true  if  a  delimiter  ends  at  the  given  end  position  false  otherwise  public  static  final  boolean  ends  with  delimiter  byte  bytes  int  end  pos  byte  delim  if  end  pos  delim  length    return  false  for  int  pos    pos  delim  length  pos  if  delim  pos  bytes  end  pos  delim  length    pos  return  false  return  true    sets  the  error  state  of  the  parser    called  by  subclasses  of  the  parser  to  set  the  type  of  error  when  failing  a  parse  param  error    the  error  state  to  set  protected  void  set  error  state    parse  error  state  error  this  error  state  error    gets  the  error  state  of  the  parser  as  a  value  of  the  enumeration  link    parse  error  state    if  no  error  occurred  the  error  state  will  be  link    parse  error  state  none  return    the  current  error  state  of  the  parser  public    parse  error  state  get  error  state  return  this  error  state    returns  the  end  position  of  a  string    sets  the  error  state  if  the  column  is  empty  return  the  end  position  of  the  string  or    if  an  error  occurred  protected  final  int  next  string  end  pos  byte  bytes  int  start  pos  int  limit  byte  delimiter  int  end  pos  start  pos  final  int  delim  limit  limit  delimiter  length    while  end  pos  limit  if  end  pos  delim  limit  delimiter  next  bytes  end  pos  delimiter  break  end  pos  if  end  pos  start  pos  set  error  state    parse  error  state  empty  column  return    return  end  pos    returns  the  length  of  a  string    throws  an  exception  if  the  column  is  empty  return  the  length  of  the  string  protected  static  final  int  next  string  length  byte  bytes  int  start  pos  int  length  char  delimiter  if  length    throw  new    illegal  argument  exception    invalid  input    empty  string  int  limited  length    final  byte  del  byte  byte  delimiter  while  limited  length  length  bytes  start  pos  limited  length  del  byte  limited  length  return  limited  length    gets  the  character  set  used  for  this  parser  return  the  charset  used  for  this  parser  public    charset  get  charset  return  this  charset    sets  the  character  set  used  for  this  parser  param  charset  charset  used  for  this  parser  public  void  set  charset    charset  charset  this  charset  charset    mapping  from  types  to  parsers    gets  the  parser  for  the  type  specified  by  the  given  class    returns  null  if  no  parser  for  that  class  is  known  param  type    the  class  of  the  type  to  get  the  parser  for  return    the  parser  for  the  given  type  or  null  if  no  such  parser  exists  public  static  t    class    field  parser  t  get  parser  for  type    class  t  type    class  extends    field  parser  parser  parsers  get  type  if  parser  null  return  null  else    suppress  warnings  unchecked    class    field  parser  t  typed  parser    class    field  parser  t  parser  return  typed  parser  private  static  final    map    class    class  extends    field  parser  parsers  new    hash  map    class    class  extends    field  parser  static  basic  types  parsers  put    byte  class    byte  parser  class  parsers  put    short  class    short  parser  class  parsers  put    integer  class    int  parser  class  parsers  put    long  class    long  parser  class  parsers  put    string  class    string  parser  class  parsers  put    float  class    float  parser  class  parsers  put    double  class    double  parser  class  parsers  put    boolean  class    boolean  parser  class  parsers  put    big  decimal  class    big  dec  parser  class  parsers  put    big  integer  class    big  int  parser  class  value  types  parsers  put    byte  value  class    byte  value  parser  class  parsers  put    short  value  class    short  value  parser  class  parsers  put    int  value  class    int  value  parser  class  parsers  put    long  value  class    long  value  parser  class  parsers  put    string  value  class    string  value  parser  class  parsers  put    float  value  class    float  value  parser  class  parsers  put    double  value  class    double  value  parser  class  parsers  put    boolean  value  class    boolean  value  parser  class  sql  date  time  types  parsers  put  java  sql    time  class    sql  time  parser  class  parsers  put  java  sql    date  class    sql  date  parser  class  parsers  put  java  sql    timestamp  class    sql  timestamp  parser  class  
public  evolving  public  class    float  parser  extends    field  parser    float  private  float  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    float  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return    if  end  pos  start  pos    character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  end  pos    set  error  state    parse  error  state  numeric  value  illegal  character  return      string  str  new    string  bytes  start  pos  end  pos  start  pos    config  constants  default  charset  try  this  result    float  parse  float  str  return  end  pos  limit  limit  end  pos  delimiter  length  catch    number  format  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    float  create  value  return    float  min  value    override  public    float  get  last  result  return    float  value  of  this  result    static  utility  to  parse  a  field  of  type  float  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  float  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type  float  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  float  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  final  int  limited  len  next  string  length  bytes  start  pos  length  delimiter  if  limited  len      character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  start  pos  limited  len    throw  new    number  format  exception    there  is  leading  or  trailing  whitespace  in  the  numeric  field  final    string  str  new    string  bytes  start  pos  limited  len    config  constants  default  charset  return    float  parse  float  str  
public  evolving  public  class    float  value  parser  extends    field  parser    float  value  private    float  value  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    float  value  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return    if  end  pos  start  pos    character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  end  pos    set  error  state    parse  error  state  numeric  value  illegal  character  return      string  str  new    string  bytes  start  pos  end  pos  start  pos    config  constants  default  charset  try  float  value    float  parse  float  str  reusable  set  value  value  this  result  reusable  return  end  pos  limit  limit  end  pos  delimiter  length  catch    number  format  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    float  value  create  value  return  new    float  value    override  public    float  value  get  last  result  return  this  result  
public  evolving  public  class    int  parser  extends    field  parser    integer  private  static  final  long  overflow  bound  0x7  fffffff  l  private  static  final  long  underflow  bound  0x80000000  l  private  int  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    integer  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  return    long  val    boolean  neg  false  final  int  delim  limit  limit  delimiter  length    if  bytes  start  pos  neg  true  start  pos  check  for  empty  field  with  only  the  sign  if  start  pos  limit  start  pos  delim  limit  delimiter  next  bytes  start  pos  delimiter  set  error  state    parse  error  state  numeric  value  orphan  sign  return    for  int  i  start  pos  i  limit  i  if  i  delim  limit  delimiter  next  bytes  i  delimiter  if  i  start  pos  set  error  state    parse  error  state  empty  column  return    this  result  int  neg  val  val  return  i  delimiter  length  if  bytes  i    bytes  i    set  error  state    parse  error  state  numeric  value  illegal  character  return    val    val  bytes  i    if  val  overflow  bound  neg  val  underflow  bound  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    this  result  int  neg  val  val  return  limit    override  public    integer  create  value  return    integer  min  value    override  public    integer  get  last  result  return    integer  value  of  this  result    static  utility  to  parse  a  field  of  type  int  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    number  format  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  int  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type  int  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    number  format  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  int  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  long  val    boolean  neg  false  if  bytes  start  pos  delimiter  throw  new    number  format  exception    empty  field  if  bytes  start  pos  neg  true  start  pos  length  if  length    bytes  start  pos  delimiter  throw  new    number  format  exception    orphaned  minus  sign  for  length    start  pos  length  if  bytes  start  pos  delimiter  return  int  neg  val  val  if  bytes  start  pos    bytes  start  pos    throw  new    number  format  exception    invalid  character  val    val  bytes  start  pos    if  val  overflow  bound  neg  val  underflow  bound  throw  new    number  format  exception    value  overflow  underflow  return  int  neg  val  val  
public  evolving  public  class    int  value  parser  extends    field  parser    int  value  private  static  final  long  overflow  bound  0x7  fffffff  l  private  static  final  long  underflow  bound  0x80000000  l  private    int  value  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    int  value  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  return    long  val    boolean  neg  false  final  int  delim  limit  limit  delimiter  length    this  result  reusable  if  bytes  start  pos  neg  true  start  pos  check  for  empty  field  with  only  the  sign  if  start  pos  limit  start  pos  delim  limit  delimiter  next  bytes  start  pos  delimiter  set  error  state    parse  error  state  numeric  value  orphan  sign  return    for  int  i  start  pos  i  limit  i  if  i  delim  limit  delimiter  next  bytes  i  delimiter  if  i  start  pos  set  error  state    parse  error  state  empty  column  return    reusable  set  value  int  neg  val  val  return  i  delimiter  length  if  bytes  i    bytes  i    set  error  state    parse  error  state  numeric  value  illegal  character  return    val    val  bytes  i    if  val  overflow  bound  neg  val  underflow  bound  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    reusable  set  value  int  neg  val  val  return  limit    override  public    int  value  create  value  return  new    int  value    override  public    int  value  get  last  result  return  this  result  
public  evolving  public  class    long  parser  extends    field  parser    long  private  long  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    long  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  return    long  val    boolean  neg  false  final  int  delim  limit  limit  delimiter  length    if  bytes  start  pos  neg  true  start  pos  check  for  empty  field  with  only  the  sign  if  start  pos  limit  start  pos  delim  limit  delimiter  next  bytes  start  pos  delimiter  set  error  state    parse  error  state  numeric  value  orphan  sign  return    for  int  i  start  pos  i  limit  i  if  i  delim  limit  delimiter  next  bytes  i  delimiter  if  i  start  pos  set  error  state    parse  error  state  empty  column  return    this  result  neg  val  val  return  i  delimiter  length  if  bytes  i    bytes  i    set  error  state    parse  error  state  numeric  value  illegal  character  return    val    val  bytes  i    check  for  overflow  underflow  if  val    this  is  an  overflow  underflow  unless  we  hit  exactly  the    long  min  value  if  neg  val    long  min  value  this  result    long  min  value  if  i    limit  return  limit  else  if  i    delim  limit  delimiter  next  bytes  i    delimiter  return  i    delimiter  length  else  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    else  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    this  result  neg  val  val  return  limit    override  public    long  create  value  return    long  min  value    override  public    long  get  last  result  return    long  value  of  this  result    static  utility  to  parse  a  field  of  type  long  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    number  format  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  long  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type  long  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    number  format  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  long  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  long  val    boolean  neg  false  if  bytes  start  pos  delimiter  throw  new    number  format  exception    empty  field  if  bytes  start  pos  neg  true  start  pos  length  if  length    bytes  start  pos  delimiter  throw  new    number  format  exception    orphaned  minus  sign  for  length    start  pos  length  if  bytes  start  pos  delimiter  return  neg  val  val  if  bytes  start  pos    bytes  start  pos    throw  new    number  format  exception    invalid  character  val    val  bytes  start  pos    check  for  overflow  underflow  if  val    this  is  an  overflow  underflow  unless  we  hit  exactly  the    long  min  value  if  neg  val    long  min  value  if  length    bytes  start  pos    delimiter  return    long  min  value  else  throw  new    number  format  exception  value  overflow  else  throw  new    number  format  exception  value  overflow  return  neg  val  val  
public  evolving  public  class    long  value  parser  extends    field  parser    long  value  private    long  value  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    long  value  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  return    long  val    boolean  neg  false  final  int  delim  limit  limit  delimiter  length    this  result  reusable  if  bytes  start  pos  neg  true  start  pos  check  for  empty  field  with  only  the  sign  if  start  pos  limit  start  pos  delim  limit  delimiter  next  bytes  start  pos  delimiter  set  error  state    parse  error  state  numeric  value  orphan  sign  return    for  int  i  start  pos  i  limit  i  if  i  delim  limit  delimiter  next  bytes  i  delimiter  if  i  start  pos  set  error  state    parse  error  state  empty  column  return    reusable  set  value  neg  val  val  return  i  delimiter  length  if  bytes  i    bytes  i    set  error  state    parse  error  state  numeric  value  illegal  character  return    val    val  bytes  i    check  for  overflow  underflow  if  val    this  is  an  overflow  underflow  unless  we  hit  exactly  the    long  min  value  if  neg  val    long  min  value  reusable  set  value    long  min  value  if  i    limit  return  limit  else  if  i    delim  limit  delimiter  next  bytes  i    delimiter  return  i    delimiter  length  else  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    else  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    reusable  set  value  neg  val  val  return  limit    override  public    long  value  create  value  return  new    long  value    override  public    long  value  get  last  result  return  this  result  
public  evolving  public  class    short  parser  extends    field  parser    short  private  static  final  int  overflow  bound  0x7  fff  private  static  final  int  underflow  bound  0x8000  private  short  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    short  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  return    int  val    boolean  neg  false  final  int  delim  limit  limit  delimiter  length    if  bytes  start  pos  neg  true  start  pos  check  for  empty  field  with  only  the  sign  if  start  pos  limit  start  pos  delim  limit  delimiter  next  bytes  start  pos  delimiter  set  error  state    parse  error  state  numeric  value  orphan  sign  return    for  int  i  start  pos  i  limit  i  if  i  delim  limit  delimiter  next  bytes  i  delimiter  if  i  start  pos  set  error  state    parse  error  state  empty  column  return    this  result  short  neg  val  val  return  i  delimiter  length  if  bytes  i    bytes  i    set  error  state    parse  error  state  numeric  value  illegal  character  return    val    val  bytes  i    if  val  overflow  bound  neg  val  underflow  bound  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    this  result  short  neg  val  val  return  limit    override  public    short  create  value  return    short  min  value    override  public    short  get  last  result  return    short  value  of  this  result    static  utility  to  parse  a  field  of  type  short  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    number  format  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  short  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type  short  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    number  format  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final  short  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  long  val    boolean  neg  false  if  bytes  start  pos  delimiter  throw  new    number  format  exception    empty  field  if  bytes  start  pos  neg  true  start  pos  length  if  length    bytes  start  pos  delimiter  throw  new    number  format  exception    orphaned  minus  sign  for  length    start  pos  length  if  bytes  start  pos  delimiter  return  short  neg  val  val  if  bytes  start  pos    bytes  start  pos    throw  new    number  format  exception    invalid  character  val    val  bytes  start  pos    if  val  overflow  bound  neg  val  underflow  bound  throw  new    number  format  exception    value  overflow  underflow  return  short  neg  val  val  
public  evolving  public  class    short  value  parser  extends    field  parser    short  value  private  static  final  int  overflow  bound  0x7  fff  private  static  final  int  underflow  bound  0x8000  private    short  value  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    short  value  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  return    int  val    boolean  neg  false  final  int  delim  limit  limit  delimiter  length    this  result  reusable  if  bytes  start  pos  neg  true  start  pos  check  for  empty  field  with  only  the  sign  if  start  pos  limit  start  pos  delim  limit  delimiter  next  bytes  start  pos  delimiter  set  error  state    parse  error  state  numeric  value  orphan  sign  return    for  int  i  start  pos  i  limit  i  if  i  delim  limit  delimiter  next  bytes  i  delimiter  if  i  start  pos  set  error  state    parse  error  state  empty  column  return    reusable  set  value  short  neg  val  val  return  i  delimiter  length  if  bytes  i    bytes  i    set  error  state    parse  error  state  numeric  value  illegal  character  return    val    val  bytes  i    if  val  overflow  bound  neg  val  underflow  bound  set  error  state    parse  error  state  numeric  value  overflow  underflow  return    reusable  set  value  short  neg  val  val  return  limit    override  public    short  value  create  value  return  new    short  value    override  public    short  value  get  last  result  return  this  result  
public  evolving  public  class    sql  date  parser  extends    field  parser    date  private  static  final    date  date  instance  new    date  0  l  private    date  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    date  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return    if  end  pos  start  pos    character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  end  pos    set  error  state    parse  error  state  numeric  value  illegal  character  return      string  str  new    string  bytes  start  pos  end  pos  start  pos    config  constants  default  charset  try  this  result    date  value  of  str  return  end  pos  limit  limit  end  pos  delimiter  length  catch    illegal  argument  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    date  create  value  return  date  instance    override  public    date  get  last  result  return  this  result    static  utility  to  parse  a  field  of  type    date  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    date  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type    date  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    date  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  final  int  limited  len  next  string  length  bytes  start  pos  length  delimiter  if  limited  len      character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  start  pos  limited  len    throw  new    number  format  exception    there  is  leading  or  trailing  whitespace  in  the  numeric  field  final    string  str  new    string  bytes  start  pos  limited  len    config  constants  default  charset  return    date  value  of  str  
public  evolving  public  class    sql  time  parser  extends    field  parser    time  private  static  final    time  time  instance  new    time  0  l  private    time  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    time  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return      string  str  new    string  bytes  start  pos  end  pos  start  pos    config  constants  default  charset  try  this  result    time  value  of  str  return  end  pos  limit  limit  end  pos  delimiter  length  catch    illegal  argument  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    time  create  value  return  time  instance    override  public    time  get  last  result  return  this  result    static  utility  to  parse  a  field  of  type    time  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    time  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type    time  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    time  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  final  int  limited  len  next  string  length  bytes  start  pos  length  delimiter  if  limited  len      character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  start  pos  limited  len    throw  new    number  format  exception    there  is  leading  or  trailing  whitespace  in  the  numeric  field  final    string  str  new    string  bytes  start  pos  limited  len    config  constants  default  charset  return    time  value  of  str  
public  evolving  public  class    sql  timestamp  parser  extends    field  parser    timestamp  private  static  final    timestamp  timestamp  instance  new    timestamp  0  l  private    timestamp  result    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    timestamp  reusable  final  int  end  pos  next  string  end  pos  bytes  start  pos  limit  delimiter  if  end  pos    return    if  end  pos  start  pos    character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  end  pos    set  error  state    parse  error  state  numeric  value  illegal  character  return      string  str  new    string  bytes  start  pos  end  pos  start  pos    config  constants  default  charset  try  this  result    timestamp  value  of  str  return  end  pos  limit  limit  end  pos  delimiter  length  catch    illegal  argument  exception  e  set  error  state    parse  error  state  numeric  value  format  error  return      override  public    timestamp  create  value  return  timestamp  instance    override  public    timestamp  get  last  result  return  this  result    static  utility  to  parse  a  field  of  type    timestamp  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    timestamp  parse  field  byte  bytes  int  start  pos  int  length  return  parse  field  bytes  start  pos  length  char  0xffff    static  utility  to  parse  a  field  of  type    timestamp  from  a  byte  sequence  that  represents  text  characters  such  as  when  read  from  a  file  stream  param  bytes    the  bytes  containing  the  text  data  that  should  be  parsed  param  start  pos    the  offset  to  start  the  parsing  param  length    the  length  of  the  byte  sequence  counting  from  the  offset  param  delimiter    the  delimiter  that  terminates  the  field  return    the  parsed  value  throws    illegal  argument  exception    thrown  when  the  value  cannot  be  parsed  because  the  text  represents  not  a  correct  number  public  static  final    timestamp  parse  field  byte  bytes  int  start  pos  int  length  char  delimiter  final  int  limited  len  next  string  length  bytes  start  pos  length  delimiter  if  limited  len      character  is  whitespace  bytes  start  pos    character  is  whitespace  bytes  start  pos  limited  len    throw  new    number  format  exception    there  is  leading  or  trailing  whitespace  in  the  numeric  field  final    string  str  new    string  bytes  start  pos  limited  len    config  constants  default  charset  return    timestamp  value  of  str  
public  evolving  public  class    string  parser  extends    field  parser    string  private  boolean  quoted  string  parsing  false  private  byte  quote  character  private  static  final  byte  backslash    private    string  result  public  void  enable  quoted  string  parsing  byte  quote  character  this  quoted  string  parsing  true  this  quote  character  quote  character    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    string  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  this  result  return  limit  int  i  start  pos  final  int  delim  limit  limit  delimiter  length    if  quoted  string  parsing  bytes  i  quote  character  quoted  string  parsing  enabled  and  first  character  is  a  quote  i  search  for  ending  quote  character  continue  when  it  is  escaped  while  i  limit  bytes  i  quote  character  bytes  i    backslash  i  if  i  limit  set  error  state    parse  error  state  unterminated  quoted  string  return    else  i  check  for  proper  termination  if  i  limit  either  by  end  of  line  this  result  new    string  bytes  start  pos    i  start  pos    get  charset  return  limit  else  if  i  delim  limit  delimiter  next  bytes  i  delimiter  or  following  field  delimiter  this  result  new    string  bytes  start  pos    i  start  pos    get  charset  return  i  delimiter  length  else  no  proper  termination  set  error  state    parse  error  state  unquoted  chars  after  quoted  string  return    else  look  for  delimiter  while  i  delim  limit  delimiter  next  bytes  i  delimiter  i  if  i  delim  limit  this  result  new    string  bytes  start  pos  limit  start  pos  get  charset  return  limit  else  delimiter  found  if  i  start  pos  set  error  state    parse  error  state  empty  column  mark  empty  column  this  result  new    string  bytes  start  pos  i  start  pos  get  charset  return  i  delimiter  length    override  public    string  create  value  return    override  public    string  get  last  result  return  this  result  
public  evolving  public  class    string  value  parser  extends    field  parser    string  value  private  boolean  quoted  string  parsing  false  private  byte  quote  character  private  static  final  byte  backslash    private    string  value  result  public  void  enable  quoted  string  parsing  byte  quote  character  this  quoted  string  parsing  true  this  quote  character  quote  character    override  public  int  parse  field  byte  bytes  int  start  pos  int  limit  byte  delimiter    string  value  reusable  if  start  pos  limit  set  error  state    parse  error  state  empty  column  reusable  set  value  ascii  bytes  start  pos    return  limit  this  result  reusable  int  i  start  pos  final  int  delim  limit  limit  delimiter  length    if  quoted  string  parsing  bytes  i  quote  character  quoted  string  parsing  enabled  and  first  character  is  a  quote  i  search  for  ending  quote  character  continue  when  it  is  escaped  while  i  limit  bytes  i  quote  character  bytes  i    backslash  i  if  i  limit  set  error  state    parse  error  state  unterminated  quoted  string  return    else  i  check  for  proper  termination  if  i  limit  either  by  end  of  line  reusable  set  value  ascii  bytes  start  pos    i  start  pos    return  limit  else  if  i  delim  limit  delimiter  next  bytes  i  delimiter  or  following  field  delimiter  reusable  set  value  ascii  bytes  start  pos    i  start  pos    return  i  delimiter  length  else  no  proper  termination  set  error  state    parse  error  state  unquoted  chars  after  quoted  string  return    else  look  for  delimiter  while  i  delim  limit  delimiter  next  bytes  i  delimiter  i  if  i  delim  limit  reusable  set  value  ascii  bytes  start  pos  limit  start  pos  return  limit  else  delimiter  found  if  i  start  pos  set  error  state    parse  error  state  empty  column  mark  empty  column  reusable  set  value  ascii  bytes  start  pos  i  start  pos  return  i  delimiter  length    override  public    string  value  create  value  return  new    string  value    override  public    string  value  get  last  result  return  this  result  
public  evolving  public  final  class    row  implements    serializable  private  static  final  long  serial  version  u  i  d  2  l    the  kind  of  change  a  row  describes  in  a  changelog  private    row  kind  kind    the  array  to  store  actual  values  private  final    object  fields    create  a  new  row  instance  p    by  default  a  row  describes  an  link    row  kind  insert  change  param  kind  kind  of  change  a  row  describes  in  a  changelog  param  arity    the  number  of  fields  in  the  row  public    row    row  kind  kind  int  arity  this  kind    preconditions  check  not  null  kind    row  kind  must  not  be  null  this  fields  new    object  arity    create  a  new  row  instance  p    by  default  a  row  describes  an  link    row  kind  insert  change  param  arity    the  number  of  fields  in  the  row  public    row  int  arity  this    row  kind  insert  arity    returns  the  kind  of  change  that  this  row  describes  in  a  changelog  p    by  default  a  row  describes  an  link    row  kind  insert  change  see    row  kind  public    row  kind  get  kind  return  kind    sets  the  kind  of  change  that  this  row  describes  in  a  changelog  p    by  default  a  row  describes  an  link    row  kind  insert  change  see    row  kind  public  void  set  kind    row  kind  kind    preconditions  check  not  null  kind    row  kind  must  not  be  null  this  kind  kind    returns  the  number  of  fields  in  the  row  p    note    the  row  kind  is  kept  separate  from  the  fields  and  is  not  included  in  this  number  return    the  number  of  fields  in  the  row  public  int  get  arity  return  fields  length    returns  the  field  s  content  at  the  specified  position  param  pos    the  position  of  the  field    based  return    the  field  s  content  at  the  specified  position  public    nullable    object  get  field  int  pos  return  fields  pos    sets  the  field  s  content  at  the  specified  position  param  pos    the  position  of  the  field    based  param  value    the  value  to  be  assigned  to  the  field  at  the  specified  position  public  void  set  field  int  pos    nullable    object  value  fields  pos  value    override  public    string  to  string    string  builder  sb  new    string  builder  for  int  i    i  fields  length  i  if  i    sb  append  sb  append    string  utils  array  aware  to  string  fields  i  return  sb  to  string    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    row  row    row  o  return  kind  row  kind    arrays  deep  equals  fields  row  fields    override  public  int  hash  code  int  result  kind  to  byte  value  for  stable  hash  across  jvm  instances  result    result    arrays  deep  hash  code  fields  return  result    utility  methods    creates  a  new  row  and  assigns  the  given  values  to  the  row  s  fields    this  is  more  convenient  than  using  the  constructor  p    for  example  pre    row  of  hello  true  1  l  pre  instead  of  pre    row  row  new    row    row  set  field    hello  row  set  field    true  row  set  field    1  l  pre  p    by  default  a  row  describes  an  link    row  kind  insert  change  public  static    row  of    object  values    row  row  new    row  values  length  for  int  i    i  values  length  i  row  set  field  i  values  i  return  row    creates  a  new  row  with  given  kind  and  assigns  the  given  values  to  the  row  s  fields    this  is  more  convenient  than  using  the  constructor  p    for  example  pre    row  of  kind    row  kind  insert  hello  true  1  l  pre  instead  of  pre    row  row  new    row    row  set  kind    row  kind  insert  row  set  field    hello  row  set  field    true  row  set  field    1  l  pre  public  static    row  of  kind    row  kind  kind    object  values    row  row  new    row  kind  values  length  for  int  i    i  values  length  i  row  set  field  i  values  i  return  row    creates  a  new  row  which  is  copied  from  another  row  including  its  link    row  kind  p    this  method  does  not  perform  a  deep  copy  public  static    row  copy    row  row  final    row  new  row  new    row  row  kind  row  fields  length    system  arraycopy  row  fields    new  row  fields    row  fields  length  return  new  row    creates  a  new  row  with  projected  fields  and  identical  link    row  kind  from  another  row  p    this  method  does  not  perform  a  deep  copy  param  fields  field  indices  to  be  projected  public  static    row  project    row  row  int  fields  final    row  new  row  new    row  row  kind  fields  length  for  int  i    i  fields  length  i  new  row  fields  i  row  fields  fields  i  return  new  row    creates  a  new  row  with  fields  that  are  copied  from  the  other  rows  and  appended  to  the  resulting  row  in  the  given  order    the  link    row  kind  of  the  first  row  determines  the  link    row  kind  of  the  result  p    this  method  does  not  perform  a  deep  copy  public  static    row  join    row  first    row  remainings  int  new  length  first  fields  length  for    row  remaining  remainings  new  length  remaining  fields  length  final    row  joined  row  new    row  first  kind  new  length  int  index    copy  the  first  row    system  arraycopy  first  fields    joined  row  fields  index  first  fields  length  index  first  fields  length  copy  the  remaining  rows  for    row  remaining  remainings    system  arraycopy  remaining  fields    joined  row  fields  index  remaining  fields  length  index  remaining  fields  length  return  joined  row  
public  evolving  public  enum    row  kind    note    enums  have  no  stable  hash  code  across  different    j  v  ms  use  to  byte  value  for  this  purpose    insertion  operation  insert  i  byte      update  operation  with  the  previous  content  of  the  updated  row  p    this  kind  should  occur  together  with  link  update  after  for  modelling  an  update  that  needs  to  retract  the  previous  row  first    it  is  useful  in  cases  of  a  non  idempotent  update  i  e  an  update  of  a  row  that  is  not  uniquely  identifiable  by  a  key  update  before  u  byte      update  operation  with  new  content  of  the  updated  row  p    this  kind  can  occur  together  with  link  update  before  for  modelling  an  update  that  needs  to  retract  the  previous  row  first  or  it  describes  an  idempotent  update  i  e  an  update  of  a  row  that  is  uniquely  identifiable  by  a  key  update  after  u  byte      deletion  operation  delete  d  byte    private  final    string  short  string  private  final  byte  value    creates  a  link    row  kind  enum  with  the  given  short  string  and  byte  value  representation  of  the  link    row  kind    row  kind    string  short  string  byte  value  this  short  string  short  string  this  value  value    returns  a  short  string  representation  of  this  link    row  kind  p  ul  li  i  represents  link  insert  li  li  u  represents  link  update  before  li  li  u  represents  link  update  after  li  li  d  represents  link  delete  li  ul  public    string  short  string  return  short  string    returns  the  byte  value  representation  of  this  link    row  kind    the  byte  value  is  used  for  serialization  and  deserialization  p  ul  li    represents  link  insert  li  li    represents  link  update  before  li  li    represents  link  update  after  li  li    represents  link  delete  li  ul  public  byte  to  byte  value  return  value    creates  a  link    row  kind  from  the  given  byte  value    each  link    row  kind  has  a  byte  value  representation  see  to  byte  value  for  mapping  of  byte  value  and  link    row  kind  public  static    row  kind  from  byte  value  byte  value  switch  value  case    return  insert  case    return  update  before  case    return  update  after  case    return  delete  default  throw  new    unsupported  operation  exception    unsupported  byte  value  value  for  row  kind  
public  evolving  public  class    abstract  i  d  implements    comparable    abstract  i  d  java  io    serializable  private  static  final  long  serial  version  u  i  d  1  l  private  static  final    random  rnd  new    random    the  size  of  a  long  in  bytes  private  static  final  int  size  of  long      the  size  of  the  id  in  byte  public  static  final  int  size    size  of  long    the  upper  part  of  the  actual  id  protected  final  long  upper  part    the  lower  part  of  the  actual  id  protected  final  long  lower  part    the  memoized  value  returned  by  to  string  private  transient    string  hex  string    constructs  a  new  id  with  a  specific  bytes  value  public    abstract  i  d  byte  bytes  if  bytes  null  bytes  length  size  throw  new    illegal  argument  exception    argument  bytes  must  by  an  array  of  size  bytes  this  lower  part  byte  array  to  long  bytes    this  upper  part  byte  array  to  long  bytes  size  of  long    constructs  a  new  abstract  id  param  lower  part  the  lower  bytes  of  the  id  param  upper  part  the  higher  bytes  of  the  id  public    abstract  i  d  long  lower  part  long  upper  part  this  lower  part  lower  part  this  upper  part  upper  part    copy  constructor    creates  a  new  abstract  id  from  the  given  one  param  id  the  abstract  id  to  copy  public    abstract  i  d    abstract  i  d  id  if  id  null  throw  new    illegal  argument  exception    id  must  not  be  null  this  lower  part  id  lower  part  this  upper  part  id  upper  part    constructs  a  new  random  id  from  a  uniform  distribution  public    abstract  i  d  this  lower  part  rnd  next  long  this  upper  part  rnd  next  long    gets  the  lower    bits  of  the  id  return    the  lower    bits  of  the  id  public  long  get  lower  part  return  lower  part    gets  the  upper    bits  of  the  id  return    the  upper    bits  of  the  id  public  long  get  upper  part  return  upper  part    gets  the  bytes  underlying  this  id  return    the  bytes  underlying  this  id  public  byte  get  bytes  byte  bytes  new  byte  size  long  to  byte  array  lower  part  bytes    long  to  byte  array  upper  part  bytes  size  of  long  return  bytes    returns  pure    string  representation  of  the  id  in  hexadecimal    this  method  should  be  used  to  construct  things  like  paths  etc  that  require  a  stable  representation  and  is  therefore  final  public  final    string  to  hex  string  if  this  hex  string  null  final  byte  ba  new  byte  size  long  to  byte  array  this  lower  part  ba    long  to  byte  array  this  upper  part  ba  size  of  long  this  hex  string    string  utils  byte  to  hex  string  ba  return  this  hex  string    standard    utilities    override  public  boolean  equals    object  obj  if  obj  this  return  true  else  if  obj  null  obj  get  class  get  class    abstract  i  d  that    abstract  i  d  obj  return  that  lower  part  this  lower  part  that  upper  part  this  upper  part  else  return  false    override  public  int  hash  code  return  int  this  lower  part  int  this  lower  part    int  this  upper  part  int  this  upper  part      override  public    string  to  string  return  to  hex  string    override  public  int  compare  to    abstract  i  d  o  int  diff1    long  compare  this  upper  part  o  upper  part  int  diff2    long  compare  this  lower  part  o  lower  part  return  diff1    diff2  diff1    conversion    utilities    converts  the  given  byte  array  to  a  long  param  ba  the  byte  array  to  be  converted  param  offset  the  offset  indicating  at  which  byte  inside  the  array  the  conversion  shall  begin  return  the  long  variable  private  static  long  byte  array  to  long  byte  ba  int  offset  long  l    for  int  i    i  size  of  long  i  l  ba  offset  size  of  long    i  0xff  l  i    return  l    converts  a  long  to  a  byte  array  param  l  the  long  variable  to  be  converted  param  ba  the  byte  array  to  store  the  result  the  of  the  conversion  param  offset  offset  indicating  at  what  position  inside  the  byte  array  the  result  of  the  conversion  shall  be  stored  private  static  void  long  to  byte  array  long  l  byte  ba  int  offset  for  int  i    i  size  of  long  i  final  int  shift  i    i    ba  offset  size  of  long    i  byte  l  0xff  l  shift  shift  
public  evolving  public  abstract  class    clock    gets  the  current  absolute  time  in  milliseconds  public  abstract  long  absolute  time  millis    gets  the  current  relative  time  in  milliseconds  public  abstract  long  relative  time  millis    gets  the  current  relative  time  in  nanoseconds  public  abstract  long  relative  time  nanos  
public  evolving  public  final  class    manual  clock  extends    clock  private  final    atomic  long  current  time  public    manual  clock  this    public    manual  clock  long  start  time  this  current  time  new    atomic  long  start  time    override  public  long  absolute  time  millis  return  current  time  get        000  l    override  public  long  relative  time  millis  return  current  time  get        000  l    override  public  long  relative  time  nanos  return  current  time  get    advances  the  time  by  the  given  duration    time  can  also  move  backwards  by  supplying  a  negative  value    this  method  performs  no  overflow  check  public  void  advance  time  long  duration    time  unit  time  unit  current  time  add  and  get  time  unit  to  nanos  duration    advances  the  time  by  the  given  duration    time  can  also  move  backwards  by  supplying  a  negative  value    this  method  performs  no  overflow  check  public  void  advance  time    duration  duration  current  time  add  and  get  duration  to  nanos  
public  evolving  public  final  class    system  clock  extends    clock  private  static  final    system  clock  instance  new    system  clock  public  static    system  clock  get  instance  return  instance    override  public  long  absolute  time  millis  return    system  current  time  millis    override  public  long  relative  time  millis  return    system  nano  time            override  public  long  relative  time  nanos  return    system  nano  time  private    system  clock  
public  evolving    functional  interface  public  interface    quad  consumer  s  t  u  v    performs  this  operation  on  the  given  arguments  param  s  first  argument  param  t  second  argument  param  u  third  argument  param  v  fourth  argument  void  accept  s  s  t  t  u  u  v  v  
public  evolving    functional  interface  public  interface    quad  function  s  t  u  v  r    applies  this  function  to  the  given  arguments  param  s  the  first  function  argument  param  t  the  second  function  argument  param  u  the  third  function  argument  oaram  v  the  fourth  function  argument  return  the  function  result  r  apply  s  s  t  t  u  u  v  v  
public  evolving    functional  interface  public  interface    throwing  runnable  e  extends    throwable    the  work  method  throws  e    exceptions  may  be  thrown  void  run  throws  e    converts  a  link    throwing  runnable  into  a  link    runnable  which  throws  all  checked  exceptions  as  unchecked  param  throwing  runnable  to  convert  into  a  link    runnable  return  link    runnable  which  throws  all  checked  exceptions  as  unchecked  static    runnable  unchecked    throwing  runnable  throwing  runnable  return  try  throwing  runnable  run  catch    throwable  t    exception  utils  rethrow  t  
public  evolving    functional  interface  public  interface    tri  consumer  s  t  u    performs  this  operation  on  the  given  arguments  param  s  first  argument  param  t  second  argument  param  u  third  argument  void  accept  s  s  t  t  u  u  
public  evolving    functional  interface  public  interface    tri  function  s  t  u  r    applies  this  function  to  the  given  arguments  param  s  the  first  function  argument  param  t  the  second  function  argument  param  u  the  third  function  argument  return  the  function  result  r  apply  s  s  t  t  u  u  
public  evolving    functional  interface  public  interface    tri  function  with  exception  s  t  u  r  e  extends    throwable    applies  this  function  to  the  given  arguments  param  s  the  first  function  argument  param  t  the  second  function  argument  param  u  the  third  function  argument  return  the  function  result  throws  e  if  it  fails  r  apply  s  s  t  t  u  u  throws  e    convert  at  link    tri  function  with  exception  into  a  link    tri  function  param  tri  function  with  exception  function  with  exception  to  convert  into  a  function  param  a  first  input  type  param  b  second  input  type  param  c  third  input  type  param  d  output  type  return  link    bi  function  which  throws  all  checked  exception  as  an  unchecked  exception  static  a  b  c  d    tri  function  a  b  c  d  unchecked    tri  function  with  exception  a  b  c  d  tri  function  with  exception  return  a  a  b  b  c  c  try  return  tri  function  with  exception  apply  a  b  c  catch    throwable  t    exception  utils  rethrow  t  we  need  this  to  appease  the  compiler  return  null  
public  evolving  public  class    output  tag  t  implements    serializable  private  static  final  long  serial  version  u  i  d  2  l  private  final    string  id  private  final    type  information  t  type  info    creates  a  new  named  code    output  tag  with  the  given  id  param  id    the  id  of  the  created  code    output  tag  public    output  tag    string  id    preconditions  check  not  null  id    output  tag  id  cannot  be  null    preconditions  check  argument  id  is  empty    output  tag  id  must  not  be  empty  this  id  id  try  this  type  info    type  extractor  create  type  info  this    output  tag  class  get  class    catch    invalid  types  exception  e  throw  new    invalid  types  exception    could  not  determine    type  information  for  the    output  tag  type    the  most  common  reason  is  forgetting  to  make  the    output  tag  an  anonymous  inner  class    it  is  also  not  possible  to  use  generic  type  variables  with    output  tags  such  as    tuple2  a  b  e    creates  a  new  named  code    output  tag  with  the  given  id  and  output  link    type  information  param  id    the  id  of  the  created  code    output  tag  param  type  info    the  code    type  information  for  the  side  output  public    output  tag    string  id    type  information  t  type  info    preconditions  check  not  null  id    output  tag  id  cannot  be  null    preconditions  check  argument  id  is  empty    output  tag  id  must  not  be  empty  this  id  id  this  type  info    preconditions  check  not  null  type  info    type  information  cannot  be  null  public    string  get  id  return  id  public    type  information  t  get  type  info  return  type  info    override  public  boolean  equals    object  obj  return  obj  instanceof    output  tag    output  tag  obj  id  equals  this  id    override  public  int  hash  code  return  id  hash  code    override  public    string  to  string  return    output  tag  get  type  info  id  
public  evolving  public  final  class    string  utils  private  static  final  char  hex  chars                      a  b  c  d  e  f    given  an  array  of  bytes  it  will  convert  the  bytes  to  a  hex  string  representation  of  the  bytes  param  bytes  the  bytes  to  convert  in  a  hex  string  param  start  start  index  inclusively  param  end  end  index  exclusively  return  hex  string  representation  of  the  byte  array  public  static    string  byte  to  hex  string  final  byte  bytes  final  int  start  final  int  end  if  bytes  null  throw  new    illegal  argument  exception  bytes  null  int  length  end  start  char  out  new  char  length    for  int  i  start  j    i  end  i  out  j  hex  chars  0x  f0  bytes  i    out  j  hex  chars  0x0  f  bytes  i  return  new    string  out    given  an  array  of  bytes  it  will  convert  the  bytes  to  a  hex  string  representation  of  the  bytes  param  bytes  the  bytes  to  convert  in  a  hex  string  return  hex  string  representation  of  the  byte  array  public  static    string  byte  to  hex  string  final  byte  bytes  return  byte  to  hex  string  bytes    bytes  length    given  a  hex  string  this  will  return  the  byte  array  corresponding  to  the  string  param  hex  the  hex    string  array  return  a  byte  array  that  is  a  hex  string  representation  of  the  given  string    the  size  of  the  byte  array  is  therefore  hex  length    public  static  byte  hex  string  to  byte  final    string  hex  final  byte  bts  new  byte  hex  length    for  int  i    i  bts  length  i  bts  i  byte    integer  parse  int  hex  substring    i    i      return  bts    converts  the  given  object  into  a  string  representation  by  calling  link    object  to  string  and  formatting  possibly  nested  arrays  and  code  null  p    see  link    arrays  deep  to  string    object  for  more  information  about  the  used  format  public  static    string  array  aware  to  string    object  o  final    string  array  string    arrays  deep  to  string  new    object  o  return  array  string  substring    array  string  length      replaces  control  characters  by  their  escape  coded  version    for  example  if  the  string  contains  a  line  break  character  n  this  character  will  be  replaced  by  the  two  characters  backslash  and  n    as  a  consequence  the  resulting  string  will  not  contain  any  more  control  characters  param  str    the  string  in  which  to  replace  the  control  characters  return    the  string  with  the  replaced  characters  public  static    string  show  control  characters    string  str  int  len  str  length    string  builder  sb  new    string  builder  for  int  i    i  len  i    char  c  str  char  at  i  switch  c  case  b  sb  append  b  break  case  t  sb  append  t  break  case  n  sb  append  n  break  case  f  sb  append  f  break  case  r  sb  append  r  break  default  sb  append  c  return  sb  to  string    creates  a  random  string  with  a  length  within  the  given  interval    the  string  contains  only  characters  that  can  be  represented  as  a  single  code  point  param  rnd    the  random  used  to  create  the  strings  param  min  length    the  minimum  string  length  param  max  length    the  maximum  string  length  inclusive  return  a  random    string  public  static    string  get  random  string    random  rnd  int  min  length  int  max  length  int  len  rnd  next  int  max  length  min  length    min  length  char  data  new  char  len  for  int  i    i  data  length  i  data  i  char  rnd  next  int  0x7  fff    return  new    string  data    creates  a  random  string  with  a  length  within  the  given  interval    the  string  contains  only  characters  that  can  be  represented  as  a  single  code  point  param  rnd    the  random  used  to  create  the  strings  param  min  length    the  minimum  string  length  param  max  length    the  maximum  string  length  inclusive  param  min  value    the  minimum  character  value  to  occur  param  max  value    the  maximum  character  value  to  occur  return  a  random    string  public  static    string  get  random  string    random  rnd  int  min  length  int  max  length  char  min  value  char  max  value  int  len  rnd  next  int  max  length  min  length    min  length  char  data  new  char  len  int  diff  max  value  min  value    for  int  i    i  data  length  i  data  i  char  rnd  next  int  diff  min  value  return  new    string  data    creates  a  random  alphanumeric  string  of  given  length  param  rnd    the  random  number  generator  to  use  param  length    the  number  of  alphanumeric  characters  to  append  public  static    string  generate  random  alphanumeric  string    random  rnd  int  length  check  not  null  rnd  check  argument  length      string  builder  buffer  new    string  builder  length  for  int  i    i  length  i  buffer  append  next  alphanumeric  char  rnd  return  buffer  to  string  private  static  char  next  alphanumeric  char    random  rnd  int  which  rnd  next  int    char  c  if  which    c  char    which  else  if  which    c  char  a    which  else  c  char  a    which  return  c    writes  a    string  to  the  given  output    the  written  string  can  be  read  with  link  read  string    data  input  view  param  str    the  string  to  write  param  out    the  output  to  write  to  throws    i  o  exception    thrown  if  the  writing  or  the  serialization  fails  public  static  void  write  string    nonnull    string  str    data  output  view  out  throws    i  o  exception  check  not  null  str    string  value  write  string  str  out    reads  a  non  null    string  from  the  given  input  param  in    the  input  to  read  from  return    the  deserialized    string  throws    i  o  exception    thrown  if  the  reading  or  the  deserialization  fails  public  static    string  read  string    data  input  view  in  throws    i  o  exception  return    string  value  read  string  in    writes  a    string  to  the  given  output    the  string  may  be  null    the  written  string  can  be  read  with  link  read  nullable  string    data  input  view  param  str    the  string  to  write  or  null  param  out    the  output  to  write  to  throws    i  o  exception    thrown  if  the  writing  or  the  serialization  fails  public  static  void  write  nullable  string    nullable    string  str    data  output  view  out  throws    i  o  exception  if  str  null  out  write  boolean  true  write  string  str  out  else  out  write  boolean  false    reads  a    string  from  the  given  input    the  string  may  be  null  and  must  have  been  written  with  link  write  nullable  string    string    data  output  view  param  in    the  input  to  read  from  return    the  deserialized  string  or  null  throws    i  o  exception    thrown  if  the  reading  or  the  deserialization  fails  public  static    nullable    string  read  nullable  string    data  input  view  in  throws    i  o  exception  if  in  read  boolean  return  read  string  in  else  return  null    checks  if  the  string  is  null  empty  or  contains  only  whitespace  characters  a  whitespace  character  is  defined  via  link    character  is  whitespace  char  param  str    the  string  to  check  return    true  if  the  string  is  null  or  blank  false  otherwise  public  static  boolean  is  null  or  whitespace  only    string  str  if  str  null  str  length    return  true  final  int  len  str  length  for  int  i    i  len  i  if    character  is  whitespace  str  char  at  i  return  false  return  true    if  both  string  arguments  are  non  null  this  method  concatenates  them  with  and    if  only  one  of  the  arguments  is  non  null  this  method  returns  the  non  null  argument    if  both  arguments  are  null  this  method  returns  null  param  s1    the  first  string  argument  param  s2    the  second  string  argument  return    the  concatenated  string  or  non  null  argument  or  null    nullable  public  static    string  concatenate  with  and    nullable    string  s1    nullable    string  s2  if  s1  null  return  s2  null  s1  s1  and  s2  else  return  s2    generates  a  string  containing  a  comma  separated  list  of  values  in  double  quotes    uses  lower  cased  values  returned  from  link    object  to  string  method  for  each  element  in  the  given  array    null  values  are  skipped  param  values  array  of  elements  for  the  list  return    the  string  with  quoted  list  of  elements  public  static    string  to  quoted  list  string    object  values  return    arrays  stream  values  filter    objects  non  null  map  v  v  to  string  to  lower  case  collect    collectors  joining    prevent  instantiation  of  this  utility  class  private    string  utils  
public  evolving  public  final  class    string  value  utils    converts  the  given  code    string  value  code  into  a  lower  case  variant  param  string    the  string  to  convert  to  lower  case  public  static  void  to  lower  case    string  value  string  final  char  chars  string  get  char  array  final  int  len  string  length  for  int  i    i  len  i  chars  i    character  to  lower  case  chars  i    replaces  all  non  word  characters  in  a  string  by  a  given  character    the  only  characters  not  replaced  are  the  characters  that  qualify  as  word  characters  or  digit  characters  with  respect  to  link    character  is  letter  char  or  link    character  is  digit  char  as  well  as  the  underscore  character  p    this  operation  is  intended  to  simplify  strings  for  counting  distinct  words  param  string    the  string  value  to  have  the  non  word  characters  replaced  param  replacement    the  character  to  use  as  the  replacement  public  static  void  replace  non  word  chars    string  value  string  char  replacement  final  char  chars  string  get  char  array  final  int  len  string  length  for  int  i    i  len  i  final  char  c  chars  i  if    character  is  letter  c    character  is  digit  c  c  chars  i  replacement  a  tokenizer  for  string  values  that  uses  whitespace  characters  as  token  delimiters    the  tokenizer  is  designed  to  have  a  resettable  state  and  operate  on  mutable  objects  sparing  object  allocation  and  garbage  collection  overhead  public  static  final  class    whitespace  tokenizer  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l  private    string  value  to  tokenize  the  string  to  tokenize  private  int  pos  the  current  position  in  the  string  private  int  limit  the  limit  in  the  string  s  character  data    creates  a  new  tokenizer  with  an  undefined  internal  state  public    whitespace  tokenizer    sets  the  string  to  be  tokenized  and  resets  the  state  of  the  tokenizer  param  string    the  string  value  to  be  tokenized  public  void  set  string  to  tokenize    string  value  string  this  to  tokenize  string  this  pos    this  limit  string  length    gets  the  next  token  from  the  string    if  another  token  is  available  the  token  is  stored  in  the  given  target    string  value  object  param  target    the    string  value  object  to  store  the  next  token  in  return    true  if  there  was  another  token  false  if  not  public  boolean  next    string  value  target  final  char  data  this  to  tokenize  get  char  array  final  int  limit  this  limit  int  pos  this  pos  skip  the  delimiter  for  pos  limit    character  is  whitespace  data  pos  pos  if  pos  limit  this  pos  pos  return  false  final  int  start  pos  for  pos  limit    character  is  whitespace  data  pos  pos  this  pos  pos  target  set  value  this  to  tokenize  start  pos  start  return  true    private  constructor  to  prevent  instantiation  as  this  is  a  utility  method  encapsulating  class  private    string  value  utils  
public  evolving  public  enum    ternary  boolean    the  value  for  true  true    the  value  for  false  false    the  value  for  undefined    in  a  configuration  setting  this  typically  means  that  the  default  value  will  be  used  or  the  value  from  a  deployment  wide  configuration  undefined    gets  the  boolean  value  corresponding  to  this  value    if  this  is  the  undefined  value  the  method  returns  the  given  default  param  default  value    the  value  to  be  returned  in  case  this  ternary  value  is  undefined  public  boolean  get  or  default  boolean  default  value  return  this  undefined  default  value  this  true    gets  the  boolean  value  corresponding  to  this  value    if  this  is  the  undefined  value  the  method  returns  the  given  value  for  undefined  param  value  for  undefined    the  value  to  be  returned  in  case  this  ternary  value  is  undefined  public    ternary  boolean  resolve  undefined  boolean  value  for  undefined  return  this  undefined  this  from  boolean  value  for  undefined    gets  this  ternary  boolean  as  a  boxed  boolean    the  value  undefined  results  in  null    nullable  public    boolean  get  as  boolean  return  this  undefined  null  this  true    boolean  true    boolean  false    converts  the  given  boolean  to  a    ternary  boolean  link  true  or  link  false  respectively  public  static    ternary  boolean  from  boolean  boolean  bool  return  bool  true  false    converts  the  given  boxed    boolean  to  a    ternary  boolean  a  null  value  results  in  link  undefined  while  a  non  null  value  results  in  link  true  or  link  false  respectively  public  static    ternary  boolean  from  boxed  boolean    nullable    boolean  bool  return  bool  null  undefined  from  boolean  bool  
public  evolving    not  thread  safe  public  final  class    s3  recoverable  fs  data  output  stream  extends    recoverable  fs  data  output  stream    lock  that  guards  the  critical  sections  when  new  parts  are  rolled  over    despite  the  class  being  declared  not  thread  safe  we  protect  certain  regions  to  at  least  enable  concurrent  close  calls  during  cancellation  or  abort  cleanup  private  final    reentrant  lock  lock  new    reentrant  lock  private  final    recoverable  multi  part  upload  upload  private  final    function  with  exception    file    ref  counted  file  with  stream    i  o  exception  tmp  file  provider    the  number  of  bytes  at  which  we  start  a  new  part  of  the  multipart  upload    this  has  to  be  greater  than  the  non  configurable  minimum    that  is  equal  to  link  org  apache  flink  fs  s3  common    flink  s3  file  system    s3  multipart  min  part  size    s3  multipart  min  part  size  and  is  set  by    amazon  private  final  long  user  defined  min  part  size  private    ref  counted  f  s  output  stream  file  stream  private  long  bytes  before  current  part    single  constructor  to  initialize  all    actual  setup  of  the  parts  happens  in  the  factory  methods    s3  recoverable  fs  data  output  stream    recoverable  multi  part  upload  upload    function  with  exception    file    ref  counted  file  with  stream    i  o  exception  temp  file  creator    ref  counted  f  s  output  stream  initial  tmp  file  long  user  defined  min  part  size  long  bytes  before  current  part  check  argument  bytes  before  current  part  0  l  this  upload  check  not  null  upload  this  tmp  file  provider  check  not  null  temp  file  creator  this  user  defined  min  part  size  user  defined  min  part  size  this  file  stream  initial  tmp  file  this  bytes  before  current  part  bytes  before  current  part  stream  methods    override  public  void  write  int  b  throws    i  o  exception  file  stream  write  b    override  public  void  write  byte  b  int  off  int  len  throws    i  o  exception  file  stream  write  b  off  len  open  new  part  if  necessary  user  defined  min  part  size    override  public  void  flush  throws    i  o  exception  file  stream  flush  open  new  part  if  necessary  user  defined  min  part  size    override  public  long  get  pos  throws    i  o  exception  return  bytes  before  current  part  file  stream  get  pos    override  public  void  sync  throws    i  o  exception  file  stream  sync    override  public  void  close  throws    i  o  exception  lock  try  file  stream  flush  finally    i  o  utils  close  quietly  file  stream  file  stream  release  unlock  recoverable  stream  methods    override  public    recoverable  writer    resume  recoverable  persist  throws    i  o  exception  lock  try  file  stream  flush  open  new  part  if  necessary  user  defined  min  part  size    we  do  not  stop  writing  to  the  current  file  we  merely  limit  the  upload  to  the  first  n  bytes  of  the  current  file  return  upload  snapshot  and  get  recoverable  file  stream  finally  unlock    override  public    committer  close  for  commit  throws    i  o  exception  lock  try  close  and  upload  part  return  upload  snapshot  and  get  committer  finally  unlock    s3  private  void  open  new  part  if  necessary  long  size  threshold  throws    i  o  exception  final  long  file  length  file  stream  get  pos  if  file  length  size  threshold  lock  try  upload  current  and  open  new  part  file  length  finally  unlock  private  void  upload  current  and  open  new  part  long  file  length  throws    i  o  exception  bytes  before  current  part  file  length  close  and  upload  part  initialize  a  new  temp  file  file  stream    ref  counted  buffering  file  stream  open  new  tmp  file  provider  private  void  close  and  upload  part  throws    i  o  exception  file  stream  flush  file  stream  close  if  file  stream  get  pos  0  l  upload  upload  part  file  stream  file  stream  release  locking  private  void  lock  throws    i  o  exception  try  lock  lock  interruptibly  catch    interrupted  exception  e    thread  current  thread  interrupt  throw  new    i  o  exception  interrupted  private  void  unlock  lock  unlock  factory  methods  public  static    s3  recoverable  fs  data  output  stream  new  stream  final    recoverable  multi  part  upload  upload  final    function  with  exception    file    ref  counted  file  with  stream    i  o  exception  tmp  file  creator  final  long  user  defined  min  part  size  throws    i  o  exception  check  argument  user  defined  min  part  size    s3  multipart  min  part  size  final    ref  counted  buffering  file  stream  file  stream  bounded  buffering  file  stream  tmp  file  creator    optional  empty  return  new    s3  recoverable  fs  data  output  stream  upload  tmp  file  creator  file  stream  user  defined  min  part  size  0  l  public  static    s3  recoverable  fs  data  output  stream  recover  stream  final    recoverable  multi  part  upload  upload  final    function  with  exception    file    ref  counted  file  with  stream    i  o  exception  tmp  file  creator  final  long  user  defined  min  part  size  final  long  bytes  before  current  part  throws    i  o  exception  check  argument  user  defined  min  part  size    s3  multipart  min  part  size  final    ref  counted  buffering  file  stream  file  stream  bounded  buffering  file  stream  tmp  file  creator  upload  get  incomplete  part  return  new    s3  recoverable  fs  data  output  stream  upload  tmp  file  creator  file  stream  user  defined  min  part  size  bytes  before  current  part  private  static    ref  counted  buffering  file  stream  bounded  buffering  file  stream  final    function  with  exception    file    ref  counted  file  with  stream    i  o  exception  tmp  file  creator  final    optional    file  incomplete  part  throws    i  o  exception  if  incomplete  part  is  present  return    ref  counted  buffering  file  stream  open  new  tmp  file  creator  final    file  file  incomplete  part  get  return    ref  counted  buffering  file  stream  restore  tmp  file  creator  file  
public  evolving  public  class    s3  recoverable  writer  implements    recoverable  writer  private  final    function  with  exception    file    ref  counted  file  with  stream    i  o  exception  temp  file  creator  private  final  long  user  defined  min  part  size  private  final    s3  access  helper  s3  access  helper  private  final    s3  recoverable  multipart  upload  factory  upload  factory    visible  for  testing    s3  recoverable  writer  final    s3  access  helper  s3  access  helper  final    s3  recoverable  multipart  upload  factory  upload  factory  final    function  with  exception    file    ref  counted  file  with  stream    i  o  exception  temp  file  creator  final  long  user  defined  min  part  size  this  s3  access  helper  check  not  null  s3  access  helper  this  upload  factory  check  not  null  upload  factory  this  temp  file  creator  check  not  null  temp  file  creator  this  user  defined  min  part  size  user  defined  min  part  size    override  public    recoverable  fs  data  output  stream  open    path  path  throws    i  o  exception  final    recoverable  multi  part  upload  upload  upload  factory  get  new  recoverable  upload  path  return    s3  recoverable  fs  data  output  stream  new  stream  upload  temp  file  creator  user  defined  min  part  size    override  public    committer  recover  for  commit    commit  recoverable  recoverable  throws    i  o  exception  final    s3  recoverable  s3recoverable  cast  to  s3  recoverable  recoverable  final    s3  recoverable  fs  data  output  stream  recovered  recover  s3recoverable  return  recovered  close  for  commit    override  public    s3  recoverable  fs  data  output  stream  recover    resume  recoverable  recoverable  throws    i  o  exception  final    s3  recoverable  s3recoverable  cast  to  s3  recoverable  recoverable  final    recoverable  multi  part  upload  upload  upload  factory  recover  recoverable  upload  s3recoverable  return    s3  recoverable  fs  data  output  stream  recover  stream  upload  temp  file  creator  user  defined  min  part  size  s3recoverable  num  bytes  in  parts    override  public  boolean  requires  cleanup  of  recoverable  state  return  true    override  public  boolean  cleanup  recoverable  state    resume  recoverable  resumable  throws    i  o  exception  final    s3  recoverable  s3recoverable  cast  to  s3  recoverable  resumable  final    string  small  part  object  to  delete  s3recoverable  incomplete  object  name  return  small  part  object  to  delete  null  s3  access  helper  delete  object  small  part  object  to  delete    override    suppress  warnings  rawtypes  unchecked  public    simple  versioned  serializer    commit  recoverable  get  commit  recoverable  serializer  return    simple  versioned  serializer    s3  recoverable  serializer  instance    override    suppress  warnings  rawtypes  unchecked  public    simple  versioned  serializer    resume  recoverable  get  resume  recoverable  serializer  return    simple  versioned  serializer    s3  recoverable  serializer  instance    override  public  boolean  supports  resume  return  true    utils  private  static    s3  recoverable  cast  to  s3  recoverable    commit  recoverable  recoverable  if  recoverable  instanceof    s3  recoverable  return    s3  recoverable  recoverable  throw  new    illegal  argument  exception    s3    file    system  cannot  recover  recoverable  for  other  file  system  recoverable    static    constructor  public  static    s3  recoverable  writer  writer  final    file  system  fs  final    function  with  exception    file    ref  counted  file  with  stream    i  o  exception  temp  file  creator  final    s3  access  helper  s3  access  helper  final    executor  upload  thread  pool  final  long  user  defined  min  part  size  final  int  max  concurrent  uploads  per  stream  check  argument  user  defined  min  part  size    s3  multipart  min  part  size  final    s3  recoverable  multipart  upload  factory  upload  factory  new    s3  recoverable  multipart  upload  factory  fs  s3  access  helper  max  concurrent  uploads  per  stream  upload  thread  pool  temp  file  creator  return  new    s3  recoverable  writer  s3  access  helper  upload  factory  temp  file  creator  user  defined  min  part  size  
public  evolving  public  class    avro  row  data  deserialization  schema  implements    deserialization  schema    row  data  private  static  final  long  serial  version  u  i  d  1  l    used  for  converting    date  type  private  static  final  int  millis  per  day        logical  type  describing  the  result  type  private  final    row  type  row  type    type  information  describing  the  result  type  private  final    type  information    row  data  type  info    runtime  instance  that  performs  the  actual  work  private  final    deserialization  runtime  converter  runtime  converter    record  to  deserialize  byte  array  private  transient    indexed  record  record    reader  that  deserializes  byte  array  into  a  record  private  transient    datum  reader    indexed  record  datum  reader    input  stream  to  read  message  from  private  transient    mutable  byte  array  input  stream  input  stream    avro  decoder  that  decodes  binary  data  private  transient    decoder  decoder    creates  a    avro  deserialization  schema  for  the  given  logical  type  param  row  type    the  logical  type  used  to  deserialize  the  data  param  type  info    the    type  information  to  be  used  by  link    avro  row  data  deserialization  schema  get  produced  type  public    avro  row  data  deserialization  schema    row  type  row  type    type  information    row  data  type  info  this  row  type  row  type  this  type  info  type  info  this  runtime  converter  create  row  converter  row  type    override  public  void  open    initialization  context  context  throws    exception  final    schema  schema    avro  schema  converter  convert  to  schema  row  type  this  record  new    generic  data    record  schema  this  datum  reader  new    specific  datum  reader  schema  this  input  stream  new    mutable  byte  array  input  stream  this  decoder    decoder  factory  get  binary  decoder  this  input  stream  null    override  public    row  data  deserialize  byte  message  throws    i  o  exception  try  input  stream  set  buffer  message  record  datum  reader  read  record  decoder  return    row  data  runtime  converter  convert  record  catch    exception  e  throw  new    i  o  exception    failed  to  deserialize    avro  record  e    override  public  boolean  is  end  of  stream    row  data  next  element  return  false    override  public    type  information    row  data  get  produced  type  return  type  info    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  final    avro  row  data  deserialization  schema  that    avro  row  data  deserialization  schema  o  return    objects  equals  row  type  that  row  type    objects  equals  type  info  that  type  info    override  public  int  hash  code  return    objects  hash  row  type  type  info    runtime    converters    runtime  converter  that  converts    avro  data  structures  into  objects  of    flink    table  sql  internal  data  structures    functional  interface  interface    deserialization  runtime  converter  extends    serializable    object  convert    object  object  static    deserialization  runtime  converter  create  row  converter    row  type  row  type  final    deserialization  runtime  converter  field  converters  row  type  get  fields  stream  map    row  type    row  field  get  type  map    avro  row  data  deserialization  schema  create  nullable  converter  to  array    deserialization  runtime  converter  new  final  int  arity  row  type  get  field  count  return  avro  object    indexed  record  record    indexed  record  avro  object    generic  row  data  row  new    generic  row  data  arity  for  int  i    i  arity  i  row  set  field  i  field  converters  i  convert  record  get  i  return  row    creates  a  runtime  converter  which  is  null  safe  private  static    deserialization  runtime  converter  create  nullable  converter    logical  type  type  final    deserialization  runtime  converter  converter  create  converter  type  return  avro  object  if  avro  object  null  return  null  return  converter  convert  avro  object    creates  a  runtime  converter  which  assuming  input  object  is  not  null  private  static    deserialization  runtime  converter  create  converter    logical  type  type  switch  type  get  type  root  case  null  return  avro  object  null  case  tinyint  return  avro  object    integer  avro  object  byte  value  case  smallint  return  avro  object    integer  avro  object  short  value  case  boolean  boolean  case  integer  int  case  interval  year  month  long  case  bigint  long  case  interval  day  time  long  case  float  float  case  double  double  return  avro  object  avro  object  case  date  return    avro  row  data  deserialization  schema  convert  to  date  case  time  without  time  zone  return    avro  row  data  deserialization  schema  convert  to  time  case  timestamp  without  time  zone  return    avro  row  data  deserialization  schema  convert  to  timestamp  case  char  case  varchar  return  avro  object    string  data  from  string  avro  object  to  string  case  binary  case  varbinary  return    avro  row  data  deserialization  schema  convert  to  bytes  case  decimal  return  create  decimal  converter    decimal  type  type  case  array  return  create  array  converter    array  type  type  case  row  return  create  row  converter    row  type  type  case  map  case  multiset  return  create  map  converter  type  case  raw  default  throw  new    unsupported  operation  exception    unsupported  type  type  private  static    deserialization  runtime  converter  create  decimal  converter    decimal  type  decimal  type  final  int  precision  decimal  type  get  precision  final  int  scale  decimal  type  get  scale  return  avro  object  final  byte  bytes  if  avro  object  instanceof    generic  fixed  bytes    generic  fixed  avro  object  bytes  else  if  avro  object  instanceof    byte  buffer    byte  buffer  byte  buffer    byte  buffer  avro  object  bytes  new  byte  byte  buffer  remaining  byte  buffer  get  bytes  else  bytes  byte  avro  object  return    decimal  data  from  unscaled  bytes  bytes  precision  scale  private  static    deserialization  runtime  converter  create  array  converter    array  type  array  type  final    deserialization  runtime  converter  element  converter  create  nullable  converter  array  type  get  element  type  final    class  element  class    logical  type  utils  to  internal  conversion  class  array  type  get  element  type  return  avro  object  final    list  list    list  avro  object  final  int  length  list  size  final    object  array    object    array  new  instance  element  class  length  for  int  i    i  length  i  array  i  element  converter  convert  list  get  i  return  new    generic  array  data  array  private  static    deserialization  runtime  converter  create  map  converter    logical  type  type  final    deserialization  runtime  converter  key  converter  create  converter    data  types  string  get  logical  type  final    deserialization  runtime  converter  value  converter  create  converter  extract  value  type  to  avro  map  type  return  avro  object  final    map  map    map  avro  object    map    object    object  result  new    hash  map  for    map    entry  entry  map  entry  set    object  key  key  converter  convert  entry  get  key    object  value  value  converter  convert  entry  get  value  result  put  key  value  return  new    generic  map  data  result  private  static    timestamp  data  convert  to  timestamp    object  object  final  long  millis  if  object  instanceof    long  millis    long  object  else  use  provided    joda  time  final    date  time  value    date  time  object  millis  value  to  date  get  time  return  to  timestamp  data  millis  private  static  int  convert  to  date    object  object  if  object  instanceof    integer  return    integer  object  else  use  provided    joda  time  final    local  date  value    local  date  object  return  int  to  timestamp  data  value  to  date  get  time  get  millisecond  millis  per  day  private  static    timestamp  data  to  timestamp  data  long  time  zone  mills  return    timestamp  data  from  timestamp  new    timestamp  time  zone  mills  private  static  int  convert  to  time    object  object  final  int  millis  if  object  instanceof    integer  millis    integer  object  else  use  provided    joda  time  final  org  joda  time    local  time  value  org  joda  time    local  time  object  millis  value  get    date  time  field  type  millis  of  day  return  millis  private  static  byte  convert  to  bytes    object  object  if  object  instanceof    generic  fixed  return    generic  fixed  object  bytes  else  if  object  instanceof    byte  buffer    byte  buffer  byte  buffer    byte  buffer  object  byte  bytes  new  byte  byte  buffer  remaining  byte  buffer  get  bytes  return  bytes  else  return  byte  object  
public  evolving  public  class    avro  row  deserialization  schema  extends    abstract  deserialization  schema    row    used  for  time  conversions  into  sql  types  private  static  final    time  zone  local  tz    time  zone  get  default    avro  record  class  for  deserialization    might  be  null  if  record  class  is  not  available  private    class  extends    specific  record  record  clazz    schema  string  for  deserialization  private    string  schema  string    avro  serialization  schema  private  transient    schema  schema    type  information  describing  the  result  type  private  transient    row  type  info  type  info    record  to  deserialize  byte  array  private  transient    indexed  record  record    reader  that  deserializes  byte  array  into  a  record  private  transient    datum  reader    indexed  record  datum  reader    input  stream  to  read  message  from  private  transient    mutable  byte  array  input  stream  input  stream    avro  decoder  that  decodes  binary  data  private  transient    decoder  decoder    creates  a    avro  deserialization  schema  for  the  given  specific  record  class    having  the  concrete    avro  record  class  might  improve  performance  param  record  clazz    avro  record  class  used  to  deserialize    avro  s  record  to    flink  s  row  public    avro  row  deserialization  schema    class  extends    specific  record  record  clazz    preconditions  check  not  null  record  clazz    avro  record  class  must  not  be  null  this  record  clazz  record  clazz  schema    specific  data  get  get  schema  record  clazz  type  info    row  type  info    avro  schema  converter  convert  to  type  info  record  clazz  schema  string  schema  to  string  record    indexed  record    specific  data  new  instance  record  clazz  schema  datum  reader  new    specific  datum  reader  schema  input  stream  new    mutable  byte  array  input  stream  decoder    decoder  factory  get  binary  decoder  input  stream  null    creates  a    avro  deserialization  schema  for  the  given    avro  schema  string  param  avro  schema  string    avro  schema  string  to  deserialize    avro  s  record  to    flink  s  row  public    avro  row  deserialization  schema    string  avro  schema  string    preconditions  check  not  null  avro  schema  string    avro  schema  must  not  be  null  record  clazz  null  final    type  information  type  info    avro  schema  converter  convert  to  type  info  avro  schema  string    preconditions  check  argument  type  info  instanceof    row  type  info    row  type  information  expected  this  type  info    row  type  info  type  info  schema  string  avro  schema  string  schema  new    schema    parser  parse  avro  schema  string  record  new    generic  data    record  schema  datum  reader  new    generic  datum  reader  schema  input  stream  new    mutable  byte  array  input  stream  decoder    decoder  factory  get  binary  decoder  input  stream  null    override  public    row  deserialize  byte  message  throws    i  o  exception  try  input  stream  set  buffer  message  record  datum  reader  read  record  decoder  return  convert  avro  record  to  row  schema  type  info  record  catch    exception  e  throw  new    i  o  exception    failed  to  deserialize    avro  record  e    override  public    type  information    row  get  produced  type  return  type  info    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  final    avro  row  deserialization  schema  that    avro  row  deserialization  schema  o  return    objects  equals  record  clazz  that  record  clazz    objects  equals  schema  string  that  schema  string    override  public  int  hash  code  return    objects  hash  record  clazz  schema  string  private    row  convert  avro  record  to  row    schema  schema    row  type  info  type  info    indexed  record  record  final    list    schema    field  fields  schema  get  fields  final    type  information  field  info  type  info  get  field  types  final  int  length  fields  size  final    row  row  new    row  length  for  int  i    i  length  i  final    schema    field  field  fields  get  i  row  set  field  i  convert  avro  type  field  schema  field  info  i  record  get  i  return  row  private    object  convert  avro  type    schema  schema    type  information  info    object  object  we  perform  the  conversion  based  on  schema  information  but  enriched  with  pre  computed  type  information  where  useful  i  e  for  arrays  if  object  null  return  null  switch  schema  get  type  case  record  if  object  instanceof    indexed  record  return  convert  avro  record  to  row  schema    row  type  info  info    indexed  record  object  throw  new    illegal  state  exception    indexed  record  expected  but  was  object  get  class  case  enum  case  string  return  object  to  string  case  array  if  info  instanceof    basic  array  type  info  final    type  information  element  info    basic  array  type  info  info  get  component  info  return  convert  to  object  array  schema  get  element  type  element  info  object  else  final    type  information  element  info    object  array  type  info  info  get  component  info  return  convert  to  object  array  schema  get  element  type  element  info  object  case  map  final    map  type  info  map  type  info    map  type  info  info  final    map    string    object  converted  map  new    hash  map  final    map  map    map  object  for    map    entry  entry  map  entry  set  converted  map  put  entry  get  key  to  string  convert  avro  type  schema  get  value  type  map  type  info  get  value  type  info  entry  get  value  return  converted  map  case  union  final    list    schema  types  schema  get  types  final  int  size  types  size  final    schema  actual  schema  if  size    types  get    get  type    schema    type  null  return  convert  avro  type  types  get    info  object  else  if  size    types  get    get  type    schema    type  null  return  convert  avro  type  types  get    info  object  else  if  size    return  convert  avro  type  types  get    info  object  else  generic  type  return  object  case  fixed  final  byte  fixed  bytes    generic  fixed  object  bytes  if  info    types  big  dec  return  convert  to  decimal  schema  fixed  bytes  return  fixed  bytes  case  bytes  final    byte  buffer  byte  buffer    byte  buffer  object  final  byte  bytes  new  byte  byte  buffer  remaining  byte  buffer  get  bytes  if  info    types  big  dec  return  convert  to  decimal  schema  bytes  return  bytes  case  int  if  info    types  sql  date  return  convert  to  date  object  else  if  info    types  sql  time  return  convert  to  time  object  return  object  case  long  if  info    types  sql  timestamp  return  convert  to  timestamp  object  return  object  case  float  case  double  case  boolean  return  object  throw  new    runtime  exception    unsupported    avro  type  schema  private    big  decimal  convert  to  decimal    schema  schema  byte  bytes  final    logical  types    decimal  decimal  type    logical  types    decimal  schema  get  logical  type  return  new    big  decimal  new    big  integer  bytes  decimal  type  get  scale  private    date  convert  to  date    object  object  final  long  millis  if  object  instanceof    integer  final    integer  value    integer  object  adopted  from    apache    calcite  final  long  t  long  value    l  millis  t  long  local  tz  get  offset  t  else  use  provided    joda  time  final    local  date  value    local  date  object  millis  value  to  date  get  time  return  new    date  millis  private    time  convert  to  time    object  object  final  long  millis  if  object  instanceof    integer  millis    integer  object  else  use  provided    joda  time  final    local  time  value    local  time  object  millis  long  value  get    date  time  field  type  millis  of  day  return  new    time  millis  local  tz  get  offset  millis  private    timestamp  convert  to  timestamp    object  object  final  long  millis  if  object  instanceof    long  millis    long  object  else  use  provided    joda  time  final    date  time  value    date  time  object  millis  value  to  date  get  time  return  new    timestamp  millis  local  tz  get  offset  millis  private    object  convert  to  object  array    schema  element  schema    type  information  element  info    object  object  final    list  list    list  object  final    object  converted  array    object    array  new  instance  element  info  get  type  class  list  size  for  int  i    i  list  size  i  converted  array  i  convert  avro  type  element  schema  element  info  list  get  i  return  converted  array  private  void  write  object    object  output  stream  output  stream  throws    i  o  exception  output  stream  write  object  record  clazz  output  stream  write  u  t  f  schema  string    suppress  warnings  unchecked  private  void  read  object    object  input  stream  input  stream  throws    class  not  found  exception    i  o  exception  record  clazz    class  extends    specific  record  input  stream  read  object  schema  string  input  stream  read  u  t  f  type  info    row  type  info    avro  schema  converter    row  convert  to  type  info  schema  string  schema  new    schema    parser  parse  schema  string  if  record  clazz  null  record    specific  record    specific  data  new  instance  record  clazz  schema  else  record  new    generic  data    record  schema  datum  reader  new    specific  datum  reader  schema  this  input  stream  new    mutable  byte  array  input  stream  decoder    decoder  factory  get  binary  decoder  this  input  stream  null  
public  evolving  public  class    avro  extends    format  descriptor  private    class  extends    specific  record  record  class  private    string  avro  schema    format  descriptor  for    apache    avro  records  public    avro  super    avro  validator  format  type  value      sets  the  class  of  the    avro  specific  record  param  record  class  class  of  the    avro  record  public    avro  record  class    class  extends    specific  record  record  class    preconditions  check  not  null  record  class  this  record  class  record  class  return  this    sets  the    avro  schema  for  specific  or  generic    avro  records  param  avro  schema    avro  schema  string  public    avro  avro  schema    string  avro  schema    preconditions  check  not  null  avro  schema  this  avro  schema  avro  schema  return  this    override  protected    map    string    string  to  format  properties  final    descriptor  properties  properties  new    descriptor  properties  if  null  record  class  properties  put  class    avro  validator  format  record  class  record  class  if  null  avro  schema  properties  put  string    avro  validator  format  avro  schema  avro  schema  return  properties  as  map  
public  evolving  public  class    compress  writer  factory  in  implements    bulk  writer    factory  in  private  final    extractor  in  extractor  private  final    map    string    string  hadoop  config  map  private  transient    compression  codec  hadoop  codec  private    string  hadoop  codec  name  private    string  codec  extension    creates  a  new    compress  writer  factory  using  the  given  link    extractor  to  assemble  either  link    hadoop  compression  bulk  writer  or  link    no  compression  bulk  writer  based  on  whether  a    hadoop    compression  codec  name  is  specified  param  extractor    extractor  to  extract  the  element  public    compress  writer  factory    extractor  in  extractor  this  extractor  check  not  null  extractor    extractor  cannot  be  null  this  hadoop  config  map  new    hash  map    compresses  the  data  using  the  provided    hadoop  link    compression  codec  param  codec  name    simple  complete  name  or  alias  of  the    compression  codec  return  the  instance  of    compression  writer  factory  throws    i  o  exception  public    compress  writer  factory  in  with  hadoop  compression    string  codec  name  throws    i  o  exception  return  with  hadoop  compression  codec  name  new    configuration    compresses  the  data  using  the  provided    hadoop  link    compression  codec  and  link    configuration  param  codec  name    simple  complete  name  or  alias  of  the    compression  codec  param  hadoop  config    hadoop    configuration  return  the  instance  of    compression  writer  factory  throws    i  o  exception  public    compress  writer  factory  in  with  hadoop  compression    string  codec  name    configuration  hadoop  config  throws    i  o  exception  this  codec  extension  get  hadoop  codec  extension  codec  name  hadoop  config  this  hadoop  codec  name  codec  name  for    map    entry    string    string  entry  hadoop  config  hadoop  config  map  put  entry  get  key  entry  get  value  return  this    override  public    bulk  writer  in  create    f  s  data  output  stream  out  throws    i  o  exception  if  hadoop  codec  name  null  hadoop  codec  name  trim  is  empty  return  new    no  compression  bulk  writer  out  extractor  initialize  compression  codec  return  new    hadoop  compression  bulk  writer  hadoop  codec  create  output  stream  out  extractor  public    string  get  extension  return  hadoop  codec  name  null  this  codec  extension  private  void  initialize  compression  codec  if  hadoop  codec  null    configuration  conf  new    configuration  for    map    entry    string    string  entry  hadoop  config  map  entry  set  conf  set  entry  get  key  entry  get  value  hadoop  codec  new    compression  codec  factory  conf  get  codec  by  name  this  hadoop  codec  name  private    string  get  hadoop  codec  extension    string  hadoop  codec  name    configuration  conf  throws    i  o  exception    compression  codec  codec  new    compression  codec  factory  conf  get  codec  by  name  hadoop  codec  name  if  codec  null  throw  new    i  o  exception    unable  to  load  the  provided    hadoop  codec  hadoop  codec  name  return  codec  get  default  extension  
public  evolving  public  final  class    csv  row  data  serialization  schema  implements    serialization  schema    row  data  private  static  final  long  serial  version  u  i  d  1  l    logical  row  type  describing  the  input  csv  data  private  final    row  type  row  type    runtime  instance  that  performs  the  actual  work  private  final    serialization  runtime  converter  runtime  converter    csv  mapper  used  to  write  link    json  node  into  bytes  private  final    csv  mapper  csv  mapper    schema  describing  the  input  csv  data  private  final    csv  schema  csv  schema    object  writer  used  to  write  rows    it  is  configured  by  link    csv  schema  private  final    object  writer  object  writer    reusable  object  node  private  transient    object  node  root  private    csv  row  data  serialization  schema    row  type  row  type    csv  schema  csv  schema  this  row  type  row  type  this  runtime  converter  create  row  converter  row  type  this  csv  mapper  new    csv  mapper  this  csv  schema  csv  schema  this  object  writer  csv  mapper  writer  csv  schema  a  builder  for  creating  a  link    csv  row  data  serialization  schema    public  evolving  public  static  class    builder  private  final    row  type  row  type  private    csv  schema  csv  schema    creates  a  link    csv  row  data  serialization  schema  expecting  the  given  link    row  type  param  row  type  logical  row  type  used  to  create  schema  public    builder    row  type  row  type    preconditions  check  not  null  row  type    row  type  must  not  be  null  this  row  type  row  type  this  csv  schema    csv  row  schema  converter  convert  row  type  public    builder  set  field  delimiter  char  c  this  csv  schema  this  csv  schema  rebuild  set  column  separator  c  build  return  this  public    builder  set  line  delimiter    string  delimiter    preconditions  check  not  null  delimiter    delimiter  must  not  be  null  if  delimiter  equals  n  delimiter  equals  r  delimiter  equals  r  n  delimiter  equals  throw  new    illegal  argument  exception    unsupported  new  line  delimiter    only  n  r  r  n  or  empty  string  are  supported  this  csv  schema  this  csv  schema  rebuild  set  line  separator  delimiter  build  return  this  public    builder  set  array  element  delimiter    string  delimiter    preconditions  check  not  null  delimiter    delimiter  must  not  be  null  this  csv  schema  this  csv  schema  rebuild  set  array  element  separator  delimiter  build  return  this  public    builder  disable  quote  character  this  csv  schema  this  csv  schema  rebuild  disable  quote  char  build  return  this  public    builder  set  quote  character  char  c  this  csv  schema  this  csv  schema  rebuild  set  quote  char  c  build  return  this  public    builder  set  escape  character  char  c  this  csv  schema  this  csv  schema  rebuild  set  escape  char  c  build  return  this  public    builder  set  null  literal    string  s  this  csv  schema  this  csv  schema  rebuild  set  null  value  s  build  return  this  public    csv  row  data  serialization  schema  build  return  new    csv  row  data  serialization  schema  row  type  csv  schema    override  public  byte  serialize    row  data  row  if  root  null  root  csv  mapper  create  object  node  try  runtime  converter  convert  csv  mapper  root  row  return  object  writer  write  value  as  bytes  root  catch    throwable  t  throw  new    runtime  exception    could  not  serialize  row  row  t    override  public  boolean  equals    object  o  if  o  null  o  get  class  this  get  class  return  false  if  this  o  return  true  final    csv  row  data  serialization  schema  that    csv  row  data  serialization  schema  o  final    csv  schema  other  schema  that  csv  schema  return  row  type  equals  that  row  type  csv  schema  get  column  separator  other  schema  get  column  separator    arrays  equals  csv  schema  get  line  separator  other  schema  get  line  separator  csv  schema  get  array  element  separator  equals  other  schema  get  array  element  separator  csv  schema  get  quote  char  other  schema  get  quote  char  csv  schema  get  escape  char  other  schema  get  escape  char    arrays  equals  csv  schema  get  null  value  other  schema  get  null  value    override  public  int  hash  code  return    objects  hash  row  type  csv  schema  get  column  separator  csv  schema  get  line  separator  csv  schema  get  array  element  separator  csv  schema  get  quote  char  csv  schema  get  escape  char  csv  schema  get  null  value    runtime    converters    runtime  converter  that  converts  objects  of    flink    table  sql  internal  data  structures  to  corresponding  link    json  node  s  private  interface    serialization  runtime  converter  extends    serializable    json  node  convert    csv  mapper  csv  mapper    container  node  container    row  data  row  private  interface    row  field  converter  extends    serializable    json  node  convert    csv  mapper  csv  mapper    container  node  container    row  data  row  int  pos  private  interface    array  element  converter  extends    serializable    json  node  convert    csv  mapper  csv  mapper    container  node  container    array  data  array  int  pos  private    serialization  runtime  converter  create  row  converter    row  type  type    logical  type  field  types  type  get  fields  stream  map    row  type    row  field  get  type  to  array    logical  type  new  final    string  field  names  type  get  field  names  to  array  new    string    final    row  field  converter  field  converters    arrays  stream  field  types  map  this  create  nullable  row  field  converter  to  array    row  field  converter  new  final  int  row  arity  type  get  field  count  return  csv  mapper  container  row  top  level  reuses  the  object  node  container  final    object  node  object  node    object  node  container  for  int  i    i  row  arity  i  object  node  set  field  names  i  field  converters  i  convert  csv  mapper  container  row  i  return  object  node  private    row  field  converter  create  nullable  row  field  converter    logical  type  field  type  final    row  field  converter  field  converter  create  row  field  converter  field  type  return  csv  mapper  container  row  pos  if  row  is  null  at  pos  return  container  null  node  return  field  converter  convert  csv  mapper  container  row  pos  private    row  field  converter  create  row  field  converter    logical  type  field  type  switch  field  type  get  type  root  case  null  return  csv  mapper  container  row  pos  container  null  node  case  boolean  return  csv  mapper  container  row  pos  container  boolean  node  row  get  boolean  pos  case  tinyint  return  csv  mapper  container  row  pos  container  number  node  row  get  byte  pos  case  smallint  return  csv  mapper  container  row  pos  container  number  node  row  get  short  pos  case  integer  case  interval  year  month  return  csv  mapper  container  row  pos  container  number  node  row  get  int  pos  case  bigint  case  interval  day  time  return  csv  mapper  container  row  pos  container  number  node  row  get  long  pos  case  float  return  csv  mapper  container  row  pos  container  number  node  row  get  float  pos  case  double  return  csv  mapper  container  row  pos  container  number  node  row  get  double  pos  case  char  case  varchar  return  csv  mapper  container  row  pos  container  text  node  row  get  string  pos  to  string  case  binary  case  varbinary  return  csv  mapper  container  row  pos  container  binary  node  row  get  binary  pos  case  date  return  csv  mapper  container  row  pos  convert  date  row  get  int  pos  container  case  time  without  time  zone  return  csv  mapper  container  row  pos  convert  time  row  get  int  pos  container  case  timestamp  with  time  zone  final  int  zoned  timestamp  precision    local  zoned  timestamp  type  field  type  get  precision  return  csv  mapper  container  row  pos  convert  timestamp  row  get  timestamp  pos  zoned  timestamp  precision  container  case  timestamp  without  time  zone  final  int  timestamp  precision    timestamp  type  field  type  get  precision  return  csv  mapper  container  row  pos  convert  timestamp  row  get  timestamp  pos  timestamp  precision  container  case  decimal  return  create  decimal  row  field  converter    decimal  type  field  type  case  array  return  create  array  row  field  converter    array  type  field  type  case  row  return  create  row  row  field  converter    row  type  field  type  case  map  case  multiset  case  raw  default  throw  new    unsupported  operation  exception    unsupported  type  field  type  private    array  element  converter  create  nullable  array  element  converter    logical  type  field  type  final    array  element  converter  element  converter  create  array  element  converter  field  type  return  csv  mapper  container  array  pos  if  array  is  null  at  pos  return  container  null  node  return  element  converter  convert  csv  mapper  container  array  pos  private    array  element  converter  create  array  element  converter    logical  type  field  type  switch  field  type  get  type  root  case  null  return  csv  mapper  container  array  pos  container  null  node  case  boolean  return  csv  mapper  container  array  pos  container  boolean  node  array  get  boolean  pos  case  tinyint  return  csv  mapper  container  array  pos  container  number  node  array  get  byte  pos  case  smallint  return  csv  mapper  container  array  pos  container  number  node  array  get  short  pos  case  integer  case  interval  year  month  return  csv  mapper  container  array  pos  container  number  node  array  get  int  pos  case  bigint  case  interval  day  time  return  csv  mapper  container  array  pos  container  number  node  array  get  long  pos  case  float  return  csv  mapper  container  array  pos  container  number  node  array  get  float  pos  case  double  return  csv  mapper  container  array  pos  container  number  node  array  get  double  pos  case  char  case  varchar  return  csv  mapper  container  array  pos  container  text  node  array  get  string  pos  to  string  case  binary  case  varbinary  return  csv  mapper  container  array  pos  container  binary  node  array  get  binary  pos  case  date  return  csv  mapper  container  array  pos  convert  date  array  get  int  pos  container  case  time  without  time  zone  return  csv  mapper  container  array  pos  convert  time  array  get  int  pos  container  case  timestamp  with  time  zone  final  int  zoned  timestamp  precision    local  zoned  timestamp  type  field  type  get  precision  return  csv  mapper  container  array  pos  convert  timestamp  array  get  timestamp  pos  zoned  timestamp  precision  container  case  timestamp  without  time  zone  final  int  timestamp  precision    timestamp  type  field  type  get  precision  return  csv  mapper  container  array  pos  convert  timestamp  array  get  timestamp  pos  timestamp  precision  container  case  decimal  return  create  decimal  array  element  converter    decimal  type  field  type  we  don  t  support  array  and  row  in  an  array  see    csv  row  schema  converter  validate  nested  field  case  array  case  row  case  map  case  multiset  case  raw  default  throw  new    unsupported  operation  exception    unsupported  type  field  type    field    element    converters  private    row  field  converter  create  decimal  row  field  converter    decimal  type  decimal  type  final  int  precision  decimal  type  get  precision  final  int  scale  decimal  type  get  scale  return  csv  mapper  container  row  pos    decimal  data  decimal  row  get  decimal  pos  precision  scale  return  convert  decimal  decimal  container  private    array  element  converter  create  decimal  array  element  converter    decimal  type  decimal  type  final  int  precision  decimal  type  get  precision  final  int  scale  decimal  type  get  scale  return  csv  mapper  container  array  pos    decimal  data  decimal  array  get  decimal  pos  precision  scale  return  convert  decimal  decimal  container  private  static    json  node  convert  decimal    decimal  data  decimal    container  node  container  return  container  number  node  decimal  to  big  decimal  private  static    json  node  convert  date  int  days    container  node  container    local  date  date    local  date  of  epoch  day  days  return  container  text  node  iso  local  date  format  date  private  static    json  node  convert  time  int  millisecond    container  node  container    local  time  time    local  time  of  second  of  day  millisecond    l  return  container  text  node  iso  local  time  format  time  private  static    json  node  convert  timestamp    timestamp  data  timestamp    container  node  container  return  container  text  node  date  time  formatter  format  timestamp  to  local  date  time  private    row  field  converter  create  array  row  field  converter    array  type  type    logical  type  element  type  type  get  element  type  final    array  element  converter  element  converter  create  nullable  array  element  converter  element  type  return  csv  mapper  container  row  pos    array  node  array  node  csv  mapper  create  array  node    array  data  array  data  row  get  array  pos  int  num  elements  array  data  size  for  int  i    i  num  elements  i  array  node  add  element  converter  convert  csv  mapper  array  node  array  data  i  return  array  node  private    row  field  converter  create  row  row  field  converter    row  type  type    logical  type  field  types  type  get  fields  stream  map    row  type    row  field  get  type  to  array    logical  type  new  final    row  field  converter  field  converters    arrays  stream  field  types  map  this  create  nullable  row  field  converter  to  array    row  field  converter  new  final  int  row  arity  type  get  field  count  return  csv  mapper  container  row  pos  final    row  data  value  row  get  row  pos  row  arity  nested  rows  use  array  node  container  final    array  node  array  node  csv  mapper  create  array  node  for  int  i    i  row  arity  i  array  node  add  field  converters  i  convert  csv  mapper  array  node  value  i  return  array  node  private  static  final    date  time  formatter  date  time  formatter  new    date  time  formatter  builder  parse  case  insensitive  append  iso  local  date  append  literal  append  iso  local  time  to  formatter  
public  evolving  public  final  class    csv  row  deserialization  schema  implements    deserialization  schema    row  private  static  final  long  serial  version  u  i  d    l    type  information  describing  the  result  type  private  final    type  information    row  type  info    runtime  instance  that  performs  the  actual  work  private  final    runtime  converter  runtime  converter    schema  describing  the  input  csv  data  private  final    csv  schema  csv  schema    object  reader  used  to  read  rows    it  is  configured  by  link    csv  schema  private  final    object  reader  object  reader    flag  indicating  whether  to  ignore  invalid  fields  rows  default  throw  an  exception  private  final  boolean  ignore  parse  errors  private    csv  row  deserialization  schema    row  type  info  type  info    csv  schema  csv  schema  boolean  ignore  parse  errors  this  type  info  type  info  this  runtime  converter  create  row  runtime  converter  type  info  ignore  parse  errors  true  this  csv  schema  csv  schema  this  object  reader  new    csv  mapper  reader  for    json  node  class  with  csv  schema  this  ignore  parse  errors  ignore  parse  errors  a  builder  for  creating  a  link    csv  row  deserialization  schema    public  evolving  public  static  class    builder  private  final    row  type  info  type  info  private    csv  schema  csv  schema  private  boolean  ignore  parse  errors    creates  a  csv  deserialization  schema  for  the  given  link    type  information  with  optional  parameters  public    builder    type  information    row  type  info    preconditions  check  not  null  type  info    type  information  must  not  be  null  if  type  info  instanceof    row  type  info  throw  new    illegal  argument  exception    row  type  information  expected  this  type  info    row  type  info  type  info  this  csv  schema    csv  row  schema  converter  convert    row  type  info  type  info  public    builder  set  field  delimiter  char  delimiter  this  csv  schema  this  csv  schema  rebuild  set  column  separator  delimiter  build  return  this  public    builder  set  allow  comments  boolean  allow  comments  this  csv  schema  this  csv  schema  rebuild  set  allow  comments  allow  comments  build  return  this  public    builder  set  array  element  delimiter    string  delimiter    preconditions  check  not  null  delimiter    array  element  delimiter  must  not  be  null  this  csv  schema  this  csv  schema  rebuild  set  array  element  separator  delimiter  build  return  this  public    builder  set  quote  character  char  c  this  csv  schema  this  csv  schema  rebuild  set  quote  char  c  build  return  this  public    builder  set  escape  character  char  c  this  csv  schema  this  csv  schema  rebuild  set  escape  char  c  build  return  this  public    builder  set  null  literal    string  null  literal    preconditions  check  not  null  null  literal    null  literal  must  not  be  null  this  csv  schema  this  csv  schema  rebuild  set  null  value  null  literal  build  return  this  public    builder  set  ignore  parse  errors  boolean  ignore  parse  errors  this  ignore  parse  errors  ignore  parse  errors  return  this  public    csv  row  deserialization  schema  build  return  new    csv  row  deserialization  schema  type  info  csv  schema  ignore  parse  errors    override  public    row  deserialize  byte  message  throws    i  o  exception  try  final    json  node  root  object  reader  read  value  message  return    row  runtime  converter  convert  root  catch    throwable  t  if  ignore  parse  errors  return  null  throw  new    i  o  exception    failed  to  deserialize  csv  row  new    string  message  t    override  public  boolean  is  end  of  stream    row  next  element  return  false    override  public    type  information    row  get  produced  type  return  type  info    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  o  get  class  this  get  class  return  false  final    csv  row  deserialization  schema  that    csv  row  deserialization  schema  o  final    csv  schema  other  schema  that  csv  schema  return  type  info  equals  that  type  info  ignore  parse  errors  that  ignore  parse  errors  csv  schema  get  column  separator  other  schema  get  column  separator  csv  schema  allows  comments  other  schema  allows  comments  csv  schema  get  array  element  separator  equals  other  schema  get  array  element  separator  csv  schema  get  quote  char  other  schema  get  quote  char  csv  schema  get  escape  char  other  schema  get  escape  char    arrays  equals  csv  schema  get  null  value  other  schema  get  null  value    override  public  int  hash  code  return    objects  hash  type  info  ignore  parse  errors  csv  schema  get  column  separator  csv  schema  allows  comments  csv  schema  get  array  element  separator  csv  schema  get  quote  char  csv  schema  get  escape  char  csv  schema  get  null  value  interface    runtime  converter  extends    serializable    object  convert    json  node  node  private  static    runtime  converter  create  row  runtime  converter    row  type  info  row  type  info  boolean  ignore  parse  errors  boolean  is  top  level  final    type  information  field  types  row  type  info  get  field  types  final    string  field  names  row  type  info  get  field  names  final    runtime  converter  field  converters  create  field  runtime  converters  ignore  parse  errors  field  types  return  assemble  row  runtime  converter  ignore  parse  errors  is  top  level  field  names  field  converters  static    runtime  converter  create  field  runtime  converters  boolean  ignore  parse  errors    type  information  field  types  final    runtime  converter  field  converters  new    runtime  converter  field  types  length  for  int  i    i  field  types  length  i  field  converters  i  create  nullable  runtime  converter  field  types  i  ignore  parse  errors  return  field  converters  private  static    runtime  converter  assemble  row  runtime  converter  boolean  ignore  parse  errors  boolean  is  top  level    string  field  names    runtime  converter  field  converters  final  int  row  arity  field  names  length  return  node  final  int  node  size  node  size  validate  arity  row  arity  node  size  ignore  parse  errors  final    row  row  new    row  row  arity  for  int  i    i    math  min  row  arity  node  size  i    jackson  only  supports  mapping  by  name  in  the  first  level  if  is  top  level  row  set  field  i  field  converters  i  convert  node  get  field  names  i  else  row  set  field  i  field  converters  i  convert  node  get  i  return  row  private  static    runtime  converter  create  nullable  runtime  converter    type  information  info  boolean  ignore  parse  errors  final    runtime  converter  value  converter  create  runtime  converter  info  ignore  parse  errors  return  node  if  node  is  null  return  null  try  return  value  converter  convert  node  catch    throwable  t  if  ignore  parse  errors  throw  t  return  null  private  static    runtime  converter  create  runtime  converter    type  information  info  boolean  ignore  parse  errors  if  info  equals    types  void  return  node  null  else  if  info  equals    types  string  return    json  node  as  text  else  if  info  equals    types  boolean  return  node    boolean  value  of  node  as  text  trim  else  if  info  equals    types  byte  return  node    byte  value  of  node  as  text  trim  else  if  info  equals    types  short  return  node    short  value  of  node  as  text  trim  else  if  info  equals    types  int  return  node    integer  value  of  node  as  text  trim  else  if  info  equals    types  long  return  node    long  value  of  node  as  text  trim  else  if  info  equals    types  float  return  node    float  value  of  node  as  text  trim  else  if  info  equals    types  double  return  node    double  value  of  node  as  text  trim  else  if  info  equals    types  big  dec  return  node  new    big  decimal  node  as  text  trim  else  if  info  equals    types  big  int  return  node  new    big  integer  node  as  text  trim  else  if  info  equals    types  sql  date  return  node    date  value  of  node  as  text  else  if  info  equals    types  sql  time  return  node    time  value  of  node  as  text  else  if  info  equals    types  sql  timestamp  return  node    timestamp  value  of  node  as  text  else  if  info  equals    types  local  date  return  node    date  value  of  node  as  text  to  local  date  else  if  info  equals    types  local  time  return  node    time  value  of  node  as  text  to  local  time  else  if  info  equals    types  local  date  time  return  node    timestamp  value  of  node  as  text  to  local  date  time  else  if  info  instanceof    row  type  info  final    row  type  info  row  type  info    row  type  info  info  return  create  row  runtime  converter  row  type  info  ignore  parse  errors  false  else  if  info  instanceof    basic  array  type  info  return  create  object  array  runtime  converter    basic  array  type  info  info  get  component  info  ignore  parse  errors  else  if  info  instanceof    object  array  type  info  return  create  object  array  runtime  converter    object  array  type  info  info  get  component  info  ignore  parse  errors  else  if  info  instanceof    primitive  array  type  info    primitive  array  type  info  info  get  component  type    types  byte  return  create  byte  array  runtime  converter  ignore  parse  errors  else  throw  new    runtime  exception    unsupported  type  information  info  private  static    runtime  converter  create  object  array  runtime  converter    type  information  element  type  boolean  ignore  parse  errors  final    class  element  class  element  type  get  type  class  final    runtime  converter  element  converter  create  nullable  runtime  converter  element  type  ignore  parse  errors  return  node  final  int  node  size  node  size  final    object  array    object    array  new  instance  element  class  node  size  for  int  i    i  node  size  i  array  i  element  converter  convert  node  get  i  return  array  private  static    runtime  converter  create  byte  array  runtime  converter  boolean  ignore  parse  errors  return  node  try  return  node  binary  value  catch    i  o  exception  e  if  ignore  parse  errors  throw  new    runtime  exception    unable  to  deserialize  byte  array  e  return  null  static  void  validate  arity  int  expected  int  actual  boolean  ignore  parse  errors  if  expected  actual  ignore  parse  errors  throw  new    runtime  exception    row  length  mismatch  expected  fields  expected  but  was  actual  
public  evolving  public  final  class    csv  row  serialization  schema  implements    serialization  schema    row  private  static  final  long  serial  version  u  i  d    l    type  information  describing  the  input  csv  data  private  final    row  type  info  type  info    runtime  instance  that  performs  the  actual  work  private  final    runtime  converter  runtime  converter    csv  mapper  used  to  write  link    json  node  into  bytes  private  final    csv  mapper  csv  mapper    schema  describing  the  input  csv  data  private  final    csv  schema  csv  schema    object  writer  used  to  write  rows    it  is  configured  by  link    csv  schema  private  final    object  writer  object  writer    reusable  object  node  private  transient    object  node  root  private    csv  row  serialization  schema    row  type  info  type  info    csv  schema  csv  schema  this  type  info  type  info  this  runtime  converter  create  row  runtime  converter  type  info  true  this  csv  mapper  new    csv  mapper  this  csv  schema  csv  schema  this  object  writer  csv  mapper  writer  csv  schema  a  builder  for  creating  a  link    csv  row  serialization  schema    public  evolving  public  static  class    builder  private  final    row  type  info  type  info  private    csv  schema  csv  schema    creates  a  link    csv  row  serialization  schema  expecting  the  given  link    type  information  param  type  info  type  information  used  to  create  schema  public    builder    type  information    row  type  info    preconditions  check  not  null  type  info    type  information  must  not  be  null  if  type  info  instanceof    row  type  info  throw  new    illegal  argument  exception    row  type  information  expected  this  type  info    row  type  info  type  info  this  csv  schema    csv  row  schema  converter  convert    row  type  info  type  info  public    builder  set  field  delimiter  char  c  this  csv  schema  this  csv  schema  rebuild  set  column  separator  c  build  return  this  public    builder  set  line  delimiter    string  delimiter    preconditions  check  not  null  delimiter    delimiter  must  not  be  null  if  delimiter  equals  n  delimiter  equals  r  delimiter  equals  r  n  delimiter  equals  throw  new    illegal  argument  exception    unsupported  new  line  delimiter    only  n  r  r  n  or  empty  string  are  supported  this  csv  schema  this  csv  schema  rebuild  set  line  separator  delimiter  build  return  this  public    builder  set  array  element  delimiter    string  delimiter    preconditions  check  not  null  delimiter    delimiter  must  not  be  null  this  csv  schema  this  csv  schema  rebuild  set  array  element  separator  delimiter  build  return  this  public    builder  disable  quote  character  this  csv  schema  this  csv  schema  rebuild  disable  quote  char  build  return  this  public    builder  set  quote  character  char  c  this  csv  schema  this  csv  schema  rebuild  set  quote  char  c  build  return  this  public    builder  set  escape  character  char  c  this  csv  schema  this  csv  schema  rebuild  set  escape  char  c  build  return  this  public    builder  set  null  literal    string  s  this  csv  schema  this  csv  schema  rebuild  set  null  value  s  build  return  this  public    csv  row  serialization  schema  build  return  new    csv  row  serialization  schema  type  info  csv  schema    override  public  byte  serialize    row  row  if  root  null  root  csv  mapper  create  object  node  try  runtime  converter  convert  csv  mapper  root  row  return  object  writer  write  value  as  bytes  root  catch    throwable  t  throw  new    runtime  exception    could  not  serialize  row  row  t    override  public  boolean  equals    object  o  if  o  null  o  get  class  this  get  class  return  false  if  this  o  return  true  final    csv  row  serialization  schema  that    csv  row  serialization  schema  o  final    csv  schema  other  schema  that  csv  schema  return  type  info  equals  that  type  info  csv  schema  get  column  separator  other  schema  get  column  separator    arrays  equals  csv  schema  get  line  separator  other  schema  get  line  separator  csv  schema  get  array  element  separator  equals  other  schema  get  array  element  separator  csv  schema  get  quote  char  other  schema  get  quote  char  csv  schema  get  escape  char  other  schema  get  escape  char    arrays  equals  csv  schema  get  null  value  other  schema  get  null  value    override  public  int  hash  code  return    objects  hash  type  info  csv  schema  get  column  separator  csv  schema  get  line  separator  csv  schema  get  array  element  separator  csv  schema  get  quote  char  csv  schema  get  escape  char  csv  schema  get  null  value  private  static  final    date  time  formatter  date  time  formatter  new    date  time  formatter  builder  parse  case  insensitive  append  iso  local  date  append  literal  append  iso  local  time  to  formatter  private  interface    runtime  converter  extends    serializable    json  node  convert    csv  mapper  csv  mapper    container  node  container    object  obj  private  static    runtime  converter  create  row  runtime  converter    row  type  info  row  type  info  boolean  is  top  level  final    type  information  field  types  row  type  info  get  field  types  final    string  field  names  row  type  info  get  field  names  final    runtime  converter  field  converters  create  field  runtime  converters  field  types  return  assemble  row  runtime  converter  is  top  level  field  names  field  converters  private  static    runtime  converter  create  field  runtime  converters    type  information  field  types  final    runtime  converter  field  converters  new    runtime  converter  field  types  length  for  int  i    i  field  types  length  i  field  converters  i  create  nullable  runtime  converter  field  types  i  return  field  converters  private  static    runtime  converter  assemble  row  runtime  converter  boolean  is  top  level    string  field  names    runtime  converter  field  converters  final  int  row  arity  field  names  length  top  level  reuses  the  object  node  container  if  is  top  level  return  csv  mapper  container  obj  final    row  row    row  obj  validate  arity  row  arity  row  get  arity  final    object  node  object  node    object  node  container  for  int  i    i  row  arity  i  object  node  set  field  names  i  field  converters  i  convert  csv  mapper  container  row  get  field  i  return  object  node  else  return  csv  mapper  container  obj  final    row  row    row  obj  validate  arity  row  arity  row  get  arity  final    array  node  array  node  csv  mapper  create  array  node  for  int  i    i  row  arity  i  array  node  add  field  converters  i  convert  csv  mapper  array  node  row  get  field  i  return  array  node  private  static    runtime  converter  create  nullable  runtime  converter    type  information  info  final    runtime  converter  value  converter  create  runtime  converter  info  return  csv  mapper  container  obj  if  obj  null  return  container  null  node  return  value  converter  convert  csv  mapper  container  obj  private  static    runtime  converter  create  runtime  converter    type  information  info  if  info  equals    types  void  return  csv  mapper  container  obj  container  null  node  else  if  info  equals    types  string  return  csv  mapper  container  obj  container  text  node    string  obj  else  if  info  equals    types  boolean  return  csv  mapper  container  obj  container  boolean  node    boolean  obj  else  if  info  equals    types  byte  return  csv  mapper  container  obj  container  number  node    byte  obj  else  if  info  equals    types  short  return  csv  mapper  container  obj  container  number  node    short  obj  else  if  info  equals    types  int  return  csv  mapper  container  obj  container  number  node    integer  obj  else  if  info  equals    types  long  return  csv  mapper  container  obj  container  number  node    long  obj  else  if  info  equals    types  float  return  csv  mapper  container  obj  container  number  node    float  obj  else  if  info  equals    types  double  return  csv  mapper  container  obj  container  number  node    double  obj  else  if  info  equals    types  big  dec  return  csv  mapper  container  obj  container  number  node    big  decimal  obj  else  if  info  equals    types  big  int  return  csv  mapper  container  obj  container  number  node    big  integer  obj  else  if  info  equals    types  sql  date  return  csv  mapper  container  obj  container  text  node  obj  to  string  else  if  info  equals    types  sql  time  return  csv  mapper  container  obj  container  text  node  obj  to  string  else  if  info  equals    types  sql  timestamp  return  csv  mapper  container  obj  container  text  node  obj  to  string  else  if  info  equals    types  local  date  return  csv  mapper  container  obj  container  text  node  obj  to  string  else  if  info  equals    types  local  time  return  csv  mapper  container  obj  container  text  node  obj  to  string  else  if  info  equals    types  local  date  time  return  csv  mapper  container  obj  container  text  node  date  time  formatter  format    local  date  time  obj  else  if  info  instanceof    row  type  info  return  create  row  runtime  converter    row  type  info  info  false  else  if  info  instanceof    basic  array  type  info  return  create  object  array  runtime  converter    basic  array  type  info  info  get  component  info  else  if  info  instanceof    object  array  type  info  return  create  object  array  runtime  converter    object  array  type  info  info  get  component  info  else  if  info  instanceof    primitive  array  type  info    primitive  array  type  info  info  get  component  type    types  byte  return  create  byte  array  runtime  converter  else  throw  new    runtime  exception    unsupported  type  information  info  private  static    runtime  converter  create  object  array  runtime  converter    type  information  element  type  final    runtime  converter  element  converter  create  nullable  runtime  converter  element  type  return  csv  mapper  container  obj  final    object  array    object  obj  final    array  node  array  node  csv  mapper  create  array  node  for    object  element  array  array  node  add  element  converter  convert  csv  mapper  array  node  element  return  array  node  private  static    runtime  converter  create  byte  array  runtime  converter  return  csv  mapper  container  obj  container  binary  node  byte  obj  private  static  void  validate  arity  int  expected  int  actual  if  expected  actual  throw  new    runtime  exception    row  length  mismatch  expected  fields  expected  but  was  actual  
public  evolving  public  class    csv  extends    format  descriptor  private    descriptor  properties  internal  properties  new    descriptor  properties  true    format  descriptor  for  comma  separated  values  csv  p    this  descriptor  aims  to  comply  with  rfc      common    format  and  mime    type  for    comma    separated    values  csv    files  proposed  by  the    internet    engineering    task    force  ietf  public    csv  super  format  type  value      sets  the  field  delimiter  character  by  default  param  delimiter  the  field  delimiter  character  public    csv  field  delimiter  char  delimiter  internal  properties  put  character  format  field  delimiter  delimiter  return  this    sets  the  line  delimiter  n  by  default  otherwise  r  r  n  or  are  allowed  param  delimiter  the  line  delimiter  public    csv  line  delimiter    string  delimiter    preconditions  check  not  null  delimiter  internal  properties  put  string  format  line  delimiter  delimiter  return  this    disable  the  quote  character  for  enclosing  field  values  public    csv  disable  quote  character  internal  properties  put  boolean  format  disable  quote  character  true  return  this    sets  the  quote  character  for  enclosing  field  values  by  default  param  quote  character  the  quote  character  public    csv  quote  character  char  quote  character  internal  properties  put  character  format  quote  character  quote  character  return  this    ignores  comment  lines  that  start  with  disabled  by  default    if  enabled  make  sure  to  also  ignore  parse  errors  to  allow  empty  rows  public    csv  allow  comments  internal  properties  put  boolean  format  allow  comments  true  return  this    skip  fields  and  rows  with  parse  errors  instead  of  failing    fields  are  set  to  code  null  in  case  of  errors    by  default  an  exception  is  thrown  public    csv  ignore  parse  errors  internal  properties  put  boolean  format  ignore  parse  errors  true  return  this    sets  the  array  element  delimiter  string  for  separating  array  or  row  element  values  by  default  param  delimiter  the  array  element  delimiter  public    csv  array  element  delimiter    string  delimiter    preconditions  check  not  null  delimiter  internal  properties  put  string  format  array  element  delimiter  delimiter  return  this    sets  the  escape  character  for  escaping  values  disabled  by  default  param  escape  character  escaping  character  e  g  backslash  public    csv  escape  character  char  escape  character  internal  properties  put  character  format  escape  character  escape  character  return  this    sets  the  null  literal  string  that  is  interpreted  as  a  null  value  disabled  by  default  param  null  literal  null  literal  e  g  null  or  n  a  public    csv  null  literal    string  null  literal    preconditions  check  not  null  null  literal  internal  properties  put  string  format  null  literal  null  literal  return  this    sets  the  format  schema  with  field  names  and  the  types    required  if  schema  is  not  derived  param  schema  type  type  information  that  describes  the  schema  deprecated  link    csv  supports  derive  schema  from  table  schema  by  default  it  is  no  longer  necessary  to  explicitly  declare  the  format  schema    this  method  will  be  removed  in  the  future    deprecated  public    csv  schema    type  information    row  schema  type    preconditions  check  not  null  schema  type  internal  properties  put  string  format  schema    type  string  utils  write  type  info  schema  type  return  this    derives  the  format  schema  from  the  table  s  schema    required  if  no  format  schema  is  defined  p    this  allows  for  defining  schema  information  only  once  p    the  names  types  and  fields  order  of  the  format  are  determined  by  the  table  s  schema    time  attributes  are  ignored  if  their  origin  is  not  a  field  a  from  definition  is  interpreted  as  a  field  renaming  in  the  format  deprecated    derivation  format  schema  from  table  s  schema  is  the  default  behavior  now    so  there  is  no  need  to  explicitly  declare  to  derive  schema    deprecated  public    csv  derive  schema  internal  properties  put  boolean  format  derive  schema  true  return  this    override  protected    map    string    string  to  format  properties  final    descriptor  properties  properties  new    descriptor  properties  properties  put  properties  internal  properties  return  properties  as  map  
public  evolving  public  class    json  node  deserialization  schema  extends    abstract  deserialization  schema    object  node  private  static  final  long  serial  version  u  i  d    l  private  final    object  mapper  mapper  new    object  mapper    override  public    object  node  deserialize  byte  message  throws    i  o  exception  return  mapper  read  value  message    object  node  class  
public  evolving  public  class    json  row  deserialization  schema  implements    deserialization  schema    row  private  static  final  long  serial  version  u  i  d    l    type  information  describing  the  result  type  private  final    row  type  info  type  info  private  boolean  fail  on  missing  field    object  mapper  for  parsing  the  json  private  final    object  mapper  object  mapper  new    object  mapper  private    deserialization  runtime  converter  runtime  converter    flag  indicating  whether  to  ignore  invalid  fields  rows  default  throw  an  exception  private  final  boolean  ignore  parse  errors  private    json  row  deserialization  schema    type  information    row  type  info  boolean  fail  on  missing  field  boolean  ignore  parse  errors  check  not  null  type  info    type  information  check  argument  type  info  instanceof    row  type  info    only    row  type  info  is  supported  if  ignore  parse  errors  fail  on  missing  field  throw  new    illegal  argument  exception  json  format  doesn  t  support  fail  on  missing  field  and  ignore  parse  errors  are  both  true  this  type  info    row  type  info  type  info  this  fail  on  missing  field  fail  on  missing  field  this  runtime  converter  create  converter  this  type  info  this  ignore  parse  errors  ignore  parse  errors  deprecated    use  the  provided  link    builder  instead    deprecated  public    json  row  deserialization  schema    type  information    row  type  info  this  type  info  false  false  deprecated    use  the  provided  link    builder  instead    deprecated  public    json  row  deserialization  schema    string  json  schema  this    json  row  schema  converter  convert  check  not  null  json  schema  false  false  deprecated    use  the  provided  link    builder  instead    deprecated  public  void  set  fail  on  missing  field  boolean  fail  on  missing  field  todo  make  this  class  immutable  once  we  drop  this  method  this  fail  on  missing  field  fail  on  missing  field  this  runtime  converter  create  converter  this  type  info    override  public    row  deserialize  byte  message  throws    i  o  exception  try  final    json  node  root  object  mapper  read  tree  message  return    row  runtime  converter  convert  object  mapper  root  catch    throwable  t  if  ignore  parse  errors  return  null  throw  new    i  o  exception  format    failed  to  deserialize  json  s  new    string  message  t    override  public  boolean  is  end  of  stream    row  next  element  return  false    override  public    type  information    row  get  produced  type  return  type  info    builder  for  link    json  row  deserialization  schema  public  static  class    builder  private  final    row  type  info  type  info  private  boolean  fail  on  missing  field  false  private  boolean  ignore  parse  errors  false    creates  a  json  deserialization  schema  for  the  given  type  information  param  type  info    type  information  describing  the  result  type    the  field  names  of  link    row  are  used  to  parse  the  json  properties  public    builder    type  information    row  type  info  check  argument  type  info  instanceof    row  type  info    only    row  type  info  is  supported  this  type  info    row  type  info  type  info    creates  a  json  deserialization  schema  for  the  given  json  schema  param  json  schema  json  schema  describing  the  result  type  see  a  href  http  json  schema  org  http  json  schema  org  a  public    builder    string  json  schema  this    json  row  schema  converter  convert  check  not  null  json  schema    configures  schema  to  fail  if  a  json  field  is  missing  p    by  default  a  missing  field  is  ignored  and  the  field  is  set  to  null  public    builder  fail  on  missing  field  this  fail  on  missing  field  true  return  this    configures  schema  to  fail  when  parsing  json  failed  p    by  default  an  exception  will  be  thrown  when  parsing  json  fails  public    builder  ignore  parse  errors  this  ignore  parse  errors  true  return  this  public    json  row  deserialization  schema  build  return  new    json  row  deserialization  schema  type  info  fail  on  missing  field  ignore  parse  errors    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  final    json  row  deserialization  schema  that    json  row  deserialization  schema  o  return    objects  equals  type  info  that  type  info    objects  equals  fail  on  missing  field  that  fail  on  missing  field    objects  equals  ignore  parse  errors  that  ignore  parse  errors    override  public  int  hash  code  return    objects  hash  type  info  fail  on  missing  field  ignore  parse  errors    runtime  converter    runtime  converter  that  maps  between  link    json  node  s  and    java  objects    functional  interface  private  interface    deserialization  runtime  converter  extends    serializable    object  convert    object  mapper  mapper    json  node  json  node  private    deserialization  runtime  converter  create  converter    type  information  type  info    deserialization  runtime  converter  base  converter  create  converter  for  simple  type  type  info  or  else  get  create  container  converter  type  info  or  else  get  create  fallback  converter  type  info  get  type  class  return  wrap  into  nullable  converter  base  converter  private    deserialization  runtime  converter  wrap  into  nullable  converter    deserialization  runtime  converter  converter  return  mapper  json  node  if  json  node  is  null  return  null  try  return  converter  convert  mapper  json  node  catch    throwable  t  if  ignore  parse  errors  throw  t  return  null  private    optional    deserialization  runtime  converter  create  container  converter    type  information  type  info  if  type  info  instanceof    row  type  info  return    optional  of  create  row  converter    row  type  info  type  info  else  if  type  info  instanceof    object  array  type  info  return    optional  of  create  object  array  converter    object  array  type  info  type  info  get  component  info  else  if  type  info  instanceof    basic  array  type  info  return    optional  of  create  object  array  converter    basic  array  type  info  type  info  get  component  info  else  if  is  primitive  byte  array  type  info  return    optional  of  create  byte  array  converter  else  if  type  info  instanceof    map  type  info    map  type  info  map  type  info    map  type  info  type  info  return    optional  of  create  map  converter  map  type  info  get  key  type  info  map  type  info  get  value  type  info  else  return    optional  empty  private    deserialization  runtime  converter  create  map  converter    type  information  key  type    type  information  value  type    deserialization  runtime  converter  value  converter  create  converter  value  type    deserialization  runtime  converter  key  converter  create  converter  key  type  return  mapper  json  node    iterator    map    entry    string    json  node  fields  json  node  fields    map    object    object  result  new    hash  map  while  fields  has  next    map    entry    string    json  node  entry  fields  next    object  key  key  converter  convert  mapper    text  node  value  of  entry  get  key    object  value  value  converter  convert  mapper  entry  get  value  result  put  key  value  return  result  private    deserialization  runtime  converter  create  byte  array  converter  return  mapper  json  node  try  return  json  node  binary  value  catch    i  o  exception  e  throw  new    json  parse  exception    unable  to  deserialize  byte  array  e  private  boolean  is  primitive  byte  array    type  information  type  info  return  type  info  instanceof    primitive  array  type  info    primitive  array  type  info  type  info  get  component  type    types  byte  private    deserialization  runtime  converter  create  object  array  converter    type  information  element  type  info    deserialization  runtime  converter  element  converter  create  converter  element  type  info  return  assemble  array  converter  element  type  info  element  converter  private    deserialization  runtime  converter  create  row  converter    row  type  info  type  info    list    deserialization  runtime  converter  field  converters    arrays  stream  type  info  get  field  types  map  this  create  converter  collect    collectors  to  list  return  assemble  row  converter  type  info  get  field  names  field  converters  private    deserialization  runtime  converter  create  fallback  converter    class  value  type  return  mapper  json  node  for  types  that  were  specified  without  json  schema  e  g    p  o  j  os  try  return  mapper  tree  to  value  json  node  value  type  catch    json  processing  exception  e  throw  new    json  parse  exception  format    could  not  convert  node  s  json  node  e  private    optional    deserialization  runtime  converter  create  converter  for  simple  type    type  information  simple  type  info  if  simple  type  info    types  void  return    optional  of  mapper  json  node  null  else  if  simple  type  info    types  boolean  return    optional  of  this  convert  to  boolean  else  if  simple  type  info    types  string  return    optional  of  this  convert  to  string  else  if  simple  type  info    types  int  return    optional  of  this  convert  to  int  else  if  simple  type  info    types  long  return    optional  of  this  convert  to  long  else  if  simple  type  info    types  double  return    optional  of  this  convert  to  double  else  if  simple  type  info    types  float  return    optional  of  mapper  json  node    float  parse  float  json  node  as  text  trim  else  if  simple  type  info    types  short  return    optional  of  mapper  json  node    short  parse  short  json  node  as  text  trim  else  if  simple  type  info    types  byte  return    optional  of  mapper  json  node    byte  parse  byte  json  node  as  text  trim  else  if  simple  type  info    types  big  dec  return    optional  of  this  convert  to  big  decimal  else  if  simple  type  info    types  big  int  return    optional  of  this  convert  to  big  integer  else  if  simple  type  info    types  sql  date  return    optional  of  this  convert  to  date  else  if  simple  type  info    types  sql  time  return    optional  of  this  convert  to  time  else  if  simple  type  info    types  sql  timestamp  return    optional  of  this  convert  to  timestamp  else  if  simple  type  info    types  local  date  return    optional  of  this  convert  to  local  date  else  if  simple  type  info    types  local  time  return    optional  of  this  convert  to  local  time  else  if  simple  type  info    types  local  date  time  return    optional  of  this  convert  to  local  date  time  else  return    optional  empty  private    string  convert  to  string    object  mapper  mapper    json  node  json  node  if  json  node  is  container  node  return  json  node  to  string  else  return  json  node  as  text  private  boolean  convert  to  boolean    object  mapper  mapper    json  node  json  node  if  json  node  is  boolean  avoid  redundant  to  string  and  parse  boolean  for  better  performance  return  json  node  as  boolean  else  return    boolean  parse  boolean  json  node  as  text  trim  private  int  convert  to  int    object  mapper  mapper    json  node  json  node  if  json  node  can  convert  to  int  avoid  redundant  to  string  and  parse  int  for  better  performance  return  json  node  as  int  else  return    integer  parse  int  json  node  as  text  trim  private  long  convert  to  long    object  mapper  mapper    json  node  json  node  if  json  node  can  convert  to  long  avoid  redundant  to  string  and  parse  long  for  better  performance  return  json  node  as  long  else  return    long  parse  long  json  node  as  text  trim  private  double  convert  to  double    object  mapper  mapper    json  node  json  node  if  json  node  is  double  avoid  redundant  to  string  and  parse  double  for  better  performance  return  json  node  as  double  else  return    double  parse  double  json  node  as  text  trim  private    big  decimal  convert  to  big  decimal    object  mapper  mapper    json  node  json  node  if  json  node  is  big  decimal  avoid  redundant  to  string  and  to  decimal  for  better  performance  return  json  node  decimal  value  else  return  new    big  decimal  json  node  as  text  trim  private    big  integer  convert  to  big  integer    object  mapper  mapper    json  node  json  node  if  json  node  is  big  integer  avoid  redundant  to  string  and  to  big  integer  for  better  performance  return  json  node  big  integer  value  else  return  new    big  integer  json  node  as  text  trim  private    local  date  convert  to  local  date    object  mapper  mapper    json  node  json  node  return  iso  local  date  parse  json  node  as  text  query    temporal  queries  local  date  private    date  convert  to  date    object  mapper  mapper    json  node  json  node  return    date  value  of  convert  to  local  date  mapper  json  node  private    local  date  time  convert  to  local  date  time    object  mapper  mapper    json  node  json  node  according  to  rfc    every  date  time  must  have  a  timezone  until  we  have  full  timezone  support  we  only  support  utc  users  can  parse  their  time  as  string  as  a  workaround    temporal  accessor  parsed  timestamp    r  f  c3339  timestamp  format  parse  json  node  as  text    zone  offset  zone  offset  parsed  timestamp  query    temporal  queries  offset  if  zone  offset  null  zone  offset  get  total  seconds    throw  new    illegal  state  exception    invalid  timestamp  format    only  a  timestamp  in  utc  timezone  is  supported  yet    format  yyyy  mm  dd  t  hh  mm  ss  sss  z    local  time  local  time  parsed  timestamp  query    temporal  queries  local  time    local  date  local  date  parsed  timestamp  query    temporal  queries  local  date  return    local  date  time  of  local  date  local  time  private    timestamp  convert  to  timestamp    object  mapper  mapper    json  node  json  node  return    timestamp  value  of  convert  to  local  date  time  mapper  json  node  private    local  time  convert  to  local  time    object  mapper  mapper    json  node  json  node  according  to  rfc    every  full  time  must  have  a  timezone  until  we  have  full  timezone  support  we  only  support  utc  users  can  parse  their  time  as  string  as  a  workaround    temporal  accessor  parsed  time    r  f  c3339  time  format  parse  json  node  as  text    zone  offset  zone  offset  parsed  time  query    temporal  queries  offset    local  time  local  time  parsed  time  query    temporal  queries  local  time  if  zone  offset  null  zone  offset  get  total  seconds    local  time  get  nano    throw  new    illegal  state  exception    invalid  time  format    only  a  time  in  utc  timezone  without  milliseconds  is  supported  yet  return  local  time  private    time  convert  to  time    object  mapper  mapper    json  node  json  node  return    time  value  of  convert  to  local  time  mapper  json  node  private    deserialization  runtime  converter  assemble  row  converter    string  field  names    list    deserialization  runtime  converter  field  converters  return  mapper  json  node    object  node  node    object  node  json  node  int  arity  field  names  length    row  row  new    row  arity  for  int  i    i  arity  i    string  field  name  field  names  i    json  node  field  node  get  field  name    object  convert  field  convert  field  mapper  field  converters  get  i  field  name  field  row  set  field  i  convert  field  return  row  private    object  convert  field    object  mapper  mapper    deserialization  runtime  converter  field  converter    string  field  name    json  node  field  if  field  null  if  fail  on  missing  field  throw  new    illegal  state  exception    could  not  find  field  with  name  field  name  else  return  null  else  return  field  converter  convert  mapper  field  private    deserialization  runtime  converter  assemble  array  converter    type  information  element  type    deserialization  runtime  converter  element  converter  final    class  element  class  element  type  get  type  class  return  mapper  json  node  final    array  node  node    array  node  json  node  final    object  array    object    array  new  instance  element  class  node  size  for  int  i    i  node  size  i  final    json  node  inner  node  node  get  i  array  i  element  converter  convert  mapper  inner  node  return  array    exception  which  refers  to  parse  errors  in  converters  private  static  final  class    json  parse  exception  extends    runtime  exception  private  static  final  long  serial  version  u  i  d  1  l  public    json  parse  exception    string  message    throwable  cause  super  message  cause  
public  evolving  public  class    json  row  serialization  schema  implements    serialization  schema    row  private  static  final  long  serial  version  u  i  d    l    type  information  describing  the  input  type  private  final    row  type  info  type  info    object  mapper  that  is  used  to  create  output  json  objects  private  final    object  mapper  mapper  new    object  mapper  private  final    serialization  runtime  converter  runtime  converter    reusable  object  node  private  transient    object  node  node  private    json  row  serialization  schema    type  information    row  type  info    preconditions  check  not  null  type  info    type  information    preconditions  check  argument  type  info  instanceof    row  type  info    only    row  type  info  is  supported  this  type  info    row  type  info  type  info  this  runtime  converter  create  converter  type  info    builder  for  link    json  row  serialization  schema    public  evolving  public  static  class    builder  private    row  type  info  type  info  private    builder  private  constructor    creates  a  json  serialization  schema  for  the  given  type  information  param  type  info    type  information  describing  the  result  type    the  field  names  of  link    row  are  used  to  parse  the  json  properties  deprecated    use  link    json  row  serialization  schema  builder  instead    deprecated  public    builder    type  information    row  type  info  check  argument  type  info  instanceof    row  type  info    only    row  type  info  is  supported  this  type  info    row  type  info  type  info    creates  a  json  serialization  schema  for  the  given  json  schema  param  json  schema  json  schema  describing  the  result  type  see  a  href  http  json  schema  org  http  json  schema  org  a  deprecated    use  link    json  row  serialization  schema  builder  instead    deprecated  public    builder    string  json  schema  this    json  row  schema  converter  convert  check  not  null  json  schema    sets  type  information  for  json  serialization  schema  param  type  info    type  information  describing  the  result  type    the  field  names  of  link    row  are  used  to  parse  the  json  properties  public    builder  with  type  info    type  information    row  type  info  check  argument  type  info  instanceof    row  type  info    only    row  type  info  is  supported  this  type  info    row  type  info  type  info  return  this    finalizes  the  configuration  and  checks  validity  return    configured  link    json  row  serialization  schema  public    json  row  serialization  schema  build  check  argument  type  info  null  type  info  should  be  set  return  new    json  row  serialization  schema  type  info    creates  a  builder  for  link    json  row  serialization  schema    builder  public  static    builder  builder  return  new    builder    override  public  byte  serialize    row  row  if  node  null  node  mapper  create  object  node  try  runtime  converter  convert  mapper  node  row  return  mapper  write  value  as  bytes  node  catch    throwable  t  throw  new    runtime  exception    could  not  serialize  row  row    make  sure  that  the  schema  matches  the  input  t    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  final    json  row  serialization  schema  that    json  row  serialization  schema  o  return    objects  equals  type  info  that  type  info    override  public  int  hash  code  return    objects  hash  type  info    runtime  converters    runtime  converter  that  maps  between    java  objects  and  corresponding  link    json  node  s    functional  interface  private  interface    serialization  runtime  converter  extends    serializable    json  node  convert    object  mapper  mapper    json  node  reuse    object  object  private    serialization  runtime  converter  create  converter    type  information  type  info    serialization  runtime  converter  base  converter  create  converter  for  simple  type  type  info  or  else  get  create  container  converter  type  info  or  else  get  this  create  fallback  converter  return  wrap  into  nullable  converter  base  converter  private    serialization  runtime  converter  wrap  into  nullable  converter    serialization  runtime  converter  converter  return  mapper  reuse  object  if  object  null  return  mapper  get  node  factory  null  node  return  converter  convert  mapper  reuse  object  private    optional    serialization  runtime  converter  create  container  converter    type  information  type  info  if  type  info  instanceof    row  type  info  return    optional  of  create  row  converter    row  type  info  type  info  else  if  type  info  instanceof    object  array  type  info  return    optional  of  create  object  array  converter    object  array  type  info  type  info  get  component  info  else  if  type  info  instanceof    basic  array  type  info  return    optional  of  create  object  array  converter    basic  array  type  info  type  info  get  component  info  else  if  is  primitive  byte  array  type  info  return    optional  of  mapper  reuse  object  mapper  get  node  factory  binary  node  byte  object  else  return    optional  empty  private  boolean  is  primitive  byte  array    type  information  type  info  return  type  info  instanceof    primitive  array  type  info    primitive  array  type  info  type  info  get  component  type    types  byte  private    serialization  runtime  converter  create  object  array  converter    type  information  element  type  info    serialization  runtime  converter  element  converter  create  converter  element  type  info  return  assemble  array  converter  element  converter  private    serialization  runtime  converter  create  row  converter    row  type  info  type  info    list    serialization  runtime  converter  field  converters    arrays  stream  type  info  get  field  types  map  this  create  converter  collect    collectors  to  list  return  assemble  row  converter  type  info  get  field  names  field  converters  private    serialization  runtime  converter  create  fallback  converter  return  mapper  reuse  object  for  types  that  were  specified  without  json  schema  e  g    p  o  j  os  try  return  mapper  value  to  tree  object  catch    illegal  argument  exception  e  throw  new    wrapping  runtime  exception  format    could  not  convert  object  s  object  e  private    optional    serialization  runtime  converter  create  converter  for  simple  type    type  information  simple  type  info  if  simple  type  info    types  void  return    optional  of  mapper  reuse  object  mapper  get  node  factory  null  node  else  if  simple  type  info    types  boolean  return    optional  of  mapper  reuse  object  mapper  get  node  factory  boolean  node    boolean  object  else  if  simple  type  info    types  string  return    optional  of  mapper  reuse  object  mapper  get  node  factory  text  node    string  object  else  if  simple  type  info    types  int  return    optional  of  mapper  reuse  object  mapper  get  node  factory  number  node    integer  object  else  if  simple  type  info    types  long  return    optional  of  mapper  reuse  object  mapper  get  node  factory  number  node    long  object  else  if  simple  type  info    types  double  return    optional  of  mapper  reuse  object  mapper  get  node  factory  number  node    double  object  else  if  simple  type  info    types  float  return    optional  of  mapper  reuse  object  mapper  get  node  factory  number  node    float  object  else  if  simple  type  info    types  short  return    optional  of  mapper  reuse  object  mapper  get  node  factory  number  node    short  object  else  if  simple  type  info    types  byte  return    optional  of  mapper  reuse  object  mapper  get  node  factory  number  node    byte  object  else  if  simple  type  info    types  big  dec  return    optional  of  create  big  decimal  converter  else  if  simple  type  info    types  big  int  return    optional  of  create  big  integer  converter  else  if  simple  type  info    types  sql  date  return    optional  of  this  convert  date  else  if  simple  type  info    types  sql  time  return    optional  of  this  convert  time  else  if  simple  type  info    types  sql  timestamp  return    optional  of  this  convert  timestamp  else  if  simple  type  info    types  local  date  return    optional  of  this  convert  local  date  else  if  simple  type  info    types  local  time  return    optional  of  this  convert  local  time  else  if  simple  type  info    types  local  date  time  return    optional  of  this  convert  local  date  time  else  return    optional  empty  private    json  node  convert  local  date    object  mapper  mapper    json  node  reuse    object  object  return  mapper  get  node  factory  text  node  iso  local  date  format    local  date  object  private    json  node  convert  date    object  mapper  mapper    json  node  reuse    object  object    date  date    date  object  return  convert  local  date  mapper  reuse  date  to  local  date  private    json  node  convert  local  date  time    object  mapper  mapper    json  node  reuse    object  object  return  mapper  get  node  factory  text  node    r  f  c3339  timestamp  format  format    local  date  time  object  private    json  node  convert  timestamp    object  mapper  mapper    json  node  reuse    object  object    timestamp  timestamp    timestamp  object  return  convert  local  date  time  mapper  reuse  timestamp  to  local  date  time  private    json  node  convert  local  time    object  mapper  mapper    json  node  reuse    object  object    json  node  factory  node  factory  mapper  get  node  factory  return  node  factory  text  node    r  f  c3339  time  format  format    local  time  object  private    json  node  convert  time    object  mapper  mapper    json  node  reuse    object  object  final    time  time    time  object  return  convert  local  time  mapper  reuse  time  to  local  time  private    serialization  runtime  converter  create  big  decimal  converter  return  mapper  reuse  object  convert  decimal  if  necessary    json  node  factory  node  factory  mapper  get  node  factory  if  object  instanceof    big  decimal  return  node  factory  number  node    big  decimal  object  return  node  factory  number  node    big  decimal  value  of    number  object  double  value  private    serialization  runtime  converter  create  big  integer  converter  return  mapper  reuse  object  convert  decimal  if  necessary    json  node  factory  node  factory  mapper  get  node  factory  if  object  instanceof    big  integer  return  node  factory  number  node    big  integer  object  return  node  factory  number  node    big  integer  value  of    number  object  long  value  private    serialization  runtime  converter  assemble  row  converter    string  field  names    list    serialization  runtime  converter  field  converters  return  mapper  reuse  object    object  node  node  reuse  could  be  a    null  node  if  last  record  is  null  if  reuse  null  reuse  is  null  node  mapper  create  object  node  else  node    object  node  reuse    row  row    row  object  for  int  i    i  field  names  length  i    string  field  name  field  names  i  node  set  field  name  field  converters  get  i  convert  mapper  node  get  field  names  i  row  get  field  i  return  node  private    serialization  runtime  converter  assemble  array  converter    serialization  runtime  converter  element  converter  return  mapper  reuse  object    array  node  node  reuse  could  be  a    null  node  if  last  record  is  null  if  reuse  null  reuse  is  null  node  mapper  create  array  node  else  node    array  node  reuse  node  remove  all    object  array    object  object  for    object  element  array  node  add  element  converter  convert  mapper  null  element  return  node  
public  evolving  public  abstract  class    vectorizer  t  implements    serializable  private  final    type  description  schema  private  transient    writer  writer  public    vectorizer  final    string  schema  check  not  null  schema  this  schema    type  description  from  string  schema    provides  the  orc  schema  return  the  orc  schema  public    type  description  get  schema  return  this  schema    users  are  not  supposed  to  use  this  method  since  this  is  intended  to  be  used  only  by  the  link    orc  bulk  writer  param  writer  the  underlying  orc    writer  public  void  set  writer    writer  writer  this  writer  writer    adds  arbitrary  user  metadata  to  the  outgoing  orc  file  p    users  who  want  to  dynamically  add  new  metadata  either  based  on  either  the  input  or  from  an  external  system  can  do  so  by  calling  code  add  user  metadata  code  inside  the  overridden  vectorize  method  param  key  a  key  to  label  the  data  with  param  value  the  contents  of  the  metadata  public  void  add  user  metadata    string  key    byte  buffer  value  this  writer  add  user  metadata  key  value    transforms  the  provided  element  to    column  vectors  and  sets  them  in  the  exposed    vectorized  row  batch  param  element    the  input  element  param  batch    the  batch  to  write  the    column  vectors  throws    i  o  exception  if  there  is  an  error  while  transforming  the  input  public  abstract  void  vectorize  t  element    vectorized  row  batch  batch  throws    i  o  exception  
public  evolving  public  class    orc  bulk  writer  factory  t  implements    bulk  writer    factory  t  a  dummy    hadoop    path  to  work  around  the  current  implementation  of  orc    writer  impl  which  works  on  the  basis  of  a    hadoop    file  system  and    hadoop    path  but  since  we  use  a  customised  orc    physical  writer  implementation  that  uses    flink  s  own    f  s  data  output  stream  as  the  underlying  internal  stream  instead  of    hadoop  s    f  s  data  output  stream  we  don  t  have  to  worry  about  this  usage  private  static  final    path  fixed  path  new    path  private  final    vectorizer  t  vectorizer  private  final    properties  writer  properties  private  final    map    string    string  conf  map  private    orc  file    writer  options  writer  options    creates  a  new    orc  bulk  writer  factory  using  the  provided    vectorizer  implementation  param  vectorizer    the  vectorizer  implementation  to  convert  input  record  to  a    vectorizer  row  batch  public    orc  bulk  writer  factory    vectorizer  t  vectorizer  this  vectorizer  new    configuration    creates  a  new    orc  bulk  writer  factory  using  the  provided    vectorizer    hadoop    configuration  param  vectorizer    the  vectorizer  implementation  to  convert  input  record  to  a    vectorizer  row  batch  public    orc  bulk  writer  factory    vectorizer  t  vectorizer    configuration  configuration  this  vectorizer  null  configuration    creates  a  new    orc  bulk  writer  factory  using  the  provided    vectorizer    hadoop    configuration  orc  writer  properties  param  vectorizer    the  vectorizer  implementation  to  convert  input  record  to  a    vectorizer  row  batch  param  writer  properties    properties  that  can  be  used  in  orc    writer  options  public    orc  bulk  writer  factory    vectorizer  t  vectorizer    properties  writer  properties    configuration  configuration  this  vectorizer  check  not  null  vectorizer  this  writer  properties  writer  properties  this  conf  map  new    hash  map    todo    replace  the    map  based  approach  with  a  better  approach  for    map    entry    string    string  entry  configuration  conf  map  put  entry  get  key  entry  get  value    override  public    bulk  writer  t  create    f  s  data  output  stream  out  throws    i  o  exception    orc  file    writer  options  opts  get  writer  options  opts  physical  writer  new    physical  writer  impl  out  opts  return  new    orc  bulk  writer  vectorizer  new    writer  impl  null  fixed  path  opts  private    orc  file    writer  options  get  writer  options  if  null  writer  options    configuration  conf  new    configuration  for    map    entry    string    string  entry  conf  map  entry  set  conf  set  entry  get  key  entry  get  value  writer  options    orc  file  writer  options  writer  properties  conf  writer  options  set  schema  this  vectorizer  get  schema  return  writer  options  
public  evolving  public  class    parquet  bulk  writer  t  implements    bulk  writer  t    the    parquet  writer  to  write  to  private  final    parquet  writer  t  parquet  writer    creates  a  new    parquet  bulk  writer  wrapping  the  given    parquet  writer  param  parquet  writer    the    parquet  writer  to  write  to  public    parquet  bulk  writer    parquet  writer  t  parquet  writer  this  parquet  writer  check  not  null  parquet  writer  parquet  writer    override  public  void  add  element  t  datum  throws    i  o  exception  parquet  writer  write  datum    override  public  void  flush  nothing  we  can  do  here    override  public  void  finish  throws    i  o  exception  parquet  writer  close  
public  evolving  public  class    parquet  writer  factory  t  implements    bulk  writer    factory  t  private  static  final  long  serial  version  u  i  d  1  l    the  builder  to  construct  the    parquet  writer  private  final    parquet  builder  t  writer  builder    creates  a  new    parquet  writer  factory  using  the  given  builder  to  assemble  the    parquet  writer  param  writer  builder    the  builder  to  construct  the    parquet  writer  public    parquet  writer  factory    parquet  builder  t  writer  builder  this  writer  builder  writer  builder    override  public    bulk  writer  t  create    f  s  data  output  stream  stream  throws    i  o  exception  final    output  file  out  new    stream  output  file  stream  final    parquet  writer  t  writer  writer  builder  create  writer  out  return  new    parquet  bulk  writer  writer  
public  evolving  public  class    sequence  file  writer  k  extends    writable  v  extends    writable  implements    bulk  writer    tuple2  k  v  private  final    sequence  file    writer  writer    sequence  file  writer    sequence  file    writer  writer  this  writer  check  not  null  writer    override  public  void  add  element    tuple2  k  v  element  throws    i  o  exception  writer  append  element  f0  element  f1    override  public  void  flush  throws    i  o  exception  writer  hsync    override  public  void  finish  throws    i  o  exception  writer  close  
public  evolving  public  class    sequence  file  writer  factory  k  extends    writable  v  extends    writable  implements    bulk  writer    factory    tuple2  k  v  private  static  final  long  serial  version  u  i  d  1  l  a  constant  specifying  that  no  compression  is  requested  public  static  final    string  no  compression  no  compression  private  final    serializable  hadoop  configuration  serializable  hadoop  config  private  final    class  k  key  class  private  final    class  v  value  class  private  final    string  compression  codec  name  private  final    sequence  file    compression  type  compression  type    creates  a  new    sequence  file  writer  factory  using  the  given  builder  to  assemble  the    sequence  file  writer  param  hadoop  conf    the    hadoop  configuration  for    sequence    file    writer  param  key  class    the  class  of  key  to  write  param  value  class    the  class  of  value  to  write  public    sequence  file  writer  factory    configuration  hadoop  conf    class  k  key  class    class  v  value  class  this  hadoop  conf  key  class  value  class  no  compression    sequence  file    compression  type  block    creates  a  new    sequence  file  writer  factory  using  the  given  builder  to  assemble  the    sequence  file  writer  param  hadoop  conf    the    hadoop  configuration  for    sequence    file    writer  param  key  class    the  class  of  key  to  write  param  value  class    the  class  of  value  to  write  param  compression  codec  name    the  name  of  compression  codec  public    sequence  file  writer  factory    configuration  hadoop  conf    class  k  key  class    class  v  value  class    string  compression  codec  name  this  hadoop  conf  key  class  value  class  compression  codec  name    sequence  file    compression  type  block    creates  a  new    sequence  file  writer  factory  using  the  given  builder  to  assemble  the    sequence  file  writer  param  hadoop  conf    the    hadoop  configuration  for    sequence    file    writer  param  key  class    the  class  of  key  to  write  param  value  class    the  class  of  value  to  write  param  compression  codec  name    the  name  of  compression  codec  param  compression  type    the  type  of  compression  level  public    sequence  file  writer  factory    configuration  hadoop  conf    class  k  key  class    class  v  value  class    string  compression  codec  name    sequence  file    compression  type  compression  type  this  serializable  hadoop  config  new    serializable  hadoop  configuration  check  not  null  hadoop  conf  this  key  class  check  not  null  key  class  this  value  class  check  not  null  value  class  this  compression  codec  name  check  not  null  compression  codec  name  this  compression  type  check  not  null  compression  type    override  public    sequence  file  writer  k  v  create    f  s  data  output  stream  out  throws    i  o  exception  org  apache  hadoop  fs    f  s  data  output  stream  stream  new  org  apache  hadoop  fs    f  s  data  output  stream  out  null    compression  codec  compression  codec  get  compression  codec  serializable  hadoop  config  get  compression  codec  name    sequence  file    writer  writer    sequence  file  create  writer  serializable  hadoop  config  get    sequence  file    writer  stream  stream    sequence  file    writer  key  class  key  class    sequence  file    writer  value  class  value  class    sequence  file    writer  compression  compression  type  compression  codec  return  new    sequence  file  writer  writer  private    compression  codec  get  compression  codec    configuration  conf    string  compression  codec  name  check  not  null  conf  check  not  null  compression  codec  name  if  compression  codec  name  equals  no  compression  return  null    compression  codec  factory  codec  factory  new    compression  codec  factory  conf    compression  codec  codec  codec  factory  get  codec  by  name  compression  codec  name  if  codec  null  throw  new    runtime  exception    codec  compression  codec  name  not  found  return  codec  
public  evolving  public  class    unsupported  aggregation  type  exception  extends    runtime  exception  private  static  final  long  serial  version  u  i  d    l  public    unsupported  aggregation  type  exception    string  message  super  message  
public  evolving  public  class    collection  environment  extends    execution  environment    override  public    job  execution  result  execute    string  job  name  throws    exception    plan  p  create  program  plan  job  name    we  need  to  reverse  here    object    reuse  enabled  means  safe  mode  is  disabled    collection  executor  exec  new    collection  executor  get  config  this  last  job  execution  result  exec  execute  p  return  this  last  job  execution  result    override  public  int  get  parallelism  return    always  serial  
public  evolving  public  abstract  class    flat  map  iterator  in  out  extends    rich  flat  map  function  in  out  private  static  final  long  serial  version  u  i  d  1  l    the  core  method  of  the  function    takes  an  element  from  the  input  data  set  and  transforms  it  into  zero  one  or  more  elements  param  value    the  input  value  return    an  iterator  over  the  returned  elements  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract    iterator  out  flat  map  in  value  throws    exception    delegates  calls  to  the  link  flat  map    object  method    override  public  final  void  flat  map  in  value    collector  out  out  throws    exception  for    iterator  out  iter  flat  map  value  iter  has  next  out  collect  iter  next  
public  evolving  public  abstract  class    group  reduce  iterator  in  out  extends    rich  group  reduce  function  in  out  private  static  final  long  serial  version  u  i  d  1  l  public  abstract    iterator  out  reduce  group    iterable  in  values  throws    exception    override  public  final  void  reduce    iterable  in  values    collector  out  out  throws    exception  for    iterator  out  iter  reduce  group  values  iter  has  next  out  collect  iter  next  
public  evolving  public  class    collection  input  format  t  extends    generic  input  format  t  implements    non  parallel  input  private  static  final  long  serial  version  u  i  d  1  l  private  static  final  int  max  to  string  len    private    type  serializer  t  serializer  private  transient    collection  t  data  set  input  data  as  collection  transient  because  it  will  be  serialized  in  a  custom  way  private  transient    iterator  t  iterator  public    collection  input  format    collection  t  data  set    type  serializer  t  serializer  if  data  set  null  throw  new    null  pointer  exception  this  serializer  serializer  this  data  set  data  set    override  public  boolean  reached  end  throws    i  o  exception  return  this  iterator  has  next    override  public  void  open    generic  input  split  split  throws    i  o  exception  super  open  split  this  iterator  this  data  set  iterator    override  public  t  next  record  t  record  throws    i  o  exception  return  this  iterator  next  private  void  write  object    object  output  stream  out  throws    i  o  exception  out  default  write  object  final  int  size  data  set  size  out  write  int  size  if  size      data  output  view  stream  wrapper  wrapper  new    data  output  view  stream  wrapper  out  for  t  element  data  set  serializer  serialize  element  wrapper  private  void  read  object    object  input  stream  in  throws    i  o  exception    class  not  found  exception  in  default  read  object  int  collection  length  in  read  int    list  t  list  new    array  list  t  collection  length  if  collection  length    try    data  input  view  stream  wrapper  wrapper  new    data  input  view  stream  wrapper  in  for  int  i    i  collection  length  i  t  element  serializer  deserialize  wrapper  list  add  element  catch    throwable  t  throw  new    i  o  exception    error  while  deserializing  element  from  collection  t  data  set  list    override  public    string  to  string    string  builder  sb  new    string  builder  sb  append  int  num    for  t  e  data  set  sb  append  e  if  num  data  set  size    sb  append  if  sb  length  max  to  string  len  sb  append  break  num  sb  append  return  sb  to  string  public  static  x  void  check  collection    collection  x  elements    class  x  viewed  as  if  elements  null  viewed  as  null  throw  new    null  pointer  exception  for  x  elem  elements  if  elem  null  throw  new    illegal  argument  exception    the  collection  must  not  contain  null  elements    the  second  part  of  the  condition  is  a  workaround  for  the  situation  that  can  arise  from  eg  env  from  elements    in  this  situation    unit  type  info  get  type  class  returns  void  class  when  we  are  in  the    java  world  but  the  actual  objects  that  we  will  be  working  with  will  be    boxed  units    note    type  information  gen  test  test  unit  tests  this  condition  if  viewed  as  is  assignable  from  elem  get  class  elem  get  class  to  string  equals  class  scala  runtime    boxed  unit  viewed  as  equals  void  class  throw  new    illegal  argument  exception    the  elements  in  the  collection  are  not  all  subclasses  of  viewed  as  get  canonical  name  
public  evolving  public  class    csv  output  format  t  extends    tuple  extends    file  output  format  t  implements    input  type  configurable  private  static  final  long  serial  version  u  i  d  1  l    suppress  warnings  unused  private  static  final    logger  log    logger  factory  get  logger    csv  output  format  class  public  static  final    string  default  line  delimiter    csv  input  format  default  line  delimiter  public  static  final    string  default  field  delimiter    string  value  of    csv  input  format  default  field  delimiter  private  transient    writer  wrt  private    string  field  delimiter  private    string  record  delimiter  private    string  charset  name  private  boolean  allow  null  values  true  private  boolean  quote  strings  false    constructors  and  getters  setters  for  the  configurable  parameters    creates  an  instance  of    csv  output  format    lines  are  separated  by  the  newline  character  n  fields  are  separated  by  param  output  path    the  path  where  the  csv  file  is  written  public    csv  output  format    path  output  path  this  output  path  default  line  delimiter  default  field  delimiter    creates  an  instance  of    csv  output  format    lines  are  separated  by  the  newline  character  n  fields  by  the  given  field  delimiter  param  output  path    the  path  where  the  csv  file  is  written  param  field  delimiter    the  delimiter  that  is  used  to  separate  fields  in  a  tuple  public    csv  output  format    path  output  path    string  field  delimiter  this  output  path  default  line  delimiter  field  delimiter    creates  an  instance  of    csv  output  format  param  output  path    the  path  where  the  csv  file  is  written  param  record  delimiter    the  delimiter  that  is  used  to  separate  the  tuples  param  field  delimiter    the  delimiter  that  is  used  to  separate  fields  in  a  tuple  public    csv  output  format    path  output  path    string  record  delimiter    string  field  delimiter  super  output  path  if  record  delimiter  null  throw  new    illegal  argument  exception    record  delmiter  shall  not  be  null  if  field  delimiter  null  throw  new    illegal  argument  exception    field  delimiter  shall  not  be  null  this  field  delimiter  field  delimiter  this  record  delimiter  record  delimiter  this  allow  null  values  false    configures  the  format  to  either  allow  null  values  writing  an  empty  field  or  to  throw  an  exception  when  encountering  a  null  field  p  by  default  null  values  are  disallowed  param  allow  nulls    flag  to  indicate  whether  the  output  format  should  accept  null  values  public  void  set  allow  null  values  boolean  allow  nulls  this  allow  null  values  allow  nulls    sets  the  charset  with  which  the  csv  strings  are  written  to  the  file    if  not  specified  the  output  format  uses  the  systems  default  character  encoding  param  charset  name    the  name  of  charset  to  use  for  encoding  the  output  public  void  set  charset  name    string  charset  name  this  charset  name  charset  name    configures  whether  the  output  format  should  quote  string  values    string  values  are  fields  of  type  link  java  lang    string  and  link  org  apache  flink  types    string  value  as  well  as  all  subclasses  of  the  latter  p    by  default  strings  are  not  quoted  param  quote  strings    flag  indicating  whether  string  fields  should  be  quoted  public  void  set  quote  strings  boolean  quote  strings  this  quote  strings  quote  strings    override  public  void  open  int  task  number  int  num  tasks  throws    i  o  exception  super  open  task  number  num  tasks  this  wrt  this  charset  name  null  new    output  stream  writer  new    buffered  output  stream  this  stream    new    output  stream  writer  new    buffered  output  stream  this  stream    this  charset  name    override  public  void  close  throws    i  o  exception  if  wrt  null  this  wrt  flush  this  wrt  close  super  close    override  public  void  write  record  t  element  throws    i  o  exception  int  num  fields  element  get  arity  for  int  i    i  num  fields  i    object  v  element  get  field  i  if  v  null  if  i    this  wrt  write  this  field  delimiter  if  quote  strings  if  v  instanceof    string  v  instanceof    string  value  this  wrt  write  this  wrt  write  v  to  string  this  wrt  write  else  this  wrt  write  v  to  string  else  this  wrt  write  v  to  string  else  if  this  allow  null  values  if  i    this  wrt  write  this  field  delimiter  else  throw  new    runtime  exception    cannot  write  tuple  with  null  value  at  position  i  add  the  record  delimiter  this  wrt  write  this  record  delimiter    override  public    string  to  string  return    csv  output  format  path  this  get  output  file  path  delimiter  this  field  delimiter    the  purpose  of  this  method  is  solely  to  check  whether  the  data  type  to  be  processed  is  in  fact  a  tuple  type    override  public  void  set  input  type    type  information  type    execution  config  execution  config  if  type  is  tuple  type  throw  new    invalid  program  exception    the    csv  output  format  class  get  simple  name  can  only  be  used  to  write  tuple  data  sets  
public  evolving  public  class    iterator  input  format  t  extends    generic  input  format  t  implements    non  parallel  input  private  static  final  long  serial  version  u  i  d  1  l  private    iterator  t  iterator  input  data  as  serializable  iterator  public    iterator  input  format    iterator  t  iterator  if  iterator  instanceof    serializable  throw  new    illegal  argument  exception    the  data  source  iterator  must  be  serializable  this  iterator  iterator    override  public  boolean  reached  end  return  this  iterator  has  next    override  public  t  next  record  t  record  return  this  iterator  next  
public  evolving  public  class    local  collection  output  format  t  extends    rich  output  format  t  implements    input  type  configurable  private  static  final  long  serial  version  u  i  d  1  l  private  static  final    map    integer    collection  result  holder  new    hash  map    integer    collection  private  transient    array  list  t  task  result  private    type  serializer  t  type  serializer  private  int  id  public    local  collection  output  format    collection  t  out  synchronized  result  holder  this  id  generate  random  id  result  holder  put  this  id  out  private  int  generate  random  id  int  num  int    math  random    integer  max  value  while  result  holder  contains  key  num  num  int    math  random    integer  max  value  return  num    override  public  void  configure    configuration  parameters    override  public  void  open  int  task  number  int  num  tasks  throws    i  o  exception  this  task  result  new    array  list  t    override  public  void  write  record  t  record  throws    i  o  exception  t  record  copy  this  type  serializer  create  instance  record  copy  this  type  serializer  copy  record  record  copy  this  task  result  add  record  copy    override  public  void  close  throws    i  o  exception  synchronized  result  holder    suppress  warnings  unchecked    collection  t  result    collection  t  result  holder  get  this  id  result  add  all  this  task  result    override    suppress  warnings  unchecked  public  void  set  input  type    type  information  type    execution  config  execution  config  this  type  serializer    type  serializer  t  type  create  serializer  execution  config  
public  evolving  public  class    parallel  iterator  input  format  t  extends    generic  input  format  t  private  static  final  long  serial  version  u  i  d  1  l  private  final    splittable  iterator  t  source  private  transient    iterator  t  split  iterator  public    parallel  iterator  input  format    splittable  iterator  t  iterator  this  source  iterator    override  public  void  open    generic  input  split  split  throws    i  o  exception  super  open  split  this  split  iterator  this  source  get  split  split  get  split  number  split  get  total  number  of  splits    override  public  boolean  reached  end  return  this  split  iterator  has  next    override  public  t  next  record  t  reuse  return  this  split  iterator  next  
public  evolving  public  class    primitive  input  format  ot  extends    delimited  input  format  ot  private  static  final  long  serial  version  u  i  d  1  l  private    class  ot  primitive  class  private  static  final  byte  carriage  return  byte  r  private  static  final  byte  new  line  byte  n  private  transient    field  parser  ot  parser  public    primitive  input  format    path  file  path    class  ot  primitive  class  super  file  path  null  this  primitive  class  primitive  class  public    primitive  input  format    path  file  path    string  delimiter    class  ot  primitive  class  super  file  path  null  this  primitive  class  primitive  class  this  set  delimiter  delimiter    override  public  void  open    file  input  split  split  throws    i  o  exception  super  open  split    class  extends    field  parser  ot  parser  type    field  parser  get  parser  for  type  primitive  class  if  parser  type  null  throw  new    illegal  argument  exception    the  type  primitive  class  get  name  is  not  supported  for  the  primitive  input  format  parser    instantiation  util  instantiate  parser  type    field  parser  class    override  public  ot  read  record  ot  reuse  byte  bytes  int  offset  int  num  bytes  throws    i  o  exception    check  if  n  is  used  as  delimiter  and  the  end  of  this  line  is  a  r  then  remove  r  from  the  line  if  this  get  delimiter  length    this  get  delimiter    new  line  offset  num  bytes    bytes  offset  num  bytes    carriage  return  num  bytes      null  character  as  delimiter  is  used  because  there  s  only    field  to  be  parsed  if  parser  reset  error  state  and  parse  bytes  offset  num  bytes  offset  new  byte    reuse    return  parser  get  last  result  else    string  s  new    string  bytes  offset  num  bytes  get  charset  throw  new    i  o  exception    could  not  parse  value  s  as  type  primitive  class  get  simple  name  
public  evolving  public  class    printing  output  format  t  extends    rich  output  format  t  private  static  final  long  serial  version  u  i  d  1  l  private  final    print  sink  output  writer  t  writer    instantiates  a  printing  output  format  that  prints  to  standard  out  public    printing  output  format  writer  new    print  sink  output  writer  false    instantiates  a  printing  output  format  that  prints  to  standard  out  param  std  err    true  if  the  format  should  print  to  standard  error  instead  of  standard  out  public    printing  output  format  final  boolean  std  err  writer  new    print  sink  output  writer  std  err    instantiates  a  printing  output  format  that  prints  to  standard  out  with  a  prefixed  message  param  sink  identifier    message  that  is  prefixed  to  the  output  of  the  value  param  std  err    true  if  the  format  should  print  to  standard  error  instead  of  standard  out  public    printing  output  format  final    string  sink  identifier  final  boolean  std  err  writer  new    print  sink  output  writer  sink  identifier  std  err    override  public  void  configure    configuration  parameters    override  public  void  open  int  task  number  int  num  tasks  writer  open  task  number  num  tasks    override  public  void  write  record  t  record  writer  write  record    override  public  void  close    override  public    string  to  string  return  writer  to  string  
public  evolving  public  class    row  csv  input  format  extends    csv  input  format    row  implements    result  type  queryable    row  private  static  final  long  serial  version  u  i  d  1  l  private  int  arity  private    type  information  field  type  infos  private  int  field  pos  map  private  boolean  empty  column  as  null  public    row  csv  input  format    path  file  path    type  information  field  type  infos    string  line  delimiter    string  field  delimiter  int  selected  fields  boolean  empty  column  as  null  super  file  path  this  arity  field  type  infos  length  if  arity  selected  fields  length  throw  new    illegal  argument  exception    number  of  field  types  and  selected  fields  must  be  the  same  this  field  type  infos  field  type  infos  this  field  pos  map  to  field  pos  map  selected  fields  this  empty  column  as  null  empty  column  as  null  boolean  fields  mask  to  field  mask  selected  fields  set  delimiter  line  delimiter  set  field  delimiter  field  delimiter  set  fields  generic  fields  mask  extract  type  classes  field  type  infos  public    row  csv  input  format    path  file  path    type  information  field  types    string  line  delimiter    string  field  delimiter  int  selected  fields  this  file  path  field  types  line  delimiter  field  delimiter  selected  fields  false  public    row  csv  input  format    path  file  path    type  information  field  types    string  line  delimiter    string  field  delimiter  this  file  path  field  types  line  delimiter  field  delimiter  sequential  scan  order  field  types  length  public    row  csv  input  format    path  file  path    type  information  field  types  int  selected  fields  this  file  path  field  types  default  line  delimiter  default  field  delimiter  selected  fields  public    row  csv  input  format    path  file  path    type  information  field  types  boolean  empty  column  as  null  this  file  path  field  types  default  line  delimiter  default  field  delimiter  sequential  scan  order  field  types  length  empty  column  as  null  public    row  csv  input  format    path  file  path    type  information  field  types  this  file  path  field  types  false  private  static    class  extract  type  classes    type  information  field  types    class  classes  new    class  field  types  length  for  int  i    i  field  types  length  i  classes  i  field  types  i  get  type  class  return  classes  private  static  int  sequential  scan  order  int  arity  int  sequential  order  new  int  arity  for  int  i    i  arity  i  sequential  order  i  i  return  sequential  order  private  static  boolean  to  field  mask  int  selected  fields  int  max  field    for  int  selected  field  selected  fields  max  field    math  max  max  field  selected  field  boolean  mask  new  boolean  max  field      arrays  fill  mask  false  for  int  selected  field  selected  fields  mask  selected  field  true  return  mask  private  static  int  to  field  pos  map  int  selected  fields  int  field  idxs    arrays  copy  of  selected  fields  selected  fields  length    arrays  sort  field  idxs  int  field  pos  map  new  int  selected  fields  length  for  int  i    i  selected  fields  length  i  int  pos    arrays  binary  search  field  idxs  selected  fields  i  field  pos  map  pos  i  return  field  pos  map    override  protected    row  fill  record    row  reuse    object  parsed  values    row  reuse  row  if  reuse  null  reuse  row  new    row  arity  else  reuse  row  reuse  for  int  i    i  parsed  values  length  i  reuse  row  set  field  i  parsed  values  i  return  reuse  row    override  protected  boolean  parse  record    object  holders  byte  bytes  int  offset  int  num  bytes  throws    parse  exception  byte  field  delimiter  this  get  field  delimiter  boolean  field  included  this  field  included  int  start  pos  offset  int  limit  offset  num  bytes  int  field    int  output    while  field  field  included  length  check  valid  start  position  if  start  pos  limit  start  pos  limit  field  field  included  length    if  is  lenient  return  false  else  throw  new    parse  exception    row  too  short  new    string  bytes  offset  num  bytes  get  charset  if  field  included  field  parse  field    field  parser    object  parser    field  parser    object  this  get  field  parsers  field  pos  map  output  int  latest  valid  pos  start  pos  start  pos  parser  reset  error  state  and  parse  bytes  start  pos  limit  field  delimiter  holders  field  pos  map  output  if  is  lenient  parser  get  error  state    field  parser    parse  error  state  none  the  error  state  empty  column  is  ignored  if  parser  get  error  state    field  parser    parse  error  state  empty  column  throw  new    parse  exception    string  format    parsing  error  for  column    s  of  row    s  originated  by    s    s  field    new    string  bytes  offset  num  bytes  parser  get  class  get  simple  name  parser  get  error  state  holders  field  pos  map  output  parser  get  last  result  check  parse  result  the  result  is  null  if  it  is  invalid  or  empty  with  empty  column  as  null  enabled  if  start  pos    empty  column  as  null  parser  get  error  state  equals    field  parser    parse  error  state  empty  column  holders  field  pos  map  output  null  start  pos  skip  fields  bytes  latest  valid  pos  limit  field  delimiter  output  else  skip  field  start  pos  skip  fields  bytes  start  pos  limit  field  delimiter  check  if  something  went  wrong  if  start  pos    throw  new    parse  exception    string  format    unexpected  parser  position  for  column    s  of  row    s  field    new    string  bytes  offset  num  bytes  else  if  start  pos  limit  field  field  included  length      field  parser  ends  with  delimiter  bytes  start  pos    field  delimiter    we  are  at  the  end  of  the  record  but  not  all  fields  have  been  read  and  the  end  is  not  a  field  delimiter  indicating  an  empty  last  field  if  is  lenient  return  false  else  throw  new    parse  exception    row  too  short  new    string  bytes  offset  num  bytes  field  return  true    override  public    type  information    row  get  produced  type  return  new    row  type  info  this  field  type  infos  
public  evolving  public  class    split  data  properties  t  implements    generic  data  source  base    split  data  properties  t  private    type  information  t  type  private  int  split  partition  keys  private    partitioner  t  split  partitioner  private  int  split  group  keys  private    ordering  split  ordering    creates    split  data  properties  for  the  given  data  types  param  type    the  data  type  of  the    split  data  properties  public    split  data  properties    type  information  t  type  this  type  type    creates    split  data  properties  for  the  given  data  types  param  source    the    data  source  for  which  the    split  data  properties  are  created  public    split  data  properties    data  source  t  source  this  type  source  get  type    defines  that  data  is  partitioned  across  input  splits  on  the  fields  defined  by  field  positions    all  records  sharing  the  same  key  combination  must  be  contained  in  a  single  input  split  p  b  important    providing  wrong  information  with    split  data  properties  can  cause  wrong  results  b  param  partition  fields    the  field  positions  of  the  partitioning  keys  return    this    split  data  properties  object  public    split  data  properties  t  splits  partitioned  by  int  partition  fields  return  this  splits  partitioned  by  null  partition  fields    defines  that  data  is  partitioned  using  a  specific  partitioning  method  across  input  splits  on  the  fields  defined  by  field  positions    all  records  sharing  the  same  key  combination  must  be  contained  in  a  single  input  split  p  b  important    providing  wrong  information  with    split  data  properties  can  cause  wrong  results  b  param  partition  method  id    an  id  for  the  method  that  was  used  to  partition  the  data  across  splits  param  partition  fields    the  field  positions  of  the  partitioning  keys  return    this    split  data  properties  object  public    split  data  properties  t  splits  partitioned  by    string  partition  method  id  int  partition  fields  if  partition  fields  null  throw  new    invalid  program  exception    partition  fields  may  not  be  null  else  if  partition  fields  length    throw  new    invalid  program  exception    partition  fields  may  not  be  empty  this  split  partition  keys  get  all  flat  keys  partition  fields  if  partition  method  id  null  this  split  partitioner  new    source  partitioner  marker  partition  method  id  else  this  split  partitioner  null  return  this    defines  that  data  is  partitioned  across  input  splits  on  the  fields  defined  by  field  expressions    multiple  field  expressions  must  be  separated  by  the  semicolon  character    all  records  sharing  the  same  key  combination  must  be  contained  in  a  single  input  split  p  b  important    providing  wrong  information  with    split  data  properties  can  cause  wrong  results  b  param  partition  fields    the  field  expressions  of  the  partitioning  keys  return    this    split  data  properties  object  public    split  data  properties  t  splits  partitioned  by    string  partition  fields  return  this  splits  partitioned  by  null  partition  fields    defines  that  data  is  partitioned  using  an  identifiable  method  across  input  splits  on  the  fields  defined  by  field  expressions    multiple  field  expressions  must  be  separated  by  the  semicolon  character    all  records  sharing  the  same  key  combination  must  be  contained  in  a  single  input  split  p  b  important    providing  wrong  information  with    split  data  properties  can  cause  wrong  results  b  param  partition  method  id    an  id  for  the  method  that  was  used  to  partition  the  data  across  splits  param  partition  fields    the  field  expressions  of  the  partitioning  keys  return    this    split  data  properties  object  public    split  data  properties  t  splits  partitioned  by    string  partition  method  id    string  partition  fields  if  partition  fields  null  throw  new    invalid  program  exception    partition  fields  may  not  be  null    string  partition  keys  a  partition  fields  split  if  partition  keys  a  length    throw  new    invalid  program  exception    partition  fields  may  not  be  empty  this  split  partition  keys  get  all  flat  keys  partition  keys  a  if  partition  method  id  null  this  split  partitioner  new    source  partitioner  marker  partition  method  id  else  this  split  partitioner  null  return  this    defines  that  the  data  within  an  input  split  is  grouped  on  the  fields  defined  by  the  field  positions    all  records  sharing  the  same  key  combination  must  be  subsequently  emitted  by  the  input  format  for  each  input  split  p  b  important    providing  wrong  information  with    split  data  properties  can  cause  wrong  results  b  param  group  fields    the  field  positions  of  the  grouping  keys  return    this    split  data  properties  object  public    split  data  properties  t  splits  grouped  by  int  group  fields  if  group  fields  null  throw  new    invalid  program  exception    group  fields  may  not  be  null  else  if  group  fields  length    throw  new    invalid  program  exception    group  fields  may  not  be  empty  if  this  split  ordering  null  throw  new    invalid  program  exception    data  source  may  either  be  grouped  or  sorted  this  split  group  keys  get  all  flat  keys  group  fields  return  this    defines  that  the  data  within  an  input  split  is  grouped  on  the  fields  defined  by  the  field  expressions    multiple  field  expressions  must  be  separated  by  the  semicolon  character    all  records  sharing  the  same  key  combination  must  be  subsequently  emitted  by  the  input  format  for  each  input  split  p  b  important    providing  wrong  information  with    split  data  properties  can  cause  wrong  results  b  param  group  fields    the  field  expressions  of  the  grouping  keys  return    this    split  data  properties  object  public    split  data  properties  t  splits  grouped  by    string  group  fields  if  group  fields  null  throw  new    invalid  program  exception    group  fields  may  not  be  null    string  group  keys  a  group  fields  split  if  group  keys  a  length    throw  new    invalid  program  exception    group  fields  may  not  be  empty  if  this  split  ordering  null  throw  new    invalid  program  exception    data  source  may  either  be  grouped  or  sorted  this  split  group  keys  get  all  flat  keys  group  keys  a  return  this    defines  that  the  data  within  an  input  split  is  sorted  on  the  fields  defined  by  the  field  positions  in  the  specified  orders    all  records  of  an  input  split  must  be  emitted  by  the  input  format  in  the  defined  order  p  b  important    providing  wrong  information  with    split  data  properties  can  cause  wrong  results  b  param  order  fields    the  field  positions  of  the  grouping  keys  param  orders    the  orders  of  the  fields  return    this    split  data  properties  object  public    split  data  properties  t  splits  ordered  by  int  order  fields    order  orders  if  order  fields  null  orders  null  throw  new    invalid  program  exception    order  fields  or    orders  may  not  be  null  else  if  order  fields  length    throw  new    invalid  program  exception    order  fields  may  not  be  empty  else  if  orders  length    throw  new    invalid  program  exception    orders  may  not  be  empty  else  if  order  fields  length  orders  length  throw  new    invalid  program  exception    number  of    order  fields  and    orders  must  match  if  this  split  group  keys  null  throw  new    invalid  program  exception    data  source  may  either  be  grouped  or  sorted  this  split  ordering  new    ordering  for  int  i    i  order  fields  length  i  int  pos  order  fields  i  int  flat  keys  this  get  all  flat  keys  new  int  pos  for  int  key  flat  keys  check  for  duplicates  for  int  okey  split  ordering  get  field  positions  if  key  okey  throw  new    invalid  program  exception    duplicate  field  in  the  field  expression  pos  append  key  this  split  ordering  append  ordering  key  null  orders  i  return  this    defines  that  the  data  within  an  input  split  is  sorted  on  the  fields  defined  by  the  field  expressions  in  the  specified  orders    multiple  field  expressions  must  be  separated  by  the  semicolon  character    all  records  of  an  input  split  must  be  emitted  by  the  input  format  in  the  defined  order  p  b  important    providing  wrong  information  with    split  data  properties  can  cause  wrong  results  b  param  order  fields    the  field  expressions  of  the  grouping  key  param  orders    the  orders  of  the  fields  return    this    split  data  properties  object  public    split  data  properties  t  splits  ordered  by    string  order  fields    order  orders  if  order  fields  null  orders  null  throw  new    invalid  program  exception    order  fields  or    orders  may  not  be  null    string  order  keys  a  order  fields  split  if  order  keys  a  length    throw  new    invalid  program  exception    order  fields  may  not  be  empty  else  if  orders  length    throw  new    invalid  program  exception    orders  may  not  be  empty  else  if  order  keys  a  length  orders  length  throw  new    invalid  program  exception    number  of    order  fields  and    orders  must  match  if  this  split  group  keys  null  throw  new    invalid  program  exception    data  source  may  either  be  grouped  or  sorted  this  split  ordering  new    ordering  for  int  i    i  order  keys  a  length  i    string  key  exp  order  keys  a  i    keys    expression  keys  t  ek  new    keys    expression  keys  key  exp  this  type  int  flat  keys  ek  compute  logical  key  positions  for  int  key  flat  keys  check  for  duplicates  for  int  okey  split  ordering  get  field  positions  if  key  okey  throw  new    invalid  program  exception    duplicate  field  in  field  expression  key  exp  append  key  this  split  ordering  append  ordering  key  null  orders  i  return  this  public  int  get  split  partition  keys  return  this  split  partition  keys  public    partitioner  t  get  split  partitioner  return  this  split  partitioner  public  int  get  split  group  keys  return  this  split  group  keys  public    ordering  get  split  order  return  this  split  ordering  flat  field  extraction  methods  private  int  get  all  flat  keys    string  field  expressions  int  all  keys  null  for    string  key  exp  field  expressions    keys    expression  keys  t  ek  new    keys    expression  keys  key  exp  this  type  int  flat  keys  ek  compute  logical  key  positions  if  all  keys  null  all  keys  flat  keys  else  check  for  duplicates  for  int  key1  flat  keys  for  int  key2  all  keys  if  key1  key2  throw  new    invalid  program  exception    duplicate  fields  in  field  expression  key  exp  append  flat  keys  int  old  length  all  keys  length  int  new  length  old  length  flat  keys  length  all  keys    arrays  copy  of  all  keys  new  length    system  arraycopy  flat  keys    all  keys  old  length  flat  keys  length  return  all  keys  private  int  get  all  flat  keys  int  field  positions    keys    expression  keys  t  ek  new    keys    expression  keys  field  positions  this  type  return  ek  compute  logical  key  positions  a  custom  partitioner  to  mark  compatible  split  partitionings  param  t    the  type  of  the  partitioned  data  public  static  class    source  partitioner  marker  t  implements    partitioner  t  private  static  final  long  serial  version  u  i  d    l    string  partition  marker  public    source  partitioner  marker    string  partition  marker  this  partition  marker  partition  marker    override  public  int  partition  t  key  int  num  partitions  throw  new    unsupported  operation  exception    the    source  partitioner  marker  is  only  used  as  a  marker  for  compatible  partitioning    it  must  not  be  invoked    override  public  boolean  equals    object  o  if  o  instanceof    source  partitioner  marker  return  this  partition  marker  equals    source  partitioner  marker  o  partition  marker  else  return  false  
public  evolving  public  class    text  input  format  extends    delimited  input  format    string  private  static  final  long  serial  version  u  i  d  1  l    code  of  r  used  to  remove  r  from  a  line  when  the  line  ends  with  r  n  private  static  final  byte  carriage  return  byte  r    code  of  n  used  to  identify  if  n  is  used  as  delimiter  private  static  final  byte  new  line  byte  n    the  name  of  the  charset  to  use  for  decoding  private    string  charset  name  utf    public    text  input  format    path  file  path  super  file  path  null  public    string  get  charset  name  return  charset  name  public  void  set  charset  name    string  charset  name  if  charset  name  null  throw  new    illegal  argument  exception    charset  must  not  be  null  this  charset  name  charset  name    override  public  void  configure    configuration  parameters  super  configure  parameters  if  charset  name  null    charset  is  supported  charset  name  throw  new    runtime  exception    unsupported  charset  charset  name    override  public    string  read  record    string  reusable  byte  bytes  int  offset  int  num  bytes  throws    i  o  exception    check  if  n  is  used  as  delimiter  and  the  end  of  this  line  is  a  r  then  remove  r  from  the  line  if  this  get  delimiter  null  this  get  delimiter  length    this  get  delimiter    new  line  offset  num  bytes    bytes  offset  num  bytes    carriage  return  num  bytes    return  new    string  bytes  offset  num  bytes  this  charset  name    override  public    string  to  string  return    text  input  format    arrays  to  string  get  file  paths  this  charset  name    override  public  boolean  supports  multi  paths  return  true  
public  evolving  public  class    text  output  format  t  extends    file  output  format  t  private  static  final  long  serial  version  u  i  d  1  l  private  static  final  int  newline  n  private    string  charset  name  private  transient    charset  charset    formatter  that  transforms  values  into  their  link    string  representations  param  in  type  of  input  elements  public  interface    text  formatter  in  extends    serializable    string  format  in  value  public    text  output  format    path  output  path  this  output  path  utf    public    text  output  format    path  output  path    string  charset  super  output  path  this  charset  name  charset  public    string  get  charset  name  return  charset  name  public  void  set  charset  name    string  charset  name  throws    illegal  charset  name  exception    unsupported  charset  exception  if  charset  name  null  throw  new    null  pointer  exception  if    charset  is  supported  charset  name  throw  new    unsupported  charset  exception    the  charset  charset  name  is  not  supported  this  charset  name  charset  name    override  public  void  open  int  task  number  int  num  tasks  throws    i  o  exception  super  open  task  number  num  tasks  try  this  charset    charset  for  name  charset  name  catch    illegal  charset  name  exception  e  throw  new    i  o  exception    the  charset  charset  name  is  not  valid  e  catch    unsupported  charset  exception  e  throw  new    i  o  exception    the  charset  charset  name  is  not  supported  e    override  public  void  write  record  t  record  throws    i  o  exception  byte  bytes  record  to  string  get  bytes  charset  this  stream  write  bytes  this  stream  write  newline    override  public    string  to  string  return    text  output  format  get  output  file  path  this  charset  name  
public  evolving  public  class    text  value  input  format  extends    delimited  input  format    string  value  private  static  final  long  serial  version  u  i  d  1  l  private    string  charset  name  utf    private  boolean  skip  invalid  lines  private  transient    charset  decoder  decoder  private  transient    byte  buffer  byte  wrapper  private  transient  boolean  ascii  public    text  value  input  format    path  file  path  super  file  path  null  public    string  get  charset  name  return  charset  name  public  void  set  charset  name    string  charset  name  if  charset  name  null  throw  new    illegal  argument  exception    the  charset  name  may  not  be  null  this  charset  name  charset  name  public  boolean  is  skip  invalid  lines  return  skip  invalid  lines  public  void  set  skip  invalid  lines  boolean  skip  invalid  lines  this  skip  invalid  lines  skip  invalid  lines    override  public  void  configure    configuration  parameters  super  configure  parameters  if  charset  name  null    charset  is  supported  charset  name  throw  new    runtime  exception    unsupported  charset  charset  name  if  charset  name  equals  ignore  case    standard  charsets  us  ascii  name  ascii  true  this  decoder    charset  for  name  charset  name  new  decoder  this  byte  wrapper    byte  buffer  allocate      override  public    string  value  read  record    string  value  reuse  byte  bytes  int  offset  int  num  bytes  if  this  ascii  reuse  set  value  ascii  bytes  offset  num  bytes  return  reuse  else    byte  buffer  byte  wrapper  this  byte  wrapper  if  bytes  byte  wrapper  array  byte  wrapper    byte  buffer  wrap  bytes    bytes  length  this  byte  wrapper  byte  wrapper  byte  wrapper  limit  offset  num  bytes  byte  wrapper  position  offset  try    char  buffer  result  this  decoder  decode  byte  wrapper  reuse  set  value  result  return  reuse  catch    character  coding  exception  e  if  skip  invalid  lines  return  null  else  byte  copy  new  byte  num  bytes    system  arraycopy  bytes  offset  copy    num  bytes  throw  new    runtime  exception    line  could  not  be  encoded    arrays  to  string  copy  e    override  public    string  to  string  return    text  value  input  format    arrays  to  string  get  file  paths  this  charset  name  this  skip  invalid  lines  skipping  invalid  lines    override  public  boolean  supports  multi  paths  return  true  
public  evolving  public  class    type  serializer  input  format  t  extends    binary  input  format  t  implements    result  type  queryable  t  private  static  final  long  serial  version  u  i  d    l  private  transient    type  information  t  result  type  private    type  serializer  t  serializer  public    type  serializer  input  format    type  information  t  result  type  this  result  type  result  type  todo  fix  this  shit  this  serializer  result  type  create  serializer  new    execution  config    override  protected  t  deserialize  t  reuse    data  input  view  data  input  throws    i  o  exception  return  serializer  deserialize  reuse  data  input    typing    override  public    type  information  t  get  produced  type  return  result  type  
public  evolving  public  class    type  serializer  output  format  t  extends    binary  output  format  t  implements    input  type  configurable  private  static  final  long  serial  version  u  i  d    l  private    type  serializer  t  serializer    override  protected  void  serialize  t  record    data  output  view  data  output  throws    i  o  exception  if  serializer  null  throw  new    runtime  exception    type  serializer  output  format  requires  a  type  serializer  to  be  defined  serializer  serialize  record  data  output  public  void  set  serializer    type  serializer  t  serializer  this  serializer  serializer    override    suppress  warnings  unchecked  public  void  set  input  type    type  information  type    execution  config  execution  config  serializer    type  serializer  t  type  create  serializer  execution  config  
public  evolving  public  class    boolean  column  summary  extends    column  summary  private  long  true  count  private  long  false  count  private  long  null  count  public    boolean  column  summary  long  true  count  long  false  count  long  null  count  this  true  count  true  count  this  false  count  false  count  this  null  count  null  count  public  long  get  true  count  return  true  count  public  long  get  false  count  return  false  count    the  number  of  non  null  values  in  this  column    override  public  long  get  non  null  count  return  true  count  false  count  public  long  get  null  count  return  null  count    override  public    string  to  string  return    boolean  column  summary  total  count  get  total  count  true  count  true  count  false  count  false  count  null  count  null  count  
public  evolving  public  abstract  class    column  summary    the  number  of  all  rows  in  this  column  including  both  nulls  and  non  nulls  public  long  get  total  count  return  get  null  count  get  non  null  count    the  number  of  non  null  values  in  this  column  public  abstract  long  get  non  null  count    the  number  of  null  values  in  this  column  public  abstract  long  get  null  count    true  if  this  column  contains  any  null  values  public  boolean  contains  null  return  get  null  count  0  l    true  if  this  column  contains  any  non  null  values  public  boolean  contains  non  null  return  get  non  null  count  0  l  
public  evolving  public  class    numeric  column  summary  t  extends    column  summary  implements  java  io    serializable  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  non  missing  count  count  of  elements  that  are  not  null    na  n  or    infinite  private  final  long  null  count  private  final  long  nan  count  always  zero  for  types  like    short    integer    long  private  final  long  infinity  count  always  zero  for  types  like    short    integer    long  private  final  t  min  private  final  t  max  private  final  t  sum  private  final    double  mean  private  final    double  variance  private  final    double  standard  deviation  public    numeric  column  summary  long  non  missing  count  long  null  count  long  nan  count  long  infinity  count  t  min  t  max  t  sum    double  mean    double  variance    double  standard  deviation  this  non  missing  count  non  missing  count  this  null  count  null  count  this  nan  count  nan  count  this  infinity  count  infinity  count  this  min  min  this  max  max  this  sum  sum  this  mean  mean  this  variance  variance  this  standard  deviation  standard  deviation    the  number  of  missing  values  where  missing  is  defined  as  null    na  n  or    infinity  p    these  values  are  ignored  in  some  calculations  like  mean  variance  and  standard  deviation  public  long  get  missing  count  return  null  count  nan  count  infinity  count    the  number  of  values  that  are  not  null    na  n  or    infinity  public  long  get  non  missing  count  return  non  missing  count    the  number  of  non  null  values  in  this  column    override  public  long  get  non  null  count  return  non  missing  count  nan  count  infinity  count    override  public  long  get  null  count  return  null  count    number  of  values  that  are    na  n  p  always  zero  for  types  like    short    integer    long  public  long  get  nan  count  return  nan  count    number  of  values  that  are  positive  or  negative  infinity  p  always  zero  for  types  like    short    integer    long  public  long  get  infinity  count  return  infinity  count  public  t  get  min  return  min  public  t  get  max  return  max  public  t  get  sum  return  sum    null    na  n  and    infinite  values  are  ignored  in  this  calculation  see  a  href  https  en  wikipedia  org  wiki    mean    arithmetic    mean  a  public    double  get  mean  return  mean    variance  is  a  measure  of  how  far  a  set  of  numbers  are  spread  out  p    null    na  n  and    infinite  values  are  ignored  in  this  calculation  see  a  href  https  en  wikipedia  org  wiki    variance    variance  a  public    double  get  variance  return  variance    standard    deviation  is  a  measure  of  variation  in  a  set  of  numbers    it  is  the  square  root  of  the  variance  p    null    na  n  and    infinite  values  are  ignored  in  this  calculation  see  a  href  https  en  wikipedia  org  wiki    standard  deviation    standard    deviation  a  public    double  get  standard  deviation  return  standard  deviation    override  public    string  to  string  return    numeric  column  summary  total  count  get  total  count  null  count  null  count  non  null  count  get  non  null  count  missing  count  get  missing  count  non  missing  count  non  missing  count  nan  count  nan  count  infinity  count  infinity  count  min  min  max  max  sum  sum  mean  mean  variance  variance  standard  deviation  standard  deviation  
public  evolving  public  class    object  column  summary  extends    column  summary  private  long  not  null  count  private  long  null  count  public    object  column  summary  long  not  null  count  long  null  count  this  not  null  count  not  null  count  this  null  count  null  count    the  number  of  non  null  values  in  this  column    override  public  long  get  non  null  count  return      override  public  long  get  null  count  return  null  count    override  public    string  to  string  return    object  column  summary  total  count  get  total  count  not  null  count  not  null  count  null  count  null  count  
public  evolving  public  class    string  column  summary  extends    column  summary  private  long  non  null  count  private  long  null  count  private  long  empty  count  private    integer  min  length  private    integer  max  length  private    double  mean  length  public    string  column  summary  long  non  null  count  long  null  count  long  empty  count    integer  min  length    integer  max  length    double  mean  length  this  non  null  count  non  null  count  this  null  count  null  count  this  empty  count  empty  count  this  min  length  min  length  this  max  length  max  length  this  mean  length  mean  length    override  public  long  get  non  null  count  return  non  null  count    override  public  long  get  null  count  return  null  count    number  of  empty  strings  e  g  java  lang    string  is  empty  public  long  get  empty  count  return  empty  count    shortest    string  length  public    integer  get  min  length  return  min  length    longest    string  length  public    integer  get  max  length  return  max  length  public    double  get  mean  length  return  mean  length    override  public    string  to  string  return    string  column  summary  total  count  get  total  count  non  null  count  non  null  count  null  count  null  count  empty  count  empty  count  min  length  min  length  max  length  max  length  mean  length  mean  length  
public  evolving  public  final  class    data  set  utils    method  that  goes  over  all  the  elements  in  each  partition  in  order  to  retrieve  the  total  number  of  elements  param  input  the    data  set  received  as  input  return  a  data  set  containing  tuples  of  subtask  index  number  of  elements  mappings  public  static  t    data  set    tuple2    integer    long  count  elements  per  partition    data  set  t  input  return  input  map  partition  new    rich  map  partition  function  t    tuple2    integer    long    override  public  void  map  partition    iterable  t  values    collector    tuple2    integer    long  out  throws    exception  long  counter    for  t  value  values  counter  out  collect  new    tuple2  get  runtime  context  get  index  of  this  subtask  counter    method  that  assigns  a  unique  link    long  value  to  all  elements  in  the  input  data  set    the  generated  values  are  consecutive  param  input  the  input  data  set  return  a  data  set  of  tuple    consisting  of  consecutive  ids  and  initial  values  public  static  t    data  set    tuple2    long  t  zip  with  index    data  set  t  input    data  set    tuple2    integer    long  element  count  count  elements  per  partition  input  return  input  map  partition  new    rich  map  partition  function  t    tuple2    long  t  long  start      override  public  void  open    configuration  parameters  throws    exception  super  open  parameters    list    tuple2    integer    long  offsets  get  runtime  context  get  broadcast  variable  with  initializer  counts  new    broadcast  variable  initializer    tuple2    integer    long    list    tuple2    integer    long    override  public    list    tuple2    integer    long  initialize  broadcast  variable    iterable    tuple2    integer    long  data  sort  the  list  by  task  id  to  calculate  the  correct  offset    list    tuple2    integer    long  sorted  data  new    array  list  for    tuple2    integer    long  datum  data  sorted  data  add  datum    collections  sort  sorted  data  new    comparator    tuple2    integer    long    override  public  int  compare    tuple2    integer    long  o1    tuple2    integer    long  o2  return  o1  f0  compare  to  o2  f0  return  sorted  data  compute  the  offset  for  each  partition  for  int  i    i  get  runtime  context  get  index  of  this  subtask  i  start  offsets  get  i  f1    override  public  void  map  partition    iterable  t  values    collector    tuple2    long  t  out  throws    exception  for  t  value  values  out  collect  new    tuple2  start  value  with  broadcast  set  element  count  counts    method  that  assigns  a  unique  link    long  value  to  all  elements  in  the  input  data  set  as  described  below  ul  li  a  map  function  is  applied  to  the  input  data  set  li  each  map  task  holds  a  counter  c  which  is  increased  for  each  record  li  c  is  shifted  by  n  bits  where  n  log2  number  of  parallel  tasks  li  to  create  a  unique  id  among  all  tasks  the  task  id  is  added  to  the  counter  li  for  each  record  the  resulting  counter  is  collected  ul  param  input  the  input  data  set  return  a  data  set  of  tuple    consisting  of  ids  and  initial  values  public  static  t    data  set    tuple2    long  t  zip  with  unique  id    data  set  t  input  return  input  map  partition  new    rich  map  partition  function  t    tuple2    long  t  long  max  bit  size  get  bit  size    long  max  value  long  shifter    long  start    long  task  id    long  label      override  public  void  open    configuration  parameters  throws    exception  super  open  parameters  shifter  get  bit  size  get  runtime  context  get  number  of  parallel  subtasks    task  id  get  runtime  context  get  index  of  this  subtask    override  public  void  map  partition    iterable  t  values    collector    tuple2    long  t  out  throws    exception  for  t  value  values  label  start  shifter  task  id  if  get  bit  size  start  shifter  max  bit  size  out  collect  new    tuple2  label  value  start  else  throw  new    exception    exceeded    long  value  range  while  generating  labels    sample    generate  a  sample  of    data  set  by  the  probability  fraction  of  each  element  param  with  replacement    whether  element  can  be  selected  more  than  once  param  fraction    probability  that  each  element  is  chosen  should  be  0  1  without  replacement  and    with  replacement    while  fraction  is  larger  than    the  elements  are  expected  to  be  selected  multi  times  into  sample  on  average  return    the  sampled    data  set  public  static  t    map  partition  operator  t  t  sample    data  set  t  input  final  boolean  with  replacement  final  double  fraction  return  sample  input  with  replacement  fraction    utils  rng  next  long    generate  a  sample  of    data  set  by  the  probability  fraction  of  each  element  param  with  replacement    whether  element  can  be  selected  more  than  once  param  fraction    probability  that  each  element  is  chosen  should  be  0  1  without  replacement  and    with  replacement    while  fraction  is  larger  than    the  elements  are  expected  to  be  selected  multi  times  into  sample  on  average  param  seed  random  number  generator  seed  return    the  sampled    data  set  public  static  t    map  partition  operator  t  t  sample    data  set  t  input  final  boolean  with  replacement  final  double  fraction  final  long  seed  return  input  map  partition  new    sample  with  fraction  t  with  replacement  fraction  seed    generate  a  sample  of    data  set  which  contains  fixed  size  elements  p  strong  note  strong    sample  with  fixed  size  is  not  as  efficient  as  sample  with  fraction  use  sample  with  fraction  unless  you  need  exact  precision  param  with  replacement    whether  element  can  be  selected  more  than  once  param  num  samples    the  expected  sample  size  return    the  sampled    data  set  public  static  t    data  set  t  sample  with  size    data  set  t  input  final  boolean  with  replacement  final  int  num  samples  return  sample  with  size  input  with  replacement  num  samples    utils  rng  next  long    generate  a  sample  of    data  set  which  contains  fixed  size  elements  p  strong  note  strong    sample  with  fixed  size  is  not  as  efficient  as  sample  with  fraction  use  sample  with  fraction  unless  you  need  exact  precision  param  with  replacement    whether  element  can  be  selected  more  than  once  param  num  samples    the  expected  sample  size  param  seed    random  number  generator  seed  return    the  sampled    data  set  public  static  t    data  set  t  sample  with  size    data  set  t  input  final  boolean  with  replacement  final  int  num  samples  final  long  seed    sample  in  partition  t  sample  in  partition  new    sample  in  partition  with  replacement  num  samples  seed    map  partition  operator  map  partition  operator  input  map  partition  sample  in  partition    there  is  no  previous  group  so  the  parallelism  of    group  reduce  operator  is  always      string  call  location    utils  get  call  location  name    sample  in  coordinator  t  sample  in  coordinator  new    sample  in  coordinator  with  replacement  num  samples  seed  return  new    group  reduce  operator  map  partition  operator  input  get  type  sample  in  coordinator  call  location    partition    range  partitions  a    data  set  on  the  specified  tuple  field  positions  public  static  t    partition  operator  t  partition  by  range    data  set  t  input    data  distribution  distribution  int  fields  return  new    partition  operator  input    partition  operator  base    partition  method  range  new    keys    expression  keys  fields  input  get  type  false  distribution    utils  get  call  location  name    range  partitions  a    data  set  on  the  specified  fields  public  static  t    partition  operator  t  partition  by  range    data  set  t  input    data  distribution  distribution    string  fields  return  new    partition  operator  input    partition  operator  base    partition  method  range  new    keys    expression  keys  fields  input  get  type  distribution    utils  get  call  location  name    range  partitions  a    data  set  using  the  specified  key  selector  function  public  static  t  k  extends    comparable  k    partition  operator  t  partition  by  range    data  set  t  input    data  distribution  distribution    key  selector  t  k  key  extractor  final    type  information  k  key  type    type  extractor  get  key  selector  types  key  extractor  input  get  type  return  new    partition  operator  input    partition  operator  base    partition  method  range  new    keys    selector  function  keys  input  clean  key  extractor  input  get  type  key  type  distribution    utils  get  call  location  name    summarize    summarize  a    data  set  of    tuples  by  collecting  single  pass  statistics  for  all  columns  p    example  usage  pre  code    dataset    tuple3    double    string    boolean  input    tuple3    numeric  column  summary    string  column  summary    boolean  column  summary  summary    data  set  utils  summarize  input  summary  f0  get  standard  deviation  summary  f1  get  max  length  pre  return  the  summary  as  a    tuple  the  same  width  as  input  rows  public  static  r  extends    tuple  t  extends    tuple  r  summarize    data  set  t  input  throws    exception  if  input  get  type  is  tuple  type  throw  new    illegal  argument  exception  summarize  is  only  implemented  for    data  set  s  of    tuples  final    tuple  type  info  base  in  type    tuple  type  info  base  input  get  type    data  set    tuple  summary  aggregator  r  result  input  map  partition  new    map  partition  function  t    tuple  summary  aggregator  r    override  public  void  map  partition    iterable  t  values    collector    tuple  summary  aggregator  r  out  throws    exception    tuple  summary  aggregator  r  aggregator    summary  aggregator  factory  create  in  type  for    tuple  value  values  aggregator  aggregate  value  out  collect  aggregator  reduce  new    reduce  function    tuple  summary  aggregator  r    override  public    tuple  summary  aggregator  r  reduce    tuple  summary  aggregator  r  agg1    tuple  summary  aggregator  r  agg2  throws    exception  agg1  combine  agg2  return  agg1  return  result  collect  get    result    checksum    convenience  method  to  get  the  count  number  of  elements  of  a    data  set  as  well  as  the  checksum  sum  over  element  hashes  return  a    checksum  hash  code  that  represents  the  count  and  checksum  of  elements  in  the  data  set  deprecated  replaced  with  code  org  apache  flink  graph  asm  dataset    checksum  hash  code  in    gelly    deprecated  public  static  t    utils    checksum  hash  code  checksum  hash  code    data  set  t  input  throws    exception  final    string  id  new    abstract  i  d  to  string  input  output  new    utils    checksum  hash  code  helper  t  id  name    checksum  hash  code    job  execution  result  res  input  get  execution  environment  execute  return  res    utils    checksum  hash  code  get  accumulator  result  id  util  methods  public  static  int  get  bit  size  long  value  if  value    integer  max  value  return      integer  number  of  leading  zeros  int  value    else  return      integer  number  of  leading  zeros  int  value    private  constructor  to  prevent  instantiation  private    data  set  utils  throw  new    runtime  exception  
public  evolving  public  class    multiple  parameter  tool  extends    abstract  parameter  tool  private  static  final  long  serial  version  u  i  d  1  l    constructors    returns  link    multiple  parameter  tool  for  the  given  arguments    the  arguments  are  keys  followed  by  values    keys  have  to  start  with  or  p  strong    example  arguments  strong  key1  value1  key2  value2  key3  value3  multi  multi  value1  multi  multi  value2  param  args    input  array  arguments  return  a  link    multiple  parameter  tool  public  static    multiple  parameter  tool  from  args    string  args  final    map    string    collection    string  map  new    hash  map  args  length    int  i    while  i  args  length  final    string  key    utils  get  key  from  args  args  i  i    try  to  find  the  value  map  put  if  absent  key  new    array  list  if  i  args  length  map  get  key  add  no  value  key  else  if    number  utils  is  number  args  i  map  get  key  add  args  i  i    else  if  args  i  starts  with  args  i  starts  with  the  argument  cannot  be  a  negative  number  because  we  checked  earlier  the  next  argument  is  a  parameter  name  map  get  key  add  no  value  key  else  map  get  key  add  args  i  i    return  from  multi  map  map    returns  link    multiple  parameter  tool  for  the  given  multi  map  param  multi  map  a  map  of  arguments    key  is    string  and  value  is  a    collection  return  a  link    multiple  parameter  tool  public  static    multiple  parameter  tool  from  multi  map    map    string    collection    string  multi  map    preconditions  check  not  null  multi  map    unable  to  initialize  from  empty  map  return  new    multiple  parameter  tool  multi  map    parameter  util  protected  final    map    string    collection    string  data  private    multiple  parameter  tool    map    string    collection    string  data  this  data    collections  unmodifiable  map  new    hash  map  data  this  default  data  new    concurrent  hash  map  data  size  this  unrequested  parameters    collections  new  set  from  map  new    concurrent  hash  map  data  size  unrequested  parameters  add  all  data  key  set    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    multiple  parameter  tool  that    multiple  parameter  tool  o  return    objects  equals  data  that  data    objects  equals  default  data  that  default  data    objects  equals  unrequested  parameters  that  unrequested  parameters    override  public  int  hash  code  return    objects  hash  data  default  data  unrequested  parameters    get  data  from  the  util    returns  number  of  parameters  in  link    parameter  tool    override  public  int  get  number  of  parameters  return  data  size    returns  the    string  value  for  the  given  key    the  value  should  only  have  one  item    use  link  get  multi  parameter    string  instead  if  want  to  get  multiple  values  parameter    if  the  key  does  not  exist  it  will  return  null    override  public    string  get    string  key  add  to  defaults  key  null  unrequested  parameters  remove  key  if  data  contains  key  key  return  null    preconditions  check  state  data  get  key  size      key  s  should  has  only  one  value  key  return    string  data  get  key  to  array      check  if  value  is  set    override  public  boolean  has    string  value  add  to  defaults  value  null  unrequested  parameters  remove  value  return  data  contains  key  value    returns  the    collection  of    string  values  for  the  given  key    if  the  key  does  not  exist  it  will  return  null  public    collection    string  get  multi  parameter    string  key  add  to  defaults  key  null  unrequested  parameters  remove  key  return  data  get  or  default  key  null    returns  the    collection  of    string  values  for  the  given  key    if  the  key  does  not  exist  it  will  throw  a  link    runtime  exception  public    collection    string  get  multi  parameter  required    string  key  add  to  defaults  key  null    collection    string  value  get  multi  parameter  key  if  value  null  throw  new    runtime  exception    no  data  for  required  key  key  return  value    export  to  different  targets    return    multi  map  of  all  the  parameters  processed  by  link    multiple  parameter  tool  return    multi  map  of  the  link    multiple  parameter  tool    key  is    string  and    value  is  a    collection  of    string  public    map    string    collection    string  to  multi  map  return  data    override  protected    object  clone  throws    clone  not  supported  exception  return  new    multiple  parameter  tool  this  data    interaction  with  other    parameter  utils    merges  two  link    multiple  parameter  tool  param  other    other  link    multiple  parameter  tool  object  return    the    merged  link    multiple  parameter  tool  public    multiple  parameter  tool  merge  with    multiple  parameter  tool  other  final    map    string    collection    string  result  data  new    hash  map  data  size  other  data  size  result  data  put  all  data  other  data  for  each  key  value  result  data  put  if  absent  key  new    array  list  result  data  get  key  add  all  value  final    multiple  parameter  tool  ret  new    multiple  parameter  tool  result  data  final    hash  set    string  requested  parameters  left  new    hash  set  data  key  set  requested  parameters  left  remove  all  unrequested  parameters  final    hash  set    string  requested  parameters  right  new    hash  set  other  data  key  set  requested  parameters  right  remove  all  other  unrequested  parameters  ret  unrequested  parameters  remove  all  requested  parameters  left  ret  unrequested  parameters  remove  all  requested  parameters  right  return  ret    execution  config    user  config  interface    override  public    map    string    string  to  map  return  get  flat  map  of  data  data    get  the  flat  map  of  the  multiple  map  data    if  the  key  have  multiple  values  only  the  last  one  will  be  used    this  is  also  the  current  behavior  when  multiple  parameters  is  specified  for  link    parameter  tool  param  data  multiple  map  of  data  return  flat  map  of  data  private  static    map    string    string  get  flat  map  of  data    map    string    collection    string  data  return  data  entry  set  stream  collect    collectors  to  map    map    entry  get  key  e  if  e  get  value  size    return    string  e  get  value  to  array  e  get  value  size    else  return  no  value  key    serialization  private  void  read  object    object  input  stream  in  throws    i  o  exception    class  not  found  exception  in  default  read  object  default  data  new    concurrent  hash  map  data  size  unrequested  parameters    collections  new  set  from  map  new    concurrent  hash  map  data  size  
public  evolving  public  class    kubernetes  config  options  public  static  final    config  option    string  context  key  kubernetes  context  string  type  no  default  value  with  description    the  desired  context  from  your    kubernetes  config  file  used  to  configure  the    kubernetes  client  for  interacting  with  the  cluster    this  could  be  helpful  if  one  has  multiple  contexts  configured  and  wants  to  administrate  different    flink  clusters  on  different    kubernetes  clusters  contexts  public  static  final    config  option    service  exposed  type  rest  service  exposed  type  key  kubernetes  rest  service  exposed  type  enum  type    service  exposed  type  class  default  value    service  exposed  type    load  balancer  with  description    the  type  of  the  rest  service    cluster  i  p  or    node  port  or    load  balancer    when  set  to    cluster  i  p  the  rest  service  will  not  be  created  public  static  final    config  option    string  job  manager  service  account  key  kubernetes  jobmanager  service  account  string  type  default  value  default  with  description    service  account  that  is  used  by  jobmanager  within  kubernetes  cluster    the  job  manager  uses  this  service  account  when  requesting  taskmanager  pods  from  the  api  server  public  static  final    config  option    double  job  manager  cpu  key  kubernetes  jobmanager  cpu  double  type  default  value  1.0  with  description    the  number  of  cpu  used  by  job  manager  public  static  final    config  option    double  task  manager  cpu  key  kubernetes  taskmanager  cpu  double  type  default  value  1.0  with  description    the  number  of  cpu  used  by  task  manager    by  default  the  cpu  is  set  to  the  number  of  slots  per    task  manager  public  static  final    config  option    image  pull  policy  container  image  pull  policy  key  kubernetes  container  image  pull  policy  enum  type    image  pull  policy  class  default  value    image  pull  policy    if  not  present  with  description    the    kubernetes  container  image  pull  policy    if  not  present  or    always  or    never    the  default  policy  is    if  not  present  to  avoid  putting  pressure  to  image  repository  public  static  final    config  option    list    string  container  image  pull  secrets  key  kubernetes  container  image  pull  secrets  string  type  as  list  no  default  value  with  description  a  semicolon  separated  list  of  the    kubernetes  secrets  used  to  access  private  image  registries  public  static  final    config  option    string  kube  config  file  key  kubernetes  config  file  string  type  no  default  value  with  description    the  kubernetes  config  file  will  be  used  to  create  the  client    the  default  is  located  at  kube  config  public  static  final    config  option    string  namespace  key  kubernetes  namespace  string  type  default  value  default  with  description    the  namespace  that  will  be  used  for  running  the  jobmanager  and  taskmanager  pods  public  static  final    config  option    string  container  start  command  template  key  kubernetes  container  start  command  template  string  type  default  value  java  classpath  jvmmem  jvmopts  logging  class  args  redirects  with  description    template  for  the  kubernetes  jobmanager  and  taskmanager  container  start  invocation  public  static  final    config  option    map    string    string  job  manager  labels  key  kubernetes  jobmanager  labels  map  type  no  default  value  with  description    the  labels  to  be  set  for    job  manager  pod    specified  as  key  value  pairs  separated  by  commas    for  example  version  alphav1  deploy  test  public  static  final    config  option    map    string    string  task  manager  labels  key  kubernetes  taskmanager  labels  map  type  no  default  value  with  description    the  labels  to  be  set  for    task  manager  pods    specified  as  key  value  pairs  separated  by  commas    for  example  version  alphav1  deploy  test  public  static  final    config  option    map    string    string  job  manager  node  selector  key  kubernetes  jobmanager  node  selector  map  type  no  default  value  with  description    the  node  selector  to  be  set  for    job  manager  pod    specified  as  key  value  pairs  separated  by  commas    for  example  environment  production  disk  ssd  public  static  final    config  option    map    string    string  task  manager  node  selector  key  kubernetes  taskmanager  node  selector  map  type  no  default  value  with  description    the  node  selector  to  be  set  for    task  manager  pods    specified  as  key  value  pairs  separated  by  commas    for  example  environment  production  disk  ssd  public  static  final    config  option    string  cluster  id  key  kubernetes  cluster  id  string  type  no  default  value  with  description    the  cluster  id  which  should  be  no  more  than    characters  is  used  for  identifying  a  unique    flink  cluster    if  not  set  the  client  will  automatically  generate  it  with  a  random  id    documentation    override  default    the  default  value  depends  on  the  actually  running  version    in  general  it  looks  like  flink  flink  version  scala  scala  version  public  static  final    config  option    string  container  image  key  kubernetes  container  image  string  type  default  value  get  default  flink  image  with  description    image  to  use  for    flink  containers    the  specified  image  must  be  based  upon  the  same    apache    flink  and    scala  versions  as  used  by  the  application    visit  https  hub  docker  com  flink  tab  tags  for  the  images  provided  by  the    flink  project    the  following  config  options  need  to  be  set  according  to  the  image  public  static  final    config  option    string  kubernetes  entry  path  key  kubernetes  entry  path  string  type  default  value  opt  flink  bin  kubernetes  entry  sh  with  description    the  entrypoint  script  of  kubernetes  in  the  image    it  will  be  used  as  command  for  jobmanager  and  taskmanager  container  public  static  final    config  option    string  flink  conf  dir  key  kubernetes  flink  conf  dir  string  type  default  value  opt  flink  conf  with  description    the  flink  conf  directory  that  will  be  mounted  in  pod    the  flink  conf  yaml  log4j  properties  logback  xml  in  this  path  will  be  overwritten  from  config  map  public  static  final    config  option    string  flink  log  dir  key  kubernetes  flink  log  dir  string  type  default  value  opt  flink  log  with  description    the  directory  that  logs  of  jobmanager  and  taskmanager  be  saved  in  the  pod  public  static  final    config  option    string  hadoop  conf  config  map  key  kubernetes  hadoop  conf  config  map  name  string  type  no  default  value  with  description    specify  the  name  of  an  existing    config  map  that  contains  custom    hadoop  configuration  to  be  mounted  on  the    job  manager  s  and    task  managers  public  static  final    config  option    map    string    string  job  manager  annotations  key  kubernetes  jobmanager  annotations  map  type  no  default  value  with  description    the  user  specified  annotations  that  are  set  to  the    job  manager  pod    the  value  could  be  in  the  form  of  a1  v1  a2  v2  public  static  final    config  option    map    string    string  task  manager  annotations  key  kubernetes  taskmanager  annotations  map  type  no  default  value  with  description    the  user  specified  annotations  that  are  set  to  the    task  manager  pod    the  value  could  be  in  the  form  of  a1  v1  a2  v2  public  static  final    config  option    list    map    string    string  job  manager  tolerations  key  kubernetes  jobmanager  tolerations  map  type  as  list  no  default  value  with  description    the  user  specified  tolerations  to  be  set  to  the    job  manager  pod    the  value  should  be  in  the  form  of  key  key1  operator    equal  value  value1  effect    no  schedule  key  key2  operator    exists  effect    no  execute  toleration  seconds    public  static  final    config  option    list    map    string    string  task  manager  tolerations  key  kubernetes  taskmanager  tolerations  map  type  as  list  no  default  value  with  description    the  user  specified  tolerations  to  be  set  to  the    task  manager  pod    the  value  should  be  in  the  form  of  key  key1  operator    equal  value  value1  effect    no  schedule  key  key2  operator    exists  effect    no  execute  toleration  seconds    public  static  final    config  option    map    string    string  rest  service  annotations  key  kubernetes  rest  service  annotations  map  type  no  default  value  with  description    the  user  specified  annotations  that  are  set  to  the  rest    service    the  value  should  be  in  the  form  of  a1  v1  a2  v2    defines  the  configuration  key  of  that  external  resource  in    kubernetes    this  is  used  as  a  suffix  in  an  actual  config  public  static  final    string  external  resource  kubernetes  config  key  suffix  kubernetes  config  key    if  configured    flink  will  add  resources  limits  gt  config  key  lt  and  resources  requests  gt  config  key  lt  to  the  main  container  of    task  executor  and  set  the  value  to  link    external  resource  options  external  resource  amount  p    it  is  intentionally  included  into  user  docs  while  unused    suppress  warnings  unused  public  static  final    config  option    string  external  resource  kubernetes  config  key  key    external  resource  options  generic  key  with  suffix  external  resource  kubernetes  config  key  suffix  string  type  no  default  value  with  description    if  configured    flink  will  add  resources  limits  config  key  and  resources  requests  config  key  to  the  main  container  of    task  executor  and  set  the  value  to  the  value  of    external  resource  options  external  resource  amount  key  private  static    string  get  default  flink  image    the  default  container  image  that  ties  to  the  exact  needed  versions  of  both    flink  and    scala  boolean  snapshot    environment  information  get  version  to  lower  case    locale  english  contains  snapshot    string  tag  snapshot  latest    environment  information  get  version  scala    environment  information  get  scala  version  return  flink  tag    the  flink  rest  service  exposed  type  public  enum    service  exposed  type    cluster  i  p    node  port    load  balancer    the  container  image  pull  policy  public  enum    image  pull  policy    if  not  present    always    never    this  class  is  not  meant  to  be  instantiated  private    kubernetes  config  options  
public  evolving  public  abstract  class    pattern  process  function  in  out  extends    abstract  rich  function    generates  resulting  elements  given  a  map  of  detected  pattern  events    the  events  are  identified  by  their  specified  names  p  link    pattern  process  function    context  timestamp  in  this  case  returns  the  time  of  the  last  element  that  was  assigned  to  the  match  resulting  in  this  partial  match  being  finished  param  match  map  containing  the  found  pattern    events  are  identified  by  their  names  param  ctx  enables  access  to  time  features  and  emitting  results  through  side  outputs  param  out    collector  used  to  output  the  generated  elements  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract  void  process  match  final    map    string    list  in  match  final    context  ctx  final    collector  out  out  throws    exception    gives  access  to  time  related  characteristics  as  well  as  enables  emitting  elements  to  side  outputs  public  interface    context  extends    time  context    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag  the  code    output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  x  void  output  final    output  tag  x  output  tag  final  x  value  
public  evolving  public  interface    timed  out  partial  match  handler  in    called  for  every  timed  out  partial  match  due  to  link  org  apache  flink  cep  pattern    pattern  within    time    it  enables  custom  handling  e  g  one  can  emit  the  timed  out  results  through  a  side  output  pre  code  private  final    output  tag  t  timed  out  partial  matches  tag  private  class    my  function  extends    pattern  process  function  in  out  implements    timed  out  partial  match  handler  in    override  public  void  process  match    map    string    list  in  match    context  ctx    collector  out  out  throws    exception    override  void  process  timed  out  match    map    string    list  in  match    pattern  process  function    context  ctx  throws    exception  ctx  output  timed  out  partial  matches  tag  match  pre  p  link    pattern  process  function    context  timestamp  in  this  case  returns  the  minimal  time  in  which  we  can  say  that  the  partial  match  will  not  become  a  match  which  is  effectively  the  timestamp  of  the  first  element  assigned  to  the  partial  match  plus  the  value  of  within  param  match  map  containing  the  timed  out  partial  match    events  are  identified  by  their  names  param  ctx  enables  access  to  time  features  and  emitting  results  through  side  outputs  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  void  process  timed  out  match  final    map    string    list  in  match  final    pattern  process  function    context  ctx  throws    exception  
public  evolving  public  abstract  class    iterative  condition  t  implements    function    serializable  private  static  final  long  serial  version  u  i  d    l    the  filter  function  that  evaluates  the  predicate  p  strong  important  strong    the  system  assumes  that  the  function  does  not  modify  the  elements  on  which  the  predicate  is  applied    violating  this  assumption  can  lead  to  incorrect  results  param  value    the  value  to  be  tested  param  ctx    the  link    context  used  for  the  evaluation  of  the  function  and  provides  access  to  the  already  accepted  events  in  the  pattern  see  link    context  get  events  for  pattern    string  return  code  true  for  values  that  should  be  retained  code  false  for  values  to  be  filtered  out  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract  boolean  filter  t  value    context  t  ctx  throws    exception    the  context  used  when  evaluating  the  link    iterative  condition  condition  public  interface    context  t  extends    time  context  return    an  link    iterable  over  the  already  accepted  elements  for  a  given  pattern    elements  are  iterated  in  the  order  they  were  inserted  in  the  pattern  param  name    the  name  of  the  pattern    iterable  t  get  events  for  pattern    string  name  throws    exception  
public  evolving  public  interface    time  context    timestamp  of  the  element  currently  being  processed  p    in  case  of  link  org  apache  flink  streaming  api    time  characteristic    processing  time  this  means  the  time  when  the  event  entered  the  cep  operator  long  timestamp    returns  the  current  processing  time  long  current  processing  time  
public  evolving    suppress  warnings    weaker  access  public  class    bootstrap  transformation  t    the  data  set  containing  the  data  to  bootstrap  the  operator  state  with  private  final    data  set  t  data  set    factory  for  the  link    stream  operator  to  consume  and  snapshot  the  bootstrapping  data  set  private  final    savepoint  writer  operator  factory  factory    partitioner  for  the  bootstrapping  data  set    only  relevant  if  this  bootstraps  partitioned  state    nullable  private  final    key  selector  t  original  key  selector    partitioner  for  distributing  data  by  key  group    only  relevant  if  this  bootstraps  partitioned  state    nullable  private  final    hash  selector  t  hash  key  selector    type  information  for  the  key  of  the  bootstrapped  state    only  relevant  if  this  bootstraps  partitioned  state    nullable  private  final    type  information  key  type    local  max  parallelism  for  the  bootstrapped  operator  private  final    optional  int  operator  max  parallelism    bootstrap  transformation    data  set  t  data  set    optional  int  operator  max  parallelism    savepoint  writer  operator  factory  factory  this  data  set  data  set  this  operator  max  parallelism  operator  max  parallelism  this  factory  factory  this  original  key  selector  null  this  hash  key  selector  null  this  key  type  null  k    bootstrap  transformation    data  set  t  data  set    optional  int  operator  max  parallelism    savepoint  writer  operator  factory  factory    nonnull    key  selector  t  k  key  selector    nonnull    type  information  k  key  type  this  data  set  data  set  this  operator  max  parallelism  operator  max  parallelism  this  factory  factory  this  original  key  selector  key  selector  this  hash  key  selector  new    hash  selector  key  selector  this  key  type  key  type  return    the  max  parallelism  for  this  operator  int  get  max  parallelism  int  global  max  parallelism  return  operator  max  parallelism  or  else  global  max  parallelism  param  operator  i  d    the  operator  id  for  the  stream  operator  param  state  backend    the  state  backend  for  the  job  param  global  max  parallelism    global  max  parallelism  set  for  the  savepoint  param  savepoint  path    the  path  where  the  savepoint  will  be  written  return    the  operator  subtask  states  for  this  bootstrap  transformation    data  set    operator  state  write  operator  state    operator  i  d  operator  i  d    state  backend  state  backend  int  global  max  parallelism    path  savepoint  path  int  local  max  parallelism  get  max  parallelism  global  max  parallelism  return  write  operator  subtask  states  operator  i  d  state  backend  savepoint  path  local  max  parallelism  reduce  group  new    operator  subtask  state  reducer  operator  i  d  local  max  parallelism  name  reduce    operator  subtask  state    visible  for  testing    map  partition  operator  t    tagged  operator  subtask  state  write  operator  subtask  states    operator  i  d  operator  i  d    state  backend  state  backend    path  savepoint  path  int  local  max  parallelism    data  set  t  input  data  set  if  original  key  selector  null  input  data  set  partition  custom  new    key  group  range  partitioner  local  max  parallelism  hash  key  selector    stream  operator    tagged  operator  subtask  state  operator  factory  create  operator    system  current  time  millis  savepoint  path  operator  data  set  clean  operator  final    stream  config  config  get  config  operator  i  d  state  backend  operator    bounded  one  input  stream  task  runner  t  operator  runner  new    bounded  one  input  stream  task  runner  config  local  max  parallelism    map  partition  operator  t    tagged  operator  subtask  state  subtask  states  input  map  partition  operator  runner  name  operator  i  d  to  hex  string  if  operator  instanceof    broadcast  state  bootstrap  operator  subtask  states  subtask  states  set  parallelism    else  int  current  parallelism  get  parallelism  subtask  states  if  current  parallelism  local  max  parallelism  subtask  states  set  parallelism  local  max  parallelism  return  subtask  states    visible  for  testing    stream  config  get  config    operator  i  d  operator  i  d    state  backend  state  backend    stream  operator    tagged  operator  subtask  state  operator    eagerly  perform  a  deep  copy  of  the  configuration  otherwise  it  will  result  in  undefined  behavior  when  deploying  with  multiple  bootstrap  transformations    configuration  deep  copy  new    configuration  data  set  get  execution  environment  get  configuration  final    stream  config  config  new    stream  config  deep  copy  config  set  chain  start  config  set  checkpointing  enabled  true  config  set  checkpoint  mode    checkpointing  mode  exactly  once  if  key  type  null    type  serializer  key  serializer  key  type  create  serializer  data  set  get  execution  environment  get  config  config  set  state  key  serializer  key  serializer  config  set  state  partitioner    original  key  selector  config  set  stream  operator  operator  config  set  operator  name  operator  i  d  to  hex  string  config  set  operator  i  d  operator  i  d  config  set  state  backend  state  backend  return  config  private  static  t  int  get  parallelism    map  partition  operator  t    tagged  operator  subtask  state  subtask  states  int  parallelism  subtask  states  get  parallelism  if  parallelism    execution  config  parallelism  default  parallelism  subtask  states  get  execution  environment  get  parallelism  return  parallelism  
public  evolving    suppress  warnings    weaker  access  public  class    existing  savepoint  extends    writable  savepoint    existing  savepoint    the  batch  execution  environment    used  for  creating  inputs  for  reading  state  private  final    execution  environment  env    the  savepoint  metadata  which  maintains  the  current  set  of  existing  newly  added  operator  states  private  final    savepoint  metadata  metadata    the  state  backend  that  was  previously  used  to  write  existing  operator  states  in  this  savepoint    this  is  also  the  state  backend  that  will  be  used  when  writing  again  this  existing  savepoint  private  final    state  backend  state  backend    existing  savepoint    execution  environment  env    savepoint  metadata  metadata    state  backend  state  backend  throws    i  o  exception  super  metadata  state  backend    preconditions  check  not  null  env    the  execution  environment  must  not  be  null    preconditions  check  not  null  metadata    the  savepoint  metadata  must  not  be  null    preconditions  check  not  null  state  backend    the  state  backend  must  not  be  null  this  env  env  this  metadata  metadata  this  state  backend  state  backend    read  operator  code    list  state  from  a  code    savepoint  param  uid    the  uid  of  the  operator  param  name    the  unique  name  for  the  state  param  type  info    the  type  of  the  elements  in  the  state  param  t    the  type  of  the  values  that  are  in  the  list  state  return  a  code    data  set  representing  the  elements  in  state  throws    i  o  exception    if  the  savepoint  path  is  invalid  or  the  uid  does  not  exist  public  t    data  set  t  read  list  state    string  uid    string  name    type  information  t  type  info  throws    i  o  exception    operator  state  operator  state  metadata  get  operator  state  uid    list  state  descriptor  t  descriptor  new    list  state  descriptor  name  type  info    list  state  input  format  t  input  format  new    list  state  input  format  operator  state  descriptor  return  env  create  input  input  format  type  info    read  operator  code    list  state  from  a  code    savepoint  when  a  custom  serializer  was  used  e  g  a  different  serializer  than  the  one  returned  by  code    type  information  create  serializer  param  uid    the  uid  of  the  operator  param  name    the  unique  name  for  the  state  param  type  info    the  type  of  the  elements  in  the  state  param  serializer    the  serializer  used  to  write  the  elements  into  state  param  t    the  type  of  the  values  that  are  in  the  list  state  return  a  code    data  set  representing  the  elements  in  state  throws    i  o  exception    if  the  savepoint  path  is  invalid  or  the  uid  does  not  exist  public  t    data  set  t  read  list  state    string  uid    string  name    type  information  t  type  info    type  serializer  t  serializer  throws    i  o  exception    operator  state  operator  state  metadata  get  operator  state  uid    list  state  descriptor  t  descriptor  new    list  state  descriptor  name  serializer    list  state  input  format  t  input  format  new    list  state  input  format  operator  state  descriptor  return  env  create  input  input  format  type  info    read  operator  code    union  state  from  a  code    savepoint  param  uid    the  uid  of  the  operator  param  name    the  unique  name  for  the  state  param  type  info    the  type  of  the  elements  in  the  state  param  t    the  type  of  the  values  that  are  in  the  union  state  return  a  code    data  set  representing  the  elements  in  state  throws    i  o  exception    if  the  savepoint  path  is  invalid  or  the  uid  does  not  exist  public  t    data  set  t  read  union  state    string  uid    string  name    type  information  t  type  info  throws    i  o  exception    operator  state  operator  state  metadata  get  operator  state  uid    list  state  descriptor  t  descriptor  new    list  state  descriptor  name  type  info    union  state  input  format  t  input  format  new    union  state  input  format  operator  state  descriptor  return  env  create  input  input  format  type  info    read  operator  code    union  state  from  a  code    savepoint  when  a  custom  serializer  was  used  e  g  a  different  serializer  than  the  one  returned  by  code    type  information  create  serializer  param  uid    the  uid  of  the  operator  param  name    the  unique  name  for  the  state  param  type  info    the  type  of  the  elements  in  the  state  param  serializer    the  serializer  used  to  write  the  elements  into  state  param  t    the  type  of  the  values  that  are  in  the  union  state  return  a  code    data  set  representing  the  elements  in  state  throws    i  o  exception    if  the  savepoint  path  is  invalid  or  the  uid  does  not  exist  public  t    data  set  t  read  union  state    string  uid    string  name    type  information  t  type  info    type  serializer  t  serializer  throws    i  o  exception    operator  state  operator  state  metadata  get  operator  state  uid    list  state  descriptor  t  descriptor  new    list  state  descriptor  name  serializer    union  state  input  format  t  input  format  new    union  state  input  format  operator  state  descriptor  return  env  create  input  input  format  type  info    read  operator  code    broadcast  state  from  a  code    savepoint  param  uid    the  uid  of  the  operator  param  name    the  unique  name  for  the  state  param  key  type  info    the  type  information  for  the  keys  in  the  state  param  value  type  info    the  type  information  for  the  values  in  the  state  param  k    the  type  of  keys  in  state  param  v    the  type  of  values  in  state  return  a  code    data  set  of  key  value  pairs  from  state  throws    i  o  exception    if  the  savepoint  does  not  contain  the  specified  uid  public  k  v    data  set    tuple2  k  v  read  broadcast  state    string  uid    string  name    type  information  k  key  type  info    type  information  v  value  type  info  throws    i  o  exception    operator  state  operator  state  metadata  get  operator  state  uid    map  state  descriptor  k  v  descriptor  new    map  state  descriptor  name  key  type  info  value  type  info    broadcast  state  input  format  k  v  input  format  new    broadcast  state  input  format  operator  state  descriptor  return  env  create  input  input  format  new    tuple  type  info  key  type  info  value  type  info    read  operator  code    broadcast  state  from  a  code    savepoint  when  a  custom  serializer  was  used  e  g  a  different  serializer  than  the  one  returned  by  code    type  information  create  serializer  param  uid    the  uid  of  the  operator  param  name    the  unique  name  for  the  state  param  key  type  info    the  type  information  for  the  keys  in  the  state  param  value  type  info    the  type  information  for  the  values  in  the  state  param  key  serializer    the  type  serializer  used  to  write  keys  into  the  state  param  value  serializer    the  type  serializer  used  to  write  values  into  the  state  param  k    the  type  of  keys  in  state  param  v    the  type  of  values  in  state  return  a  code    data  set  of  key  value  pairs  from  state  throws    i  o  exception    if  the  savepoint  path  is  invalid  or  the  uid  does  not  exist  public  k  v    data  set    tuple2  k  v  read  broadcast  state    string  uid    string  name    type  information  k  key  type  info    type  information  v  value  type  info    type  serializer  k  key  serializer    type  serializer  v  value  serializer  throws    i  o  exception    operator  state  operator  state  metadata  get  operator  state  uid    map  state  descriptor  k  v  descriptor  new    map  state  descriptor  name  key  serializer  value  serializer    broadcast  state  input  format  k  v  input  format  new    broadcast  state  input  format  operator  state  descriptor  return  env  create  input  input  format  new    tuple  type  info  key  type  info  value  type  info    read  keyed  state  from  an  operator  in  a  code    savepoint  param  uid    the  uid  of  the  operator  param  function    the  link    keyed  state  reader  function  that  is  called  for  each  key  in  state  param  k    the  type  of  the  key  in  state  param  out    the  output  type  of  the  transform  function  return  a  code    data  set  of  objects  read  from  keyed  state  throws    i  o  exception    if  the  savepoint  does  not  contain  operator  state  with  the  given  uid  public  k  out    data  set  out  read  keyed  state    string  uid    keyed  state  reader  function  k  out  function  throws    i  o  exception    type  information  k  key  type  info    type  information  out  out  type  try  key  type  info    type  extractor  create  type  info    keyed  state  reader  function  class  function  get  class    null  null  catch    invalid  types  exception  e  throw  new    invalid  program  exception    the  key  type  of  the    keyed  state  reader  function  could  not  be  automatically  determined    please  use    savepoint  read  keyed  state    string    keyed  state  reader  function    type  information    type  information  instead  e  try  out  type    type  extractor  get  unary  operator  return  type  function    keyed  state  reader  function  class        type  extractor  no  index  key  type  info    utils  get  call  location  name  false  catch    invalid  types  exception  e  throw  new    invalid  program  exception    the  output  type  of  the    keyed  state  reader  function  could  not  be  automatically  determined    please  use    savepoint  read  keyed  state    string    keyed  state  reader  function    type  information    type  information  instead  e  return  read  keyed  state  uid  function  key  type  info  out  type    read  keyed  state  from  an  operator  in  a  code    savepoint  param  uid    the  uid  of  the  operator  param  function    the  link    keyed  state  reader  function  that  is  called  for  each  key  in  state  param  key  type  info    the  type  information  of  the  key  in  state  param  out  type  info    the  type  information  of  the  output  of  the  transform  reader  function  param  k    the  type  of  the  key  in  state  param  out    the  output  type  of  the  transform  function  return  a  code    data  set  of  objects  read  from  keyed  state  throws    i  o  exception    if  the  savepoint  does  not  contain  operator  state  with  the  given  uid  public  k  out    data  set  out  read  keyed  state    string  uid    keyed  state  reader  function  k  out  function    type  information  k  key  type  info    type  information  out  out  type  info  throws    i  o  exception    operator  state  operator  state  metadata  get  operator  state  uid    keyed  state  input  format  k    void  namespace  out  input  format  new    keyed  state  input  format  operator  state  state  backend  env  get  configuration  new    keyed  state  reader  operator  function  key  type  info  return  env  create  input  input  format  out  type  info  
public  evolving  public  abstract  class    broadcast  state  bootstrap  function  in  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d  1  l    writes  the  given  value  to  operator  state    this  function  is  called  for  every  record  param  value    the  input  record  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract  void  process  element  in  value    context  ctx  throws    exception    context  that  link    broadcast  state  bootstrap  function  s  can  use  for  getting  additional  data  about  an  input  record  p    the  context  is  only  valid  for  the  duration  of  a  link  process  element    object    context  call    do  not  store  the  context  and  use  afterwards  public  interface    context    returns  the  current  processing  time  long  current  processing  time    fetches  the  link    broadcast  state  with  the  specified  name  param  descriptor  the  link    map  state  descriptor  of  the  state  to  be  fetched  return    the  required  link    broadcast  state  broadcast  state  k  v    broadcast  state  k  v  get  broadcast  state    map  state  descriptor  k  v  descriptor  
public  evolving  public  abstract  class    keyed  state  bootstrap  function  k  in  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d  1  l    process  one  element  from  the  input  stream  p    this  function  can  update  internal  state  or  set  timers  using  the  link    context  parameter  param  value    the  input  value  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract  void  process  element  in  value    context  ctx  throws    exception    information  available  in  an  invocation  of  link  process  element    object    context  public  abstract  class    context  a  link    timer  service  for  querying  time  and  registering  timers  public  abstract    timer  service  timer  service    get  key  of  the  element  being  processed  public  abstract  k  get  current  key  
public  evolving  public  abstract  class    keyed  state  reader  function  k  out  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d    l    initialization  method  for  the  function    it  is  called  before  link  read  key    object    context    collector  and  thus  suitable  for  one  time  setup  work  p    this  is  the  only  method  that  my  register  state  descriptors  within  a  code    keyed  state  reader  function  public  abstract  void  open    configuration  parameters  throws    exception    process  one  key  from  the  restored  state  backend  p    this  function  can  read  partitioned  state  from  the  restored  state  backend  and  output  zero  or  more  elements  using  the  link    collector  parameter  param  key    the  input  value  param  out    the  collector  for  returning  result  values  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract  void  read  key  k  key    context  ctx    collector  out  out  throws    exception    context  that  link    keyed  state  reader  function  s  can  use  for  getting  additional  data  about  an  input  record  p    the  context  is  only  valid  for  the  duration  of  a  link    keyed  state  reader  function  read  key    object    context    collector  call    do  not  store  the  context  and  use  afterwards  public  interface    context  return    all  registered  event  time  timers  for  the  current  key    set    long  registered  event  time  timers  throws    exception  return    all  registered  processing  time  timers  for  the  current  key    set    long  registered  processing  time  timers  throws    exception  
public  evolving  public  abstract  class    state  bootstrap  function  in  extends    abstract  rich  function  implements    checkpointed  function  private  static  final  long  serial  version  u  i  d  1  l    writes  the  given  value  to  operator  state    this  function  is  called  for  every  record  param  value    the  input  record  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract  void  process  element  in  value    context  ctx  throws    exception    context  that  link    state  bootstrap  function  s  can  use  for  getting  additional  data  about  an  input  record  p    the  context  is  only  valid  for  the  duration  of  a  link    state  bootstrap  function  process  element    object    context  call    do  not  store  the  context  and  use  afterwards  public  interface    context    returns  the  current  processing  time  long  current  processing  time  
public  evolving    suppress  warnings    weaker  access  public  class    keyed  operator  transformation  k  t    the  data  set  containing  the  data  to  bootstrap  the  operator  state  with  private  final    data  set  t  data  set    local  max  parallelism  for  the  bootstrapped  operator  private  final    optional  int  operator  max  parallelism    partitioner  for  the  bootstrapping  data  set  private  final    key  selector  t  k  key  selector    type  information  for  the  key  of  the  bootstrapped  operator  private  final    type  information  k  key  type    keyed  operator  transformation    data  set  t  data  set    optional  int  operator  max  parallelism    key  selector  t  k  key  selector    type  information  k  key  type  this  data  set  data  set  this  operator  max  parallelism  operator  max  parallelism  this  key  selector  key  selector  this  key  type  key  type    applies  the  given  link    keyed  state  bootstrap  function  on  the  keyed  input  p    the  function  will  be  called  for  every  element  in  the  input  and  can  be  used  for  writing  both  keyed  and  operator  state  into  a  link    savepoint  param  process  function    the  link    keyed  state  bootstrap  function  that  is  called  for  each  element  return    an  link    operator  transformation  that  can  be  added  to  a  link    savepoint  public    bootstrap  transformation  t  transform    keyed  state  bootstrap  function  k  t  process  function    savepoint  writer  operator  factory  factory  timestamp  path  new    keyed  state  bootstrap  operator  timestamp  path  process  function  return  transform  factory    method  for  passing  user  defined  operators  along  with  the  type  information  that  will  transform  the    operator  transformation  p  b  important  b    any  output  from  this  operator  will  be  discarded  param  factory  a  factory  returning  transformation  logic  type  of  the  return  stream  return    an  link    bootstrap  transformation  that  can  be  added  to  a  link    savepoint  public    bootstrap  transformation  t  transform    savepoint  writer  operator  factory  factory  return  new    bootstrap  transformation  data  set  operator  max  parallelism  factory  key  selector  key  type  
public  evolving  public  class    new  savepoint  extends    writable  savepoint    new  savepoint    new  savepoint    savepoint  metadata  metadata    state  backend  state  backend  super  metadata  state  backend  
public  evolving    suppress  warnings    weaker  access  public  class    one  input  operator  transformation  t    the  data  set  containing  the  data  to  bootstrap  the  operator  state  with  private  final    data  set  t  data  set    local  max  parallelism  for  the  bootstrapped  operator  private    optional  int  operator  max  parallelism    optional  int  empty    one  input  operator  transformation    data  set  t  data  set  this  data  set  data  set    sets  the  maximum  parallelism  of  this  operator  p    the  maximum  parallelism  specifies  the  upper  bound  for  dynamic  scaling    it  also  defines  the  number  of  key  groups  used  for  partitioned  state  param  max  parallelism    maximum  parallelism  return    the  operator  with  set  maximum  parallelism    public  evolving  public    one  input  operator  transformation  t  set  max  parallelism  int  max  parallelism  this  operator  max  parallelism    optional  int  of  max  parallelism  return  this    applies  the  given  link    state  bootstrap  function  on  the  non  keyed  input  p    the  function  will  be  called  for  every  element  in  the  input  and  can  be  used  for  writing  operator  state  into  a  link    savepoint  param  process  function    the  link    state  bootstrap  function  that  is  called  for  each  element  return    an  link    operator  transformation  that  can  be  added  to  a  link    savepoint  public    bootstrap  transformation  t  transform    state  bootstrap  function  t  process  function    savepoint  writer  operator  factory  factory  timestamp  path  new    state  bootstrap  operator  timestamp  path  process  function  return  transform  factory    applies  the  given  link    broadcast  state  bootstrap  function  on  the  non  keyed  input  p    the  function  will  be  called  for  every  element  in  the  input  and  can  be  used  for  writing  broadcast  state  into  a  link    savepoint  param  process  function    the  link    broadcast  state  bootstrap  function  that  is  called  for  each  element  return    an  link    bootstrap  transformation  that  can  be  added  to  a  link    savepoint  public    bootstrap  transformation  t  transform    broadcast  state  bootstrap  function  t  process  function    savepoint  writer  operator  factory  factory  timestamp  path  new    broadcast  state  bootstrap  operator  timestamp  path  process  function  return  transform  factory    method  for  passing  user  defined  operators  along  with  the  type  information  that  will  transform  the    operator  transformation  p  b  important  b    any  output  from  this  operator  will  be  discarded  param  factory  a  factory  returning  transformation  logic  type  of  the  return  stream  return    an  link    bootstrap  transformation  that  can  be  added  to  a  link    savepoint  public    bootstrap  transformation  t  transform    savepoint  writer  operator  factory  factory  return  new    bootstrap  transformation  data  set  operator  max  parallelism  factory    it  creates  a  new  link    keyed  operator  transformation  that  uses  the  provided  key  for  partitioning  its  operator  states  param  key  selector    the    key  selector  to  be  used  for  extracting  the  key  for  partitioning  return    the  code    bootstrap  transformation  with  partitioned  state  public  k    keyed  operator  transformation  k  t  key  by    key  selector  t  k  key  selector    type  information  k  key  type    type  extractor  get  key  selector  types  key  selector  data  set  get  type  return  new    keyed  operator  transformation  data  set  operator  max  parallelism  key  selector  key  type    it  creates  a  new  link    keyed  operator  transformation  that  uses  the  provided  key  with  explicit  type  information  for  partitioning  its  operator  states  param  key  selector    the    key  selector  to  be  used  for  extracting  the  key  for  partitioning  param  key  type    the  type  information  describing  the  key  type  return    the  code    bootstrap  transformation  with  partitioned  state  public  k    keyed  operator  transformation  k  t  key  by    key  selector  t  k  key  selector    type  information  k  key  type  return  new    keyed  operator  transformation  data  set  operator  max  parallelism  key  selector  key  type    partitions  the  operator  state  of  a  link    operator  transformation  by  the  given  key  positions  param  fields    the  position  of  the  fields  on  which  the  code    operator  transformation  will  be  grouped  return    the  code    operator  transformation  with  partitioned  state  public    keyed  operator  transformation    tuple  t  key  by  int  fields  if  data  set  get  type  instanceof    basic  array  type  info  data  set  get  type  instanceof    primitive  array  type  info  return  key  by    key  selector  util  get  selector  for  array  fields  data  set  get  type  else  return  key  by  new    keys    expression  keys  fields  data  set  get  type    partitions  the  operator  state  of  a  link    operator  transformation  using  field  expressions  a  field  expression  is  either  the  name  of  a  public  field  or  a  getter  method  with  parentheses  of  the  code    operator  transformation  s  underlying  type  a  dot  can  be  used  to  drill  down  into  objects  as  in  code  field1  get  inner  field2  param  fields    one  or  more  field  expressions  on  which  the  state  of  the  link    operator  transformation  operators  will  be  partitioned  return    the  code    operator  transformation  with  partitioned  state  i  e    keyed  stream  public    keyed  operator  transformation    tuple  t  key  by    string  fields  return  key  by  new    keys    expression  keys  fields  data  set  get  type  private    keyed  operator  transformation    tuple  t  key  by    keys  t  keys    key  selector  t    tuple  key  selector    key  selector  util  get  selector  for  keys  keys  data  set  get  type  data  set  get  execution  environment  get  config    type  information    tuple  key  type    type  extractor  get  key  selector  types  key  selector  data  set  get  type  return  new    keyed  operator  transformation  data  set  operator  max  parallelism  key  selector  key  type  
public  evolving    suppress  warnings    weaker  access  public  final  class    operator  transformation  private    operator  transformation    create  a  new  link    operator  transformation  from  a  link    data  set  param  data  set  a  dataset  of  elements  param  t    the  type  of  the  input  return  a  link    one  input  operator  transformation  public  static  t    one  input  operator  transformation  t  bootstrap  with    data  set  t  data  set  return  new    one  input  operator  transformation  data  set  
public  evolving  public  final  class    savepoint  private    savepoint    loads  an  existing  savepoint    useful  if  you  want  to  query  modify  or  extend  the  state  of  an  existing  application  param  env    the  execution  environment  used  to  transform  the  savepoint  param  path    the  path  to  an  existing  savepoint  on  disk  param  state  backend    the  state  backend  of  the  savepoint  public  static    existing  savepoint  load    execution  environment  env    string  path    state  backend  state  backend  throws    i  o  exception    checkpoint  metadata  metadata    savepoint  loader  load  savepoint  metadata  path  int  max  parallelism  metadata  get  operator  states  stream  map    operator  state  get  max  parallelism  max    comparator  natural  order  or  else  throw  new    runtime  exception    savepoint  must  contain  at  least  one  operator  state    savepoint  metadata  savepoint  metadata  new    savepoint  metadata  max  parallelism  metadata  get  master  states  metadata  get  operator  states  return  new    existing  savepoint  env  savepoint  metadata  state  backend    creates  a  new  savepoint  param  state  backend    the  state  backend  of  the  savepoint  used  for  keyed  state  param  max  parallelism    the  max  parallelism  of  the  savepoint  return  a  new  savepoint  public  static    new  savepoint  create    state  backend  state  backend  int  max  parallelism    preconditions  check  argument  max  parallelism    max  parallelism  upper  bound  max  parallelism    maximum  parallelism  must  be  between    and  upper  bound  max  parallelism    found  max  parallelism    savepoint  metadata  metadata  new    savepoint  metadata  max  parallelism    collections  empty  list    collections  empty  list  return  new    new  savepoint  metadata  state  backend  
public  evolving    functional  interface  public  interface    savepoint  writer  operator  factory    creates  a  link    stream  operator  to  be  used  for  generating  and  snapshotting  state  param  savepoint  timestamp  the  timestamp  to  associate  with  the  generated  savepoint  param  savepoint  path  the  path  to  write  the  savepoint  to  return  a  stream  operator  for  writing  the  savepoint    stream  operator    tagged  operator  subtask  state  create  operator  long  savepoint  timestamp    path  savepoint  path  
public  evolving    suppress  warnings    weaker  access  public  abstract  class    writable  savepoint  f  extends    writable  savepoint    the  savepoint  metadata  which  maintains  the  current  set  of  existing  newly  added  operator  states  protected  final    savepoint  metadata  metadata    the  state  backend  to  use  when  writing  this  savepoint  protected  final    state  backend  state  backend    writable  savepoint    savepoint  metadata  metadata    state  backend  state  backend    preconditions  check  not  null  metadata    the  savepoint  metadata  must  not  be  null    preconditions  check  not  null  state  backend    the  state  backend  must  not  be  null  this  metadata  metadata  this  state  backend  state  backend    drop  an  existing  operator  from  the  savepoint  param  uid    the  uid  of  the  operator  return  a  modified  savepoint    suppress  warnings  unchecked  public  f  remove  operator    string  uid  metadata  remove  operator  uid  return  f  this    adds  a  new  operator  to  the  savepoint  param  uid    the  uid  of  the  operator  param  transformation    the  operator  to  be  included  return    the  modified  savepoint    suppress  warnings  unchecked  public  t  f  with  operator    string  uid    bootstrap  transformation  t  transformation  metadata  add  operator  uid  transformation  return  f  this    write  out  a  new  or  updated  savepoint  param  path    the  path  to  where  the  savepoint  should  be  written  public  final  void  write    string  path  final    path  savepoint  path  new    path  path    list    bootstrap  transformation  with  i  d  new  operator  transformations  metadata  get  new  operators    data  set    operator  state  new  operator  states  write  operator  states  new  operator  transformations  savepoint  path    list    operator  state  existing  operators  metadata  get  existing  operators    data  set    operator  state  final  operator  states  union  operator  states  new  operator  states  existing  operators  final  operator  states  reduce  group  new    merge  operator  states  metadata  get  master  states  name  reduce    operator  state  output  new    savepoint  output  format  savepoint  path  name  path  private    data  set    operator  state  union  operator  states    data  set    operator  state  new  operator  states    list    operator  state  existing  operators    data  set    operator  state  final  operator  states  if  existing  operators  is  empty  final  operator  states  new  operator  states  else    data  set    operator  state  wrapped  collection  new  operator  states  get  execution  environment  from  collection  existing  operators  final  operator  states  new  operator  states  union  wrapped  collection  return  final  operator  states  private    data  set    operator  state  write  operator  states    list    bootstrap  transformation  with  i  d  new  operator  states    path  savepoint  write  path  return  new  operator  states  stream  map  new  operator  state  new  operator  state  get  bootstrap  transformation  write  operator  state  new  operator  state  get  operator  i  d  state  backend  metadata  get  max  parallelism  savepoint  write  path  reduce    data  set  union  or  else  throw  new    illegal  state  exception    savepoint  s  must  contain  at  least  one  operator  
public  evolving  public  abstract  class    scheduled  dropwizard  reporter  implements    metric  reporter    scheduled    reporter    character  filter  protected  final    logger  log    logger  factory  get  logger  get  class  public  static  final    string  arg  host  host  public  static  final    string  arg  port  port  public  static  final    string  arg  prefix  prefix  public  static  final    string  arg  conversion  rate  rate  conversion  public  static  final    string  arg  conversion  duration  duration  conversion  protected  final    metric  registry  registry  protected    scheduled  reporter  reporter  private  final    map    gauge    string  gauges  new    hash  map  private  final    map    counter    string  counters  new    hash  map  private  final    map    histogram    string  histograms  new    hash  map  private  final    map    meter    string  meters  new    hash  map  protected    scheduled  dropwizard  reporter  this  registry  new    metric  registry    getters    visible  for  testing    map    counter    string  get  counters  return  counters    visible  for  testing    map    meter    string  get  meters  return  meters    visible  for  testing    map    gauge    string  get  gauges  return  gauges    visible  for  testing    map    histogram    string  get  histograms  return  histograms  life  cycle    override  public  void  open    metric  config  config  this  reporter  get  reporter  config    override  public  void  close  this  reporter  stop  adding  removing  metrics    override  public  void  notify  of  added  metric    metric  metric    string  metric  name    metric  group  group  final    string  full  name  group  get  metric  identifier  metric  name  this  synchronized  this  if  metric  instanceof    counter  counters  put    counter  metric  full  name  registry  register  full  name  new    flink  counter  wrapper    counter  metric  else  if  metric  instanceof    gauge  gauges  put    gauge  metric  full  name  registry  register  full  name    flink  gauge  wrapper  from  gauge    gauge  metric  else  if  metric  instanceof    histogram    histogram  histogram    histogram  metric  histograms  put  histogram  full  name  if  histogram  instanceof    dropwizard  histogram  wrapper  registry  register  full  name    dropwizard  histogram  wrapper  histogram  get  dropwizard  histogram  else  registry  register  full  name  new    flink  histogram  wrapper  histogram  else  if  metric  instanceof    meter    meter  meter    meter  metric  meters  put  meter  full  name  if  meter  instanceof    dropwizard  meter  wrapper  registry  register  full  name    dropwizard  meter  wrapper  meter  get  dropwizard  meter  else  registry  register  full  name  new    flink  meter  wrapper  meter  else  log  warn    cannot  add  metric  of  type    this  indicates  that  the  reporter  does  not  support  this  metric  type  metric  get  class  get  name    override  public  void  notify  of  removed  metric    metric  metric    string  metric  name    metric  group  group  synchronized  this    string  full  name  if  metric  instanceof    counter  full  name  counters  remove  metric  else  if  metric  instanceof    gauge  full  name  gauges  remove  metric  else  if  metric  instanceof    histogram  full  name  histograms  remove  metric  else  if  metric  instanceof    meter  full  name  meters  remove  metric  else  full  name  null  if  full  name  null  registry  remove  full  name    override  public    string  filter  characters    string  metric  name  char  chars  null  final  int  str  len  metric  name  length  int  pos    for  int  i    i  str  len  i  final  char  c  metric  name  char  at  i  switch  c  case  if  chars  null  chars  metric  name  to  char  array  chars  pos  break  case  if  chars  null  chars  metric  name  to  char  array  break  default  if  chars  null  chars  pos  c  pos  return  chars  null  metric  name  new    string  chars    pos  scheduled  reporting    override  public  void  report  we  do  not  need  to  lock  here  because  the  dropwizard  registry  is  internally  a  concurrent  map    suppress  warnings  rawtypes  final    sorted  map    string  com  codahale  metrics    gauge  gauges  registry  get  gauges  final    sorted  map    string  com  codahale  metrics    counter  counters  registry  get  counters  final    sorted  map    string  com  codahale  metrics    histogram  histograms  registry  get  histograms  final    sorted  map    string  com  codahale  metrics    meter  meters  registry  get  meters  final    sorted  map    string  com  codahale  metrics    timer  timers  registry  get  timers  this  reporter  report  gauges  counters  histograms  meters  timers  public  abstract    scheduled  reporter  get  reporter    metric  config  config  
public  evolving    instantiate  via  factory  factory  class  name  org  apache  flink  metrics  graphite    graphite  reporter  factory  public  class    graphite  reporter  extends    scheduled  dropwizard  reporter  public  static  final    string  arg  protocol  protocol  private  enum    protocol  tcp  udp    override  public    scheduled  reporter  get  reporter    metric  config  config    string  host  config  get  string  arg  host  null  int  port  config  get  integer  arg  port    if  host  null  host  length    port    throw  new    illegal  argument  exception    invalid  host  port  configuration    host  host    port  port    string  prefix  config  get  string  arg  prefix  null    string  conversion  rate  config  get  string  arg  conversion  rate  null    string  conversion  duration  config  get  string  arg  conversion  duration  null    string  protocol  config  get  string  arg  protocol  tcp  com  codahale  metrics  graphite    graphite  reporter    builder  builder  com  codahale  metrics  graphite    graphite  reporter  for  registry  registry  if  prefix  null  builder  prefixed  with  prefix  if  conversion  rate  null  builder  convert  rates  to    time  unit  value  of  conversion  rate  if  conversion  duration  null  builder  convert  durations  to    time  unit  value  of  conversion  duration    protocol  prot  try  prot    protocol  value  of  protocol  catch    illegal  argument  exception  iae  log  warn    invalid  protocol  configuration  protocol    expected  tcp  or  udp  defaulting  to  tcp  prot    protocol  tcp  log  info    configured    graphite  reporter  with  host  port  protocol  host  port  prot  switch  prot  case  udp  return  builder  build  new    graphite  u  d  p  host  port  case  tcp  default  return  builder  build  new    graphite  host  port  
public  evolving  public  abstract  class    abstract  prometheus  reporter  implements    metric  reporter  protected  final    logger  log    logger  factory  get  logger  get  class  private  static  final    pattern  unallowed  char  pattern    pattern  compile  a  z  a    z0    private  static  final    character  filter  character  filter  new    character  filter    override  public    string  filter  characters    string  input  return  replace  invalid  chars  input  private  static  final  char  scope  separator  private  static  final    string  scope  prefix  flink  scope  separator  private  final    map    string    abstract  map    simple  immutable  entry    collector    integer  collectors  with  count  by  metric  name  new    hash  map    visible  for  testing  static    string  replace  invalid  chars  final    string  input  https  prometheus  io  docs  instrumenting  writing  exporters    only  a  z  a    z0    are  valid  in  metric  names  any  other  characters  should  be  sanitized  to  an  underscore  return  unallowed  char  pattern  matcher  input  replace  all  private    character  filter  label  value  characters  filter  character  filter    override  public  void  open    metric  config  config  boolean  filter  label  value  characters  config  get  boolean  filter  label  value  character  key  filter  label  value  character  default  value  if  filter  label  value  characters  label  value  characters  filter  input  input    override  public  void  close    collector  registry  default  registry  clear    override  public  void  notify  of  added  metric  final    metric  metric  final    string  metric  name  final    metric  group  group    list    string  dimension  keys  new    linked  list    list    string  dimension  values  new    linked  list  for  final    map    entry    string    string  dimension  group  get  all  variables  entry  set  final    string  key  dimension  get  key  dimension  keys  add  character  filter  filter  characters  key  substring    key  length    dimension  values  add  label  value  characters  filter  filter  characters  dimension  get  value  final    string  scoped  metric  name  get  scoped  name  metric  name  group  final    string  help  string  metric  name  scope  get  logical  scope  group  final    collector  collector    integer  count    synchronized  this  if  collectors  with  count  by  metric  name  contains  key  scoped  metric  name  final    abstract  map    simple  immutable  entry    collector    integer  collector  with  count  collectors  with  count  by  metric  name  get  scoped  metric  name  collector  collector  with  count  get  key  count  collector  with  count  get  value  else  collector  create  collector  metric  dimension  keys  dimension  values  scoped  metric  name  help  string  try  collector  register  catch    exception  e  log  warn    there  was  a  problem  registering  metric  metric  name  e  add  metric  metric  dimension  values  collector  collectors  with  count  by  metric  name  put  scoped  metric  name  new    abstract  map    simple  immutable  entry  collector  count    private  static    string  get  scoped  name    string  metric  name    metric  group  group  return  scope  prefix  get  logical  scope  group  scope  separator  character  filter  filter  characters  metric  name  private    collector  create  collector    metric  metric    list    string  dimension  keys    list    string  dimension  values    string  scoped  metric  name    string  help  string    collector  collector  if  metric  instanceof    gauge  metric  instanceof    counter  metric  instanceof    meter  collector  io  prometheus  client    gauge  build  name  scoped  metric  name  help  help  string  label  names  to  array  dimension  keys  create  else  if  metric  instanceof    histogram  collector  new    histogram  summary  proxy    histogram  metric  scoped  metric  name  help  string  dimension  keys  dimension  values  else  log  warn    cannot  create  collector  for  unknown  metric  type    this  indicates  that  the  metric  type  is  not  supported  by  this  reporter  metric  get  class  get  name  collector  null  return  collector  private  void  add  metric    metric  metric    list    string  dimension  values    collector  collector  if  metric  instanceof    gauge  io  prometheus  client    gauge  collector  set  child  gauge  from    gauge  metric  to  array  dimension  values  else  if  metric  instanceof    counter  io  prometheus  client    gauge  collector  set  child  gauge  from    counter  metric  to  array  dimension  values  else  if  metric  instanceof    meter  io  prometheus  client    gauge  collector  set  child  gauge  from    meter  metric  to  array  dimension  values  else  if  metric  instanceof    histogram    histogram  summary  proxy  collector  add  child    histogram  metric  dimension  values  else  log  warn    cannot  add  unknown  metric  type    this  indicates  that  the  metric  type  is  not  supported  by  this  reporter  metric  get  class  get  name  private  void  remove  metric    metric  metric    list    string  dimension  values    collector  collector  if  metric  instanceof    gauge  io  prometheus  client    gauge  collector  remove  to  array  dimension  values  else  if  metric  instanceof    counter  io  prometheus  client    gauge  collector  remove  to  array  dimension  values  else  if  metric  instanceof    meter  io  prometheus  client    gauge  collector  remove  to  array  dimension  values  else  if  metric  instanceof    histogram    histogram  summary  proxy  collector  remove  dimension  values  else  log  warn    cannot  remove  unknown  metric  type    this  indicates  that  the  metric  type  is  not  supported  by  this  reporter  metric  get  class  get  name    override  public  void  notify  of  removed  metric  final    metric  metric  final    string  metric  name  final    metric  group  group    list    string  dimension  values  new    linked  list  for  final    map    entry    string    string  dimension  group  get  all  variables  entry  set  dimension  values  add  label  value  characters  filter  filter  characters  dimension  get  value  final    string  scoped  metric  name  get  scoped  name  metric  name  group  synchronized  this  final    abstract  map    simple  immutable  entry    collector    integer  collector  with  count  collectors  with  count  by  metric  name  get  scoped  metric  name  final    integer  count  collector  with  count  get  value  final    collector  collector  collector  with  count  get  key  remove  metric  metric  dimension  values  collector  if  count    try    collector  registry  default  registry  unregister  collector  catch    exception  e  log  warn    there  was  a  problem  unregistering  metric  scoped  metric  name  e  collectors  with  count  by  metric  name  remove  scoped  metric  name  else  collectors  with  count  by  metric  name  put  scoped  metric  name  new    abstract  map    simple  immutable  entry  collector  count      suppress  warnings  unchecked  private  static    string  get  logical  scope    metric  group  group  return    front  metric  group    abstract  metric  group  group  get  logical  scope  character  filter  scope  separator    visible  for  testing  io  prometheus  client    gauge    child  gauge  from    gauge  gauge  return  new  io  prometheus  client    gauge    child    override  public  double  get  final    object  value  gauge  get  value  if  value  null  log  debug    gauge  is  null  valued  defaulting  to    gauge  return    if  value  instanceof    double  return  double  value  if  value  instanceof    number  return    number  value  double  value  if  value  instanceof    boolean  return    boolean  value      log  debug    invalid  type  for    gauge  only  number  types  and  booleans  are  supported  by  this  reporter  gauge  value  get  class  get  name  return    private  static  io  prometheus  client    gauge    child  gauge  from    counter  counter  return  new  io  prometheus  client    gauge    child    override  public  double  get  return  double  counter  get  count  private  static  io  prometheus  client    gauge    child  gauge  from    meter  meter  return  new  io  prometheus  client    gauge    child    override  public  double  get  return  meter  get  rate    visible  for  testing  static  class    histogram  summary  proxy  extends    collector  static  final    list    double  quantiles    arrays  as  list              private  final    string  metric  name  private  final    string  help  string  private  final    list    string  label  names  with  quantile  private  final    map    list    string    histogram  histograms  by  label  values  new    hash  map    histogram  summary  proxy  final    histogram  histogram  final    string  metric  name  final    string  help  string  final    list    string  label  names  final    list    string  label  values  this  metric  name  metric  name  this  help  string  help  string  this  label  names  with  quantile  add  to  list  label  names  quantile  histograms  by  label  values  put  label  values  histogram    override  public    list    metric  family  samples  collect    we  cannot  use    summary  metric  family  because  it  is  impossible  to  get  a  sum  of  all  values  at  least  for    dropwizard  histograms  whose  snapshot  s  values  array  only  holds  a  sample  of  recent  values    list    metric  family  samples    sample  samples  new    linked  list  for    map    entry    list    string    histogram  label  values  to  histogram  histograms  by  label  values  entry  set  add  samples  label  values  to  histogram  get  key  label  values  to  histogram  get  value  samples  return    collections  singleton  list  new    metric  family  samples  metric  name    type  summary  help  string  samples  void  add  child  final    histogram  histogram  final    list    string  label  values  histograms  by  label  values  put  label  values  histogram  void  remove  final    list    string  label  values  histograms  by  label  values  remove  label  values  private  void  add  samples  final    list    string  label  values  final    histogram  histogram  final    list    metric  family  samples    sample  samples  samples  add  new    metric  family  samples    sample  metric  name    count  label  names  with  quantile  sub  list    label  names  with  quantile  size    label  values  histogram  get  count  final    histogram  statistics  statistics  histogram  get  statistics  for  final    double  quantile  quantiles  samples  add  new    metric  family  samples    sample  metric  name  label  names  with  quantile  add  to  list  label  values  quantile  to  string  statistics  get  quantile  quantile  private  static    list    string  add  to  list    list    string  list    string  element  final    list    string  result  new    array  list  list  result  add  element  return  result  private  static    string  to  array    list    string  list  return  list  to  array  new    string  list  size  
public  evolving    instantiate  via  factory  factory  class  name  org  apache  flink  metrics  prometheus    prometheus  push  gateway  reporter  factory  public  class    prometheus  push  gateway  reporter  extends    abstract  prometheus  reporter  implements    scheduled  private    push  gateway  push  gateway  private    string  job  name  private  boolean  delete  on  shutdown  private    map    string    string  grouping  key    override  public  void  open    metric  config  config  super  open  config    string  host  config  get  string  host  key  host  default  value  int  port  config  get  integer  port  key  port  default  value    string  configured  job  name  config  get  string  job  name  key  job  name  default  value  boolean  random  suffix  config  get  boolean  random  job  name  suffix  key  random  job  name  suffix  default  value  delete  on  shutdown  config  get  boolean  delete  on  shutdown  key  delete  on  shutdown  default  value  grouping  key  parse  grouping  key  config  get  string  grouping  key  key  grouping  key  default  value  if  host  null  host  is  empty  port    throw  new    illegal  argument  exception    invalid  host  port  configuration    host  host    port  port  if  random  suffix  this  job  name  configured  job  name  new    abstract  i  d  else  this  job  name  configured  job  name  push  gateway  new    push  gateway  host  port  log  info    configured    prometheus  push  gateway  reporter  with  host  port  job  name  random  job  name  suffix  delete  on  shutdown  grouping  key  host  port  job  name  random  suffix  delete  on  shutdown  grouping  key    map    string    string  parse  grouping  key  final    string  grouping  key  config  if  grouping  key  config  is  empty    map    string    string  grouping  key  new    hash  map    string  kvs  grouping  key  config  split  for    string  kv  kvs  int  idx  kv  index  of  if  idx    log  warn    invalid  prometheus  push  gateway  grouping  key  will  be  ignored  kv  continue    string  label  key  kv  substring    idx    string  label  value  kv  substring  idx    if    string  utils  is  null  or  whitespace  only  label  key    string  utils  is  null  or  whitespace  only  label  value  log  warn    invalid  grouping  key  label  key  label  value  must  not  be  empty  label  key  label  value  continue  grouping  key  put  label  key  label  value  return  grouping  key  return    collections  empty  map    override  public  void  report  try  push  gateway  push    collector  registry  default  registry  job  name  grouping  key  catch    exception  e  log  warn    failed  to  push  metrics  to    push  gateway  with  job  name  grouping  key  job  name  grouping  key  e    override  public  void  close  if  delete  on  shutdown  push  gateway  null  try  push  gateway  delete  job  name  grouping  key  catch    i  o  exception  e  log  warn    failed  to  delete  metrics  from    push  gateway  with  job  name  grouping  key  job  name  grouping  key  e  super  close  
public  evolving    instantiate  via  factory  factory  class  name  org  apache  flink  metrics  prometheus    prometheus  reporter  factory  public  class    prometheus  reporter  extends    abstract  prometheus  reporter  static  final    string  arg  port  port  private  static  final    string  default  port    private    h  t  t  p  server  http  server  private  int  port    visible  for  testing  int  get  port    preconditions  check  state  http  server  null    server  has  not  been  initialized  return  port    override  public  void  open    metric  config  config  super  open  config    string  ports  config  config  get  string  arg  port  default  port    iterator    integer  ports    net  utils  get  port  range  from  string  ports  config  while  ports  has  next  int  port  ports  next  try  internally  accesses    collector  registry  default  registry  http  server  new    h  t  t  p  server  port  this  port  port  log  info    started    prometheus  reporter  http  server  on  port  port  break  catch    i  o  exception  ioe  assume  port  conflict  log  debug    could  not  start    prometheus  reporter  http  server  on  port  port  ioe  if  http  server  null  throw  new    runtime  exception    could  not  start    prometheus  reporter  http  server  on  any  configured  port    ports  ports  config    override  public  void  close  if  http  server  null  http  server  stop  super  close  
public  evolving    instantiate  via  factory  factory  class  name  org  apache  flink  metrics  statsd    stats  d  reporter  factory  public  class    stats  d  reporter  extends    abstract  reporter  implements    scheduled  private  static  final    logger  log    logger  factory  get  logger    stats  d  reporter  class  public  static  final    string  arg  host  host  public  static  final    string  arg  port  port  private  boolean  closed  false  private    datagram  socket  socket  private    inet  socket  address  address    override  public  void  open    metric  config  config    string  host  config  get  string  arg  host  null  int  port  config  get  integer  arg  port    if  host  null  host  length    port    throw  new    illegal  argument  exception    invalid  host  port  configuration    host  host    port  port  this  address  new    inet  socket  address  host  port  try  this  socket  new    datagram  socket    catch    socket  exception  e  throw  new    runtime  exception    could  not  create  datagram  socket  e  log  info    configured    stats  d  reporter  with  host  port  host  port    override  public  void  close  closed  true  if  socket  null  socket  is  closed  socket  close    override  public  void  report  instead  of  locking  here  we  tolerate  exceptions  we  do  this  to  prevent  holding  the  lock  for  very  long  and  blocking  operator  creation  and  shutdown  try  for    map    entry    gauge    string  entry  gauges  entry  set  if  closed  return  report  gauge  entry  get  value  entry  get  key  for    map    entry    counter    string  entry  counters  entry  set  if  closed  return  report  counter  entry  get  value  entry  get  key  for    map    entry    histogram    string  entry  histograms  entry  set  report  histogram  entry  get  value  entry  get  key  for    map    entry    meter    string  entry  meters  entry  set  report  meter  entry  get  value  entry  get  key  catch    concurrent  modification  exception    no  such  element  exception  e  ignore  may  happen  when  metrics  are  concurrently  added  or  removed  report  next  time  private  void  report  counter  final    string  name  final    counter  counter  send  name  counter  get  count  private  void  report  gauge  final    string  name  final    gauge  gauge    object  value  gauge  get  value  if  value  null  return  if  value  instanceof    number  send  number  is  negative    number  value  name  value  to  string  send  name  value  to  string  private  void  report  histogram  final    string  name  final    histogram  histogram  if  histogram  null    histogram  statistics  statistics  histogram  get  statistics  if  statistics  null  send  prefix  name  count  histogram  get  count  send  prefix  name  max  statistics  get  max  send  prefix  name  min  statistics  get  min  send  prefix  name  mean  statistics  get  mean  send  prefix  name  stddev  statistics  get  std  dev  send  prefix  name  p50  statistics  get  quantile  0.5  send  prefix  name  p75  statistics  get  quantile  0.75  send  prefix  name  p95  statistics  get  quantile  0.95  send  prefix  name  p98  statistics  get  quantile  0.98  send  prefix  name  p99  statistics  get  quantile  0.99  send  prefix  name  p999  statistics  get  quantile  0.999  private  void  report  meter  final    string  name  final    meter  meter  if  meter  null  send  prefix  name  rate  meter  get  rate  send  prefix  name  count  meter  get  count  private    string  prefix    string  names  if  names  length      string  builder  string  builder  new    string  builder  names    for  int  i    i  names  length  i  string  builder  append  append  names  i  return  string  builder  to  string  else  return  private  void  send    string  name  double  value  send  number  is  negative  value  name    string  value  of  value  private  void  send    string  name  long  value  send  value    name    string  value  of  value  private  void  send  boolean  reset  to  zero    string  name    string  value  if  reset  to  zero  negative  values  are  interpreted  as  reductions  instead  of  absolute  values  reset  value  to    before  applying  reduction  as  a  workaround  send  name    send  name  value  private  void  send  final    string  name  final    string  value  try    string  formatted    string  format  s  s  g  name  value  byte  data  formatted  get  bytes    standard  charsets  utf    socket  send  new    datagram  packet  data  data  length  this  address  catch    i  o  exception  e  log  error  unable  to  send  packet  to  statsd  at  address  get  host  name  address  get  port    override  public    string  filter  characters    string  input  char  chars  null  final  int  str  len  input  length  int  pos    for  int  i    i  str  len  i  final  char  c  input  char  at  i  switch  c  case  if  chars  null  chars  input  to  char  array  chars  pos  break  default  if  chars  null  chars  pos  c  pos  return  chars  null  input  new    string  chars    pos  private  boolean  number  is  negative    number  input  return    double  compare  input  double  value      
public  evolving  public  interface    estimator  e  extends    estimator  e  m  m  extends    model  m  extends    pipeline  stage  e    train  and  produce  a  link    model  which  fits  the  records  in  the  given  link    table  param  t  env  the  table  environment  to  which  the  input  table  is  bound  param  input  the  table  with  records  to  train  the    model  return  a  model  trained  to  fit  on  the  given    table  m  fit    table  environment  t  env    table  input  
public  evolving  public  interface    model  m  extends    model  m  extends    transformer  m  
public  evolving  public  final  class    pipeline  implements    estimator    pipeline    pipeline    transformer    pipeline    model    pipeline  private  static  final  long  serial  version  u  i  d  1  l  private  final    list    pipeline  stage  stages  new    array  list  private  final    params  params  new    params  private  int  last  estimator  index    public    pipeline  public    pipeline    string  pipeline  json  this  load  json  pipeline  json  public    pipeline    list    pipeline  stage  stages  for    pipeline  stage  s  stages  append  stage  s  is  the  stage  a  simple    estimator  or  pipeline  with    estimator  private  static  boolean  is  stage  need  fit    pipeline  stage  stage  return  stage  instanceof    pipeline    pipeline  stage  need  fit  stage  instanceof    pipeline  stage  instanceof    estimator    appends  a    pipeline  stage  to  the  tail  of  this  pipeline    pipeline  is  editable  only  via  this  method    the    pipeline  stage  must  be    estimator    transformer    model  or    pipeline  param  stage  the  stage  to  be  appended  public    pipeline  append  stage    pipeline  stage  stage  if  is  stage  need  fit  stage  last  estimator  index  stages  size  else  if  stage  instanceof    transformer  throw  new    runtime  exception    all    pipeline  stages  should  be    estimator  or    transformer  got  stage  get  class  get  simple  name  stages  add  stage  return  this    returns  a  list  of  all  stages  in  this  pipeline  in  order  the  list  is  immutable  return  an  immutable  list  of  all  stages  in  this  pipeline  in  order  public    list    pipeline  stage  get  stages  return    collections  unmodifiable  list  stages    check  whether  the  pipeline  acts  as  an  link    estimator  or  not    when  the  return  value  is  true  that  means  this  pipeline  contains  an  link    estimator  and  thus  users  must  invoke  link  fit    table  environment    table  before  they  can  use  this  pipeline  as  a  link    transformer    otherwise  the  pipeline  can  be  used  as  a  link    transformer  directly  return  code  true  if  this  pipeline  has  an    estimator  code  false  otherwise  public  boolean  need  fit  return  this  get  index  of  last  estimator    public    params  get  params  return  params  find  the  last    estimator  or    pipeline  that  needs  fit  in  stages    stand  for  no    estimator  in    pipeline  private  int  get  index  of  last  estimator  return  last  estimator  index    train  the  pipeline  to  fit  on  the  records  in  the  given  link    table  p    this  method  go  through  all  the  link    pipeline  stage  s  in  order  and  does  the  following  on  each  stage  until  the  last  link    estimator  inclusive  ul  li    if  a  stage  is  an  link    estimator  invoke  link    estimator  fit    table  environment    table  with  the  input  table  to  generate  a  link    model  transform  the  the  input  table  with  the  generated  link    model  to  get  a  result  table  then  pass  the  result  table  to  the  next  stage  as  input  li  li    if  a  stage  is  a  link    transformer  invoke  link    transformer  transform    table  environment    table  on  the  input  table  to  get  a  result  table  and  pass  the  result  table  to  the  next  stage  as  input  li  ul  p    after  all  the  link    estimator  s  are  trained  to  fit  their  input  tables  a  new  pipeline  will  be  created  with  the  same  stages  in  this  pipeline  except  that  all  the    estimators  in  the  new  pipeline  are  replaced  with  their  corresponding    models  generated  in  the  above  process  p    if  there  is  no  link    estimator  in  the  pipeline  the  method  returns  a  copy  of  this  pipeline  param  t  env  the  table  environment  to  which  the  input  table  is  bound  param  input  the  table  with  records  to  train  the    pipeline  return  a  pipeline  with  same  stages  as  this    pipeline  except  all    estimators  replaced  with  their  corresponding    models    override  public    pipeline  fit    table  environment  t  env    table  input    list    pipeline  stage  transform  stages  new    array  list  stages  size  int  last  estimator  idx  get  index  of  last  estimator  for  int  i    i  stages  size  i    pipeline  stage  s  stages  get  i  if  i  last  estimator  idx    transformer  t  boolean  need  fit  is  stage  need  fit  s  if  need  fit  t    estimator  s  fit  t  env  input  else  stage  is    transformer  guaranteed  in  append  stage  method  t    transformer  s  transform  stages  add  t  input  t  transform  t  env  input  else  transform  stages  add  s  return  new    pipeline  transform  stages    generate  a  result  table  by  applying  all  the  stages  in  this  pipeline  to  the  input  table  in  order  param  t  env  the  table  environment  to  which  the  input  table  is  bound  param  input  the  table  to  be  transformed  return  a  result  table  with  all  the  stages  applied  to  the  input  tables  in  order    override  public    table  transform    table  environment  t  env    table  input  if  need  fit  throw  new    runtime  exception    pipeline  contains    estimator  need  to  fit  first  for    pipeline  stage  s  stages  input    transformer  s  transform  t  env  input  return  input    override  public    string  to  json    object  mapper  mapper  new    object  mapper    list    map    string    string  stage  jsons  new    array  list  for    pipeline  stage  s  get  stages    map    string    string  stage  map  new    hash  map  stage  map  put  stage  class  name  s  get  class  get  type  name  stage  map  put  stage  json  s  to  json  stage  jsons  add  stage  map  try  return  mapper  write  value  as  string  stage  jsons  catch    json  processing  exception  e  throw  new    runtime  exception    failed  to  serialize  pipeline  e    override    suppress  warnings  unchecked  public  void  load  json    string  json    object  mapper  mapper  new    object  mapper    list    map    string    string  stage  jsons  try  stage  jsons  mapper  read  value  json    list  class  catch    i  o  exception  e  throw  new    runtime  exception    failed  to  deserialize  pipeline  json  json  e  for    map    string    string  stage  map  stage  jsons  append  stage  restore  inner  stage  stage  map  private    pipeline  stage  restore  inner  stage    map    string    string  stage  map    string  class  name  stage  map  get  stage  class  name    class  clz  try  clz    class  for  name  class  name  catch    class  not  found  exception  e  throw  new    runtime  exception    pipeline  stage  class  class  name  not  exists  e    instantiation  util  check  for  instantiation  clz    pipeline  stage  s  try  s    pipeline  stage  clz  new  instance  catch    exception  e  throw  new    runtime  exception    class  is  instantiable  but  failed  to  new  an  instance  e    string  stage  json  stage  map  get  stage  json  s  load  json  stage  json  return  s  
public  evolving  public  interface    transformer  t  extends    transformer  t  extends    pipeline  stage  t    applies  the  transformer  on  the  input  table  and  returns  the  result  table  param  t  env  the  table  environment  to  which  the  input  table  is  bound  param  input  the  table  to  be  transformed  return  the  transformed  table    table  transform    table  environment  t  env    table  input  
public  evolving  public  class    param  info  v  private  final    string  name  private  final    string  alias  private  final    string  description  private  final  boolean  is  optional  private  final  boolean  has  default  value  private  final  v  default  value  private  final    param  validator  v  validator  private  final    class  v  value  class    param  info    string  name    string  alias    string  description  boolean  is  optional  boolean  has  default  value  v  default  value    param  validator  v  validator    class  v  value  class  this  name  name  this  alias  alias  this  description  description  this  is  optional  is  optional  this  has  default  value  has  default  value  this  default  value  default  value  this  validator  validator  this  value  class  value  class    returns  the  name  of  the  parameter    the  name  must  be  unique  in  the  stage  the    param  info  belongs  to  return  the  name  of  the  parameter  public    string  get  name  return  name    returns  the  aliases  of  the  parameter    the  alias  will  be  an  empty  string  array  by  default  return  the  aliases  of  the  parameter  public    string  get  alias    preconditions  check  not  null  alias  return  alias    returns  the  description  of  the  parameter  return  the  description  of  the  parameter  public    string  get  description  return  description    returns  whether  the  parameter  is  optional  return  code  true  if  the  param  is  optional  code  false  otherwise  public  boolean  is  optional  return  is  optional    returns  whether  the  parameter  has  a  default  value    since  code  null  may  also  be  a  valid  default  value  of  a  parameter  the  return  of  get  default  value  may  be  code  null  even  when  this  method  returns  true  return  code  true  if  the  param  is  has  a  default  value  even  if  it  s  a  code  null  code  false  otherwise  public  boolean  has  default  value  return  has  default  value    returns  the  default  value  of  the  parameter    the  default  value  should  be  defined  whenever  possible    the  default  value  can  be  a  code  null  even  if  has  default  value  returns  true  return  the  default  value  of  the  param  code  null  if  not  defined  public  v  get  default  value  return  default  value    returns  the  validator  to  validate  the  value  of  the  parameter  return  the  validator  to  validate  the  value  of  the  parameter  public    param  validator  v  get  validator  return  validator    returns  the  class  of  the  param  value    it  s  usually  needed  in  serialization  return  the  class  of  the  param  value  public    class  v  get  value  class  return  value  class  
public  evolving  public  class    params  implements    serializable    cloneable  private  static  final  long  serial  version  u  i  d  1  l  a  mapping  from  param  name  to  its  value  p    the  value  is  stored  in  map  using  json  format  private  final    map    string    string  params  private  transient    object  mapper  mapper  public    params  this  params  new    hash  map    return  the  number  of  params  return    return  the  number  of  params  public  int  size  return  params  size    removes  all  of  the  params    the  params  will  be  empty  after  this  call  returns  public  void  clear  params  clear    returns  tt  true  tt  if  this  params  contains  no  mappings  return  tt  true  tt  if  this  map  contains  no  mappings  public  boolean  is  empty  return  params  is  empty    returns  the  value  of  the  specific  parameter  or  default  value  defined  in  the  code  info  if  this    params  doesn  t  have  a  value  set  for  the  parameter    an  exception  will  be  thrown  in  the  following  cases  because  no  value  could  be  found  for  the  specified  parameter  ul  li    non  optional  parameter  no  value  is  defined  in  this  params  for  a  non  optional  parameter  li  li    optional  parameter  no  value  is  defined  in  this  params  and  no  default  value  is  defined  li  ul  param  info  the  info  of  the  specific  parameter  usually  with  default  value  param  v  the  type  of  the  specific  parameter  return  the  value  of  the  specific  parameter  or  default  value  defined  in  the  code  info  if  this    params  doesn  t  contain  the  parameter  throws    illegal  argument  exception  if  no  value  can  be  found  for  specified  parameter  public  v  v  get    param  info  v  info    string  value  null    string  used  param  name  null  for    string  name  or  alias  get  param  name  and  alias  info  if  params  contains  key  name  or  alias  if  used  param  name  null  throw  new    illegal  argument  exception    string  format    duplicate  parameters  of  s  and  s  used  param  name  name  or  alias  used  param  name  name  or  alias  value  params  get  name  or  alias  if  used  param  name  null    the  param  value  was  set  by  the  user  return  value  from  json  value  info  get  value  class  else    the  param  value  was  not  set  by  the  user  if  info  is  optional  throw  new    illegal  argument  exception    missing  non  optional  parameter  info  get  name  else  if  info  has  default  value  throw  new    illegal  argument  exception    cannot  find  default  value  for  optional  parameter  info  get  name  return  info  get  default  value    set  the  value  of  the  specific  parameter  param  info  the  info  of  the  specific  parameter  to  set  param  value  the  value  to  be  set  to  the  specific  parameter  param  v  the  type  of  the  specific  parameter  return  the  previous  value  of  the  specific  parameter  or  null  if  this    params  didn  t  contain  the  parameter  before  throws    runtime  exception  if  the  code  info  has  a  validator  and  the  code  value  is  evaluated  as  illegal  by  the  validator  public  v    params  set    param  info  v  info  v  value  if  info  get  validator  null  info  get  validator  validate  value  throw  new    runtime  exception    setting  info  get  name  as  a  invalid  value  value  params  put  info  get  name  value  to  json  value  return  this    removes  the  specific  parameter  from  this    params  param  info  the  info  of  the  specific  parameter  to  remove  param  v  the  type  of  the  specific  parameter  public  v  void  remove    param  info  v  info  params  remove  info  get  name  for    string  a  info  get  alias  params  remove  a    check  whether  this  params  has  a  value  set  for  the  given  code  info  return  tt  true  tt  if  this  params  has  a  value  set  for  the  specified  code  info  false  otherwise  public  v  boolean  contains    param  info  v  info  return  params  contains  key  info  get  name    arrays  stream  info  get  alias  any  match  params  contains  key    returns  a  json  containing  all  parameters  in  this    params    the  json  should  be  human  readable  if  possible  return  a  json  containing  all  parameters  in  this    params  public    string  to  json  assert  mapper  inited  try  return  mapper  write  value  as  string  params  catch    json  processing  exception  e  throw  new    runtime  exception    failed  to  serialize  params  to  json  e    restores  the  parameters  from  the  given  json    the  parameters  should  be  exactly  the  same  with  the  one  who  was  serialized  to  the  input  json  after  the  restoration  param  json  the  json    string  to  restore  from    suppress  warnings  unchecked  public  void  load  json    string  json  assert  mapper  inited    map    string    string  params  try  params  mapper  read  value  json    map  class  catch    i  o  exception  e  throw  new    runtime  exception    failed  to  deserialize  json  json  e  this  params  put  all  params    factory  method  for  constructing  params  param  json  the  json  string  to  load  return  the  code    params  loaded  from  the  json  string  public  static    params  from  json    string  json    params  params  new    params  params  load  json  json  return  params    merge  other  params  into  this  param  other  params  other  params  return  this  public    params  merge    params  other  params  if  other  params  null  this  params  put  all  other  params  params  return  this    creates  and  returns  a  deep  clone  of  this    params  return  a  deep  clone  of  this    params    override  public    params  clone    params  new  params  new    params  new  params  params  put  all  this  params  return  new  params  private  void  assert  mapper  inited  if  mapper  null  mapper  new    object  mapper  private    string  value  to  json    object  value  assert  mapper  inited  try  if  value  null  return  null  return  mapper  write  value  as  string  value  catch    json  processing  exception  e  throw  new    runtime  exception    failed  to  serialize  to  json  value  e  private  t  t  value  from  json    string  json    class  t  clazz  assert  mapper  inited  try  if  json  null  return  null  return  mapper  read  value  json  clazz  catch    i  o  exception  e  throw  new    runtime  exception    failed  to  deserialize  json  json  e  private  v    list    string  get  param  name  and  alias    param  info  v  info    list    string  param  names  new    array  list  info  get  alias  length    param  names  add  info  get  name  param  names  add  all    arrays  as  list  info  get  alias  return  param  names  
public  evolving  public  interface    param  validator  v  extends    serializable    validates  a  parameter  value  param  value  value  to  validate  return  code  true  if  the  value  is  valid  code  false  otherwise  boolean  validate  v  value  
public  evolving  public  class    python  options    the  maximum  number  of  elements  to  include  in  a  bundle  public  static  final    config  option    integer  max  bundle  size    config  options  key  python  fn  execution  bundle  size  default  value    with  description    the  maximum  number  of  elements  to  include  in  a  bundle  for    python  user  defined  function  execution    the  elements  are  processed  asynchronously    one  bundle  of  elements  are  processed  before  processing  the  next  bundle  of  elements  a  larger  value  can  improve  the  throughput  but  at  the  cost  of  more  memory  usage  and  higher  latency    the  maximum  time  to  wait  before  finalising  a  bundle  in  milliseconds  public  static  final    config  option    long  max  bundle  time  mills    config  options  key  python  fn  execution  bundle  time  default  value    l  with  description    sets  the  waiting  timeout  in  milliseconds  before  processing  a  bundle  for    python  user  defined  function  execution    the  timeout  defines  how  long  the  elements  of  a  bundle  will  be  buffered  before  being  processed    lower  timeouts  lead  to  lower  tail  latencies  but  may  affect  throughput    the  maximum  number  of  elements  to  include  in  an  arrow  batch  public  static  final    config  option    integer  max  arrow  batch  size    config  options  key  python  fn  execution  arrow  batch  size  default  value    with  description    the  maximum  number  of  elements  to  include  in  an  arrow  batch  for    python  user  defined  function  execution    the  arrow  batch  size  should  not  exceed  the  bundle  size    otherwise  the  bundle  size  will  be  used  as  the  arrow  batch  size    the  amount  of  memory  to  be  allocated  by  the    python  framework  public  static  final    config  option    string  python  framework  memory  size    config  options  key  python  fn  execution  framework  memory  size  default  value  64mb  with  description    the  amount  of  memory  to  be  allocated  by  the    python  framework    the  sum  of  the  value  of  this  configuration  and  python  fn  execution  buffer  memory  size  represents  the  total  memory  of  a    python  worker    the  memory  will  be  accounted  as  managed  memory  if  the  actual  memory  allocated  to  an  operator  is  no  less  than  the  total  memory  of  a    python  worker    otherwise  this  configuration  takes  no  effect    the  amount  of  memory  to  be  allocated  by  the  input  output  buffer  of  a    python  worker  public  static  final    config  option    string  python  data  buffer  memory  size    config  options  key  python  fn  execution  buffer  memory  size  default  value  15mb  with  description    the  amount  of  memory  to  be  allocated  by  the  input  buffer  and  output  buffer  of  a    python  worker    the  memory  will  be  accounted  as  managed  memory  if  the  actual  memory  allocated  to  an  operator  is  no  less  than  the  total  memory  of  a    python  worker    otherwise  this  configuration  takes  no  effect    the  configuration  to  enable  or  disable  metric  for    python  execution  public  static  final    config  option    boolean  python  metric  enabled    config  options  key  python  metric  enabled  default  value  true  with  description    when  it  is  false  metric  for    python  will  be  disabled    you  can  disable  the  metric  to  achieve  better  performance  at  some  circumstance  public  static  final    config  option    string  python  files    config  options  key  python  files  string  type  no  default  value  with  description    attach  custom  python  files  for  job    these  files  will  be  added  to  the  pythonpath  of  both  the  local  client  and  the  remote  python  udf  worker    the  standard  python  resource  file  suffixes  such  as  py  egg  zip  or  directory  are  all  supported    comma  could  be  used  as  the  separator  to  specify  multiple  files    the  option  is  equivalent  to  the  command  line  option  pyfs  public  static  final    config  option    string  python  requirements    config  options  key  python  requirements  string  type  no  default  value  with  description    specify  a  requirements  txt  file  which  defines  the  third  party  dependencies    these  dependencies  will  be  installed  and  added  to  the  pythonpath  of  the  python  udf  worker  a  directory  which  contains  the  installation  packages  of  these  dependencies  could  be  specified  optionally    use  as  the  separator  if  the  optional  parameter  exists    the  option  is  equivalent  to  the  command  line  option  pyreq  public  static  final    config  option    string  python  archives    config  options  key  python  archives  string  type  no  default  value  with  description    add  python  archive  files  for  job    the  archive  files  will  be  extracted  to  the  working  directory  of  python  udf  worker    currently  only  zip  format  is  supported    for  each  archive  file  a  target  directory  is  specified    if  the  target  directory  name  is  specified  the  archive  file  will  be  extracted  to  a  name  can  directory  with  the  specified  name    otherwise  the  archive  file  will  be  extracted  to  a  directory  with  the  same  name  of  the  archive  file    the  files  uploaded  via  this  option  are  accessible  via  relative  path  could  be  used  as  the  separator  of  the  archive  file  path  and  the  target  directory  name    comma  could  be  used  as  the  separator  to  specify  multiple  archive  files    this  option  can  be  used  to  upload  the  virtual  environment  the  data  files  used  in    python  udf    the  data  files  could  be  accessed  in    python  udf  e  g  f  open  data  data  txt  r    the  option  is  equivalent  to  the  command  line  option  pyarch  public  static  final    config  option    string  python  executable    config  options  key  python  executable  string  type  default  value  python  with  description    specify  the  path  of  the  python  interpreter  used  to  execute  the  python  udf  worker    the  python  udf  worker  depends  on    python  3.5    apache    beam  version  2.19      pip  version  7.1    and    setup  tools  version  37.0      please  ensure  that  the  specified  environment  meets  the  above  requirements    the  option  is  equivalent  to  the  command  line  option  pyexec  public  static  final    config  option    string  python  client  executable    config  options  key  python  client  executable  string  type  default  value  python  with  description    the  python  interpreter  used  to  launch  the  python  process  when  compiling  the  jobs  containing    python    u  d  fs    equivalent  to  the  environment  variable  pyflink  executable    the  priority  is  as  following    the  configuration  python  client  executable  defined  in  the  source  code    the  environment  variable  pyflink  executable    the  configuration  python  client  executable  defined  in  flink  conf  yaml    whether  the  memory  used  by  the    python  framework  is  managed  memory  public  static  final    config  option    boolean  use  managed  memory    config  options  key  python  fn  execution  memory  managed  default  value  false  with  description    string  format    if  set  the    python  worker  will  configure  itself  to  use  the  managed  memory  budget  of  the  task  slot    otherwise  it  will  use  the    off    heap    memory  of  the  task  slot    in  this  case  users  should  set  the    task    off    heap    memory  using  the  configuration  key  s    for  each    python  worker  the  required    task    off    heap    memory  is  the  sum  of  the  value  of  s  and  s    task  manager  options  task  off  heap  memory  key  python  framework  memory  size  key  python  data  buffer  memory  size  key  
public  evolving  public  class    queryable  state  client  private  static  final    logger  log    logger  factory  get  logger    queryable  state  client  class  private  static  final    map    class  extends    state  descriptor    state  factory  state  factories    stream  of    tuple2  of    value  state  descriptor  class    state  factory    immutable  value  state  create  state    tuple2  of    list  state  descriptor  class    state  factory    immutable  list  state  create  state    tuple2  of    map  state  descriptor  class    state  factory    immutable  map  state  create  state    tuple2  of    aggregating  state  descriptor  class    state  factory    immutable  aggregating  state  create  state    tuple2  of    reducing  state  descriptor  class    state  factory    immutable  reducing  state  create  state    tuple2  of    folding  state  descriptor  class    state  factory    immutable  folding  state  create  state  collect    collectors  to  map  t  t  f0  t  t  f1  private  interface    state  factory  t  s  extends    state  s  create  state    state  descriptor  s  t  state  desc  byte  serialized  state  throws    exception    the  client  that  forwards  the  requests  to  the  proxy  private  final    client    kv  state  request    kv  state  response  client    the  address  of  the  proxy  this  client  is  connected  to  private  final    inet  socket  address  remote  address    the  execution  configuration  used  to  instantiate  the  different  de  serializers  private    execution  config  execution  config    create  the    queryable    state    client  param  remote  hostname  the  hostname  of  the  code    client    proxy  to  connect  to  param  remote  port  the  port  of  the  proxy  to  connect  to  public    queryable  state  client  final    string  remote  hostname  final  int  remote  port  throws    unknown  host  exception  this    inet  address  get  by  name    preconditions  check  not  null  remote  hostname  remote  port    create  the    queryable    state    client  param  remote  address  the  link    inet  address  address  of  the  code    client    proxy  to  connect  to  param  remote  port  the  port  of  the  proxy  to  connect  to  public    queryable  state  client  final    inet  address  remote  address  final  int  remote  port    preconditions  check  argument    net  utils  is  valid  host  port  remote  port    remote    port  remote  port  is  out  of  valid  port  range  0-65535  this  remote  address  new    inet  socket  address  remote  address  remote  port  final    message  serializer    kv  state  request    kv  state  response  message  serializer  new    message  serializer  new    kv  state  request    kv  state  request  deserializer  new    kv  state  response    kv  state  response  deserializer  this  client  new    client    queryable    state    client    message  serializer  new    disabled  kv  state  request  stats    shuts  down  the  client  and  returns  a  link    completable  future  that  will  be  completed  when  the  shutdown  process  is  completed  p    if  an  exception  is  thrown  for  any  reason  then  the  returned  future  will  be  completed  exceptionally  with  that  exception  return  a  link    completable  future  for  further  handling  of  the  shutdown  result  public    completable  future  shutdown  and  handle  return  client  shutdown    shuts  down  the  client  and  waits  until  shutdown  is  completed  p    if  an  exception  is  thrown  a  warning  is  logged  containing  the  exception  message  public  void  shutdown  and  wait  try  client  shutdown  get  log  info    the    queryable    state    client  was  shutdown  successfully  catch    exception  e  log  warn    the    queryable    state    client  shutdown  failed  e    gets  the  link    execution  config  public    execution  config  get  execution  config  return  execution  config    replaces  the  existing  link    execution  config  possibly  code  null  with  the  provided  one  param  config    the  new  code  configuration  return    the  old  configuration  or  code  null  if  none  was  specified  public    execution  config  set  execution  config    execution  config  config    execution  config  prev  execution  config  this  execution  config  config  return  prev    returns  a  future  holding  the  request  result  param  job  id    job  i  d  of  the  job  the  queryable  state  belongs  to  param  queryable  state  name    name  under  which  the  state  is  queryable  param  key    the  key  we  are  interested  in  param  key  type  hint  a  link    type  hint  used  to  extract  the  type  of  the  key  param  state  descriptor    the  link    state  descriptor  of  the  state  we  want  to  query  return    future  holding  the  immutable  link    state  object  containing  the  result    public  evolving  public  k  s  extends    state  v    completable  future  s  get  kv  state  final    job  i  d  job  id  final    string  queryable  state  name  final  k  key  final    type  hint  k  key  type  hint  final    state  descriptor  s  v  state  descriptor    preconditions  check  not  null  key  type  hint    type  information  k  key  type  info  key  type  hint  get  type  info  return  get  kv  state  job  id  queryable  state  name  key  key  type  info  state  descriptor    returns  a  future  holding  the  request  result  param  job  id    job  i  d  of  the  job  the  queryable  state  belongs  to  param  queryable  state  name    name  under  which  the  state  is  queryable  param  key    the  key  we  are  interested  in  param  key  type  info    the  link    type  information  of  the  key  param  state  descriptor    the  link    state  descriptor  of  the  state  we  want  to  query  return    future  holding  the  immutable  link    state  object  containing  the  result    public  evolving  public  k  s  extends    state  v    completable  future  s  get  kv  state  final    job  i  d  job  id  final    string  queryable  state  name  final  k  key  final    type  information  k  key  type  info  final    state  descriptor  s  v  state  descriptor  return  get  kv  state  job  id  queryable  state  name  key    void  namespace  instance  key  type  info    void  namespace  type  info  instance  state  descriptor    returns  a  future  holding  the  request  result  param  job  id    job  i  d  of  the  job  the  queryable  state  belongs  to  param  queryable  state  name    name  under  which  the  state  is  queryable  param  key    the  key  that  the  state  we  request  is  associated  with  param  namespace    the  namespace  of  the  state  param  key  type  info    the  link    type  information  of  the  keys  param  namespace  type  info    the  link    type  information  of  the  namespace  param  state  descriptor    the  link    state  descriptor  of  the  state  we  want  to  query  return    future  holding  the  immutable  link    state  object  containing  the  result  private  k  n  s  extends    state  v    completable  future  s  get  kv  state  final    job  i  d  job  id  final    string  queryable  state  name  final  k  key  final  n  namespace  final    type  information  k  key  type  info  final    type  information  n  namespace  type  info  final    state  descriptor  s  v  state  descriptor    preconditions  check  not  null  job  id    preconditions  check  not  null  queryable  state  name    preconditions  check  not  null  key    preconditions  check  not  null  namespace    preconditions  check  not  null  key  type  info    preconditions  check  not  null  namespace  type  info    preconditions  check  not  null  state  descriptor    type  serializer  k  key  serializer  key  type  info  create  serializer  execution  config    type  serializer  n  namespace  serializer  namespace  type  info  create  serializer  execution  config  state  descriptor  initialize  serializer  unless  set  execution  config  final  byte  serialized  key  and  namespace  try  serialized  key  and  namespace    kv  state  serializer  serialize  key  and  namespace  key  key  serializer  namespace  namespace  serializer  catch    i  o  exception  e  return    future  utils  get  failed  future  e  return  get  kv  state  job  id  queryable  state  name  key  hash  code  serialized  key  and  namespace  then  apply  state  response  create  state  state  response  state  descriptor  private  t  s  extends    state  s  create  state    kv  state  response  state  response    state  descriptor  s  t  state  descriptor    state  factory  state  factory  state  factories  get  state  descriptor  get  class  if  state  factory  null    string  message    string  format    state  s  is  not  supported  by  s  state  descriptor  get  class  this  get  class  throw  new    flink  runtime  exception  message  try  return  state  factory  create  state  state  descriptor  state  response  get  content  catch    exception  e  throw  new    flink  runtime  exception  e    returns  a  future  holding  the  serialized  request  result  param  job  id    job  i  d  of  the  job  the  queryable  state  belongs  to  param  queryable  state  name    name  under  which  the  state  is  queryable  param  key  hash  code    integer  hash  code  of  the  key  result  of  a  call  to  link    object  hash  code  param  serialized  key  and  namespace    serialized  key  and  namespace  to  query    kv  state  instance  with  return    future  holding  the  serialized  result  private    completable  future    kv  state  response  get  kv  state  final    job  i  d  job  id  final    string  queryable  state  name  final  int  key  hash  code  final  byte  serialized  key  and  namespace  log  debug    sending    state    request  to  remote  address  try    kv  state  request  request  new    kv  state  request  job  id  queryable  state  name  key  hash  code  serialized  key  and  namespace  return  client  send  request  remote  address  request  catch    exception  e  log  error    unable  to  send    k  v  state  request  e  return    future  utils  get  failed  future  e  
public  evolving  public  class    savepoint  config  options    the  path  to  a  savepoint  that  will  be  used  to  bootstrap  the  pipeline  s  state  public  static  final    config  option    string  savepoint  path  key  execution  savepoint  path  string  type  no  default  value  with  description    path  to  a  savepoint  to  restore  the  job  from  for  example  hdfs  flink  savepoint    a  flag  indicating  if  we  allow    flink  to  skip  savepoint  state  that  cannot  be  restored  e  g  because  the  corresponding  operator  has  been  removed  public  static  final    config  option    boolean  savepoint  ignore  unclaimed  state  key  execution  savepoint  ignore  unclaimed  state  boolean  type  default  value  false  with  description    allow  to  skip  savepoint  state  that  cannot  be  restored    allow  this  if  you  removed  an  operator  from  your  pipeline  after  the  savepoint  was  triggered  
public  evolving  public  abstract  class    abstract  state  backend  implements    state  backend  java  io    serializable  private  static  final  long  serial  version  u  i  d    l  public  static    stream  compression  decorator  get  compression  decorator    execution  config  execution  config  if  execution  config  null  execution  config  is  use  snapshot  compression  return    snappy  stream  compression  decorator  instance  else  return    uncompressed  stream  compression  decorator  instance    state    backend    state    holding    backends    override  public  abstract  k    abstract  keyed  state  backend  k  create  keyed  state  backend    environment  env    job  i  d  job  i  d    string  operator  identifier    type  serializer  k  key  serializer  int  number  of  key  groups    key  group  range  key  group  range    task  kv  state  registry  kv  state  registry    ttl  time  provider  ttl  time  provider    metric  group  metric  group    nonnull    collection    keyed  state  handle  state  handles    closeable  registry  cancel  stream  registry  throws    i  o  exception    override  public  abstract    operator  state  backend  create  operator  state  backend    environment  env    string  operator  identifier    nonnull    collection    operator  state  handle  state  handles    closeable  registry  cancel  stream  registry  throws    exception  
public  evolving  public  abstract  class    abstract  file  state  backend  extends    abstract  state  backend  private  static  final  long  serial  version  u  i  d  1  l    state    backend    properties    the  path  where  checkpoints  will  be  stored  or  null  if  none  has  been  configured    nullable  private  final    path  base  checkpoint  path    the  path  where  savepoints  will  be  stored  or  null  if  none  has  been  configured    nullable  private  final    path  base  savepoint  path    creates  a  backend  with  the  given  optional  checkpoint  and  savepoint  base  directories  param  base  checkpoint  path    the  base  directory  for  checkpoints  or  null  if  none  is  configured  param  base  savepoint  path    the  default  directory  for  savepoints  or  null  if  none  is  set  protected    abstract  file  state  backend    nullable  uri  base  checkpoint  path    nullable  uri  base  savepoint  path  this  base  checkpoint  path  null  null  new    path  base  checkpoint  path  base  savepoint  path  null  null  new    path  base  savepoint  path    creates  a  backend  with  the  given  optional  checkpoint  and  savepoint  base  directories  param  base  checkpoint  path    the  base  directory  for  checkpoints  or  null  if  none  is  configured  param  base  savepoint  path    the  default  directory  for  savepoints  or  null  if  none  is  set  protected    abstract  file  state  backend    nullable    path  base  checkpoint  path    nullable    path  base  savepoint  path  this  base  checkpoint  path  base  checkpoint  path  null  null  validate  path  base  checkpoint  path  this  base  savepoint  path  base  savepoint  path  null  null  validate  path  base  savepoint  path    creates  a  new  backend  using  the  given  checkpoint  savepoint  directories  or  the  values  defined  in  the  given  configuration    if  a  checkpoint  savepoint  parameter  is  not  null  that  value  takes  precedence  over  the  value  in  the  configuration    if  the  configuration  does  not  specify  a  value  it  is  possible  that  the  checkpoint  savepoint  directories  in  the  backend  will  be  null  p    this  constructor  can  be  used  to  create  a  backend  that  is  based  partially  on  a  given  backend  and  partially  on  a  configuration  param  base  checkpoint  path    the  checkpoint  base  directory  to  use  or  null  param  base  savepoint  path    the  default  savepoint  directory  to  use  or  null  param  configuration    the  configuration  to  read  values  from  protected    abstract  file  state  backend    nullable    path  base  checkpoint  path    nullable    path  base  savepoint  path    readable  config  configuration  this  parameter  or  configured  base  checkpoint  path  configuration    checkpointing  options  checkpoints  directory  parameter  or  configured  base  savepoint  path  configuration    checkpointing  options  savepoint  directory    gets  the  checkpoint  base  directory    jobs  will  create  job  specific  subdirectories  for  checkpoints  within  this  directory    may  be  null  if  not  configured  return    the  checkpoint  base  directory    nullable  public    path  get  checkpoint  path  return  base  checkpoint  path    gets  the  directory  where  savepoints  are  stored  by  default  when  no  custom  path  is  given  to  the  savepoint  trigger  command  return    the  default  directory  for  savepoints  or  null  if  no  default  directory  has  been  configured    nullable  public    path  get  savepoint  path  return  base  savepoint  path    initialization  and  metadata  storage    override  public    completed  checkpoint  storage  location  resolve  checkpoint    string  pointer  throws    i  o  exception  return    abstract  fs  checkpoint  storage  resolve  checkpoint  pointer  pointer    utilities    checks  the  validity  of  the  path  s  scheme  and  path  param  path    the  path  to  check  return    the  uri  as  a    path  throws    illegal  argument  exception    thrown  if  the  uri  misses  scheme  or  path  private  static    path  validate  path    path  path  final  uri  uri  path  to  uri  final    string  scheme  uri  get  scheme  final    string  path  part  uri  get  path  some  validity  checks  if  scheme  null  throw  new    illegal  argument  exception    the  scheme  hdfs  file  etc  is  null    please  specify  the  file  system  scheme  explicitly  in  the  uri  if  path  part  null  throw  new    illegal  argument  exception    the  path  to  store  the  checkpoint  data  in  is  null    please  specify  a  directory  path  for  the  checkpoint  data  if  path  part  length    path  part  equals  throw  new    illegal  argument  exception    cannot  use  the  root  directory  for  checkpoints  return  path    nullable  private  static    path  parameter  or  configured    nullable    path  path    readable  config  config    config  option    string  option  if  path  null  return  path  else    string  config  value  config  get  option  try  return  config  value  null  null  new    path  config  value  catch    illegal  argument  exception  e  throw  new    illegal  configuration  exception    cannot  parse  value  for  option  key  config  value    not  a  valid  path  
public  evolving  public  class    fs  state  backend  extends    abstract  file  state  backend  implements    configurable  state  backend  private  static  final  long  serial  version  u  i  d    l    maximum  size  of  state  that  is  stored  with  the  metadata  rather  than  in  files      mi  byte  private  static  final  int  max  file  state  threshold        state  below  this  size  will  be  stored  as  part  of  the  metadata  rather  than  in  files  a  value  of    means  not  yet  configured  in  which  case  the  default  will  be  used  private  final  int  file  state  threshold    switch  to  chose  between  synchronous  and  asynchronous  snapshots  a  value  of  undefined  means  not  yet  configured  in  which  case  the  default  will  be  used  private  final    ternary  boolean  asynchronous  snapshots    the  write  buffer  size  for  created  checkpoint  stream  this  should  not  be  less  than  file  state  threshold  when  we  want  state  below  that  threshold  stored  as  part  of  metadata  not  files  a  value  of    means  not  yet  configured  in  which  case  the  default  will  be  used  private  final  int  write  buffer  size    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  public    fs  state  backend    string  checkpoint  data  uri  this  new    path  checkpoint  data  uri    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  param  asynchronous  snapshots    switch  to  enable  asynchronous  snapshots  public    fs  state  backend    string  checkpoint  data  uri  boolean  asynchronous  snapshots  this  new    path  checkpoint  data  uri  asynchronous  snapshots    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  public    fs  state  backend    path  checkpoint  data  uri  this  checkpoint  data  uri  to  uri    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  param  asynchronous  snapshots    switch  to  enable  asynchronous  snapshots  public    fs  state  backend    path  checkpoint  data  uri  boolean  asynchronous  snapshots  this  checkpoint  data  uri  to  uri  asynchronous  snapshots    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  public    fs  state  backend  uri  checkpoint  data  uri  this  checkpoint  data  uri  null        ternary  boolean  undefined    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri    optionally  this  constructor  accepts  a  default  savepoint  storage  directory  to  which  savepoints  are  stored  when  no  custom  target  path  is  give  to  the  savepoint  command  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  param  default  savepoint  directory    the  default  directory  to  store  savepoints  to    may  be  null  public    fs  state  backend  uri  checkpoint  data  uri    nullable  uri  default  savepoint  directory  this  checkpoint  data  uri  default  savepoint  directory        ternary  boolean  undefined    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  param  asynchronous  snapshots    switch  to  enable  asynchronous  snapshots  public    fs  state  backend  uri  checkpoint  data  uri  boolean  asynchronous  snapshots  this  checkpoint  data  uri  null        ternary  boolean  from  boolean  asynchronous  snapshots    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  param  file  state  size  threshold    state  up  to  this  size  will  be  stored  as  part  of  the  metadata  rather  than  in  files  public    fs  state  backend  uri  checkpoint  data  uri  int  file  state  size  threshold  this  checkpoint  data  uri  null  file  state  size  threshold      ternary  boolean  undefined    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  data  uri    the  uri  describing  the  filesystem  scheme  and  optionally  authority  and  the  path  to  the  checkpoint  data  directory  param  file  state  size  threshold    state  up  to  this  size  will  be  stored  as  part  of  the  metadata  rather  than  in  files    for  default  value  param  asynchronous  snapshots    switch  to  enable  asynchronous  snapshots  public    fs  state  backend  uri  checkpoint  data  uri  int  file  state  size  threshold  boolean  asynchronous  snapshots  this  checkpoint  data  uri  null  file  state  size  threshold      ternary  boolean  from  boolean  asynchronous  snapshots    creates  a  new  state  backend  that  stores  its  checkpoint  data  in  the  file  system  and  location  defined  by  the  given  uri  p  a  file  system  for  the  file  system  scheme  in  the  uri  e  g  file  hdfs  or    s3  must  be  accessible  via  link    file  system  get  uri  p    for  a  state  backend  targeting  hdfs  this  means  that  the  uri  must  either  specify  the  authority  host  and  port  or  that  the    hadoop  configuration  that  describes  that  information  must  be  in  the  classpath  param  checkpoint  directory    the  path  to  write  checkpoint  metadata  to  param  default  savepoint  directory    the  path  to  write  savepoints  to    if  null  the  value  from  the  runtime  configuration  will  be  used  or  savepoint  target  locations  need  to  be  passed  when  triggering  a  savepoint  param  file  state  size  threshold    state  below  this  size  will  be  stored  as  part  of  the  metadata  rather  than  in  files    if    the  value  configured  in  the  runtime  configuration  will  be  used  or  the  default  value  1  k  b  if  nothing  is  configured  param  write  buffer  size    write  buffer  size  used  to  serialize  state    if    the  value  configured  in  the  runtime  configuration  will  be  used  or  the  default  value  4  k  b  if  nothing  is  configured  param  asynchronous  snapshots    flag  to  switch  between  synchronous  and  asynchronous  snapshot  mode    if  undefined  the  value  configured  in  the  runtime  configuration  will  be  used  public    fs  state  backend  uri  checkpoint  directory    nullable  uri  default  savepoint  directory  int  file  state  size  threshold  int  write  buffer  size    ternary  boolean  asynchronous  snapshots  super  check  not  null  checkpoint  directory  checkpoint  directory  is  null  default  savepoint  directory  check  not  null  asynchronous  snapshots  asynchronous  snapshots  check  argument  file  state  size  threshold    file  state  size  threshold  max  file  state  threshold    the  threshold  for  file  state  size  must  be  in    s  where    means  to  use  the  value  from  the  deployment  s  configuration  max  file  state  threshold  check  argument  write  buffer  size      the  write  buffer  size  must  be  not  less  than    where    means  to  use  the  value  from  the  deployment  s  configuration  this  file  state  threshold  file  state  size  threshold  this  write  buffer  size  write  buffer  size  this  asynchronous  snapshots  asynchronous  snapshots    private  constructor  that  creates  a  re  configured  copy  of  the  state  backend  param  original    the  state  backend  to  re  configure  param  configuration    the  configuration  private    fs  state  backend    fs  state  backend  original    readable  config  configuration    class  loader  class  loader  super  original  get  checkpoint  path  original  get  savepoint  path  configuration  if  asynchronous  snapshots  were  configured  use  that  setting  else  check  the  configuration  this  asynchronous  snapshots  original  asynchronous  snapshots  resolve  undefined  configuration  get    checkpointing  options  async  snapshots  if  get  valid  file  state  threshold  original  file  state  threshold    this  file  state  threshold  original  file  state  threshold  else  final  int  configured  state  threshold  get  valid  file  state  threshold  configuration  get  fs  small  file  threshold  get  bytes  if  configured  state  threshold    this  file  state  threshold  configured  state  threshold  else  this  file  state  threshold    math  utils  checked  down  cast  fs  small  file  threshold  default  value  get  bytes  because  this  is  the  only  place  we  unlikely  ever  log  we  lazily  create  the  logger  here    logger  factory  get  logger    abstract  file  state  backend  class  warn    ignoring  invalid  file  size  threshold  value  using  default  value  instead  fs  small  file  threshold  key  configuration  get  fs  small  file  threshold  get  bytes  fs  small  file  threshold  default  value  final  int  buffer  size  original  write  buffer  size    original  write  buffer  size  configuration  get    checkpointing  options  fs  write  buffer  size  this  write  buffer  size    math  max  buffer  size  this  file  state  threshold  private  int  get  valid  file  state  threshold  long  file  state  threshold  if  file  state  threshold    file  state  threshold  max  file  state  threshold  return  int  file  state  threshold  return      properties    gets  the  base  directory  where  all  the  checkpoints  are  stored    the  job  specific  checkpoint  directory  is  created  inside  this  directory  return    the  base  directory  for  checkpoints  deprecated    deprecated  in  favor  of  link  get  checkpoint  path    deprecated  public    path  get  base  path  return  get  checkpoint  path    gets  the  base  directory  where  all  the  checkpoints  are  stored    the  job  specific  checkpoint  directory  is  created  inside  this  directory  return    the  base  directory  for  checkpoints    nonnull    override  public    path  get  checkpoint  path  we  know  that  this  can  never  be  null  by  the  way  of  constructor  checks  noinspection    constant  conditions  return  super  get  checkpoint  path    gets  the  threshold  below  which  state  is  stored  as  part  of  the  metadata  rather  than  in  files    this  threshold  ensures  that  the  backend  does  not  create  a  large  amount  of  very  small  files  where  potentially  the  file  pointers  are  larger  than  the  state  itself  p    if  not  explicitly  configured  this  is  the  default  value  of  link    checkpointing  options  fs  small  file  threshold  return    the  file  size  threshold  in  bytes  public  int  get  min  file  size  threshold  return  file  state  threshold    file  state  threshold    math  utils  checked  down  cast  fs  small  file  threshold  default  value  get  bytes    gets  the  write  buffer  size  for  created  checkpoint  stream  p    if  not  explicitly  configured  this  is  the  default  value  of  link    checkpointing  options  fs  write  buffer  size  return    the  write  buffer  size  in  bytes  public  int  get  write  buffer  size  return  write  buffer  size    write  buffer  size    checkpointing  options  fs  write  buffer  size  default  value    gets  whether  the  key  value  data  structures  are  asynchronously  snapshotted  p    if  not  explicitly  configured  this  is  the  default  value  of  link    checkpointing  options  async  snapshots  public  boolean  is  using  asynchronous  snapshots  return  asynchronous  snapshots  get  or  default    checkpointing  options  async  snapshots  default  value    reconfiguration    creates  a  copy  of  this  state  backend  that  uses  the  values  defined  in  the  configuration  for  fields  where  that  were  not  specified  in  this  state  backend  param  config  the  configuration  return    the  re  configured  variant  of  the  state  backend    override  public    fs  state  backend  configure    readable  config  config    class  loader  class  loader  return  new    fs  state  backend  this  config  class  loader  initialization  and  cleanup    override  public    checkpoint  storage  create  checkpoint  storage    job  i  d  job  id  throws    i  o  exception  check  not  null  job  id  job  id  return  new    fs  checkpoint  storage  get  checkpoint  path  get  savepoint  path  job  id  get  min  file  size  threshold  get  write  buffer  size  state  holding  structures    override  public  k    abstract  keyed  state  backend  k  create  keyed  state  backend    environment  env    job  i  d  job  i  d    string  operator  identifier    type  serializer  k  key  serializer  int  number  of  key  groups    key  group  range  key  group  range    task  kv  state  registry  kv  state  registry    ttl  time  provider  ttl  time  provider    metric  group  metric  group    nonnull    collection    keyed  state  handle  state  handles    closeable  registry  cancel  stream  registry  throws    backend  building  exception    task  state  manager  task  state  manager  env  get  task  state  manager    local  recovery  config  local  recovery  config  task  state  manager  create  local  recovery  config    heap  priority  queue  set  factory  priority  queue  set  factory  new    heap  priority  queue  set  factory  key  group  range  number  of  key  groups    return  new    heap  keyed  state  backend  builder  kv  state  registry  key  serializer  env  get  user  class  loader  number  of  key  groups  key  group  range  env  get  execution  config  ttl  time  provider  state  handles    abstract  state  backend  get  compression  decorator  env  get  execution  config  local  recovery  config  priority  queue  set  factory  is  using  asynchronous  snapshots  cancel  stream  registry  build    override  public    operator  state  backend  create  operator  state  backend    environment  env    string  operator  identifier    nonnull    collection    operator  state  handle  state  handles    closeable  registry  cancel  stream  registry  throws    backend  building  exception  return  new    default  operator  state  backend  builder  env  get  user  class  loader  env  get  execution  config  is  using  asynchronous  snapshots  state  handles  cancel  stream  registry  build  utilities    override  public    string  to  string  return    file    state    backend  checkpoints  get  checkpoint  path  savepoints  get  savepoint  path  asynchronous  asynchronous  snapshots  file  state  threshold  file  state  threshold  
public  evolving  public  class    fs  state  backend  factory  implements    state  backend  factory    fs  state  backend    override  public    fs  state  backend  create  from  config    readable  config  config    class  loader  class  loader  throws    illegal  configuration  exception  we  need  to  explicitly  read  the  checkpoint  directory  here  because  that  is  a  required  constructor  parameter  final    string  checkpoint  dir  config  get    checkpointing  options  checkpoints  directory  if  checkpoint  dir  null  throw  new    illegal  configuration  exception    cannot  create  the  file  system  state  backend    the  configuration  does  not  specify  the  checkpoint  directory    checkpointing  options  checkpoints  directory  key  try  return  new    fs  state  backend  checkpoint  dir  configure  config  class  loader  catch    illegal  argument  exception  e  throw  new    illegal  configuration  exception    invalid  configuration  for  the  state  backend  e  
public  evolving  public  interface    function  initialization  context  extends    managed  initialization  context  
public  evolving  public  interface    function  snapshot  context  extends    managed  snapshot  context  
public  evolving  public  final  class    keyed  state  checkpoint  output  stream  extends    non  closing  checkpoint  output  stream    key  groups  state  handle  public  static  final  long  no  offset  set  1  l  public  static  final  int  no  current  key  group    private  int  current  key  group  private  final    key  group  range  offsets  key  group  range  offsets  public    keyed  state  checkpoint  output  stream    checkpoint  stream  factory    checkpoint  state  output  stream  delegate    key  group  range  key  group  range  super  delegate    preconditions  check  not  null  key  group  range    preconditions  check  argument  key  group  range    key  group  range  empty  key  group  range  this  current  key  group  no  current  key  group  long  empty  offsets  new  long  key  group  range  get  number  of  key  groups  mark  offsets  as  currently  not  set    arrays  fill  empty  offsets  no  offset  set  this  key  group  range  offsets  new    key  group  range  offsets  key  group  range  empty  offsets    override  public  void  close  throws    i  o  exception  users  should  not  be  able  to  actually  close  the  stream  it  is  closed  by  the  system  todo  if  we  want  to  support  async  writes  this  call  could  trigger  a  callback  to  the  snapshot  context  that  a  handle  is  available    returns  a  list  of  all  key  groups  which  can  be  written  to  this  stream  public    key  groups  list  get  key  group  list  return  key  group  range  offsets  get  key  group  range    user  code  can  call  this  method  to  signal  that  it  begins  to  write  a  new  key  group  with  the  given  key  group  id    this  id  must  be  within  the  link    key  groups  list  provided  by  the  stream    each  key  group  can  only  be  started  once  and  is  considered  final  immutable  as  soon  as  this  method  is  called  again  public  void  start  new  key  group  int  key  group  id  throws    i  o  exception  if  is  key  group  already  started  key  group  id  throw  new    i  o  exception    key  group  key  group  id  already  registered  key  group  range  offsets  set  key  group  offset  key  group  id  delegate  get  pos  current  key  group  key  group  id    returns  true  if  the  key  group  with  the  given  id  was  already  started    the  key  group  might  not  yet  be  finished  if  it  s  id  is  equal  to  the  return  value  of  link  get  current  key  group  public  boolean  is  key  group  already  started  int  key  group  id  return  no  offset  set  key  group  range  offsets  get  key  group  offset  key  group  id    returns  true  if  the  key  group  is  already  completely  written  and  immutable    it  was  started  and  since  then  another  key  group  has  been  started  public  boolean  is  key  group  already  finished  int  key  group  id  return  is  key  group  already  started  key  group  id  key  group  id  get  current  key  group    returns  the  key  group  that  is  currently  being  written    the  key  group  was  started  but  not  yet  finished  i  e  data  can  still  be  added    if  no  key  group  was  started  this  returns  link  no  current  key  group  public  int  get  current  key  group  return  current  key  group    override    key  groups  state  handle  close  and  get  handle  throws    i  o  exception    stream  state  handle  stream  state  handle  super  close  and  get  handle  after  leases  released  return  stream  state  handle  null  new    key  groups  state  handle  key  group  range  offsets  stream  state  handle  null  
public  evolving  public  class    key  group  state  partition  stream  provider  extends    state  partition  stream  provider    key  group  that  corresponds  to  the  data  in  the  provided  stream  private  final  int  key  group  id  public    key  group  state  partition  stream  provider    i  o  exception  creation  exception  int  key  group  id  super  creation  exception  this  key  group  id  key  group  id  public    key  group  state  partition  stream  provider    input  stream  stream  int  key  group  id  super  stream  this  key  group  id  key  group  id    returns  the  key  group  that  corresponds  to  the  data  in  the  provided  stream  public  int  get  key  group  id  return  key  group  id  
public  evolving  public  interface    managed  snapshot  context    returns  the  id  of  the  checkpoint  for  which  the  snapshot  is  taken  p    the  checkpoint  id  is  guaranteed  to  be  strictly  monotonously  increasing  across  checkpoints    for  two  completed  checkpoints  i  a  i  and  i  b  i  code  id  b  id  a  means  that  checkpoint  i  b  i  subsumes  checkpoint  i  a  i  i  e  checkpoint  i  b  i  contains  a  later  state  than  checkpoint  i  a  i  long  get  checkpoint  id    returns  timestamp  wall  clock  time  when  the  master  node  triggered  the  checkpoint  for  which  the  state  snapshot  is  taken  long  get  checkpoint  timestamp  
public  evolving  public  class    memory  state  backend  extends    abstract  file  state  backend  implements    configurable  state  backend  private  static  final  long  serial  version  u  i  d    l    the  default  maximal  size  that  the  snapshotted  memory  state  may  have      mi  bytes  public  static  final  int  default  max  state  size          the  maximal  size  that  the  snapshotted  memory  state  may  have  private  final  int  max  state  size    switch  to  chose  between  synchronous  and  asynchronous  snapshots  a  value  of  undefined  means  not  yet  configured  in  which  case  the  default  will  be  used  private  final    ternary  boolean  asynchronous  snapshots    creates  a  new  memory  state  backend  that  accepts  states  whose  serialized  forms  are  up  to  the  default  state  size    mb  p    checkpoint  and  default  savepoint  locations  are  used  as  specified  in  the  runtime  configuration  public    memory  state  backend  this  null  null  default  max  state  size    ternary  boolean  undefined    creates  a  new  memory  state  backend  that  accepts  states  whose  serialized  forms  are  up  to  the  default  state  size    mb    the  state  backend  uses  asynchronous  snapshots  or  synchronous  snapshots  as  configured  p    checkpoint  and  default  savepoint  locations  are  used  as  specified  in  the  runtime  configuration  param  asynchronous  snapshots    switch  to  enable  asynchronous  snapshots  public    memory  state  backend  boolean  asynchronous  snapshots  this  null  null  default  max  state  size    ternary  boolean  from  boolean  asynchronous  snapshots    creates  a  new  memory  state  backend  that  accepts  states  whose  serialized  forms  are  up  to  the  given  number  of  bytes  p    checkpoint  and  default  savepoint  locations  are  used  as  specified  in  the  runtime  configuration  p  b  warning  b    increasing  the  size  of  this  value  beyond  the  default  value  value  default  max  state  size  should  be  done  with  care    the  checkpointed  state  needs  to  be  send  to  the    job  manager  via  limited  size  rpc  messages  and  there  and  the    job  manager  needs  to  be  able  to  hold  all  aggregated  state  in  its  memory  param  max  state  size    the  maximal  size  of  the  serialized  state  public    memory  state  backend  int  max  state  size  this  null  null  max  state  size    ternary  boolean  undefined    creates  a  new  memory  state  backend  that  accepts  states  whose  serialized  forms  are  up  to  the  given  number  of  bytes  and  that  uses  asynchronous  snashots  as  configured  p    checkpoint  and  default  savepoint  locations  are  used  as  specified  in  the  runtime  configuration  p  b  warning  b    increasing  the  size  of  this  value  beyond  the  default  value  value  default  max  state  size  should  be  done  with  care    the  checkpointed  state  needs  to  be  send  to  the    job  manager  via  limited  size  rpc  messages  and  there  and  the    job  manager  needs  to  be  able  to  hold  all  aggregated  state  in  its  memory  param  max  state  size    the  maximal  size  of  the  serialized  state  param  asynchronous  snapshots    switch  to  enable  asynchronous  snapshots  public    memory  state  backend  int  max  state  size  boolean  asynchronous  snapshots  this  null  null  max  state  size    ternary  boolean  from  boolean  asynchronous  snapshots    creates  a  new    memory  state  backend  setting  optionally  the  path  to  persist  checkpoint  metadata  to  and  to  persist  savepoints  to  param  checkpoint  path    the  path  to  write  checkpoint  metadata  to    if  null  the  value  from  the  runtime  configuration  will  be  used  param  savepoint  path    the  path  to  write  savepoints  to    if  null  the  value  from  the  runtime  configuration  will  be  used  public    memory  state  backend    nullable    string  checkpoint  path    nullable    string  savepoint  path  this  checkpoint  path  savepoint  path  default  max  state  size    ternary  boolean  undefined    creates  a  new    memory  state  backend  setting  optionally  the  paths  to  persist  checkpoint  metadata  and  savepoints  to  as  well  as  configuring  state  thresholds  and  asynchronous  operations  p  b  warning  b    increasing  the  size  of  this  value  beyond  the  default  value  value  default  max  state  size  should  be  done  with  care    the  checkpointed  state  needs  to  be  send  to  the    job  manager  via  limited  size  rpc  messages  and  there  and  the    job  manager  needs  to  be  able  to  hold  all  aggregated  state  in  its  memory  param  checkpoint  path    the  path  to  write  checkpoint  metadata  to    if  null  the  value  from  the  runtime  configuration  will  be  used  param  savepoint  path    the  path  to  write  savepoints  to    if  null  the  value  from  the  runtime  configuration  will  be  used  param  max  state  size    the  maximal  size  of  the  serialized  state  param  asynchronous  snapshots    flag  to  switch  between  synchronous  and  asynchronous  snapshot  mode    if  null  the  value  configured  in  the  runtime  configuration  will  be  used  public    memory  state  backend    nullable    string  checkpoint  path    nullable    string  savepoint  path  int  max  state  size    ternary  boolean  asynchronous  snapshots  super  checkpoint  path  null  null  new    path  checkpoint  path  savepoint  path  null  null  new    path  savepoint  path  check  argument  max  state  size    max  state  size  must  be    this  max  state  size  max  state  size  this  asynchronous  snapshots  asynchronous  snapshots    private  constructor  that  creates  a  re  configured  copy  of  the  state  backend  param  original    the  state  backend  to  re  configure  param  configuration    the  configuration  param  class  loader    the  class  loader  private    memory  state  backend    memory  state  backend  original    readable  config  configuration    class  loader  class  loader  super  original  get  checkpoint  path  original  get  savepoint  path  configuration  this  max  state  size  original  max  state  size  if  asynchronous  snapshots  were  configured  use  that  setting  else  check  the  configuration  this  asynchronous  snapshots  original  asynchronous  snapshots  resolve  undefined  configuration  get    checkpointing  options  async  snapshots    properties    gets  the  maximum  size  that  an  individual  state  can  have  as  configured  in  the  constructor  by  default  value  default  max  state  size  return    the  maximum  size  that  an  individual  state  can  have  public  int  get  max  state  size  return  max  state  size    gets  whether  the  key  value  data  structures  are  asynchronously  snapshotted  p    if  not  explicitly  configured  this  is  the  default  value  of  link    checkpointing  options  async  snapshots  public  boolean  is  using  asynchronous  snapshots  return  asynchronous  snapshots  get  or  default    checkpointing  options  async  snapshots  default  value    reconfiguration    creates  a  copy  of  this  state  backend  that  uses  the  values  defined  in  the  configuration  for  fields  where  that  were  not  specified  in  this  state  backend  param  config    the  configuration  param  class  loader    the  class  loader  return    the  re  configured  variant  of  the  state  backend    override  public    memory  state  backend  configure    readable  config  config    class  loader  class  loader  return  new    memory  state  backend  this  config  class  loader  checkpoint  state  persistence    override  public    checkpoint  storage  create  checkpoint  storage    job  i  d  job  id  throws    i  o  exception  return  new    memory  backend  checkpoint  storage  job  id  get  checkpoint  path  get  savepoint  path  max  state  size  state  holding  structures    override  public    operator  state  backend  create  operator  state  backend    environment  env    string  operator  identifier    nonnull    collection    operator  state  handle  state  handles    closeable  registry  cancel  stream  registry  throws    exception  return  new    default  operator  state  backend  builder  env  get  user  class  loader  env  get  execution  config  is  using  asynchronous  snapshots  state  handles  cancel  stream  registry  build    override  public  k    abstract  keyed  state  backend  k  create  keyed  state  backend    environment  env    job  i  d  job  i  d    string  operator  identifier    type  serializer  k  key  serializer  int  number  of  key  groups    key  group  range  key  group  range    task  kv  state  registry  kv  state  registry    ttl  time  provider  ttl  time  provider    metric  group  metric  group    nonnull    collection    keyed  state  handle  state  handles    closeable  registry  cancel  stream  registry  throws    backend  building  exception    task  state  manager  task  state  manager  env  get  task  state  manager    heap  priority  queue  set  factory  priority  queue  set  factory  new    heap  priority  queue  set  factory  key  group  range  number  of  key  groups    return  new    heap  keyed  state  backend  builder  kv  state  registry  key  serializer  env  get  user  class  loader  number  of  key  groups  key  group  range  env  get  execution  config  ttl  time  provider  state  handles    abstract  state  backend  get  compression  decorator  env  get  execution  config  task  state  manager  create  local  recovery  config  priority  queue  set  factory  is  using  asynchronous  snapshots  cancel  stream  registry  build  utilities    override  public    string  to  string  return    memory  state  backend  data  in  heap  memory  checkpoints  to    job  manager  checkpoints  get  checkpoint  path  savepoints  get  savepoint  path  asynchronous  asynchronous  snapshots  max  state  size  max  state  size  
public  evolving  public  class    memory  state  backend  factory  implements    state  backend  factory    memory  state  backend    override  public    memory  state  backend  create  from  config    readable  config  config    class  loader  class  loader  return  new    memory  state  backend  configure  config  class  loader  
public  evolving  public  final  class    operator  state  checkpoint  output  stream  extends    non  closing  checkpoint  output  stream    operator  state  handle  private    long  array  list  partition  offsets  private  final  long  initial  position  public    operator  state  checkpoint  output  stream    checkpoint  stream  factory    checkpoint  state  output  stream  delegate  throws    i  o  exception  super  delegate  this  partition  offsets  new    long  array  list    this  initial  position  delegate  get  pos    user  code  can  call  this  method  to  signal  that  it  begins  to  write  a  new  partition  of  operator  state    each  previously  written  partition  is  considered  final  immutable  as  soon  as  this  method  is  called  again  public  void  start  new  partition  throws    i  o  exception  partition  offsets  add  delegate  get  pos    this  method  should  not  be  public  so  as  to  not  expose  internals  to  user  code    override    operator  state  handle  close  and  get  handle  throws    i  o  exception    stream  state  handle  stream  state  handle  super  close  and  get  handle  after  leases  released  if  null  stream  state  handle  return  null  if  partition  offsets  is  empty  delegate  get  pos  initial  position  start  new  partition    map    string    operator  state  handle    state  meta  info  offsets  map  new    hash  map      operator  state  handle    state  meta  info  meta  info  new    operator  state  handle    state  meta  info  partition  offsets  to  array    operator  state  handle    mode  split  distribute  offsets  map  put    default  operator  state  backend  default  operator  state  name  meta  info  return  new    operator  stream  state  handle  offsets  map  stream  state  handle  public  int  get  number  of  partitions  return  partition  offsets  size  
public  evolving  public  interface    state  backend  extends  java  io    serializable    checkpoint  storage  the  durable  persistence  of  checkpoint  data    resolves  the  given  pointer  to  a  checkpoint  savepoint  into  a  checkpoint  location    the  location  supports  reading  the  checkpoint  metadata  or  disposing  the  checkpoint  storage  location  p    if  the  state  backend  cannot  understand  the  format  of  the  pointer  for  example  because  it  was  created  by  a  different  state  backend  this  method  should  throw  an  code    i  o  exception  param  external  pointer    the  external  checkpoint  pointer  to  resolve  return    the  checkpoint  location  handle  throws    i  o  exception    thrown  if  the  state  backend  does  not  understand  the  pointer  or  if  the  pointer  could  not  be  resolved  due  to  an  i  o  error    completed  checkpoint  storage  location  resolve  checkpoint    string  external  pointer  throws    i  o  exception    creates  a  storage  for  checkpoints  for  the  given  job    the  checkpoint  storage  is  used  to  write  checkpoint  data  and  metadata  param  job  id    the  job  to  store  checkpoint  data  for  return  a  checkpoint  storage  for  the  given  job  throws    i  o  exception    thrown  if  the  checkpoint  storage  cannot  be  initialized    checkpoint  storage  create  checkpoint  storage    job  i  d  job  id  throws    i  o  exception    structure    backends    creates  a  new  link    abstract  keyed  state  backend  that  is  responsible  for  holding  b  keyed  state  b  and  checkpointing  it  p  i    keyed    state  i  is  state  where  each  value  is  bound  to  a  key  param  env    the  environment  of  the  task  param  job  i  d    the  id  of  the  job  that  the  task  belongs  to  param  operator  identifier    the  identifier  text  of  the  operator  param  key  serializer    the  key  serializer  for  the  operator  param  number  of  key  groups    the  number  of  key  groups  aka  max  parallelism  param  key  group  range    range  of  key  groups  for  which  the  to  be  created  backend  is  responsible  param  kv  state  registry    kv  state  registry  helper  for  this  task  param  ttl  time  provider    provider  for  ttl  logic  to  judge  about  state  expiration  param  metric  group    the  parent  metric  group  for  all  state  backend  metrics  param  state  handles    the  state  handles  for  restore  param  cancel  stream  registry    the  registry  to  which  created  closeable  objects  will  be  registered  during  restore  param  k    the  type  of  the  keys  by  which  the  state  is  organized  return    the    keyed    state    backend  for  the  given  job  operator  and  key  group  range  throws    exception    this  method  may  forward  all  exceptions  that  occur  while  instantiating  the  backend  k    abstract  keyed  state  backend  k  create  keyed  state  backend    environment  env    job  i  d  job  i  d    string  operator  identifier    type  serializer  k  key  serializer  int  number  of  key  groups    key  group  range  key  group  range    task  kv  state  registry  kv  state  registry    ttl  time  provider  ttl  time  provider    metric  group  metric  group    nonnull    collection    keyed  state  handle  state  handles    closeable  registry  cancel  stream  registry  throws    exception    creates  a  new  link    operator  state  backend  that  can  be  used  for  storing  operator  state  p    operator  state  is  state  that  is  associated  with  parallel  operator  or  function  instances  rather  than  with  keys  param  env    the  runtime  environment  of  the  executing  task  param  operator  identifier    the  identifier  of  the  operator  whose  state  should  be  stored  param  state  handles    the  state  handles  for  restore  param  cancel  stream  registry    the  registry  to  register  streams  to  close  if  task  canceled  return    the    operator  state  backend  for  operator  identified  by  the  job  and  operator  identifier  throws    exception    this  method  may  forward  all  exceptions  that  occur  while  instantiating  the  backend    operator  state  backend  create  operator  state  backend    environment  env    string  operator  identifier    nonnull    collection    operator  state  handle  state  handles    closeable  registry  cancel  stream  registry  throws    exception  
public  evolving  public  interface    state  backend  factory  t  extends    state  backend    creates  the  state  backend  optionally  using  the  given  configuration  param  config    the    flink  configuration  loaded  by  the    task  manager  param  class  loader    the  class  loader  that  should  be  used  to  load  the  state  backend  return    the  created  state  backend  throws    illegal  configuration  exception    if  the  configuration  misses  critical  values  or  specifies  invalid  values  throws    i  o  exception    if  the  state  backend  initialization  failed  due  to  an  i  o  exception  t  create  from  config    readable  config  config    class  loader  class  loader  throws    illegal  configuration  exception    i  o  exception  
public  evolving  public  interface    state  initialization  context  extends    function  initialization  context    returns  an  iterable  to  obtain  input  streams  for  previously  stored  operator  state  partitions  that  are  assigned  to  this  operator    iterable    state  partition  stream  provider  get  raw  operator  state  inputs    returns  an  iterable  to  obtain  input  streams  for  previously  stored  keyed  state  partitions  that  are  assigned  to  this  operator    iterable    key  group  state  partition  stream  provider  get  raw  keyed  state  inputs  
public  evolving  public  class    state  partition  stream  provider  a  ready  made  stream  that  contains  data  for  one  state  partition  private  final    input  stream  stream    holds  potential  exception  that  happened  when  actually  trying  to  create  the  stream  private  final    i  o  exception  creation  exception  public    state  partition  stream  provider    i  o  exception  creation  exception  this  creation  exception    preconditions  check  not  null  creation  exception  this  stream  null  public    state  partition  stream  provider    input  stream  stream  this  stream  new    non  closing  input  stream  decorator    preconditions  check  not  null  stream  this  creation  exception  null    returns  a  stream  with  the  data  of  one  state  partition  public    input  stream  get  stream  throws    i  o  exception  if  creation  exception  null  throw  new    i  o  exception  creation  exception  return  stream  
public  evolving  public  interface    state  snapshot  context  extends    function  snapshot  context    returns  an  output  stream  for  keyed  state    keyed  state  checkpoint  output  stream  get  raw  keyed  operator  state  output  throws    exception    returns  an  output  stream  for  operator  state    operator  state  checkpoint  output  stream  get  raw  operator  state  output  throws    exception  
public  evolving  public  class    scala  csv  output  format  t  extends    product  extends    file  output  format  t  implements    input  type  configurable  private  static  final  long  serial  version  u  i  d  1  l    suppress  warnings  unused  private  static  final    logger  log    logger  factory  get  logger    scala  csv  output  format  class  public  static  final    string  default  line  delimiter    csv  input  format  default  line  delimiter  public  static  final    string  default  field  delimiter    string  value  of    csv  input  format  default  field  delimiter  private  transient    writer  wrt  private    string  field  delimiter  private    string  record  delimiter  private    string  charset  name  private  boolean  allow  null  values  true  private  boolean  quote  strings  false    constructors  and  getters  setters  for  the  configurable  parameters    creates  an  instance  of    csv  output  format    lines  are  separated  by  the  newline  character  n  fields  are  separated  by  param  output  path    the  path  where  the  csv  file  is  written  public    scala  csv  output  format    path  output  path  this  output  path  default  line  delimiter  default  field  delimiter    creates  an  instance  of    csv  output  format    lines  are  separated  by  the  newline  character  n  fields  by  the  given  field  delimiter  param  output  path    the  path  where  the  csv  file  is  written  param  field  delimiter    the  delimiter  that  is  used  to  separate  fields  in  a  tuple  public    scala  csv  output  format    path  output  path    string  field  delimiter  this  output  path  default  line  delimiter  field  delimiter    creates  an  instance  of    csv  output  format  param  output  path    the  path  where  the  csv  file  is  written  param  record  delimiter    the  delimiter  that  is  used  to  separate  the  tuples  param  field  delimiter    the  delimiter  that  is  used  to  separate  fields  in  a  tuple  public    scala  csv  output  format    path  output  path    string  record  delimiter    string  field  delimiter  super  output  path  if  record  delimiter  null  throw  new    illegal  argument  exception    record  delmiter  shall  not  be  null  if  field  delimiter  null  throw  new    illegal  argument  exception    field  delimiter  shall  not  be  null  this  field  delimiter  field  delimiter  this  record  delimiter  record  delimiter  this  allow  null  values  false    configures  the  format  to  either  allow  null  values  writing  an  empty  field  or  to  throw  an  exception  when  encountering  a  null  field  p    by  default  null  values  are  allowed  param  allow  nulls    flag  to  indicate  whether  the  output  format  should  accept  null  values  public  void  set  allow  null  values  boolean  allow  nulls  this  allow  null  values  allow  nulls    sets  the  charset  with  which  the  csv  strings  are  written  to  the  file    if  not  specified  the  output  format  uses  the  systems  default  character  encoding  param  charset  name    the  name  of  charset  to  use  for  encoding  the  output  public  void  set  charset  name    string  charset  name  this  charset  name  charset  name    configures  whether  the  output  format  should  quote  string  values    string  values  are  fields  of  type  link    string  and  link  org  apache  flink  types    string  value  as  well  as  all  subclasses  of  the  latter  p    by  default  strings  are  not  quoted  param  quote  strings    flag  indicating  whether  string  fields  should  be  quoted  public  void  set  quote  strings  boolean  quote  strings  this  quote  strings  quote  strings    override  public  void  open  int  task  number  int  num  tasks  throws    i  o  exception  super  open  task  number  num  tasks  this  wrt  this  charset  name  null  new    output  stream  writer  new    buffered  output  stream  this  stream    new    output  stream  writer  new    buffered  output  stream  this  stream    this  charset  name    override  public  void  close  throws    i  o  exception  if  wrt  null  this  wrt  flush  this  wrt  close  super  close    override  public  void  write  record  t  element  throws    i  o  exception  int  num  fields  element  product  arity  for  int  i    i  num  fields  i    object  v  element  product  element  i  if  v  null  if  i    this  wrt  write  this  field  delimiter  if  quote  strings  if  v  instanceof    string  v  instanceof    string  value  this  wrt  write  this  wrt  write  v  to  string  this  wrt  write  else  this  wrt  write  v  to  string  else  this  wrt  write  v  to  string  else  if  this  allow  null  values  if  i    this  wrt  write  this  field  delimiter  else  throw  new    runtime  exception    cannot  write  tuple  with  null  value  at  position  i  add  the  record  delimiter  this  wrt  write  this  record  delimiter    override  public    string  to  string  return    csv  output  format  path  this  get  output  file  path  delimiter  this  field  delimiter    the  purpose  of  this  method  is  solely  to  check  whether  the  data  type  to  be  processed  is  in  fact  a  tuple  type    override  public  void  set  input  type    type  information  type    execution  config  execution  config  if  type  is  tuple  type  throw  new    invalid  program  exception    the    scala  csv  output  format  class  get  simple  name  can  only  be  used  to  write  tuple  data  sets  
public  evolving  public  interface    externally  induced  source  t  cd  extends    source  function  t    with  master  checkpoint  hook  cd    sets  the  checkpoint  trigger  through  which  the  source  can  trigger  the  checkpoint  param  checkpoint  trigger    the  checkpoint  trigger  to  set  void  set  checkpoint  trigger    checkpoint  trigger  checkpoint  trigger    through  the  code    checkpoint  trigger  the  source  function  notifies  the    flink  source  operator  when  to  trigger  the  checkpoint  interface    checkpoint  trigger    triggers  a  checkpoint    this  method  should  be  called  by  the  source  when  it  sees  the  event  that  indicates  that  a  checkpoint  should  be  triggered  p    when  this  method  is  called  the  parallel  operator  instance  in  which  the  calling  source  function  runs  will  perform  its  checkpoint  and  insert  the  checkpoint  barrier  into  the  data  stream  param  checkpoint  id    the  id  that  identifies  the  checkpoint  throws    flink  exception    thrown  when  the  checkpoint  could  not  be  triggered  for  example  because  of  an  invalid  state  or  errors  when  storing  the  checkpoint  state  void  trigger  checkpoint  long  checkpoint  id  throws    flink  exception  
public  evolving    deprecated  public  interface    list  checkpointed  t  extends    serializable    gets  the  current  state  of  the  function    the  state  must  reflect  the  result  of  all  prior  invocations  to  this  function  p    the  returned  list  should  contain  one  entry  for  redistributable  unit  of  state    see  the  link    list  checkpointed  class  docs  for  an  illustration  how  list  style  state  redistribution  works  p    as  special  case  the  returned  list  may  be  null  or  empty  if  the  operator  has  no  state  or  it  may  contain  a  single  element  if  the  operator  state  is  indivisible  param  checkpoint  id    the  id  of  the  checkpoint  a  unique  and  monotonously  increasing  value  param  timestamp    the  wall  clock  timestamp  when  the  checkpoint  was  triggered  by  the  master  return    the  operator  state  in  a  list  of  redistributable  atomic  sub  states    should  not  return  null  but  empty  list  instead  throws    exception    thrown  if  the  creation  of  the  state  object  failed    this  causes  the  checkpoint  to  fail    the  system  may  decide  to  fail  the  operation  and  trigger  recovery  or  to  discard  this  checkpoint  attempt  and  to  continue  running  and  to  try  again  with  the  next  checkpoint  attempt    list  t  snapshot  state  long  checkpoint  id  long  timestamp  throws    exception    restores  the  state  of  the  function  or  operator  to  that  of  a  previous  checkpoint    this  method  is  invoked  when  the  function  is  executed  after  a  failure  recovery    the  state  list  may  be  empty  if  no  state  is  to  be  recovered  by  the  particular  parallel  instance  of  the  function  p    the  given  state  list  will  contain  all  the  i  sub  states  i  that  this  parallel  instance  of  the  function  needs  to  handle    refer  to  the  link    list  checkpointed  class  docs  for  an  illustration  how  list  style  state  redistribution  works  p  b    important  b    when  implementing  this  interface  together  with  link    rich  function  then  the  code  restore  state  method  is  called  before  link    rich  function  open    configuration  param  state    the  state  to  be  restored  as  a  list  of  atomic  sub  states  throws    exception    throwing  an  exception  in  this  method  causes  the  recovery  to  fail    the  exact  consequence  depends  on  the  configured  failure  handling  strategy  but  typically  the  system  will  re  attempt  the  recovery  or  try  recovering  from  a  different  checkpoint  void  restore  state    list  t  state  throws    exception  
public  evolving  public  interface    with  master  checkpoint  hook  e  extends  java  io    serializable    creates  the  hook  that  should  be  called  by  the  checkpoint  coordinator    master  trigger  restore  hook  e  create  master  trigger  restore  hook  
public  evolving  public  interface    output  selector  out  extends    serializable    method  for  selecting  output  names  for  the  emitted  objects  when  using  the  link    single  output  stream  operator  split  method    the  values  will  be  emitted  only  to  output  names  which  are  contained  in  the  returned  iterable  param  value    output  object  for  which  the  output  selection  should  be  made    iterable    string  select  out  value  
public  evolving  public  class    async  data  stream    output  mode  for  asynchronous  operations  public  enum    output  mode  ordered  unordered  private  static  final  int  default  queue  capacity      add  an    async  wait  operator  param  in    the  link    data  stream  where  the  link    async  wait  operator  will  be  added  param  func  link    async  function  wrapped  inside  link    async  wait  operator  param  timeout  for  the  asynchronous  operation  to  complete  param  buf  size    the  max  number  of  inputs  the  link    async  wait  operator  can  hold  inside  param  mode    processing  mode  for  link    async  wait  operator  param  in    input  type  param  out    output  type  return  a  new  link    single  output  stream  operator  private  static  in  out    single  output  stream  operator  out  add  operator    data  stream  in  in    async  function  in  out  func  long  timeout  int  buf  size    output  mode  mode    type  information  out  out  type  info    type  extractor  get  unary  operator  return  type  func    async  function  class      new  int      in  get  type    utils  get  call  location  name  true  create  transform    async  wait  operator  factory  in  out  operator  factory  new    async  wait  operator  factory  in  get  execution  environment  clean  func  timeout  buf  size  mode  return  in  transform  async  wait  operator  out  type  info  operator  factory    add  an    async  wait  operator    the  order  of  output  stream  records  may  be  reordered  param  in    input  link    data  stream  param  func  link    async  function  param  timeout  for  the  asynchronous  operation  to  complete  param  time  unit  of  the  given  timeout  param  capacity    the  max  number  of  async  i  o  operation  that  can  be  triggered  param  in    type  of  input  record  param  out    type  of  output  record  return  a  new  link    single  output  stream  operator  public  static  in  out    single  output  stream  operator  out  unordered  wait    data  stream  in  in    async  function  in  out  func  long  timeout    time  unit  time  unit  int  capacity  return  add  operator  in  func  time  unit  to  millis  timeout  capacity    output  mode  unordered    add  an    async  wait  operator    the  order  of  output  stream  records  may  be  reordered  param  in    input  link    data  stream  param  func  link    async  function  param  timeout  for  the  asynchronous  operation  to  complete  param  time  unit  of  the  given  timeout  param  in    type  of  input  record  param  out    type  of  output  record  return  a  new  link    single  output  stream  operator  public  static  in  out    single  output  stream  operator  out  unordered  wait    data  stream  in  in    async  function  in  out  func  long  timeout    time  unit  time  unit  return  add  operator  in  func  time  unit  to  millis  timeout  default  queue  capacity    output  mode  unordered    add  an    async  wait  operator    the  order  to  process  input  records  is  guaranteed  to  be  the  same  as  input  ones  param  in    input  link    data  stream  param  func  link    async  function  param  timeout  for  the  asynchronous  operation  to  complete  param  time  unit  of  the  given  timeout  param  capacity    the  max  number  of  async  i  o  operation  that  can  be  triggered  param  in    type  of  input  record  param  out    type  of  output  record  return  a  new  link    single  output  stream  operator  public  static  in  out    single  output  stream  operator  out  ordered  wait    data  stream  in  in    async  function  in  out  func  long  timeout    time  unit  time  unit  int  capacity  return  add  operator  in  func  time  unit  to  millis  timeout  capacity    output  mode  ordered    add  an    async  wait  operator    the  order  to  process  input  records  is  guaranteed  to  be  the  same  as  input  ones  param  in    input  link    data  stream  param  func  link    async  function  param  timeout  for  the  asynchronous  operation  to  complete  param  time  unit  of  the  given  timeout  param  in    type  of  input  record  param  out    type  of  output  record  return  a  new  link    single  output  stream  operator  public  static  in  out    single  output  stream  operator  out  ordered  wait    data  stream  in  in    async  function  in  out  func  long  timeout    time  unit  time  unit  return  add  operator  in  func  time  unit  to  millis  timeout  default  queue  capacity    output  mode  ordered  
public  evolving  public  class    broadcast  connected  stream    i  n1    i  n2  private  final    stream  execution  environment  environment  private  final    data  stream    i  n1  input  stream1  private  final    broadcast  stream    i  n2  input  stream2  private  final    list    map  state  descriptor  broadcast  state  descriptors  protected    broadcast  connected  stream  final    stream  execution  environment  env  final    data  stream    i  n1  input1  final    broadcast  stream    i  n2  input2  final    list    map  state  descriptor  broadcast  state  descriptors  this  environment  require  non  null  env  this  input  stream1  require  non  null  input1  this  input  stream2  require  non  null  input2  this  broadcast  state  descriptors  require  non  null  broadcast  state  descriptors  public    stream  execution  environment  get  execution  environment  return  environment    returns  the  non  broadcast  link    data  stream  return    the  stream  which  by  convention  is  not  broadcasted  public    data  stream    i  n1  get  first  input  return  input  stream1    returns  the  link    broadcast  stream  return    the  stream  which  by  convention  is  the  broadcast  one  public    broadcast  stream    i  n2  get  second  input  return  input  stream2    gets  the  type  of  the  first  input  return    the  type  of  the  first  input  public    type  information    i  n1  get  type1  return  input  stream1  get  type    gets  the  type  of  the  second  input  return    the  type  of  the  second  input  public    type  information    i  n2  get  type2  return  input  stream2  get  type    assumes  as  inputs  a  link    broadcast  stream  and  a  link    keyed  stream  and  applies  the  given  link    keyed  broadcast  process  function  on  them  thereby  creating  a  transformed  output  stream  param  function    the  link    keyed  broadcast  process  function  that  is  called  for  each  element  in  the  stream  param  ks    the  type  of  the  keys  in  the  keyed  stream  param  out    the  type  of  the  output  elements  return    the  transformed  link    data  stream    public  evolving  public  ks  out    single  output  stream  operator  out  process  final    keyed  broadcast  process  function  ks    i  n1    i  n2  out  function    type  information  out  out  type  info    type  extractor  get  binary  operator  return  type  function    keyed  broadcast  process  function  class          type  extractor  no  index  get  type1  get  type2    utils  get  call  location  name  true  return  process  function  out  type  info    assumes  as  inputs  a  link    broadcast  stream  and  a  link    keyed  stream  and  applies  the  given  link    keyed  broadcast  process  function  on  them  thereby  creating  a  transformed  output  stream  param  function    the  link    keyed  broadcast  process  function  that  is  called  for  each  element  in  the  stream  param  out  type  info    the  type  of  the  output  elements  param  ks    the  type  of  the  keys  in  the  keyed  stream  param  out    the  type  of  the  output  elements  return    the  transformed  link    data  stream    public  evolving  public  ks  out    single  output  stream  operator  out  process  final    keyed  broadcast  process  function  ks    i  n1    i  n2  out  function  final    type  information  out  out  type  info    preconditions  check  not  null  function    preconditions  check  argument  input  stream1  instanceof    keyed  stream  a    keyed  broadcast  process  function  can  only  be  used  on  a  keyed  stream    two  input  stream  operator    i  n1    i  n2  out  operator  new    co  broadcast  with  keyed  operator  clean  function  broadcast  state  descriptors  return  transform    co    process    broadcast    keyed  out  type  info  operator    assumes  as  inputs  a  link    broadcast  stream  and  a  non  keyed  link    data  stream  and  applies  the  given  link    broadcast  process  function  on  them  thereby  creating  a  transformed  output  stream  param  function    the  link    broadcast  process  function  that  is  called  for  each  element  in  the  stream  param  out    the  type  of  the  output  elements  return    the  transformed  link    data  stream    public  evolving  public  out    single  output  stream  operator  out  process  final    broadcast  process  function    i  n1    i  n2  out  function    type  information  out  out  type  info    type  extractor  get  binary  operator  return  type  function    broadcast  process  function  class          type  extractor  no  index  get  type1  get  type2    utils  get  call  location  name  true  return  process  function  out  type  info    assumes  as  inputs  a  link    broadcast  stream  and  a  non  keyed  link    data  stream  and  applies  the  given  link    broadcast  process  function  on  them  thereby  creating  a  transformed  output  stream  param  function    the  link    broadcast  process  function  that  is  called  for  each  element  in  the  stream  param  out  type  info    the  type  of  the  output  elements  param  out    the  type  of  the  output  elements  return    the  transformed  link    data  stream    public  evolving  public  out    single  output  stream  operator  out  process  final    broadcast  process  function    i  n1    i  n2  out  function  final    type  information  out  out  type  info    preconditions  check  not  null  function    preconditions  check  argument  input  stream1  instanceof    keyed  stream  a    broadcast  process  function  can  only  be  used  on  a  non  keyed  stream    two  input  stream  operator    i  n1    i  n2  out  operator  new    co  broadcast  with  non  keyed  operator  clean  function  broadcast  state  descriptors  return  transform    co    process    broadcast  out  type  info  operator    internal  private  out    single  output  stream  operator  out  transform  final    string  function  name  final    type  information  out  out  type  info  final    two  input  stream  operator    i  n1    i  n2  out  operator  read  the  output  type  of  the  input    transforms  to  coax  out  errors  about    missing  type  info  input  stream1  get  type  input  stream2  get  type    two  input  transformation    i  n1    i  n2  out  transform  new    two  input  transformation  input  stream1  get  transformation  input  stream2  get  transformation  function  name  operator  out  type  info  environment  get  parallelism  if  input  stream1  instanceof    keyed  stream    keyed  stream    i  n1  keyed  input1    keyed  stream    i  n1  input  stream1    type  information  key  type1  keyed  input1  get  key  type  transform  set  state  key  selectors  keyed  input1  get  key  selector  null  transform  set  state  key  type  key  type1    suppress  warnings  unchecked  rawtypes    single  output  stream  operator  out  return  stream  new    single  output  stream  operator  environment  transform  get  execution  environment  add  operator  transform  return  return  stream  protected  f  f  clean  f  f  return  get  execution  environment  clean  f  
public  evolving  public  class    broadcast  stream  t  private  final    stream  execution  environment  environment  private  final    data  stream  t  input  stream    the  link  org  apache  flink  api  common  state    state  descriptor  state  descriptors  of  the  registered  link  org  apache  flink  api  common  state    broadcast  state  broadcast  states    these  states  have  code  key  value  format  private  final    list    map  state  descriptor  broadcast  state  descriptors  protected    broadcast  stream  final    stream  execution  environment  env  final    data  stream  t  input  final    map  state  descriptor  broadcast  state  descriptors  this  environment  require  non  null  env  this  input  stream  require  non  null  input  this  broadcast  state  descriptors    arrays  as  list  require  non  null  broadcast  state  descriptors  public    type  information  t  get  type  return  input  stream  get  type  public  f  f  clean  f  f  return  environment  clean  f  public    transformation  t  get  transformation  return  input  stream  get  transformation  public    list    map  state  descriptor  get  broadcast  state  descriptor  return  broadcast  state  descriptors  public    stream  execution  environment  get  environment  return  environment  
public  evolving  public  class    iterative  stream  t  extends    single  output  stream  operator  t    we  store  these  so  that  we  can  create  a  co  iteration  if  we  need  to  private    data  stream  t  original  input  private  long  max  wait  time  protected    iterative  stream    data  stream  t  data  stream  long  max  wait  time  super  data  stream  get  execution  environment  new    feedback  transformation  data  stream  get  transformation  max  wait  time  this  original  input  data  stream  this  max  wait  time  max  wait  time  set  buffer  timeout  data  stream  environment  get  buffer  timeout    closes  the  iteration    this  method  defines  the  end  of  the  iterative  program  part  that  will  be  fed  back  to  the  start  of  the  iteration  p  a  common  usage  pattern  for  streaming  iterations  is  to  use  output  splitting  to  send  a  part  of  the  closing  data  stream  to  the  head    refer  to  link    data  stream  split  org  apache  flink  streaming  api  collector  selector    output  selector  for  more  information  param  feedback  stream  link    data  stream  that  will  be  used  as  input  to  the  iteration  head  return    the  feedback  stream    suppress  warnings  unchecked  rawtypes  public    data  stream  t  close  with    data  stream  t  feedback  stream    collection    transformation  predecessors  feedback  stream  get  transformation  get  transitive  predecessors  if  predecessors  contains  this  transformation  throw  new    unsupported  operation  exception    cannot  close  an  iteration  with  a  feedback    data  stream  that  does  not  originate  from  said  iteration    feedback  transformation  get  transformation  add  feedback  edge  feedback  stream  get  transformation  return  feedback  stream    changes  the  feedback  type  of  the  iteration  and  allows  the  user  to  apply  co  transformations  on  the  input  and  feedback  stream  as  in  a  link    connected  streams  p    for  type  safety  the  user  needs  to  define  the  feedback  type  param  feedback  type  class    class  of  the  elements  in  the  feedback  stream  return  a  link    connected  iterative  streams  public  f    connected  iterative  streams  t  f  with  feedback  type    class  f  feedback  type  class  return  with  feedback  type    type  information  of  feedback  type  class    changes  the  feedback  type  of  the  iteration  and  allows  the  user  to  apply  co  transformations  on  the  input  and  feedback  stream  as  in  a  link    connected  streams  p    for  type  safety  the  user  needs  to  define  the  feedback  type  param  feedback  type  hint    class  of  the  elements  in  the  feedback  stream  return  a  link    connected  iterative  streams  public  f    connected  iterative  streams  t  f  with  feedback  type    type  hint  f  feedback  type  hint  return  with  feedback  type    type  information  of  feedback  type  hint    changes  the  feedback  type  of  the  iteration  and  allows  the  user  to  apply  co  transformations  on  the  input  and  feedback  stream  as  in  a  link    connected  streams  p    for  type  safety  the  user  needs  to  define  the  feedback  type  param  feedback  type    the  type  information  of  the  feedback  stream  return  a  link    connected  iterative  streams  public  f    connected  iterative  streams  t  f  with  feedback  type    type  information  f  feedback  type  return  new    connected  iterative  streams  original  input  feedback  type  max  wait  time    the  link    connected  iterative  streams  represent  a  start  of  an  iterative  part  of  a  streaming  program  where  the  original  input  of  the  iteration  and  the  feedback  of  the  iteration  are  connected  as  in  a  link    connected  streams  p    the  user  can  distinguish  between  the  two  inputs  using  co  transformation  thus  eliminating  the  need  for  mapping  the  inputs  and  outputs  to  a  common  type  param  i    type  of  the  input  of  the  iteration  param  f    type  of  the  feedback  of  the  iteration    public  public  static  class    connected  iterative  streams  i  f  extends    connected  streams  i  f  private    co  feedback  transformation  f  co  feedback  transformation  public    connected  iterative  streams    data  stream  i  input    type  information  f  feedback  type  long  wait  time  super  input  get  execution  environment  input  new    data  stream  input  get  execution  environment  new    co  feedback  transformation  input  get  parallelism  feedback  type  wait  time  this  co  feedback  transformation    co  feedback  transformation  f  get  second  input  get  transformation    closes  the  iteration    this  method  defines  the  end  of  the  iterative  program  part  that  will  be  fed  back  to  the  start  of  the  iteration  as  the  second  input  in  the  link    connected  streams  param  feedback  stream  link    data  stream  that  will  be  used  as  second  input  to  the  iteration  head  return    the  feedback  stream  public    data  stream  f  close  with    data  stream  f  feedback  stream    collection    transformation  predecessors  feedback  stream  get  transformation  get  transitive  predecessors  if  predecessors  contains  this  co  feedback  transformation  throw  new    unsupported  operation  exception    cannot  close  an  iteration  with  a  feedback    data  stream  that  does  not  originate  from  said  iteration  co  feedback  transformation  add  feedback  edge  feedback  stream  get  transformation  return  feedback  stream  private    unsupported  operation  exception  grouping  exception  new    unsupported  operation  exception    cannot  change  the  input  partitioning  of  an  iteration  head  directly    apply  the  partitioning  on  the  input  and  feedback  streams  instead    override  public    connected  streams  i  f  key  by  int  key  positions1  int  key  positions2  throw  grouping  exception    override  public    connected  streams  i  f  key  by    string  field1    string  field2  throw  grouping  exception    override  public    connected  streams  i  f  key  by    string  fields1    string  fields2  throw  grouping  exception    override  public    connected  streams  i  f  key  by    key  selector  i  key  selector1    key  selector  f  key  selector2  throw  grouping  exception    override  public  key    connected  streams  i  f  key  by    key  selector  i  key  key  selector1    key  selector  f  key  key  selector2    type  information  key  key  type  throw  grouping  exception  
public  evolving  public  class    queryable  state  stream  k  v    name  under  which  the  state  is  queryable  private  final    string  queryable  state  name    key  serializer  for  the  state  instance  private  final    type  serializer  k  key  serializer    state  descriptor  for  the  state  instance  private  final    state  descriptor  v  state  descriptor    creates  a  queryable  state  stream  param  queryable  state  name    name  under  which  to  publish  the  queryable  state  instance  param  state  descriptor    the  state  descriptor  for  the  state  instance  param  key  serializer    key  serializer  for  the  state  instance  public    queryable  state  stream    string  queryable  state  name    state  descriptor  v  state  descriptor    type  serializer  k  key  serializer  this  queryable  state  name    preconditions  check  not  null  queryable  state  name    queryable  state  name  this  state  descriptor    preconditions  check  not  null  state  descriptor    state    descriptor  this  key  serializer    preconditions  check  not  null  key  serializer    key  serializer    returns  the  name  under  which  the  state  can  be  queried  return    name  under  which  state  can  be  queried  public    string  get  queryable  state  name  return  queryable  state  name    returns  the  key  serializer  for  the  queryable  state  instance  return    key  serializer  for  the  state  instance  public    type  serializer  k  get  key  serializer  return  key  serializer    returns  the  state  descriptor  for  the  queryable  state  instance  return    state  descriptor  for  the  state  instance  public    state  descriptor  v  get  state  descriptor  return  state  descriptor  
public  evolving  public  class    stream  projection  in  private    data  stream  in  data  stream  private  int  field  indexes  protected    stream  projection    data  stream  in  data  stream  int  field  indexes  if  data  stream  get  type  is  tuple  type  throw  new    runtime  exception    only    tuple    data  streams  can  be  projected  if  field  indexes  length    throw  new    illegal  argument  exception  project  needs  to  select  at  least  one    field  else  if  field  indexes  length    tuple  max  arity    throw  new    illegal  argument  exception  project  may  select  only  up  to    tuple  max  arity    fields  int  max  field  index  data  stream  get  type  get  arity  for  int  i    i  field  indexes  length  i    preconditions  check  element  index  field  indexes  i  max  field  index  this  data  stream  data  stream  this  field  indexes  field  indexes    chooses  a  project  tuple  x  according  to  the  length  of  link  org  apache  flink  streaming  api  datastream    stream  projection  field  indexes  return    the  projected    data  stream  see  org  apache  flink  api  java  operators    project  operator    projection    suppress  warnings  unchecked  public  out  extends    tuple    single  output  stream  operator  out  project  tuple  x    single  output  stream  operator  out  proj  operator  null  switch  field  indexes  length  case    proj  operator    single  output  stream  operator  out  project  tuple1  break  case    proj  operator    single  output  stream  operator  out  project  tuple2  break  case    proj  operator    single  output  stream  operator  out  project  tuple3  break  case    proj  operator    single  output  stream  operator  out  project  tuple4  break  case    proj  operator    single  output  stream  operator  out  project  tuple5  break  case    proj  operator    single  output  stream  operator  out  project  tuple6  break  case    proj  operator    single  output  stream  operator  out  project  tuple7  break  case    proj  operator    single  output  stream  operator  out  project  tuple8  break  case    proj  operator    single  output  stream  operator  out  project  tuple9  break  case    proj  operator    single  output  stream  operator  out  project  tuple10  break  case    proj  operator    single  output  stream  operator  out  project  tuple11  break  case    proj  operator    single  output  stream  operator  out  project  tuple12  break  case    proj  operator    single  output  stream  operator  out  project  tuple13  break  case    proj  operator    single  output  stream  operator  out  project  tuple14  break  case    proj  operator    single  output  stream  operator  out  project  tuple15  break  case    proj  operator    single  output  stream  operator  out  project  tuple16  break  case    proj  operator    single  output  stream  operator  out  project  tuple17  break  case    proj  operator    single  output  stream  operator  out  project  tuple18  break  case    proj  operator    single  output  stream  operator  out  project  tuple19  break  case    proj  operator    single  output  stream  operator  out  project  tuple20  break  case    proj  operator    single  output  stream  operator  out  project  tuple21  break  case    proj  operator    single  output  stream  operator  out  project  tuple22  break  case    proj  operator    single  output  stream  operator  out  project  tuple23  break  case    proj  operator    single  output  stream  operator  out  project  tuple24  break  case    proj  operator    single  output  stream  operator  out  project  tuple25  break  default  throw  new    illegal  state  exception    excessive  arity  in  tuple  return  proj  operator    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    single  output  stream  operator    tuple1    t0  project  tuple1    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple1    t0  t  type  new    tuple  type  info    tuple1    t0  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple1    t0  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    single  output  stream  operator    tuple2    t0    t1  project  tuple2    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple2    t0    t1  t  type  new    tuple  type  info    tuple2    t0    t1  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple2    t0    t1  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    single  output  stream  operator    tuple3    t0    t1    t2  project  tuple3    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple3    t0    t1    t2  t  type  new    tuple  type  info    tuple3    t0    t1    t2  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple3    t0    t1    t2  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    single  output  stream  operator    tuple4    t0    t1    t2    t3  project  tuple4    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple4    t0    t1    t2    t3  t  type  new    tuple  type  info    tuple4    t0    t1    t2    t3  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple4    t0    t1    t2    t3  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    single  output  stream  operator    tuple5    t0    t1    t2    t3    t4  project  tuple5    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple5    t0    t1    t2    t3    t4  t  type  new    tuple  type  info    tuple5    t0    t1    t2    t3    t4  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple5    t0    t1    t2    t3    t4  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    single  output  stream  operator    tuple6    t0    t1    t2    t3    t4    t5  project  tuple6    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple6    t0    t1    t2    t3    t4    t5  t  type  new    tuple  type  info    tuple6    t0    t1    t2    t3    t4    t5  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple6    t0    t1    t2    t3    t4    t5  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    single  output  stream  operator    tuple7    t0    t1    t2    t3    t4    t5    t6  project  tuple7    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple7    t0    t1    t2    t3    t4    t5    t6  t  type  new    tuple  type  info    tuple7    t0    t1    t2    t3    t4    t5    t6  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple7    t0    t1    t2    t3    t4    t5    t6  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    single  output  stream  operator    tuple8    t0    t1    t2    t3    t4    t5    t6    t7  project  tuple8    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple8    t0    t1    t2    t3    t4    t5    t6    t7  t  type  new    tuple  type  info    tuple8    t0    t1    t2    t3    t4    t5    t6    t7  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple8    t0    t1    t2    t3    t4    t5    t6    t7  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    single  output  stream  operator    tuple9    t0    t1    t2    t3    t4    t5    t6    t7    t8  project  tuple9    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple9    t0    t1    t2    t3    t4    t5    t6    t7    t8  t  type  new    tuple  type  info    tuple9    t0    t1    t2    t3    t4    t5    t6    t7    t8  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple9    t0    t1    t2    t3    t4    t5    t6    t7    t8  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    single  output  stream  operator    tuple10    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9  project  tuple10    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple10    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9  t  type  new    tuple  type  info    tuple10    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple10    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    single  output  stream  operator    tuple11    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10  project  tuple11    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple11    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10  t  type  new    tuple  type  info    tuple11    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple11    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    single  output  stream  operator    tuple12    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11  project  tuple12    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple12    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11  t  type  new    tuple  type  info    tuple12    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple12    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    single  output  stream  operator    tuple13    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12  project  tuple13    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple13    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12  t  type  new    tuple  type  info    tuple13    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple13    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    single  output  stream  operator    tuple14    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13  project  tuple14    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple14    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13  t  type  new    tuple  type  info    tuple14    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple14    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    single  output  stream  operator    tuple15    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14  project  tuple15    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple15    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14  t  type  new    tuple  type  info    tuple15    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple15    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    single  output  stream  operator    tuple16    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15  project  tuple16    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple16    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15  t  type  new    tuple  type  info    tuple16    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple16    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    single  output  stream  operator    tuple17    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16  project  tuple17    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple17    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16  t  type  new    tuple  type  info    tuple17    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple17    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    single  output  stream  operator    tuple18    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17  project  tuple18    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple18    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17  t  type  new    tuple  type  info    tuple18    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple18    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    single  output  stream  operator    tuple19    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18  project  tuple19    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple19    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18  t  type  new    tuple  type  info    tuple19    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple19    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    single  output  stream  operator    tuple20    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19  project  tuple20    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple20    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19  t  type  new    tuple  type  info    tuple20    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple20    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    single  output  stream  operator    tuple21    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20  project  tuple21    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple21    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20  t  type  new    tuple  type  info    tuple21    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple21    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    single  output  stream  operator    tuple22    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21  project  tuple22    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple22    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21  t  type  new    tuple  type  info    tuple22    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple22    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    single  output  stream  operator    tuple23    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22  project  tuple23    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple23    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22  t  type  new    tuple  type  info    tuple23    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple23    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23    single  output  stream  operator    tuple24    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23  project  tuple24    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple24    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23  t  type  new    tuple  type  info    tuple24    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple24    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23  field  indexes  t  type  create  serializer  data  stream  get  execution  config    projects  a  link    tuple  link    data  stream  to  the  previously  selected  fields  return    the  projected    data  stream  see    tuple  see    data  stream  public    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23    t24    single  output  stream  operator    tuple25    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23    t24  project  tuple25    type  information  f  types  extract  field  types  field  indexes  data  stream  get  type    tuple  type  info    tuple25    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23    t24  t  type  new    tuple  type  info    tuple25    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23    t24  f  types  return  data  stream  transform    projection  t  type  new    stream  project  in    tuple25    t0    t1    t2    t3    t4    t5    t6    t7    t8    t9    t10    t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21    t22    t23    t24  field  indexes  t  type  create  serializer  data  stream  get  execution  config  public  static    type  information  extract  field  types  int  fields    type  information  in  type    tuple  type  info  in  tuple  type    tuple  type  info  in  type    type  information  field  types  new    type  information  fields  length  for  int  i    i  fields  length  i  field  types  i  in  tuple  type  get  type  at  fields  i  return  field  types  
public  evolving  public  class    unsupported  time  characteristic  exception  extends    flink  runtime  exception  private  static  final  long  serial  version  u  i  d    l  public    unsupported  time  characteristic  exception    string  message  super  message  
public  evolving  public  class    execution  checkpointing  options  public  static  final    config  option    checkpointing  mode  checkpointing  mode    config  options  key  execution  checkpointing  mode  enum  type    checkpointing  mode  class  default  value    checkpointing  mode  exactly  once  with  description    the  checkpointing  mode  exactly  once  vs  at  least  once  public  static  final    config  option    duration  checkpointing  timeout    config  options  key  execution  checkpointing  timeout  duration  type  default  value    duration  of  minutes    with  description    the  maximum  time  that  a  checkpoint  may  take  before  being  discarded  public  static  final    config  option    integer  max  concurrent  checkpoints    config  options  key  execution  checkpointing  max  concurrent  checkpoints  int  type  default  value    with  description    the  maximum  number  of  checkpoint  attempts  that  may  be  in  progress  at  the  same  time    if  this  value  is  n  then  no  checkpoints  will  be  triggered  while  n  checkpoint  attempts  are  currently  in  flight    for  the  next  checkpoint  to  be  triggered  one  checkpoint  attempt  would  need  to  finish  or  expire  public  static  final    config  option    duration  min  pause  between  checkpoints    config  options  key  execution  checkpointing  min  pause  duration  type  default  value    duration  zero  with  description    description  builder  text    the  minimal  pause  between  checkpointing  attempts    this  setting  defines  how  soon  the  checkpoint  coordinator  may  trigger  another  checkpoint  after  it  becomes  possible  to  trigger  another  checkpoint  with  respect  to  the  maximum  number  of  concurrent  checkpoints  see  s    text  element  code  max  concurrent  checkpoints  key  linebreak  linebreak  text    if  the  maximum  number  of  concurrent  checkpoints  is  set  to  one  this  setting  makes  effectively  sure  that  a  minimum  amount  of  time  passes  where  no  checkpoint  is  in  progress  at  all  build  public  static  final    config  option    boolean  prefer  checkpoint  for  recovery    config  options  key  execution  checkpointing  prefer  checkpoint  for  recovery  boolean  type  default  value  false  with  description    if  enabled  a  job  recovery  should  fallback  to  checkpoint  when  there  is  a  more  recent  savepoint  public  static  final    config  option    integer  tolerable  failure  number    config  options  key  execution  checkpointing  tolerable  failed  checkpoints  int  type  no  default  value  with  description    the  tolerable  checkpoint  failure  number    if  set  to    that  means  we  do  not  tolerance  any  checkpoint  failure  public  static  final    config  option    checkpoint  config    externalized  checkpoint  cleanup  externalized  checkpoint    config  options  key  execution  checkpointing  externalized  checkpoint  retention  enum  type    checkpoint  config    externalized  checkpoint  cleanup  class  no  default  value  with  description    description  builder  text    externalized  checkpoints  write  their  meta  data  out  to  persistent  storage  and  are  not  automatically  cleaned  up  when  the  owning  job  fails  or  is  suspended  terminating  with  job  status  s  or  s    in  this  case  you  have  to  manually  clean  up  the  checkpoint  state  both  the  meta  data  and  actual  program  state    text  element  code    job  status  failed    text  element  code    job  status  suspended  linebreak  linebreak  text    the  mode  defines  how  an  externalized  checkpoint  should  be  cleaned  up  on  job  cancellation    if  you  choose  to  retain  externalized  checkpoints  on  cancellation  you  have  to  handle  checkpoint  clean  up  manually  when  you  cancel  the  job  as  well  terminating  with  job  status  s    text  element  code    job  status  canceled  linebreak  linebreak  text    the  target  directory  for  externalized  checkpoints  is  configured  via  s    text  element  code    checkpointing  options  checkpoints  directory  key  build  public  static  final    config  option    duration  checkpointing  interval    config  options  key  execution  checkpointing  interval  duration  type  no  default  value  with  description    description  builder  text    gets  the  interval  in  which  checkpoints  are  periodically  scheduled  linebreak  linebreak  text    this  setting  defines  the  base  interval    checkpoint  triggering  may  be  delayed  by  the  settings  s  and  s    text  element  code  max  concurrent  checkpoints  key    text  element  code  min  pause  between  checkpoints  key  build  public  static  final    config  option    boolean  enable  unaligned    config  options  key  execution  checkpointing  unaligned  boolean  type  default  value  false  with  description    description  builder  text    enables  unaligned  checkpoints  which  greatly  reduce  checkpointing  times  under  backpressure  linebreak  linebreak  text    unaligned  checkpoints  contain  data  stored  in  buffers  as  part  of  the  checkpoint  state  which  allows  checkpoint  barriers  to  overtake  these  buffers    thus  the  checkpoint  duration  becomes  independent  of  the  current  throughput  as  checkpoint  barriers  are  effectively  not  embedded  into  the  stream  of  data  anymore  linebreak  linebreak  text    unaligned  checkpoints  can  only  be  enabled  if  s  is  s  and  if  s  is      text  element  code  checkpointing  mode  key    text  element  code    checkpointing  mode  exactly  once  to  string    text  element  code  max  concurrent  checkpoints  key  build  
public  evolving  public  interface    stream  execution  environment  factory    creates  a    stream  execution  environment  from  this  factory  return  a    stream  execution  environment    stream  execution  environment  create  execution  environment  
public  evolving  public  class    stream  pipeline  options  public  static  final    config  option    time  characteristic  time  characteristic    config  options  key  pipeline  time  characteristic  enum  type    time  characteristic  class  default  value    time  characteristic    processing  time  with  description    description  builder  text    the  time  characteristic  for  all  created  streams  e  g  processing  time  event  time  or  ingestion  time  linebreak  linebreak  text    if  you  set  the  characteristic  to    ingestion  time  or    event  time  this  will  set  a  default  watermark  update  interval  of    ms    if  this  is  not  applicable  for  your  application  you  should  change  it  using  s    text  element  code    pipeline  options  auto  watermark  interval  key  build  
public  evolving    deprecated  public  abstract  class    ascending  timestamp  extractor  t  extends  org  apache  flink  streaming  api  functions  timestamps    ascending  timestamp  extractor  t  
public  evolving  public  interface    async  function  in  out  extends    function    serializable    trigger  async  operation  for  each  stream  input  param  input  element  coming  from  an  upstream  task  param  result  future  to  be  completed  with  the  result  data  exception    exception  in  case  of  a  user  code  error    an  exception  will  make  the  task  fail  and  trigger  fail  over  process  void  async  invoke  in  input    result  future  out  result  future  throws    exception  link    async  function  async  invoke  timeout  occurred    by  default  the  result  future  is  exceptionally  completed  with  a  timeout  exception  param  input  element  coming  from  an  upstream  task  param  result  future  to  be  completed  with  the  result  data  default  void  timeout  in  input    result  future  out  result  future  throws    exception  result  future  complete  exceptionally  new    timeout  exception    async  function  call  has  timed  out  
public  evolving  public  interface    result  future  out    completes  the  result  future  with  a  collection  of  result  objects  p    note  that  it  should  be  called  for  exactly  one  time  in  the  user  code    calling  this  function  for  multiple  times  will  cause  data  lose  p    put  all  results  in  a  link    collection  and  then  emit  output  param  result  a  list  of  results  void  complete    collection  out  result    completes  the  result  future  exceptionally  with  an  exception  param  error  a    throwable  object  void  complete  exceptionally    throwable  error  
public  evolving  public  abstract  class    rich  async  function  in  out  extends    abstract  rich  function  implements    async  function  in  out  private  static  final  long  serial  version  u  i  d    l    override  public  void  set  runtime  context    runtime  context  runtime  context    preconditions  check  not  null  runtime  context  if  runtime  context  instanceof    iteration  runtime  context  super  set  runtime  context  new    rich  async  function  iteration  runtime  context    iteration  runtime  context  runtime  context  else  super  set  runtime  context  new    rich  async  function  runtime  context  runtime  context    override  public  abstract  void  async  invoke  in  input    result  future  out  result  future  throws    exception    wrapper  classes  a  wrapper  class  for  async  function  s  link    runtime  context    the  async  function  runtime  context  only  supports  basic  operations  which  are  thread  safe    consequently  state  access  accumulators  broadcast  variables  and  the  distributed  cache  are  disabled  private  static  class    rich  async  function  runtime  context  implements    runtime  context  private  final    runtime  context  runtime  context    rich  async  function  runtime  context    runtime  context  context  runtime  context    preconditions  check  not  null  context    override  public    string  get  task  name  return  runtime  context  get  task  name    override  public    metric  group  get  metric  group  return  runtime  context  get  metric  group    override  public  int  get  number  of  parallel  subtasks  return  runtime  context  get  number  of  parallel  subtasks    override  public  int  get  max  number  of  parallel  subtasks  return  runtime  context  get  max  number  of  parallel  subtasks    override  public  int  get  index  of  this  subtask  return  runtime  context  get  index  of  this  subtask    override  public  int  get  attempt  number  return  runtime  context  get  attempt  number    override  public    string  get  task  name  with  subtasks  return  runtime  context  get  task  name  with  subtasks    override  public    execution  config  get  execution  config  return  runtime  context  get  execution  config    override  public    class  loader  get  user  code  class  loader  return  runtime  context  get  user  code  class  loader    override  public    set    external  resource  info  get  external  resource  infos    string  resource  name  return  runtime  context  get  external  resource  infos  resource  name    unsupported  operations    override  public    distributed  cache  get  distributed  cache  throw  new    unsupported  operation  exception    distributed  cache  is  not  supported  in  rich  async  functions    override  public  t    value  state  t  get  state    value  state  descriptor  t  state  properties  throw  new    unsupported  operation  exception    state  is  not  supported  in  rich  async  functions    override  public  t    list  state  t  get  list  state    list  state  descriptor  t  state  properties  throw  new    unsupported  operation  exception    state  is  not  supported  in  rich  async  functions    override  public  t    reducing  state  t  get  reducing  state    reducing  state  descriptor  t  state  properties  throw  new    unsupported  operation  exception    state  is  not  supported  in  rich  async  functions    override  public  in  acc  out    aggregating  state  in  out  get  aggregating  state    aggregating  state  descriptor  in  acc  out  state  properties  throw  new    unsupported  operation  exception    state  is  not  supported  in  rich  async  functions    override  public  uk  uv    map  state  uk  uv  get  map  state    map  state  descriptor  uk  uv  state  properties  throw  new    unsupported  operation  exception    state  is  not  supported  in  rich  async  functions    override  public  v  a  extends    serializable  void  add  accumulator    string  name    accumulator  v  a  accumulator  throw  new    unsupported  operation  exception    accumulators  are  not  supported  in  rich  async  functions    override  public  v  a  extends    serializable    accumulator  v  a  get  accumulator    string  name  throw  new    unsupported  operation  exception    accumulators  are  not  supported  in  rich  async  functions    override  public    map    string    accumulator  get  all  accumulators  throw  new    unsupported  operation  exception    accumulators  are  not  supported  in  rich  async  functions    override  public    int  counter  get  int  counter    string  name  throw  new    unsupported  operation  exception    int  counters  are  not  supported  in  rich  async  functions    override  public    long  counter  get  long  counter    string  name  throw  new    unsupported  operation  exception    long  counters  are  not  supported  in  rich  async  functions    override  public    double  counter  get  double  counter    string  name  throw  new    unsupported  operation  exception    long  counters  are  not  supported  in  rich  async  functions    override  public    histogram  get  histogram    string  name  throw  new    unsupported  operation  exception    histograms  are  not  supported  in  rich  async  functions    override  public  boolean  has  broadcast  variable    string  name  throw  new    unsupported  operation  exception    broadcast  variables  are  not  supported  in  rich  async  functions    override  public  rt    list  rt  get  broadcast  variable    string  name  throw  new    unsupported  operation  exception    broadcast  variables  are  not  supported  in  rich  async  functions    override  public  t  c  c  get  broadcast  variable  with  initializer    string  name    broadcast  variable  initializer  t  c  initializer  throw  new    unsupported  operation  exception    broadcast  variables  are  not  supported  in  rich  async  functions  private  static  class    rich  async  function  iteration  runtime  context  extends    rich  async  function  runtime  context  implements    iteration  runtime  context  private  final    iteration  runtime  context  iteration  runtime  context    rich  async  function  iteration  runtime  context    iteration  runtime  context  iteration  runtime  context  super  iteration  runtime  context  this  iteration  runtime  context    preconditions  check  not  null  iteration  runtime  context    override  public  int  get  superstep  number  return  iteration  runtime  context  get  superstep  number    unsupported  operations    override  public  t  extends    aggregator  t  get  iteration  aggregator    string  name  throw  new    unsupported  operation  exception    iteration  aggregators  are  not  supported  in  rich  async  functions    override  public  t  extends    value  t  get  previous  iteration  aggregate    string  name  throw  new    unsupported  operation  exception    iteration  aggregators  are  not  supported  in  rich  async  functions  
public  evolving  public  abstract  class    base  broadcast  process  function  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d    l    the  base  context  available  to  all  methods  in  a  broadcast  process  function    this  include  link    broadcast  process  function    broadcast  process  functions  and  link    keyed  broadcast  process  function    keyed  broadcast  process  functions  abstract  class    base  context    timestamp  of  the  element  currently  being  processed  or  timestamp  of  a  firing  timer  p    this  might  be  code  null  for  example  if  the  time  characteristic  of  your  program  is  set  to  link  org  apache  flink  streaming  api    time  characteristic    processing  time  public  abstract    long  timestamp    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag  the  code    output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  public  abstract  x  void  output  final    output  tag  x  output  tag  final  x  value    returns  the  current  processing  time  public  abstract  long  current  processing  time    returns  the  current  event  time  watermark  public  abstract  long  current  watermark  a  base  link    base  context  context  available  to  the  broadcasted  stream  side  of  a  link  org  apache  flink  streaming  api  datastream    broadcast  connected  stream    broadcast  connected  stream  p    apart  from  the  basic  functionality  of  a  link    base  context  context  this  also  allows  to  get  and  update  the  elements  stored  in  the  link    broadcast  state  broadcast  state    in  other  words  it  gives  read  write  access  to  the  broadcast  state  public  abstract  class    context  extends    base  context    fetches  the  link    broadcast  state  with  the  specified  name  param  state  descriptor  the  link    map  state  descriptor  of  the  state  to  be  fetched  return    the  required  link    broadcast  state  broadcast  state  public  abstract  k  v    broadcast  state  k  v  get  broadcast  state  final    map  state  descriptor  k  v  state  descriptor  a  link    base  context  context  available  to  the  non  broadcasted  stream  side  of  a  link  org  apache  flink  streaming  api  datastream    broadcast  connected  stream    broadcast  connected  stream  p    apart  from  the  basic  functionality  of  a  link    base  context  context  this  also  allows  to  get  a  b  read  only  b  link    iterable  over  the  elements  stored  in  the  broadcast  state  public  abstract  class    read  only  context  extends    base  context    fetches  a  read  only  view  of  the  broadcast  state  with  the  specified  name  param  state  descriptor  the  link    map  state  descriptor  of  the  state  to  be  fetched  return    the  required  read  only  view  of  the  broadcast  state  public  abstract  k  v    read  only  broadcast  state  k  v  get  broadcast  state  final    map  state  descriptor  k  v  state  descriptor  
public  evolving  public  abstract  class    broadcast  process  function    i  n1    i  n2  out  extends    base  broadcast  process  function  private  static  final  long  serial  version  u  i  d    l    this  method  is  called  for  each  element  in  the  non  broadcast  link  org  apache  flink  streaming  api  datastream    data  stream  data  stream  p    this  function  can  output  zero  or  more  elements  using  the  link    collector  parameter  query  the  current  processing  event  time  and  also  query  and  update  the  local  keyed  state    finally  it  has  b  read  only  b  access  to  the  broadcast  state    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  value    the  stream  element  param  ctx  a  link    read  only  context  that  allows  querying  the  timestamp  of  the  element  querying  the  current  processing  event  time  and  updating  the  broadcast  state    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    the  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  into  recovery  public  abstract  void  process  element  final    i  n1  value  final    read  only  context  ctx  final    collector  out  out  throws    exception    this  method  is  called  for  each  element  in  the  link  org  apache  flink  streaming  api  datastream    broadcast  stream  broadcast  stream  p    this  function  can  output  zero  or  more  elements  using  the  link    collector  parameter  query  the  current  processing  event  time  and  also  query  and  update  the  internal  link  org  apache  flink  api  common  state    broadcast  state  broadcast  state    these  can  be  done  through  the  provided  link    context    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  value    the  stream  element  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  querying  the  current  processing  event  time  and  updating  the  broadcast  state    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    the  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  into  recovery  public  abstract  void  process  broadcast  element  final    i  n2  value  final    context  ctx  final    collector  out  out  throws    exception  a  link    base  broadcast  process  function    context  context  available  to  the  broadcast  side  of  a  link  org  apache  flink  streaming  api  datastream    broadcast  connected  stream  public  abstract  class    context  extends    base  broadcast  process  function    context  a  link    base  broadcast  process  function    context  context  available  to  the  non  keyed  side  of  a  link  org  apache  flink  streaming  api  datastream    broadcast  connected  stream  if  any  public  abstract  class    read  only  context  extends    base  broadcast  process  function    read  only  context  
public  evolving  public  abstract  class    co  process  function    i  n1    i  n2  out  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d  1  l    this  method  is  called  for  each  element  in  the  first  of  the  connected  streams  p    this  function  can  output  zero  or  more  elements  using  the  link    collector  parameter  and  also  update  internal  state  or  set  timers  using  the  link    context  parameter  param  value    the  stream  element  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  querying  the  link    time  domain  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    the  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  into  recovery  public  abstract  void  process  element1    i  n1  value    context  ctx    collector  out  out  throws    exception    this  method  is  called  for  each  element  in  the  second  of  the  connected  streams  p    this  function  can  output  zero  or  more  elements  using  the  link    collector  parameter  and  also  update  internal  state  or  set  timers  using  the  link    context  parameter  param  value    the  stream  element  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  querying  the  link    time  domain  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    the  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  into  recovery  public  abstract  void  process  element2    i  n2  value    context  ctx    collector  out  out  throws    exception    called  when  a  timer  set  using  link    timer  service  fires  param  timestamp    the  timestamp  of  the  firing  timer  param  ctx    an  link    on  timer  context  that  allows  querying  the  timestamp  of  the  firing  timer  querying  the  link    time  domain  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  for  returning  result  values  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  void  on  timer  long  timestamp    on  timer  context  ctx    collector  out  out  throws    exception    information  available  in  an  invocation  of  link  process  element1    object    context    collector  link  process  element2    object    context    collector  or  link  on  timer  long    on  timer  context    collector  public  abstract  class    context    timestamp  of  the  element  currently  being  processed  or  timestamp  of  a  firing  timer  p    this  might  be  code  null  for  example  if  the  time  characteristic  of  your  program  is  set  to  link  org  apache  flink  streaming  api    time  characteristic    processing  time  public  abstract    long  timestamp  a  link    timer  service  for  querying  time  and  registering  timers  public  abstract    timer  service  timer  service    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag  the  code    output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  public  abstract  x  void  output    output  tag  x  output  tag  x  value    information  available  in  an  invocation  of  link  on  timer  long    on  timer  context    collector  public  abstract  class    on  timer  context  extends    context    the  link    time  domain  of  the  firing  timer  public  abstract    time  domain  time  domain  
public  evolving  public  abstract  class    keyed  broadcast  process  function  ks    i  n1    i  n2  out  extends    base  broadcast  process  function  private  static  final  long  serial  version  u  i  d    l    this  method  is  called  for  each  element  in  the  non  broadcast  link  org  apache  flink  streaming  api  datastream    keyed  stream  keyed  stream  p    it  can  output  zero  or  more  elements  using  the  link    collector  parameter  query  the  current  processing  event  time  and  also  query  and  update  the  local  keyed  state    in  addition  it  can  get  a  link    timer  service  for  registering  timers  and  querying  the  time    finally  it  has  b  read  only  b  access  to  the  broadcast  state    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  value    the  stream  element  param  ctx  a  link    read  only  context  that  allows  querying  the  timestamp  of  the  element  querying  the  current  processing  event  time  and  iterating  the  broadcast  state  with  b  read  only  b  access    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    the  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  into  recovery  public  abstract  void  process  element  final    i  n1  value  final    read  only  context  ctx  final    collector  out  out  throws    exception    this  method  is  called  for  each  element  in  the  link  org  apache  flink  streaming  api  datastream    broadcast  stream  broadcast  stream  p    it  can  output  zero  or  more  elements  using  the  link    collector  parameter  query  the  current  processing  event  time  and  also  query  and  update  the  internal  link  org  apache  flink  api  common  state    broadcast  state  broadcast  state    in  addition  it  can  register  a  link    keyed  state  function  function  to  be  applied  to  all  keyed  states  on  the  local  partition    these  can  be  done  through  the  provided  link    context    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  value    the  stream  element  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  querying  the  current  processing  event  time  and  updating  the  broadcast  state    in  addition  it  allows  the  registration  of  a  link    keyed  state  function  function  to  be  applied  to  all  keyed  state  with  a  given  link    state  descriptor  on  the  local  partition    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    the  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  into  recovery  public  abstract  void  process  broadcast  element  final    i  n2  value  final    context  ctx  final    collector  out  out  throws    exception    called  when  a  timer  set  using  link    timer  service  fires  param  timestamp    the  timestamp  of  the  firing  timer  param  ctx    an  link    on  timer  context  that  allows  querying  the  timestamp  of  the  firing  timer  querying  the  current  processing  event  time  iterating  the  broadcast  state  with  b  read  only  b  access  querying  the  link    time  domain  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  for  returning  result  values  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  void  on  timer  final  long  timestamp  final    on  timer  context  ctx  final    collector  out  out  throws    exception  the  default  implementation  does  nothing  a  link    base  broadcast  process  function    context  context  available  to  the  broadcast  side  of  a  link  org  apache  flink  streaming  api  datastream    broadcast  connected  stream  p    apart  from  the  basic  functionality  of  a  link    base  broadcast  process  function    context  context  this  also  allows  to  apply  a  link    keyed  state  function  to  the  local  states  of  all  active  keys  in  the  your  backend  public  abstract  class    context  extends    base  broadcast  process  function    context    applies  the  provided  code  function  to  the  state  associated  with  the  provided  code  state  descriptor  param  state  descriptor  the  descriptor  of  the  state  to  be  processed  param  function  the  function  to  be  applied  public  abstract  vs  s  extends    state  void  apply  to  keyed  state  final    state  descriptor  s  vs  state  descriptor  final    keyed  state  function  ks  s  function  throws    exception  a  link    base  broadcast  process  function    context  context  available  to  the  keyed  stream  side  of  a  link  org  apache  flink  streaming  api  datastream    broadcast  connected  stream  if  any  p    apart  from  the  basic  functionality  of  a  link    base  broadcast  process  function    context  context  this  also  allows  to  get  a  b  read  only  b  link    iterable  over  the  elements  stored  in  the  broadcast  state  and  a  link    timer  service  for  querying  time  and  registering  timers  public  abstract  class    read  only  context  extends    base  broadcast  process  function    read  only  context  a  link    timer  service  for  querying  time  and  registering  timers  public  abstract    timer  service  timer  service    get  key  of  the  element  being  processed  public  abstract  ks  get  current  key    information  available  in  an  invocation  of  link  on  timer  long    on  timer  context    collector  public  abstract  class    on  timer  context  extends    read  only  context    the  link    time  domain  of  the  firing  timer  i  e  if  it  is  event  or  processing  time  timer  public  abstract    time  domain  time  domain    get  the  key  of  the  firing  timer    override  public  abstract  ks  get  current  key  
public  evolving  public  abstract  class    keyed  co  process  function  k    i  n1    i  n2  out  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d  1  l    this  method  is  called  for  each  element  in  the  first  of  the  connected  streams  p    this  function  can  output  zero  or  more  elements  using  the  link    collector  parameter  and  also  update  internal  state  or  set  timers  using  the  link    context  parameter  param  value    the  stream  element  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  querying  the  link    time  domain  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    the  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  into  recovery  public  abstract  void  process  element1    i  n1  value    context  ctx    collector  out  out  throws    exception    this  method  is  called  for  each  element  in  the  second  of  the  connected  streams  p    this  function  can  output  zero  or  more  elements  using  the  link    collector  parameter  and  also  update  internal  state  or  set  timers  using  the  link    context  parameter  param  value    the  stream  element  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  querying  the  link    time  domain  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    the  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  into  recovery  public  abstract  void  process  element2    i  n2  value    context  ctx    collector  out  out  throws    exception    called  when  a  timer  set  using  link    timer  service  fires  param  timestamp    the  timestamp  of  the  firing  timer  param  ctx    an  link    on  timer  context  that  allows  querying  the  timestamp  of  the  firing  timer  querying  the  link    time  domain  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  for  returning  result  values  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  void  on  timer  long  timestamp    on  timer  context  ctx    collector  out  out  throws    exception    information  available  in  an  invocation  of  link  process  element1    object    context    collector  link  process  element2    object    context    collector  or  link  on  timer  long    on  timer  context    collector  public  abstract  class    context    timestamp  of  the  element  currently  being  processed  or  timestamp  of  a  firing  timer  p    this  might  be  code  null  for  example  if  the  time  characteristic  of  your  program  is  set  to  link  org  apache  flink  streaming  api    time  characteristic    processing  time  public  abstract    long  timestamp  a  link    timer  service  for  querying  time  and  registering  timers  public  abstract    timer  service  timer  service    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag  the  code    output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  public  abstract  x  void  output    output  tag  x  output  tag  x  value    get  key  of  the  element  being  processed  public  abstract  k  get  current  key    information  available  in  an  invocation  of  link  on  timer  long    on  timer  context    collector  public  abstract  class    on  timer  context  extends    context    the  link    time  domain  of  the  firing  timer  public  abstract    time  domain  time  domain    get  key  of  the  firing  timer    override  public  abstract  k  get  current  key  
public  evolving  public  abstract  class    process  join  function    i  n1    i  n2  out  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d    l    this  method  is  called  for  each  joined  pair  of  elements    it  can  output  zero  or  more  elements  through  the  provided  link    collector  and  has  access  to  the  timestamps  of  the  joined  elements  and  the  result  through  the  link    context  param  left    the  left  element  of  the  joined  pair  param  right    the  right  element  of  the  joined  pair  param  ctx  a  context  that  allows  querying  the  timestamps  of  the  left  right  and  joined  pair    in  addition  this  context  allows  to  emit  elements  on  a  side  output  param  out    the  collector  to  emit  resulting  elements  to  throws    exception    this  function  may  throw  exceptions  which  cause  the  streaming  program  to  fail  and  go  in  recovery  mode  public  abstract  void  process  element    i  n1  left    i  n2  right    context  ctx    collector  out  out  throws    exception    the  context  that  is  available  during  an  invocation  of  link  process  element    object    object    context    collector    it  gives  access  to  the  timestamps  of  the  left  element  in  the  joined  pair  the  right  one  and  that  of  the  joined  pair    in  addition  this  context  allows  to  emit  elements  on  a  side  output  public  abstract  class    context  return    the  timestamp  of  the  left  element  of  a  joined  pair  public  abstract  long  get  left  timestamp  return    the  timestamp  of  the  right  element  of  a  joined  pair  public  abstract  long  get  right  timestamp  return    the  timestamp  of  the  joined  pair  public  abstract  long  get  timestamp    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag    the  output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  public  abstract  x  void  output    output  tag  x  output  tag  x  value  
public  evolving  public  abstract  class    keyed  process  function  k  i  o  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d  1  l    process  one  element  from  the  input  stream  p    this  function  can  output  zero  or  more  elements  using  the  link    collector  parameter  and  also  update  internal  state  or  set  timers  using  the  link    context  parameter  param  value    the  input  value  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  for  returning  result  values  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract  void  process  element  i  value    context  ctx    collector  o  out  throws    exception    called  when  a  timer  set  using  link    timer  service  fires  param  timestamp    the  timestamp  of  the  firing  timer  param  ctx    an  link    on  timer  context  that  allows  querying  the  timestamp  the  link    time  domain  and  the  key  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  for  returning  result  values  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  void  on  timer  long  timestamp    on  timer  context  ctx    collector  o  out  throws    exception    information  available  in  an  invocation  of  link  process  element    object    context    collector  or  link  on  timer  long    on  timer  context    collector  public  abstract  class    context    timestamp  of  the  element  currently  being  processed  or  timestamp  of  a  firing  timer  p    this  might  be  code  null  for  example  if  the  time  characteristic  of  your  program  is  set  to  link  org  apache  flink  streaming  api    time  characteristic    processing  time  public  abstract    long  timestamp  a  link    timer  service  for  querying  time  and  registering  timers  public  abstract    timer  service  timer  service    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag  the  code    output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  public  abstract  x  void  output    output  tag  x  output  tag  x  value    get  key  of  the  element  being  processed  public  abstract  k  get  current  key    information  available  in  an  invocation  of  link  on  timer  long    on  timer  context    collector  public  abstract  class    on  timer  context  extends    context    the  link    time  domain  of  the  firing  timer  public  abstract    time  domain  time  domain    get  key  of  the  firing  timer    override  public  abstract  k  get  current  key  
public  evolving  public  abstract  class    process  function  i  o  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d  1  l    process  one  element  from  the  input  stream  p    this  function  can  output  zero  or  more  elements  using  the  link    collector  parameter  and  also  update  internal  state  or  set  timers  using  the  link    context  parameter  param  value    the  input  value  param  ctx  a  link    context  that  allows  querying  the  timestamp  of  the  element  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  for  returning  result  values  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  abstract  void  process  element  i  value    context  ctx    collector  o  out  throws    exception    called  when  a  timer  set  using  link    timer  service  fires  param  timestamp    the  timestamp  of  the  firing  timer  param  ctx    an  link    on  timer  context  that  allows  querying  the  timestamp  of  the  firing  timer  querying  the  link    time  domain  of  the  firing  timer  and  getting  a  link    timer  service  for  registering  timers  and  querying  the  time    the  context  is  only  valid  during  the  invocation  of  this  method  do  not  store  it  param  out    the  collector  for  returning  result  values  throws    exception    this  method  may  throw  exceptions    throwing  an  exception  will  cause  the  operation  to  fail  and  may  trigger  recovery  public  void  on  timer  long  timestamp    on  timer  context  ctx    collector  o  out  throws    exception    information  available  in  an  invocation  of  link  process  element    object    context    collector  or  link  on  timer  long    on  timer  context    collector  public  abstract  class    context    timestamp  of  the  element  currently  being  processed  or  timestamp  of  a  firing  timer  p    this  might  be  code  null  for  example  if  the  time  characteristic  of  your  program  is  set  to  link  org  apache  flink  streaming  api    time  characteristic    processing  time  public  abstract    long  timestamp  a  link    timer  service  for  querying  time  and  registering  timers  public  abstract    timer  service  timer  service    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag  the  code    output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  public  abstract  x  void  output    output  tag  x  output  tag  x  value    information  available  in  an  invocation  of  link  on  timer  long    on  timer  context    collector  public  abstract  class    on  timer  context  extends    context    the  link    time  domain  of  the  firing  timer  public  abstract    time  domain  time  domain  
public  evolving  public  interface    bucket  assigner  in    bucket  i  d  extends    serializable    returns  the  identifier  of  the  bucket  the  provided  element  should  be  put  into  param  element    the  current  element  being  processed  param  context    the  link    sink  function    context  context  used  by  the  link    streaming  file  sink  sink  return  a  string  representing  the  identifier  of  the  bucket  the  element  should  be  put  into    the  actual  path  to  the  bucket  will  result  from  the  concatenation  of  the  returned  string  and  the  code  base  path  provided  during  the  initialization  of  the  link    streaming  file  sink  sink    bucket  i  d  get  bucket  id  in  element    bucket  assigner    context  context  return  a  link    simple  versioned  serializer  capable  of  serializing  deserializing  the  elements  of  type  code    bucket  i  d    that  is  the  type  of  the  objects  returned  by  the  link  get  bucket  id    object    bucket  assigner    context    simple  versioned  serializer    bucket  i  d  get  serializer    context  that  the  link    bucket  assigner  can  use  for  getting  additional  data  about  an  input  record  p    the  context  is  only  valid  for  the  duration  of  a  link    bucket  assigner  get  bucket  id    object    bucket  assigner    context  call    do  not  store  the  context  and  use  afterwards    public  evolving  interface    context    returns  the  current  processing  time  long  current  processing  time    returns  the  current  event  time  watermark  long  current  watermark    returns  the  timestamp  of  the  current  input  record  or  code  null  if  the  element  does  not  have  an  assigned  timestamp    nullable    long  timestamp  
public  evolving  public  class    base  path  bucket  assigner  t  implements    bucket  assigner  t    string  private  static  final  long  serial  version  u  i  d    l    override  public    string  get  bucket  id  t  element    bucket  assigner    context  context  return    override  public    simple  versioned  serializer    string  get  serializer  in  the  future  this  could  be  optimized  as  it  is  the  empty  string  return    simple  versioned  string  serializer  instance    override  public    string  to  string  return    base  path  bucket  assigner  
public  evolving  public  class    date  time  bucket  assigner  in  implements    bucket  assigner  in    string  private  static  final  long  serial  version  u  i  d  1  l  private  static  final    string  default  format  string  yyyy  mm  dd  hh  private  final    string  format  string  private  final    zone  id  zone  id  private  transient    date  time  formatter  date  time  formatter    creates  a  new  code    date  time  bucket  assigner  with  format  string  code  yyyy  mm  dd  hh  public    date  time  bucket  assigner  this  default  format  string    creates  a  new  code    date  time  bucket  assigner  with  the  given  date  time  format  string  param  format  string    the  format  string  that  will  be  given  to  code    simple  date  format  to  determine  the  bucket  id  public    date  time  bucket  assigner    string  format  string  this  format  string    zone  id  system  default    creates  a  new  code    date  time  bucket  assigner  with  format  string  code  yyyy  mm  dd  hh  using  the  given  timezone  param  zone  id    the  timezone  used  to  format  code    date  time  formatter  for  bucket  id  public    date  time  bucket  assigner    zone  id  zone  id  this  default  format  string  zone  id    creates  a  new  code    date  time  bucket  assigner  with  the  given  date  time  format  string  using  the  given  timezone  param  format  string    the  format  string  that  will  be  given  to  code    date  time  formatter  to  determine  the  bucket  path  param  zone  id    the  timezone  used  to  format  code    date  time  formatter  for  bucket  id  public    date  time  bucket  assigner    string  format  string    zone  id  zone  id  this  format  string    preconditions  check  not  null  format  string  this  zone  id    preconditions  check  not  null  zone  id    override  public    string  get  bucket  id  in  element    bucket  assigner    context  context  if  date  time  formatter  null  date  time  formatter    date  time  formatter  of  pattern  format  string  with  zone  zone  id  return  date  time  formatter  format    instant  of  epoch  milli  context  current  processing  time    override  public    simple  versioned  serializer    string  get  serializer  return    simple  versioned  string  serializer  instance    override  public    string  to  string  return    date  time  bucket  assigner  format  string  format  string  zone  id  zone  id  
public  evolving  public  final  class    simple  versioned  string  serializer  implements    simple  versioned  serializer    string  private  static  final    charset  charset    standard  charsets  utf    public  static  final    simple  versioned  string  serializer  instance  new    simple  versioned  string  serializer    override  public  int  get  version  return      override  public  byte  serialize    string  value  final  byte  serialized  value  get  bytes    standard  charsets  utf    final  byte  target  bytes  new  byte    integer  bytes  serialized  length  final    byte  buffer  bb    byte  buffer  wrap  target  bytes  order    byte  order  little  endian  bb  put  int  serialized  length  bb  put  serialized  return  target  bytes    override  public    string  deserialize  int  version  byte  serialized  throws    i  o  exception  switch  version  case    return  deserialize  v1  serialized  default  throw  new    i  o  exception    unrecognized  version  or  corrupt  state  version  private  static    string  deserialize  v1  byte  serialized  final    byte  buffer  bb    byte  buffer  wrap  serialized  order    byte  order  little  endian  final  byte  target  string  bytes  new  byte  bb  get  int  bb  get  target  string  bytes  return  new    string  target  string  bytes  charset    private  constructor  to  prevent  instantiation    access  the  serializer  through  the  link  instance  private    simple  versioned  string  serializer  
public  evolving  public  static  class    output  file  config  builder  private  static  final    string  default  part  prefix  part  private  static  final    string  default  part  suffix  private    string  part  prefix  private    string  part  suffix  private    output  file  config  builder  this  part  prefix  default  part  prefix  this  part  suffix  default  part  suffix  public    output  file  config  builder  with  part  prefix    string  prefix  this  part  prefix  prefix  return  this  public    output  file  config  builder  with  part  suffix    string  suffix  this  part  suffix  suffix  return  this  public    output  file  config  build  return  new    output  file  config  part  prefix  part  suffix  
public  evolving  public  interface    part  file  info    bucket  i  d  return    the  bucket  identifier  of  the  current  buffer  as  returned  by  the  link    bucket  assigner  get  bucket  id    object    bucket  assigner    context    bucket  i  d  get  bucket  id  return    the  creation  time  in  ms  of  the  currently  open  part  file  long  get  creation  time  return    the  size  of  the  currently  open  part  file  long  get  size  throws    i  o  exception  return    the  last  time  in  ms  the  currently  open  part  file  was  written  to  long  get  last  update  time  
public  evolving  public  abstract  class    checkpoint  rolling  policy  in    bucket  i  d  implements    rolling  policy  in    bucket  i  d  public  boolean  should  roll  on  checkpoint    part  file  info    bucket  i  d  part  file  state  return  true  public  abstract  boolean  should  roll  on  event  final    part  file  info    bucket  i  d  part  file  state  in  element  throws    i  o  exception  public  abstract  boolean  should  roll  on  processing  time  final    part  file  info    bucket  i  d  part  file  state  final  long  current  time  throws    i  o  exception    the  base  abstract  builder  class  for  link    checkpoint  rolling  policy  public  abstract  static  class    policy  builder  in    bucket  i  d  t  extends    policy  builder  in    bucket  i  d  t    suppress  warnings  unchecked  protected  t  self  return  t  this  public  abstract    checkpoint  rolling  policy  in    bucket  i  d  build  
public  evolving  public  final  class    default  rolling  policy  in    bucket  i  d  implements    rolling  policy  in    bucket  i  d  private  static  final  long  serial  version  u  i  d  1  l  private  static  final  long  default  inactivity  interval  60  l    l  private  static  final  long  default  rollover  interval  60  l    l  private  static  final  long  default  max  part  size    l    l    l  private  final  long  part  size  private  final  long  rollover  interval  private  final  long  inactivity  interval    private  constructor  to  avoid  direct  instantiation  private    default  rolling  policy  long  part  size  long  rollover  interval  long  inactivity  interval    preconditions  check  argument  part  size  0  l    preconditions  check  argument  rollover  interval  0  l    preconditions  check  argument  inactivity  interval  0  l  this  part  size  part  size  this  rollover  interval  rollover  interval  this  inactivity  interval  inactivity  interval    override  public  boolean  should  roll  on  checkpoint    part  file  info    bucket  i  d  part  file  state  throws    i  o  exception  return  part  file  state  get  size  part  size    override  public  boolean  should  roll  on  event    part  file  info    bucket  i  d  part  file  state  in  element  throws    i  o  exception  return  part  file  state  get  size  part  size    override  public  boolean  should  roll  on  processing  time  final    part  file  info    bucket  i  d  part  file  state  final  long  current  time  return  current  time  part  file  state  get  creation  time  rollover  interval  current  time  part  file  state  get  last  update  time  inactivity  interval    returns  the  maximum  part  file  size  before  rolling  return    max  size  in  bytes  public  long  get  max  part  size  return  part  size    returns  the  maximum  time  duration  a  part  file  can  stay  open  before  rolling  return    time  duration  in  milliseconds  public  long  get  rollover  interval  return  rollover  interval    returns  time  duration  of  allowed  inactivity  after  which  a  part  file  will  have  to  roll  return    time  duration  in  milliseconds  public  long  get  inactivity  interval  return  inactivity  interval    creates  a  new  link    policy  builder  that  is  used  to  configure  and  build  an  instance  of  code    default  rolling  policy  public  static    default  rolling  policy    policy  builder  builder  return  new    default  rolling  policy    policy  builder  default  max  part  size  default  rollover  interval  default  inactivity  interval    this  method  is  link    deprecated  use  link    default  rolling  policy  builder  instead    deprecated  public  static    default  rolling  policy    policy  builder  create  return  builder  a  helper  class  that  holds  the  configuration  properties  for  the  link    default  rolling  policy    the  link    policy  builder  build  method  must  be  called  to  instantiate  the  policy    public  evolving  public  static  final  class    policy  builder  private  final  long  part  size  private  final  long  rollover  interval  private  final  long  inactivity  interval  private    policy  builder  final  long  part  size  final  long  rollover  interval  final  long  inactivity  interval  this  part  size  part  size  this  rollover  interval  rollover  interval  this  inactivity  interval  inactivity  interval    sets  the  part  size  above  which  a  part  file  will  have  to  roll  param  size  the  allowed  part  size  public    default  rolling  policy    policy  builder  with  max  part  size  final  long  size    preconditions  check  state  size  0  l  return  new    policy  builder  size  rollover  interval  inactivity  interval    sets  the  interval  of  allowed  inactivity  after  which  a  part  file  will  have  to  roll    the  frequency  at  which  this  is  checked  is  controlled  by  the  link  org  apache  flink  streaming  api  functions  sink  filesystem    streaming  file  sink    row  format  builder  with  bucket  check  interval  long  setting  param  interval  the  allowed  inactivity  interval  public    default  rolling  policy    policy  builder  with  inactivity  interval  final  long  interval    preconditions  check  state  interval  0  l  return  new    policy  builder  part  size  rollover  interval  interval    sets  the  max  time  a  part  file  can  stay  open  before  having  to  roll    the  frequency  at  which  this  is  checked  is  controlled  by  the  link  org  apache  flink  streaming  api  functions  sink  filesystem    streaming  file  sink    row  format  builder  with  bucket  check  interval  long  setting  param  interval  the  desired  rollover  interval  public    default  rolling  policy    policy  builder  with  rollover  interval  final  long  interval    preconditions  check  state  interval  0  l  return  new    policy  builder  part  size  interval  inactivity  interval    creates  the  actual  policy  public  in    bucket  i  d    default  rolling  policy  in    bucket  i  d  build  return  new    default  rolling  policy  part  size  rollover  interval  inactivity  interval  
public  evolving  public  final  class    on  checkpoint  rolling  policy  in    bucket  i  d  extends    checkpoint  rolling  policy  in    bucket  i  d  private  static  final  long  serial  version  u  i  d  1  l  private    on  checkpoint  rolling  policy    override  public  boolean  should  roll  on  event    part  file  info    bucket  i  d  part  file  state  in  element  return  false    override  public  boolean  should  roll  on  processing  time    part  file  info    bucket  i  d  part  file  state  long  current  time  return  false  public  static  in    bucket  i  d    on  checkpoint  rolling  policy  in    bucket  i  d  build  return  new    on  checkpoint  rolling  policy  
public  evolving  public  interface    rolling  policy  in    bucket  i  d  extends    serializable    determines  if  the  in  progress  part  file  for  a  bucket  should  roll  on  every  checkpoint  param  part  file  state  the  state  of  the  currently  open  part  file  of  the  bucket  return  code    true  if  the  part  file  should  roll  link  false  otherwise  boolean  should  roll  on  checkpoint  final    part  file  info    bucket  i  d  part  file  state  throws    i  o  exception    determines  if  the  in  progress  part  file  for  a  bucket  should  roll  based  on  its  current  state  e  g  its  size  param  element  the  element  being  processed  param  part  file  state  the  state  of  the  currently  open  part  file  of  the  bucket  return  code    true  if  the  part  file  should  roll  link  false  otherwise  boolean  should  roll  on  event  final    part  file  info    bucket  i  d  part  file  state  in  element  throws    i  o  exception    determines  if  the  in  progress  part  file  for  a  bucket  should  roll  based  on  a  time  condition  param  part  file  state  the  state  of  the  currently  open  part  file  of  the  bucket  param  current  time  the  current  processing  time  return  code    true  if  the  part  file  should  roll  link  false  otherwise  boolean  should  roll  on  processing  time  final    part  file  info    bucket  i  d  part  file  state  final  long  current  time  throws    i  o  exception  
public  evolving  public  class    streaming  file  sink  in  extends    rich  sink  function  in  implements    checkpointed  function    checkpoint  listener  private  static  final  long  serial  version  u  i  d  1  l  configuration  fields  private  final  long  bucket  check  interval  private  final    buckets  builder  in  extends    buckets  builder  in  buckets  builder  runtime  fields  private  transient    streaming  file  sink  helper  in  helper    creates  a  new  code    streaming  file  sink  that  writes  files  to  the  given  base  directory  with  the  give  buckets  properties  protected    streaming  file  sink    buckets  builder  in  extends    buckets  builder  in  buckets  builder  long  bucket  check  interval    preconditions  check  argument  bucket  check  interval  0  l  this  buckets  builder    preconditions  check  not  null  buckets  builder  this  bucket  check  interval  bucket  check  interval    sink    builders    creates  the  builder  for  a  code    streaming  file  sink  with  row  encoding  format  param  base  path  the  base  path  where  all  the  buckets  are  going  to  be  created  as  sub  directories  param  encoder  the  link    encoder  to  be  used  when  writing  elements  in  the  buckets  param  in  the  type  of  incoming  elements  return    the  builder  where  the  remaining  of  the  configuration  parameters  for  the  sink  can  be  configured    in  order  to  instantiate  the  sink  call  link    row  format  builder  build  after  specifying  the  desired  parameters  public  static  in    streaming  file  sink    default  row  format  builder  in  for  row  format  final    path  base  path  final    encoder  in  encoder  return  new    default  row  format  builder  base  path  encoder  new    date  time  bucket  assigner    creates  the  builder  for  a  link    streaming  file  sink  with  row  encoding  format  param  base  path  the  base  path  where  all  the  buckets  are  going  to  be  created  as  sub  directories  param  writer  factory  the  link    bulk  writer    factory  to  be  used  when  writing  elements  in  the  buckets  param  in  the  type  of  incoming  elements  return    the  builder  where  the  remaining  of  the  configuration  parameters  for  the  sink  can  be  configured    in  order  to  instantiate  the  sink  call  link    row  format  builder  build  after  specifying  the  desired  parameters  public  static  in    streaming  file  sink    default  bulk  format  builder  in  for  bulk  format  final    path  base  path  final    bulk  writer    factory  in  writer  factory  return  new    streaming  file  sink    default  bulk  format  builder  base  path  writer  factory  new    date  time  bucket  assigner    the  base  abstract  class  for  the  link    row  format  builder  and  link    bulk  format  builder    internal  public  abstract  static  class    buckets  builder  in    bucket  i  d  t  extends    buckets  builder  in    bucket  i  d  t  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l  public  static  final  long  default  bucket  check  interval  60  l    l    suppress  warnings  unchecked  protected  t  self  return  t  this    internal  public  abstract    buckets  in    bucket  i  d  create  buckets  final  int  subtask  index  throws    i  o  exception  a  builder  for  configuring  the  sink  for  row  wise  encoding  formats    public  evolving  public  static  class    row  format  builder  in    bucket  i  d  t  extends    row  format  builder  in    bucket  i  d  t  extends    streaming  file  sink    buckets  builder  in    bucket  i  d  t  private  static  final  long  serial  version  u  i  d  1  l  private  long  bucket  check  interval  private  final    path  base  path  private    encoder  in  encoder  private    bucket  assigner  in    bucket  i  d  bucket  assigner  private    rolling  policy  in    bucket  i  d  rolling  policy  private    bucket  factory  in    bucket  i  d  bucket  factory  private    output  file  config  output  file  config  protected    row  format  builder    path  base  path    encoder  in  encoder    bucket  assigner  in    bucket  i  d  bucket  assigner  this  base  path  encoder  bucket  assigner    default  rolling  policy  builder  build  default  bucket  check  interval  new    default  bucket  factory  impl    output  file  config  builder  build  protected    row  format  builder    path  base  path    encoder  in  encoder    bucket  assigner  in    bucket  i  d  assigner    rolling  policy  in    bucket  i  d  policy  long  bucket  check  interval    bucket  factory  in    bucket  i  d  bucket  factory    output  file  config  output  file  config  this  base  path    preconditions  check  not  null  base  path  this  encoder    preconditions  check  not  null  encoder  this  bucket  assigner    preconditions  check  not  null  assigner  this  rolling  policy    preconditions  check  not  null  policy  this  bucket  check  interval  bucket  check  interval  this  bucket  factory    preconditions  check  not  null  bucket  factory  this  output  file  config    preconditions  check  not  null  output  file  config  public  long  get  bucket  check  interval  return  bucket  check  interval  public  t  with  bucket  check  interval  final  long  interval  this  bucket  check  interval  interval  return  self  public  t  with  bucket  assigner  final    bucket  assigner  in    bucket  i  d  assigner  this  bucket  assigner    preconditions  check  not  null  assigner  return  self  public  t  with  rolling  policy  final    rolling  policy  in    bucket  i  d  policy  this  rolling  policy    preconditions  check  not  null  policy  return  self  public  t  with  output  file  config  final    output  file  config  output  file  config  this  output  file  config  output  file  config  return  self  public  id    streaming  file  sink    row  format  builder  in  id  extends    row  format  builder  in  id  with  new  bucket  assigner  and  policy  final    bucket  assigner  in  id  assigner  final    rolling  policy  in  id  policy    preconditions  check  state  bucket  factory  get  class    default  bucket  factory  impl  class  new  builder  with  bucket  assigner  and  policy  cannot  be  called  after  specifying  a  customized  bucket  factory  return  new    row  format  builder  base  path  encoder    preconditions  check  not  null  assigner    preconditions  check  not  null  policy  bucket  check  interval  new    default  bucket  factory  impl  output  file  config    creates  the  actual  sink  public    streaming  file  sink  in  build  return  new    streaming  file  sink  this  bucket  check  interval    visible  for  testing  t  with  bucket  factory  final    bucket  factory  in    bucket  i  d  factory  this  bucket  factory    preconditions  check  not  null  factory  return  self    internal    override  public    buckets  in    bucket  i  d  create  buckets  int  subtask  index  throws    i  o  exception  return  new    buckets  base  path  bucket  assigner  bucket  factory  new    row  wise  bucket  writer    file  system  get  base  path  to  uri  create  recoverable  writer  encoder  rolling  policy  subtask  index  output  file  config    builder  for  the  vanilla  link    streaming  file  sink  using  a  row  format  param  in  record  type  public  static  final  class    default  row  format  builder  in  extends    row  format  builder  in    string    default  row  format  builder  in  private  static  final  long  serial  version  u  i  d    l  private    default  row  format  builder    path  base  path    encoder  in  encoder    bucket  assigner  in    string  bucket  assigner  super  base  path  encoder  bucket  assigner  a  builder  for  configuring  the  sink  for  bulk  encoding  formats  e  g    parquet  orc    public  evolving  public  static  class    bulk  format  builder  in    bucket  i  d  t  extends    bulk  format  builder  in    bucket  i  d  t  extends    streaming  file  sink    buckets  builder  in    bucket  i  d  t  private  static  final  long  serial  version  u  i  d  1  l  private  long  bucket  check  interval  private  final    path  base  path  private    bulk  writer    factory  in  writer  factory  private    bucket  assigner  in    bucket  i  d  bucket  assigner  private    checkpoint  rolling  policy  in    bucket  i  d  rolling  policy  private    bucket  factory  in    bucket  i  d  bucket  factory  private    output  file  config  output  file  config  protected    bulk  format  builder    path  base  path    bulk  writer    factory  in  writer  factory    bucket  assigner  in    bucket  i  d  assigner  this  base  path  writer  factory  assigner    on  checkpoint  rolling  policy  build  default  bucket  check  interval  new    default  bucket  factory  impl    output  file  config  builder  build  protected    bulk  format  builder    path  base  path    bulk  writer    factory  in  writer  factory    bucket  assigner  in    bucket  i  d  assigner    checkpoint  rolling  policy  in    bucket  i  d  policy  long  bucket  check  interval    bucket  factory  in    bucket  i  d  bucket  factory    output  file  config  output  file  config  this  base  path    preconditions  check  not  null  base  path  this  writer  factory  writer  factory  this  bucket  assigner    preconditions  check  not  null  assigner  this  rolling  policy    preconditions  check  not  null  policy  this  bucket  check  interval  bucket  check  interval  this  bucket  factory    preconditions  check  not  null  bucket  factory  this  output  file  config    preconditions  check  not  null  output  file  config  public  long  get  bucket  check  interval  return  bucket  check  interval  public  t  with  bucket  check  interval  long  interval  this  bucket  check  interval  interval  return  self  public  t  with  bucket  assigner    bucket  assigner  in    bucket  i  d  assigner  this  bucket  assigner    preconditions  check  not  null  assigner  return  self  public  t  with  rolling  policy    checkpoint  rolling  policy  in    bucket  i  d  rolling  policy  this  rolling  policy    preconditions  check  not  null  rolling  policy  return  self    visible  for  testing  t  with  bucket  factory  final    bucket  factory  in    bucket  i  d  factory  this  bucket  factory    preconditions  check  not  null  factory  return  self  public  t  with  output  file  config  final    output  file  config  output  file  config  this  output  file  config  output  file  config  return  self  public  id    streaming  file  sink    bulk  format  builder  in  id  extends    bulk  format  builder  in  id  with  new  bucket  assigner  final    bucket  assigner  in  id  assigner    preconditions  check  state  bucket  factory  get  class    default  bucket  factory  impl  class  new  builder  with  bucket  assigner  cannot  be  called  after  specifying  a  customized  bucket  factory  return  new    bulk  format  builder  base  path  writer  factory    preconditions  check  not  null  assigner  rolling  policy  bucket  check  interval  new    default  bucket  factory  impl  output  file  config    creates  the  actual  sink  public    streaming  file  sink  in  build  return  new    streaming  file  sink  this  bucket  check  interval    internal    override  public    buckets  in    bucket  i  d  create  buckets  int  subtask  index  throws    i  o  exception  return  new    buckets  base  path  bucket  assigner  bucket  factory  new    bulk  bucket  writer    file  system  get  base  path  to  uri  create  recoverable  writer  writer  factory  rolling  policy  subtask  index  output  file  config    builder  for  the  vanilla  link    streaming  file  sink  using  a  bulk  format  param  in  record  type  public  static  final  class    default  bulk  format  builder  in  extends    bulk  format  builder  in    string    default  bulk  format  builder  in  private  static  final  long  serial  version  u  i  d    l  private    default  bulk  format  builder    path  base  path    bulk  writer    factory  in  writer  factory    bucket  assigner  in    string  assigner  super  base  path  writer  factory  assigner    sink    methods    override  public  void  initialize  state    function  initialization  context  context  throws    exception  this  helper  new    streaming  file  sink  helper  buckets  builder  create  buckets  get  runtime  context  get  index  of  this  subtask  context  is  restored  context  get  operator  state  store    streaming  runtime  context  get  runtime  context  get  processing  time  service  bucket  check  interval    override  public  void  notify  checkpoint  complete  long  checkpoint  id  throws    exception  this  helper  commit  up  to  checkpoint  checkpoint  id    override  public  void  notify  checkpoint  aborted  long  checkpoint  id    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception    preconditions  check  state  helper  null  sink  has  not  been  initialized  this  helper  snapshot  state  context  get  checkpoint  id    override  public  void  invoke  in  value    sink  function    context  context  throws    exception  this  helper  on  element  value  context  current  processing  time  context  timestamp  context  current  watermark    override  public  void  close  throws    exception  if  this  helper  null  this  helper  close  
public  evolving    deprecated  public  class    output  format  sink  function  in  extends    rich  sink  function  in  implements    input  type  configurable  private  static  final  long  serial  version  u  i  d  1  l  private  static  final    logger  log    logger  factory  get  logger    output  format  sink  function  class  private    output  format  in  format  private  boolean  cleanup  called  false  public    output  format  sink  function    output  format  in  format  this  format  format    override  public  void  open    configuration  parameters  throws    exception    runtime  context  context  get  runtime  context  format  configure  parameters  int  index  in  subtask  group  context  get  index  of  this  subtask  int  current  number  of  subtasks  context  get  number  of  parallel  subtasks  format  open  index  in  subtask  group  current  number  of  subtasks    override  public  void  set  runtime  context    runtime  context  context  super  set  runtime  context  context  if  format  instanceof    rich  output  format    rich  output  format  format  set  runtime  context  context    override  public  void  set  input  type    type  information  type    execution  config  execution  config  if  format  instanceof    input  type  configurable    input  type  configurable  itc    input  type  configurable  format  itc  set  input  type  type  execution  config    override  public  void  invoke  in  record  throws    exception  try  format  write  record  record  catch    exception  ex  cleanup  throw  ex    override  public  void  close  throws    i  o  exception  try  format  close  catch    exception  ex  cleanup  throw  ex  private  void  cleanup  try  if  cleanup  called  format  instanceof    cleanup  when  unsuccessful  cleanup  called  true    cleanup  when  unsuccessful  format  try  cleanup  on  error  catch    throwable  t  log  error    cleanup  on  error  failed  t  public    output  format  in  get  format  return  format  
public  evolving  public  class    print  sink  function  in  extends    rich  sink  function  in  private  static  final  long  serial  version  u  i  d  1  l  private  final    print  sink  output  writer  in  writer    instantiates  a  print  sink  function  that  prints  to  standard  out  public    print  sink  function  writer  new    print  sink  output  writer  false    instantiates  a  print  sink  function  that  prints  to  standard  out  param  std  err    true  if  the  format  should  print  to  standard  error  instead  of  standard  out  public    print  sink  function  final  boolean  std  err  writer  new    print  sink  output  writer  std  err    instantiates  a  print  sink  function  that  prints  to  standard  out  and  gives  a  sink  identifier  param  std  err    true  if  the  format  should  print  to  standard  error  instead  of  standard  out  param  sink  identifier    message  that  identify  sink  and  is  prefixed  to  the  output  of  the  value  public    print  sink  function  final    string  sink  identifier  final  boolean  std  err  writer  new    print  sink  output  writer  sink  identifier  std  err    override  public  void  open    configuration  parameters  throws    exception  super  open  parameters    streaming  runtime  context  context    streaming  runtime  context  get  runtime  context  writer  open  context  get  index  of  this  subtask  context  get  number  of  parallel  subtasks    override  public  void  invoke  in  record  writer  write  record    override  public    string  to  string  return  writer  to  string  
public  evolving  public  class    socket  client  sink  in  extends    rich  sink  function  in  private  static  final  long  serial  version  u  i  d  1  l  private  static  final    logger  log    logger  factory  get  logger    socket  client  sink  class  private  static  final  int  connection  retry  delay    private  final    serializable  object  lock  new    serializable  object  private  final    serialization  schema  in  schema  private  final    string  host  name  private  final  int  port  private  final  int  max  num  retries  private  final  boolean  auto  flush  private  transient    socket  client  private  transient    output  stream  output  stream  private  int  retries  private  volatile  boolean  is  running  true    creates  a  new    socket  client  sink    the  sink  will  not  attempt  to  retry  connections  upon  failure  and  will  not  auto  flush  the  stream  param  host  name    hostname  of  the  server  to  connect  to  param  port    port  of  the  server  param  schema    schema  used  to  serialize  the  data  into  bytes  public    socket  client  sink    string  host  name  int  port    serialization  schema  in  schema  this  host  name  port  schema      creates  a  new    socket  client  sink  that  retries  connections  upon  failure  up  to  a  given  number  of  times  a  value  of    for  the  number  of  retries  will  cause  the  system  to  retry  an  infinite  number  of  times    the  sink  will  not  auto  flush  the  stream  param  host  name    hostname  of  the  server  to  connect  to  param  port    port  of  the  server  param  schema    schema  used  to  serialize  the  data  into  bytes  param  max  num  retries    the  maximum  number  of  retries  after  a  message  send  failed  public    socket  client  sink    string  host  name  int  port    serialization  schema  in  schema  int  max  num  retries  this  host  name  port  schema  max  num  retries  false    creates  a  new    socket  client  sink  that  retries  connections  upon  failure  up  to  a  given  number  of  times  a  value  of    for  the  number  of  retries  will  cause  the  system  to  retry  an  infinite  number  of  times  param  host  name    hostname  of  the  server  to  connect  to  param  port    port  of  the  server  param  schema    schema  used  to  serialize  the  data  into  bytes  param  max  num  retries    the  maximum  number  of  retries  after  a  message  send  failed  param  autoflush    flag  to  indicate  whether  the  socket  stream  should  be  flushed  after  each  message  public    socket  client  sink    string  host  name  int  port    serialization  schema  in  schema  int  max  num  retries  boolean  autoflush  check  argument  is  valid  client  port  port  port  is  out  of  range  check  argument  max  num  retries    max  num  retries  must  be  zero  or  larger  num  retries  or    infinite  retries  this  host  name  check  not  null  host  name  hostname  must  not  be  null  this  port  port  this  schema  check  not  null  schema  this  max  num  retries  max  num  retries  this  auto  flush  autoflush    life  cycle    initialize  the  connection  with  the    socket  in  the  server  param  parameters    configuration    override  public  void  open    configuration  parameters  throws    exception  try  synchronized  lock  create  connection  catch    i  o  exception  e  throw  new    i  o  exception    cannot  connect  to  socket  server  at  host  name  port  e    called  when  new  data  arrives  to  the  sink  and  forwards  it  to    socket  param  value    the  value  to  write  to  the  socket    override  public  void  invoke  in  value  throws    exception  byte  msg  schema  serialize  value  try  output  stream  write  msg  if  auto  flush  output  stream  flush  catch    i  o  exception  e  if  no  re  tries  are  enable  fail  immediately  if  max  num  retries    throw  new    i  o  exception    failed  to  send  message  value  to  socket  server  at  host  name  port    connection  re  tries  are  not  enabled  e  log  error    failed  to  send  message  value  to  socket  server  at  host  name  port    trying  to  reconnect  e  do  the  retries  in  locked  scope  to  guard  against  concurrent  close  calls  note  that  the  first  re  try  comes  immediately  without  a  wait  synchronized  lock    i  o  exception  last  exception  null  retries    while  is  running  max  num  retries    retries  max  num  retries  first  clean  up  the  old  resources  try  if  output  stream  null  output  stream  close  catch    i  o  exception  ee  log  error    could  not  close  output  stream  from  failed  write  attempt  ee  try  if  client  null  client  close  catch    i  o  exception  ee  log  error    could  not  close  socket  from  failed  write  attempt  ee  try  again  retries  try  initialize  a  new  connection  create  connection  re  try  the  write  output  stream  write  msg  success  return  catch    i  o  exception  ee  last  exception  ee  log  error    re  connect  to  socket  server  and  send  message  failed    retry  time  s  retries  ee  wait  before  re  attempting  to  connect  lock  wait  connection  retry  delay  throw  an  exception  if  the  task  is  still  running  otherwise  simply  leave  the  method  if  is  running  throw  new    i  o  exception    failed  to  send  message  value  to  socket  server  at  host  name  port    failed  after  retries  retries  last  exception    closes  the  connection  with  the    socket  server    override  public  void  close  throws    exception  flag  this  as  not  running  any  more  is  running  false  clean  up  in  locked  scope  so  there  is  no  concurrent  change  to  the  stream  and  client  synchronized  lock  we  notify  first  this  statement  cannot  fail    the  notified  thread  will  not  continue  anyways  before  it  can  re  acquire  the  lock  lock  notify  all  try  if  output  stream  null  output  stream  close  finally  if  client  null  client  close    utilities  private  void  create  connection  throws    i  o  exception  client  new    socket  host  name  port  client  set  keep  alive  true  client  set  tcp  no  delay  true  output  stream  client  get  output  stream    for  testing  int  get  current  number  of  retries  synchronized  lock  return  retries  
public  evolving  public  abstract  class    two  phase  commit  sink  function  in  txn  context  extends    rich  sink  function  in  implements    checkpointed  function    checkpoint  listener  private  static  final    logger  log    logger  factory  get  logger    two  phase  commit  sink  function  class  protected  final    linked  hash  map    long    transaction  holder  txn  pending  commit  transactions  new    linked  hash  map  protected  transient    optional  context  user  context  protected  transient    list  state    state  txn  context  state  private  final    clock  clock  private  final    list  state  descriptor    state  txn  context  state  descriptor  private    transaction  holder  txn  current  transaction  holder    specifies  the  maximum  time  a  transaction  should  remain  open  private  long  transaction  timeout    long  max  value    if  true  any  exception  thrown  in  link  recover  and  commit    object  will  be  caught  instead  of  propagated  private  boolean  ignore  failures  after  transaction  timeout    if  a  transaction  s  elapsed  time  reaches  this  percentage  of  the  transaction  timeout  a  warning  message  will  be  logged    value  must  be  in  range  0  1    negative  value  disables  warnings  private  double  transaction  timeout  warning  ratio      use  default  link    list  state  descriptor  for  internal  state  serialization    helpful  utilities  for  using  this  constructor  are  link    type  information  of    class  link  org  apache  flink  api  common  typeinfo    type  hint  and  link    type  information  of    type  hint    example  pre  code    two  phase  commit  sink  function    type  information  of  new    type  hint    state  txn  context  pre  param  transaction  serializer  link    type  serializer  for  the  transaction  type  of  this  sink  param  context  serializer  link    type  serializer  for  the  context  type  of  this  sink  public    two  phase  commit  sink  function    type  serializer  txn  transaction  serializer    type  serializer  context  context  serializer  this  transaction  serializer  context  serializer    clock  system  u  t  c    visible  for  testing    two  phase  commit  sink  function    type  serializer  txn  transaction  serializer    type  serializer  context  context  serializer    clock  clock  this  state  descriptor  new    list  state  descriptor  state  new    state  serializer  transaction  serializer  context  serializer  this  clock  clock  protected    optional  context  initialize  user  context  return    optional  empty  protected    optional  context  get  user  context  return  user  context    nullable  protected  txn  current  transaction  return  current  transaction  holder  null  null  current  transaction  holder  handle    nonnull  protected    stream    map    entry    long  txn  pending  transactions  return  pending  commit  transactions  entry  set  stream  map  e  new    abstract  map    simple  entry  e  get  key  e  get  value  handle  methods  that  should  be  implemented  in  child  class  to  support  two  phase  commit  algorithm    write  value  within  a  transaction  protected  abstract  void  invoke  txn  transaction  in  value    context  context  throws    exception    method  that  starts  a  new  transaction  return  newly  created  transaction  protected  abstract  txn  begin  transaction  throws    exception    pre  commit  previously  created  transaction    pre  commit  must  make  all  of  the  necessary  steps  to  prepare  the  transaction  for  a  commit  that  might  happen  in  the  future    after  this  point  the  transaction  might  still  be  aborted  but  underlying  implementation  must  ensure  that  commit  calls  on  already  pre  committed  transactions  will  always  succeed  p    usually  implementation  involves  flushing  the  data  protected  abstract  void  pre  commit  txn  transaction  throws    exception    commit  a  pre  committed  transaction    if  this  method  fail    flink  application  will  be  restarted  and  link    two  phase  commit  sink  function  recover  and  commit    object  will  be  called  again  for  the  same  transaction  protected  abstract  void  commit  txn  transaction    invoked  on  recovered  transactions  after  a  failure    user  implementation  must  ensure  that  this  call  will  eventually  succeed    if  it  fails    flink  application  will  be  restarted  and  it  will  be  invoked  again    if  it  does  not  succeed  eventually  a  data  loss  will  occur    transactions  will  be  recovered  in  an  order  in  which  they  were  created  protected  void  recover  and  commit  txn  transaction  commit  transaction    abort  a  transaction  protected  abstract  void  abort  txn  transaction    abort  a  transaction  that  was  rejected  by  a  coordinator  after  a  failure  protected  void  recover  and  abort  txn  transaction  abort  transaction    callback  for  subclasses  which  is  called  after  restoring  each  user  context  param  handled  transactions  transactions  which  were  already  committed  or  aborted  and  do  not  need  further  handling  protected  void  finish  recovering  context    collection  txn  handled  transactions  entry  points  for  above  methods  implementing    check  pointed  function  and    checkpoint  listener    this  should  not  be  implemented  by  subclasses    override  public  final  void  invoke  in  value  throws    exception    override  public  final  void  invoke  in  value    context  context  throws    exception  invoke  current  transaction  holder  handle  value  context    override  public  final  void  notify  checkpoint  complete  long  checkpoint  id  throws    exception  the  following  scenarios  are  possible  here    there  is  exactly  one  transaction  from  the  latest  checkpoint  that  was  triggered  and  completed    that  should  be  the  common  case    simply  commit  that  transaction  in  that  case    there  are  multiple  pending  transactions  because  one  previous  checkpoint  was  skipped    that  is  a  rare  case  but  can  happen  for  example  when  the  master  cannot  persist  the  metadata  of  the  last  checkpoint  temporary  outage  in  the  storage  system  but  could  persist  a  successive  checkpoint  the  one  notified  here  other  tasks  could  not  persist  their  status  during  the  previous  checkpoint  but  did  not  trigger  a  failure  because  they  could  hold  onto  their  state  and  could  successfully  persist  it  in  a  successive  checkpoint  the  one  notified  here    in  both  cases  the  prior  checkpoint  never  reach  a  committed  state  but  this  checkpoint  is  always  expected  to  subsume  the  prior  one  and  cover  all  changes  since  the  last  successful  one    as  a  consequence  we  need  to  commit  all  pending  transactions      multiple  transactions  are  pending  but  the  checkpoint  complete  notification  relates  not  to  the  latest    that  is  possible  because  notification  messages  can  be  delayed  in  an  extreme  case  till  arrive  after  a  succeeding  checkpoint  was  triggered  and  because  there  can  be  concurrent  overlapping  checkpoints  a  new  one  is  started  before  the  previous  fully  finished    there  should  never  be  a  case  where  we  have  no  pending  transaction  here    iterator    map    entry    long    transaction  holder  txn  pending  transaction  iterator  pending  commit  transactions  entry  set  iterator    throwable  first  error  null  while  pending  transaction  iterator  has  next    map    entry    long    transaction  holder  txn  entry  pending  transaction  iterator  next    long  pending  transaction  checkpoint  id  entry  get  key    transaction  holder  txn  pending  transaction  entry  get  value  if  pending  transaction  checkpoint  id  checkpoint  id  continue  log  info  checkpoint  complete  committing  transaction  from  checkpoint  name  checkpoint  id  pending  transaction  pending  transaction  checkpoint  id  log  warning  if  timeout  almost  reached  pending  transaction  try  commit  pending  transaction  handle  catch    throwable  t  if  first  error  null  first  error  t  log  debug  committed  checkpoint  transaction  name  pending  transaction  pending  transaction  iterator  remove  if  first  error  null  throw  new    flink  runtime  exception    committing  one  of  transactions  failed  logging  first  encountered  failure  first  error    override  public  void  notify  checkpoint  aborted  long  checkpoint  id    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception  this  is  like  the  pre  commit  of  a    phase  commit  transaction  we  are  ready  to  commit  and  remember  the  transaction  check  state  current  transaction  holder  null  bug  no  transaction  object  when  performing  state  snapshot  long  checkpoint  id  context  get  checkpoint  id  log  debug  checkpoint  triggered  flushing  transaction  name  context  get  checkpoint  id  current  transaction  holder  pre  commit  current  transaction  holder  handle  pending  commit  transactions  put  checkpoint  id  current  transaction  holder  log  debug  stored  pending  transactions  name  pending  commit  transactions  current  transaction  holder  begin  transaction  internal  log  debug  started  new  transaction  name  current  transaction  holder  state  clear  state  add  new    state  this  current  transaction  holder  new    array  list  pending  commit  transactions  values  user  context    override  public  void  initialize  state    function  initialization  context  context  throws    exception  when  we  are  restoring  state  with  pending  commit  transactions  we  don  t  really  know  whether  the  transactions  were  already  committed  or  whether  there  was  a  failure  between  completing  the  checkpoint  on  the  master  and  notifying  the  writer  here  the  common  case  is  actually  that  is  was  already  committed  the  window  between  the  commit  on  the  master  and  the  notification  here  is  very  small  it  is  possible  to  not  have  any  transactions  at  all  if  there  was  a  failure  before  the  first  completed  checkpoint  or  in  case  of  a  scale  out  event  where  some  of  the  new  task  do  not  have  and  transactions  assigned  to  check  we  can  have  more  than  one  transaction  to  check  in  case  of  a  scale  in  event  or  for  the  reasons  discussed  in  the  notify  checkpoint  complete  method  state  context  get  operator  state  store  get  list  state  state  descriptor  boolean  recovered  user  context  false  if  context  is  restored  log  info  restoring  state  name  for    state  txn  context  operator  state  state  get  user  context  operator  state  get  context    list    transaction  holder  txn  recovered  transactions  operator  state  get  pending  commit  transactions    list  txn  handled  transactions  new    array  list  recovered  transactions  size    for    transaction  holder  txn  recovered  transaction  recovered  transactions    if  this  fails  to  succeed  eventually  there  is  actually  data  loss  recover  and  commit  internal  recovered  transaction  handled  transactions  add  recovered  transaction  handle  log  info  committed  recovered  transaction  name  recovered  transaction  txn  transaction  operator  state  get  pending  transaction  handle  recover  and  abort  transaction  handled  transactions  add  transaction  log  info  aborted  recovered  transaction  name  operator  state  get  pending  transaction  if  user  context  is  present  finish  recovering  context  handled  transactions  recovered  user  context  true  if  in  restore  we  didn  t  get  any  user  context  or  we  are  initializing  from  scratch  if  recovered  user  context  log  info  no  state  to  restore  name  user  context  initialize  user  context  this  pending  commit  transactions  clear  current  transaction  holder  begin  transaction  internal  log  debug  started  new  transaction  name  current  transaction  holder    this  method  must  be  the  only  place  to  call  link  begin  transaction  to  ensure  that  the  link    transaction  holder  is  created  at  the  same  time  private    transaction  holder  txn  begin  transaction  internal  throws    exception  return  new    transaction  holder  begin  transaction  clock  millis    this  method  must  be  the  only  place  to  call  link  recover  and  commit    object  to  ensure  that  the  configuration  parameters  link  transaction  timeout  and  link  ignore  failures  after  transaction  timeout  are  respected  private  void  recover  and  commit  internal    transaction  holder  txn  transaction  holder  try  log  warning  if  timeout  almost  reached  transaction  holder  recover  and  commit  transaction  holder  handle  catch  final    exception  e  final  long  elapsed  time  clock  millis  transaction  holder  transaction  start  time  if  ignore  failures  after  transaction  timeout  elapsed  time  transaction  timeout  log  error    error  while  committing  transaction    transaction  has  been  open  for  longer  than  the  transaction  timeout    commit  will  not  be  attempted  again    data  loss  might  have  occurred  transaction  holder  handle  transaction  timeout  e  else  throw  e  private  void  log  warning  if  timeout  almost  reached    transaction  holder  txn  transaction  holder  final  long  elapsed  time  transaction  holder  elapsed  time  clock  if  transaction  timeout  warning  ratio    elapsed  time  transaction  timeout  transaction  timeout  warning  ratio  log  warn    transaction  has  been  open  for  ms    this  is  close  to  or  even  exceeding  the  transaction  timeout  of  ms  transaction  holder  handle  elapsed  time  transaction  timeout    override  public  void  close  throws    exception  super  close  if  current  transaction  holder  null  abort  current  transaction  holder  handle  current  transaction  holder  null    sets  the  transaction  timeout    setting  only  the  transaction  timeout  has  no  effect  in  itself  param  transaction  timeout    the  transaction  timeout  in  ms  see  ignore  failures  after  transaction  timeout  see  enable  transaction  timeout  warnings  double  protected    two  phase  commit  sink  function  in  txn  context  set  transaction  timeout  long  transaction  timeout  check  argument  transaction  timeout    transaction  timeout  must  not  be  negative  this  transaction  timeout  transaction  timeout  return  this    if  called  the  sink  will  only  log  but  not  propagate  exceptions  thrown  in  link  recover  and  commit    object  if  the  transaction  is  older  than  a  specified  transaction  timeout    the  start  time  of  an  transaction  is  determined  by  link    system  current  time  millis    by  default  failures  are  propagated  protected    two  phase  commit  sink  function  in  txn  context  ignore  failures  after  transaction  timeout  this  ignore  failures  after  transaction  timeout  true  return  this    enables  logging  of  warnings  if  a  transaction  s  elapsed  time  reaches  a  specified  ratio  of  the  code  transaction  timeout  code    if  code  warning  ratio  code  is    a  warning  will  be  always  logged  when  committing  the  transaction  param  warning  ratio  a  value  in  the  range  0  1  return  protected    two  phase  commit  sink  function  in  txn  context  enable  transaction  timeout  warnings  double  warning  ratio  check  argument  warning  ratio    warning  ratio    warning  ratio  must  be  in  range  0  1  this  transaction  timeout  warning  ratio  warning  ratio  return  this  private    string  name  return    string  format  s  s  s  this  get  class  get  simple  name  get  runtime  context  get  index  of  this  subtask    get  runtime  context  get  number  of  parallel  subtasks    state  pojo  class  coupling  pending  transaction  context  and  pending  commit  transactions    visible  for  testing    internal  public  static  final  class    state  txn  context  protected    transaction  holder  txn  pending  transaction  protected    list    transaction  holder  txn  pending  commit  transactions  new    array  list  protected    optional  context  context  public    state  public    state    transaction  holder  txn  pending  transaction    list    transaction  holder  txn  pending  commit  transactions    optional  context  context  this  context  require  non  null  context  context  is  null  this  pending  transaction  require  non  null  pending  transaction  pending  transaction  is  null  this  pending  commit  transactions  require  non  null  pending  commit  transactions  pending  commit  transactions  is  null  public    transaction  holder  txn  get  pending  transaction  return  pending  transaction  public  void  set  pending  transaction    transaction  holder  txn  pending  transaction  this  pending  transaction  pending  transaction  public    list    transaction  holder  txn  get  pending  commit  transactions  return  pending  commit  transactions  public  void  set  pending  commit  transactions    list    transaction  holder  txn  pending  commit  transactions  this  pending  commit  transactions  pending  commit  transactions  public    optional  context  get  context  return  context  public  void  set  context    optional  context  context  this  context  context    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    state  state    state  o  if  pending  transaction  null  pending  transaction  equals  state  pending  transaction  state  pending  transaction  null  return  false  if  pending  commit  transactions  null  pending  commit  transactions  equals  state  pending  commit  transactions  state  pending  commit  transactions  null  return  false  return  context  null  context  equals  state  context  state  context  null    override  public  int  hash  code  int  result  pending  transaction  null  pending  transaction  hash  code    result    result  pending  commit  transactions  null  pending  commit  transactions  hash  code    result    result  context  null  context  hash  code    return  result    adds  metadata  currently  only  the  start  time  of  the  transaction  to  the  transaction  object    visible  for  testing    internal  public  static  final  class    transaction  holder  txn  private  final  txn  handle    the  system  time  when  link  handle  was  created    used  to  determine  if  the  current  transaction  has  exceeded  its  timeout  specified  by  link  transaction  timeout  private  final  long  transaction  start  time    visible  for  testing  public    transaction  holder  txn  handle  long  transaction  start  time  this  handle  handle  this  transaction  start  time  transaction  start  time  long  elapsed  time    clock  clock  return  clock  millis  transaction  start  time    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    transaction  holder  that    transaction  holder  o  if  transaction  start  time  that  transaction  start  time  return  false  return  handle  null  handle  equals  that  handle  that  handle  null    override  public  int  hash  code  int  result  handle  null  handle  hash  code    result    result  int  transaction  start  time  transaction  start  time    return  result    override  public    string  to  string  return    transaction  holder  handle  handle  transaction  start  time  transaction  start  time    custom  link    type  serializer  for  the  sink  state    visible  for  testing    internal  public  static  final  class    state  serializer  txn  context  extends    type  serializer    state  txn  context  private  static  final  long  serial  version  u  i  d  1  l  private  final    type  serializer  txn  transaction  serializer  private  final    type  serializer  context  context  serializer  public    state  serializer    type  serializer  txn  transaction  serializer    type  serializer  context  context  serializer  this  transaction  serializer  check  not  null  transaction  serializer  this  context  serializer  check  not  null  context  serializer    override  public  boolean  is  immutable  type  return  transaction  serializer  is  immutable  type  context  serializer  is  immutable  type    override  public    type  serializer    state  txn  context  duplicate  return  new    state  serializer  transaction  serializer  duplicate  context  serializer  duplicate    override  public    state  txn  context  create  instance  return  null    override  public    state  txn  context  copy    state  txn  context  from  final    transaction  holder  txn  pending  transaction  from  get  pending  transaction  final    transaction  holder  txn  copied  pending  transaction  new    transaction  holder  transaction  serializer  copy  pending  transaction  handle  pending  transaction  transaction  start  time  final    list    transaction  holder  txn  copied  pending  commit  transactions  new    array  list  for    transaction  holder  txn  txn  from  get  pending  commit  transactions  final  txn  txn  handle  copy  transaction  serializer  copy  txn  handle  copied  pending  commit  transactions  add  new    transaction  holder  txn  handle  copy  txn  transaction  start  time  final    optional  context  copied  context  from  get  context  map  context  serializer  copy  return  new    state  copied  pending  transaction  copied  pending  commit  transactions  copied  context    override  public    state  txn  context  copy    state  txn  context  from    state  txn  context  reuse  return  copy  from    override  public  int  get  length  return      override  public  void  serialize    state  txn  context  record    data  output  view  target  throws    i  o  exception  final    transaction  holder  txn  pending  transaction  record  get  pending  transaction  transaction  serializer  serialize  pending  transaction  handle  target  target  write  long  pending  transaction  transaction  start  time  final    list    transaction  holder  txn  pending  commit  transactions  record  get  pending  commit  transactions  target  write  int  pending  commit  transactions  size  for    transaction  holder  txn  pending  txn  pending  commit  transactions  transaction  serializer  serialize  pending  txn  handle  target  target  write  long  pending  txn  transaction  start  time    optional  context  context  record  get  context  if  context  is  present  target  write  boolean  true  context  serializer  serialize  context  get  target  else  target  write  boolean  false    override  public    state  txn  context  deserialize    data  input  view  source  throws    i  o  exception  txn  pending  txn  handle  transaction  serializer  deserialize  source  final  long  pending  txn  start  time  source  read  long  final    transaction  holder  txn  pending  txn  new    transaction  holder  pending  txn  handle  pending  txn  start  time  int  num  pending  commit  txns  source  read  int    list    transaction  holder  txn  pending  commit  txns  new    array  list  num  pending  commit  txns  for  int  i    i  num  pending  commit  txns  i  final  txn  pending  commit  txn  handle  transaction  serializer  deserialize  source  final  long  pending  commit  txn  start  time  source  read  long  pending  commit  txns  add  new    transaction  holder  pending  commit  txn  handle  pending  commit  txn  start  time    optional  context  context    optional  empty  boolean  has  context  source  read  boolean  if  has  context  context    optional  of  context  serializer  deserialize  source  return  new    state  pending  txn  pending  commit  txns  context    override  public    state  txn  context  deserialize    state  txn  context  reuse    data  input  view  source  throws    i  o  exception  return  deserialize  source    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  txn  pending  txn  handle  transaction  serializer  deserialize  source  transaction  serializer  serialize  pending  txn  handle  target  final  long  pending  txn  start  time  source  read  long  target  write  long  pending  txn  start  time  int  num  pending  commit  txns  source  read  int  target  write  int  num  pending  commit  txns  for  int  i    i  num  pending  commit  txns  i  txn  pending  commit  txn  handle  transaction  serializer  deserialize  source  transaction  serializer  serialize  pending  commit  txn  handle  target  final  long  pending  commit  txn  start  time  source  read  long  target  write  long  pending  commit  txn  start  time  boolean  has  context  source  read  boolean  target  write  boolean  has  context  if  has  context  context  context  context  serializer  deserialize  source  context  serializer  serialize  context  target    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    state  serializer  that    state  serializer  o  if  transaction  serializer  equals  that  transaction  serializer  return  false  return  context  serializer  equals  that  context  serializer    override  public  int  hash  code  int  result  transaction  serializer  hash  code  result    result  context  serializer  hash  code  return  result    override  public    state  serializer  snapshot  txn  context  snapshot  configuration  return  new    state  serializer  snapshot  this  link    type  serializer  config  snapshot  for  sink  state    this  has  to  be  public  so  that  it  can  be  deserialized  instantiated  should  not  be  used  anywhere  outside  code    two  phase  commit  sink  function  deprecated  this  snapshot  class  is  no  longer  in  use  and  is  maintained  only  for  backwards  compatibility  purposes    it  is  fully  replaced  by  link    state  serializer  snapshot    internal    deprecated  public  static  final  class    state  serializer  config  snapshot  txn  context  extends    composite  type  serializer  config  snapshot    state  txn  context  private  static  final  int  version      this  empty  nullary  constructor  is  required  for  deserializing  the  configuration  public    state  serializer  config  snapshot  public    state  serializer  config  snapshot    type  serializer  txn  transaction  serializer    type  serializer  context  context  serializer  super  transaction  serializer  context  serializer    override  public  int  get  version  return  version    override  public    type  serializer  schema  compatibility    state  txn  context  resolve  schema  compatibility    type  serializer    state  txn  context  new  serializer  final    type  serializer  snapshot  nested  snapshots  get  nested  serializers  and  configs  stream  map  t  t  f1  to  array    type  serializer  snapshot  new  return    composite  type  serializer  util  delegate  compatibility  check  to  new  snapshot  new  serializer  new    state  serializer  snapshot  nested  snapshots    snapshot  for  the  link    state  serializer    internal  public  static  final  class    state  serializer  snapshot  txn  context  extends    composite  type  serializer  snapshot    state  txn  context    state  serializer  txn  context  private  static  final  int  version      suppress  warnings    weaker  access  public    state  serializer  snapshot  super    state  serializer  class    state  serializer  snapshot    state  serializer  txn  context  serializer  instance  super  serializer  instance    override  protected  int  get  current  outer  snapshot  version  return  version    override  protected    state  serializer  txn  context  create  outer  serializer  with  nested  serializers    type  serializer  nested  serializers    suppress  warnings  unchecked  final    type  serializer  txn  transaction  serializer    type  serializer  txn  nested  serializers      suppress  warnings  unchecked  final    type  serializer  context  context  serializer    type  serializer  context  nested  serializers    return  new    state  serializer  transaction  serializer  context  serializer    override  protected    type  serializer  get  nested  serializers    state  serializer  txn  context  outer  serializer  return  new    type  serializer  outer  serializer  transaction  serializer  outer  serializer  context  serializer  
public  evolving    deprecated  public  abstract  class    write  format  in  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l    writes  the  contents  of  tuple  list  to  the  file  specified  by  path  param  path  is  the  path  to  the  location  where  the  tuples  are  written  param  tuple  list  is  the  list  of  tuples  to  be  written  protected  abstract  void  write    string  path    array  list  in  tuple  list  
public  evolving    deprecated  public  class    write  format  as  csv  in  extends    write  format  in  private  static  final  long  serial  version  u  i  d  1  l    override  protected  void  write    string  path    array  list  in  tuple  list  try    print  writer  out  stream  new    print  writer  new    buffered  writer  new    file  writer  path  true  for  in  tuple  to  write  tuple  list    string  str  tuple  tuple  to  write  to  string  out  stream  println  str  tuple  substring    str  tuple  length    catch    i  o  exception  e  throw  new    runtime  exception    exception  occured  while  writing  file  path  e  
public  evolving    deprecated  public  class    write  format  as  text  in  extends    write  format  in  private  static  final  long  serial  version  u  i  d  1  l    override  public  void  write    string  path    array  list  in  tuple  list  try    print  writer  out  stream  new    print  writer  new    buffered  writer  new    file  writer  path  true  for  in  tuple  to  write  tuple  list  out  stream  println  tuple  to  write  catch    i  o  exception  e  throw  new    runtime  exception    exception  occured  while  writing  file  path  e  
public  evolving    deprecated  public  abstract  class    write  sink  function  in  implements    sink  function  in  private  static  final  long  serial  version  u  i  d  1  l  protected  final    string  path  protected    array  list  in  tuple  list  new    array  list  in  protected    write  format  in  format  public    write  sink  function    string  path    write  format  in  format  this  path  path  this  format  format  clean  file  path    creates  target  file  if  it  does  not  exist  cleans  it  if  it  exists  param  path  is  the  path  to  the  location  where  the  tuples  are  written  protected  void  clean  file    string  path  try    print  writer  writer  writer  new    print  writer  path  writer  print  writer  close  catch    file  not  found  exception  e  throw  new    runtime  exception    an  error  occurred  while  cleaning  the  file  e  get  message  e    condition  for  writing  the  contents  of  tuple  list  and  clearing  it  return  value  of  the  updating  condition  protected  abstract  boolean  update  condition    statements  to  be  executed  after  writing  a  batch  goes  here  protected  abstract  void  reset  parameters    implementation  of  the  invoke  method  of  the    sink  function  class    collects  the  incoming  tuples  in  tuple  list  and  appends  the  list  to  the  end  of  the  target  file  if  update  condition  is  true  or  the  current  tuple  is  the  end  tuple    override  public  void  invoke  in  tuple  tuple  list  add  tuple  if  update  condition  format  write  path  tuple  list  reset  parameters  
public  evolving    deprecated  public  class    write  sink  function  by  millis  in  extends    write  sink  function  in  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  millis  private  long  last  time  public    write  sink  function  by  millis    string  path    write  format  in  format  long  millis  super  path  format  this  millis  millis  last  time    system  current  time  millis    override  protected  boolean  update  condition  return    system  current  time  millis  last  time  millis    override  protected  void  reset  parameters  tuple  list  clear  last  time    system  current  time  millis  
public  evolving  public  enum    file  processing  mode    processes  the  current  contents  of  the  path  and  exits  process  once    periodically  scans  the  path  for  new  data  process  continuously  
public  evolving  public  class    from  elements  function  t  implements    source  function  t    checkpointed  function  private  static  final  long  serial  version  u  i  d  1  l    the  de  serializer  to  be  used  for  the  data  elements  private  final    type  serializer  t  serializer    the  actual  data  elements  in  serialized  form  private  final  byte  elements  serialized    the  number  of  serialized  elements  private  final  int  num  elements    the  number  of  elements  emitted  already  private  volatile  int  num  elements  emitted    the  number  of  elements  to  skip  initially  private  volatile  int  num  elements  to  skip    flag  to  make  the  source  cancelable  private  volatile  boolean  is  running  true  private  transient    list  state    integer  checkpointed  state  public    from  elements  function    type  serializer  t  serializer  t  elements  throws    i  o  exception  this  serializer    arrays  as  list  elements  public    from  elements  function    type  serializer  t  serializer    iterable  t  elements  throws    i  o  exception    byte  array  output  stream  baos  new    byte  array  output  stream    data  output  view  stream  wrapper  wrapper  new    data  output  view  stream  wrapper  baos  int  count    try  for  t  element  elements  serializer  serialize  element  wrapper  count  catch    exception  e  throw  new    i  o  exception    serializing  the  source  elements  failed  e  get  message  e  this  serializer  serializer  this  elements  serialized  baos  to  byte  array  this  num  elements  count    override  public  void  initialize  state    function  initialization  context  context  throws    exception    preconditions  check  state  this  checkpointed  state  null    the  get  class  get  simple  name  has  already  been  initialized  this  checkpointed  state  context  get  operator  state  store  get  list  state  new    list  state  descriptor  from  elements  state    int  serializer  instance  if  context  is  restored    list    integer  retrieved  states  new    array  list  for    integer  entry  this  checkpointed  state  get  retrieved  states  add  entry  given  that  the  parallelism  of  the  function  is    we  can  only  have    state    preconditions  check  argument  retrieved  states  size    get  class  get  simple  name  retrieved  invalid  state  this  num  elements  to  skip  retrieved  states  get      override  public  void  run    source  context  t  ctx  throws    exception    byte  array  input  stream  bais  new    byte  array  input  stream  elements  serialized  final    data  input  view  input  new    data  input  view  stream  wrapper  bais  if  we  are  restored  from  a  checkpoint  and  need  to  skip  elements  skip  them  now  int  to  skip  num  elements  to  skip  if  to  skip    try  while  to  skip    serializer  deserialize  input  to  skip  catch    exception  e  throw  new    i  o  exception    failed  to  deserialize  an  element  from  the  source    if  you  are  using  user  defined  serialization    value  and    writable  types  check  the  serialization  functions  n  serializer  is  serializer  e  this  num  elements  emitted  this  num  elements  to  skip  final    object  lock  ctx  get  checkpoint  lock  while  is  running  num  elements  emitted  num  elements  t  next  try  next  serializer  deserialize  input  catch    exception  e  throw  new    i  o  exception    failed  to  deserialize  an  element  from  the  source    if  you  are  using  user  defined  serialization    value  and    writable  types  check  the  serialization  functions  n  serializer  is  serializer  e  synchronized  lock  ctx  collect  next  num  elements  emitted    override  public  void  cancel  is  running  false    gets  the  number  of  elements  produced  in  total  by  this  function  return    the  number  of  elements  produced  in  total  public  int  get  num  elements  return  num  elements    gets  the  number  of  elements  emitted  so  far  return    the  number  of  elements  emitted  so  far  public  int  get  num  elements  emitted  return  num  elements  emitted    checkpointing    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception    preconditions  check  state  this  checkpointed  state  null    the  get  class  get  simple  name  has  not  been  properly  initialized  this  checkpointed  state  clear  this  checkpointed  state  add  this  num  elements  emitted    utilities    verifies  that  all  elements  in  the  collection  are  non  null  and  are  of  the  given  class  or  a  subclass  thereof  param  elements    the  collection  to  check  param  viewed  as    the  class  to  which  the  elements  must  be  assignable  to  param  out    the  generic  type  of  the  collection  to  be  checked  public  static  out  void  check  collection    collection  out  elements    class  out  viewed  as  for  out  elem  elements  if  elem  null  throw  new    illegal  argument  exception    the  collection  contains  a  null  element  if  viewed  as  is  assignable  from  elem  get  class  throw  new    illegal  argument  exception    the  elements  in  the  collection  are  not  all  subclasses  of  viewed  as  get  canonical  name  
public  evolving  public  class    from  iterator  function  t  implements    source  function  t  private  static  final  long  serial  version  u  i  d  1  l  private  final    iterator  t  iterator  private  volatile  boolean  is  running  true  public    from  iterator  function    iterator  t  iterator  this  iterator  iterator    override  public  void  run    source  context  t  ctx  throws    exception  while  is  running  iterator  has  next  ctx  collect  iterator  next    override  public  void  cancel  is  running  false  
public  evolving  public  class    from  splittable  iterator  function  t  extends    rich  parallel  source  function  t  private  static  final  long  serial  version  u  i  d  1  l  private    splittable  iterator  t  full  iterator  private  transient    iterator  t  iterator  private  volatile  boolean  is  running  true  public    from  splittable  iterator  function    splittable  iterator  t  iterator  this  full  iterator  iterator    override  public  void  open    configuration  parameters  throws    exception  int  number  of  sub  tasks  get  runtime  context  get  number  of  parallel  subtasks  int  indexof  this  sub  task  get  runtime  context  get  index  of  this  subtask  iterator  full  iterator  split  number  of  sub  tasks  indexof  this  sub  task  is  running  true    override  public  void  run    source  context  t  ctx  throws    exception  while  is  running  iterator  has  next  ctx  collect  iterator  next    override  public  void  cancel  is  running  false  
public  evolving  public  abstract  class    message  acknowledging  source  base    type    u  id  extends    rich  source  function    type  implements    checkpointed  function    checkpoint  listener  private  static  final  long  serial  version  u  i  d    l  private  static  final    logger  log    logger  factory  get  logger    message  acknowledging  source  base  class    serializer  used  to  serialize  the    i  ds  for  checkpoints  private  final    type  serializer    u  id  id  serializer    the  list  gathering  the    i  ds  of  messages  emitted  during  the  current  checkpoint  private  transient    set    u  id  ids  for  current  checkpoint    the  list  with    i  ds  from  checkpoints  that  were  triggered  but  not  yet  completed  or  notified  of  completion  protected  transient    array  deque    tuple2    long    set    u  id  pending  checkpoints    set  which  contain  all  processed  ids    ids  are  acknowledged  after  checkpoints    when  restoring  a  checkpoint  ids  may  be  processed  again    this  happens  when  the  checkpoint  completed  but  the  ids  for  a  checkpoint  haven  t  been  acknowledged  yet  private  transient    set    u  id  ids  processed  but  not  acknowledged  private  transient    list  state    serialized  checkpoint  data  checkpointed  state    creates  a  new    message  acknowledging  source  base  for    i  ds  of  the  given  type  param  id  class    the  class  of  the  message  id  type  used  to  create  a  serializer  for  the  message    i  ds  protected    message  acknowledging  source  base    class    u  id  id  class  this    type  extractor  get  for  class  id  class    creates  a  new    message  acknowledging  source  base  for    i  ds  of  the  given  type  param  id  type  info    the  type  information  of  the  message  id  type  used  to  create  a  serializer  for  the  message    i  ds  protected    message  acknowledging  source  base    type  information    u  id  id  type  info  this  id  serializer  id  type  info  create  serializer  new    execution  config    override  public  void  initialize  state    function  initialization  context  context  throws    exception    preconditions  check  state  this  checkpointed  state  null    the  get  class  get  simple  name  has  already  been  initialized    we  are  using    java  serializer  from  the  flink  runtime  module  here    this  is  very  naughty  and  we  shouldn  t  be  doing  it  because  ideally  nothing  in  the  api  modules  connector  depends  directly  on  flink  runtime    we  are  doing  it  here  because  we  need  to  maintain  backwards  compatibility  with  old  state  and  because  we  will  have  to  rework  remove  this  code  soon  this  checkpointed  state  context  get  operator  state  store  get  list  state  new    list  state  descriptor  message  acknowledging  source  state  new    java  serializer  this  ids  for  current  checkpoint  new    hash  set    this  pending  checkpoints  new    array  deque  this  ids  processed  but  not  acknowledged  new    hash  set  if  context  is  restored  log  info    restoring  state  for  the  get  class  get  simple  name    list    serialized  checkpoint  data  retrieved  states  new    array  list  for    serialized  checkpoint  data  entry  this  checkpointed  state  get  retrieved  states  add  entry  given  that  the  parallelism  of  the  function  is    we  can  only  have  at  most    state    preconditions  check  argument  retrieved  states  size    get  class  get  simple  name  retrieved  invalid  state  pending  checkpoints    serialized  checkpoint  data  to  deque  retrieved  states  get    id  serializer  build  a  set  which  contains  all  processed  ids    it  may  be  used  to  check  if  we  have  already  processed  an  incoming  message  for    tuple2    long    set    u  id  checkpoint  pending  checkpoints  ids  processed  but  not  acknowledged  add  all  checkpoint  f1  else  log  info    no  state  to  restore  for  the  get  class  get  simple  name    override  public  void  close  throws    exception  ids  for  current  checkpoint  clear  pending  checkpoints  clear  id    checkpointing    this  method  must  be  implemented  to  acknowledge  the  given  set  of    i  ds  back  to  the  message  queue  param  u  ids    the  list  od    i  ds  to  acknowledge  protected  abstract  void  acknowledge  i  ds  long  checkpoint  id    set    u  id  u  ids    adds  an  id  to  be  stored  with  the  current  checkpoint    in  order  to  achieve  exactly  once  guarantees  implementing  classes  should  only  emit  records  with    i  ds  for  which  this  method  return  true  param  uid    the  id  to  add  return    true  if  the  id  has  not  been  processed  previously  protected  boolean  add  id    u  id  uid  ids  for  current  checkpoint  add  uid  return  ids  processed  but  not  acknowledged  add  uid    checkpointing  the  data    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception    preconditions  check  state  this  checkpointed  state  null    the  get  class  get  simple  name  has  not  been  properly  initialized  if  log  is  debug  enabled  log  debug    checkpointing    messages  checkpoint  id  timestamp  ids  for  current  checkpoint  context  get  checkpoint  id  context  get  checkpoint  timestamp  pending  checkpoints  add  last  new    tuple2  context  get  checkpoint  id  ids  for  current  checkpoint  ids  for  current  checkpoint  new    hash  set    this  checkpointed  state  clear  this  checkpointed  state  add    serialized  checkpoint  data  from  deque  pending  checkpoints  id  serializer    override  public  void  notify  checkpoint  complete  long  checkpoint  id  throws    exception  log  debug    committing    messages  externally  for  checkpoint  checkpoint  id  for    iterator    tuple2    long    set    u  id  iter  pending  checkpoints  iterator  iter  has  next    tuple2    long    set    u  id  checkpoint  iter  next  long  id  checkpoint  f0  if  id  checkpoint  id  log  trace    committing    messages  with  following    i  ds  checkpoint  f1  acknowledge  i  ds  checkpoint  id  checkpoint  f1  remove  deduplication  data  ids  processed  but  not  acknowledged  remove  all  checkpoint  f1  remove  checkpoint  data  iter  remove  else  break    override  public  void  notify  checkpoint  aborted  long  checkpoint  id  
public  evolving  public  abstract  class    multiple  ids  message  acknowledging  source  base    type    u  id    session  id  extends    message  acknowledging  source  base    type    u  id  private  static  final  long  serial  version  u  i  d  42  l  private  static  final    logger  log    logger  factory  get  logger    multiple  ids  message  acknowledging  source  base  class    session  ids  per  pending  snapshot  protected  transient    deque    tuple2    long    list    session  id  session  ids  per  snapshot    current  session  ids  for  this  snapshot  protected  transient    list    session  id  session  ids    creates  a  new    message  acknowledging  source  base  for    i  ds  of  the  given  type  param  id  class    the  class  of  the  message  id  type  used  to  create  a  serializer  for  the  message    i  ds  protected    multiple  ids  message  acknowledging  source  base    class    u  id  id  class  super  id  class    creates  a  new    message  acknowledging  source  base  for    i  ds  of  the  given  type  param  id  type  info    the  type  information  of  the  message  id  type  used  to  create  a  serializer  for  the  message    i  ds  protected    multiple  ids  message  acknowledging  source  base    type  information    u  id  id  type  info  super  id  type  info    override  public  void  open    configuration  parameters  throws    exception  super  open  parameters  session  ids  new    array  list    session  ids  per  snapshot  new    array  deque    override  public  void  close  throws    exception  super  close  session  ids  clear  session  ids  per  snapshot  clear  id    checkpointing    acknowledges  the  session  ids  param  checkpoint  id    the  id  of  the  current  checkout  to  acknowledge  ids  for  param  unique  ids    the  checkpointed  unique  ids  which  are  ignored  here    they  only  serve  as  a  means  of  de  duplicating  messages  when  the  acknowledgment  after  a  checkpoint  fails    override  protected  final  void  acknowledge  i  ds  long  checkpoint  id    set    u  id  unique  ids  log  debug    acknowledging  ids  for  checkpoint  checkpoint  id    iterator    tuple2    long    list    session  id  iterator  session  ids  per  snapshot  iterator  while  iterator  has  next  final    tuple2    long    list    session  id  next  iterator  next  long  id  next  f0  if  id  checkpoint  id  acknowledge  session  i  ds  next  f1  remove  ids  for  this  session  iterator  remove    acknowledges  the  session  ids  param  session  ids    the  message  ids  for  this  session  protected  abstract  void  acknowledge  session  i  ds    list    session  id  session  ids    checkpointing  the  data    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception  session  ids  per  snapshot  add  new    tuple2  context  get  checkpoint  id  session  ids  session  ids  new    array  list    super  snapshot  state  context  
public  evolving  public  class    socket  text  stream  function  implements    source  function    string  private  static  final  long  serial  version  u  i  d  1  l  private  static  final    logger  log    logger  factory  get  logger    socket  text  stream  function  class    default  delay  between  successive  connection  attempts  private  static  final  int  default  connection  retry  sleep      default  connection  timeout  when  connecting  to  the  server  socket  infinite  private  static  final  int  connection  timeout  time    private  final    string  hostname  private  final  int  port  private  final    string  delimiter  private  final  long  max  num  retries  private  final  long  delay  between  retries  private  transient    socket  current  socket  private  volatile  boolean  is  running  true  public    socket  text  stream  function    string  hostname  int  port    string  delimiter  long  max  num  retries  this  hostname  port  delimiter  max  num  retries  default  connection  retry  sleep  public    socket  text  stream  function    string  hostname  int  port    string  delimiter  long  max  num  retries  long  delay  between  retries  check  argument  is  valid  client  port  port  port  is  out  of  range  check  argument  max  num  retries    max  num  retries  must  be  zero  or  larger  num  retries  or    infinite  retries  check  argument  delay  between  retries    delay  between  retries  must  be  zero  or  positive  this  hostname  check  not  null  hostname  hostname  must  not  be  null  this  port  port  this  delimiter  delimiter  this  max  num  retries  max  num  retries  this  delay  between  retries  delay  between  retries    override  public  void  run    source  context    string  ctx  throws    exception  final    string  builder  buffer  new    string  builder  long  attempt    while  is  running  try    socket  socket  new    socket  current  socket  socket  log  info    connecting  to  server  socket  hostname  port  socket  connect  new    inet  socket  address  hostname  port  connection  timeout  time  try    buffered  reader  reader  new    buffered  reader  new    input  stream  reader  socket  get  input  stream  char  cbuf  new  char    int  bytes  read  while  is  running  bytes  read  reader  read  cbuf    buffer  append  cbuf    bytes  read  int  delim  pos  while  buffer  length  delimiter  length  delim  pos  buffer  index  of  delimiter      string  record  buffer  substring    delim  pos  truncate  trailing  carriage  return  if  delimiter  equals  n  record  ends  with  r  record  record  substring    record  length    ctx  collect  record  buffer  delete    delim  pos  delimiter  length  if  we  dropped  out  of  this  loop  due  to  an  eof  sleep  and  retry  if  is  running  attempt  if  max  num  retries    attempt  max  num  retries  log  warn    lost  connection  to  server  socket    retrying  in  delay  between  retries  msecs    thread  sleep  delay  between  retries  else  this  should  probably  be  here  but  some  examples  expect  simple  exists  of  the  stream  source  throw  new    e  o  f  exception    reached  end  of  stream  and  reconnects  are  not  enabled  break  collect  trailing  data  if  buffer  length    ctx  collect  buffer  to  string    override  public  void  cancel  is  running  false  we  need  to  close  the  socket  as  well  because  the    thread  interrupt  function  will  not  wake  the  thread  in  the  socket  stream  read  method  when  blocked    socket  the  socket  this  current  socket  if  the  socket  null    i  o  utils  close  socket  the  socket  
public  evolving  public  class    stateful  sequence  source  extends    rich  parallel  source  function    long  implements    checkpointed  function  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  start  private  final  long  end  private  volatile  boolean  is  running  true  private  transient    deque    long  values  to  emit  private  transient    list  state    long  checkpointed  state    creates  a  source  that  emits  all  numbers  from  the  given  interval  exactly  once  param  start    start  of  the  range  of  numbers  to  emit  param  end    end  of  the  range  of  numbers  to  emit  public    stateful  sequence  source  long  start  long  end  this  start  start  this  end  end    override  public  void  initialize  state    function  initialization  context  context  throws    exception    preconditions  check  state  this  checkpointed  state  null    the  get  class  get  simple  name  has  already  been  initialized  this  checkpointed  state  context  get  operator  state  store  get  list  state  new    list  state  descriptor  stateful  sequence  source  state    long  serializer  instance  this  values  to  emit  new    array  deque  if  context  is  restored  upon  restoring  for    long  v  this  checkpointed  state  get  this  values  to  emit  add  v  else  the  first  time  the  job  is  executed  final  int  step  size  get  runtime  context  get  number  of  parallel  subtasks  final  int  task  idx  get  runtime  context  get  index  of  this  subtask  final  long  congruence  start  task  idx  long  total  no  of  elements    math  abs  end  start    final  int  base  size  safe  divide  total  no  of  elements  step  size  final  int  to  collect  total  no  of  elements  step  size  task  idx  base  size    base  size  for  long  collected    collected  to  collect  collected  this  values  to  emit  add  collected  step  size  congruence    override  public  void  run    source  context    long  ctx  throws    exception  while  is  running  this  values  to  emit  is  empty  synchronized  ctx  get  checkpoint  lock  ctx  collect  this  values  to  emit  poll    override  public  void  cancel  is  running  false    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception    preconditions  check  state  this  checkpointed  state  null    the  get  class  get  simple  name  state  has  not  been  properly  initialized  this  checkpointed  state  clear  for    long  v  this  values  to  emit  this  checkpointed  state  add  v  private  static  int  safe  divide  long  left  long  right    preconditions  check  argument  right      preconditions  check  argument  left      preconditions  check  argument  left    integer  max  value  right  return  int  left  right  
public  evolving  public  class    cosine  distance  data  extends    extraction  aware  delta  function  data  double  private  static  final  long  serial  version  u  i  d    l  public    cosine  distance  super  null  public    cosine  distance    extractor  data  double  converter  super  converter    override  public  double  get  nested  delta  double  old  data  point  double  new  data  point  if  is  nullvector  old  data  point  new  data  point  return    if  old  data  point  length  new  data  point  length  throw  new    illegal  argument  exception    the  size  of  two  input  arrays  are  not  same  can  not  compute  cosine  distance  double  sum1    double  sum2    for  int  i    i  old  data  point  length  i  sum1  old  data  point  i  old  data  point  i  sum2  new  data  point  i  new  data  point  i  sum1    math  sqrt  sum1  sum2    math  sqrt  sum2  return  1d  dot  product  old  data  point  new  data  point  sum1  sum2  private  double  dot  product  double  a  double  b  double  result    for  int  i    i  a  length  i  result  a  i  b  i  return  result  private  boolean  is  nullvector  double  vectors  outer  for  double  v  vectors  for  double  field  v  if  field    continue  outer    this  position  is  only  reached  in  case  all  fields  are    return  true  return  false  
public  evolving  public  interface    delta  function  data  extends    serializable    calculates  the  delta  between  two  given  data  points  param  old  data  point  the  old  data  point  param  new  data  point  the  new  data  point  return  the  delta  between  the  two  given  points  double  get  delta  data  old  data  point  data  new  data  point  
public  evolving  public  class    euclidean  distance  data  extends    extraction  aware  delta  function  data  double  public    euclidean  distance  super  null  public    euclidean  distance    extractor  data  double  converter  super  converter  private  static  final  long  serial  version  u  i  d    l    override  public  double  get  nested  delta  double  old  data  point  double  new  data  point  double  result    for  int  i    i  old  data  point  length  i  result  old  data  point  i  new  data  point  i  old  data  point  i  new  data  point  i  return    math  sqrt  result  
public  evolving  public  abstract  class    extraction  aware  delta  function  data  to  implements    delta  function  data  private  static  final  long  serial  version  u  i  d    l  private    extractor  data  to  converter  public    extraction  aware  delta  function    extractor  data  to  converter  this  converter  converter    this  method  takes  the  two  data  point  and  runs  the  set  extractor  on  it    the  delta  function  implemented  at  link  get  nested  delta  is  then  called  with  the  extracted  data    in  case  no  extractor  is  set  the  input  data  gets  passes  to  link  get  nested  delta  as  is    the  return  value  is  just  forwarded  from  link  get  nested  delta  param  old  data  point  the  older  data  point  as  raw  data  before  extraction  param  new  data  point  the  new  data  point  as  raw  data  before  extraction  return  the  delta  between  the  two  points    suppress  warnings  unchecked    override  public  double  get  delta  data  old  data  point  data  new  data  point  if  converter  null    in  case  no  conversion  extraction  is  required  we  can  cast  data  to  to    therefore  unchecked  warning  is  suppressed  for  this  method  return  get  nested  delta  to  old  data  point  to  new  data  point  else  return  get  nested  delta  converter  extract  old  data  point  converter  extract  new  data  point    this  method  is  exactly  the  same  as  link    delta  function  get  delta    object    object  except  that  it  gets  the  result  of  the  previously  done  extractions  as  input    therefore  this  method  only  does  the  actual  calculation  of  the  delta  but  no  data  extraction  or  conversion  param  old  data  point  the  older  data  point  param  new  data  point  the  new  data  point  return  the  delta  between  the  two  points  public  abstract  double  get  nested  delta  to  old  data  point  to  new  data  point  
public  evolving  public  abstract  class    process  all  window  function  in  out  w  extends    window  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d  1  l    evaluates  the  window  and  outputs  none  or  several  elements  param  context    the  context  in  which  the  window  is  being  evaluated  param  elements    the  elements  in  the  window  being  evaluated  param  out  a  collector  for  emitting  elements  throws    exception    the  function  may  throw  exceptions  to  fail  the  program  and  trigger  recovery  public  abstract  void  process    context  context    iterable  in  elements    collector  out  out  throws    exception    deletes  any  state  in  the  code    context  when  the    window  expires  the  watermark  passes  its  code  max  timestamp  code  allowed  lateness  param  context    the  context  to  which  the  window  is  being  evaluated  throws    exception    the  function  may  throw  exceptions  to  fail  the  program  and  trigger  recovery  public  void  clear    context  context  throws    exception    the  context  holding  window  metadata  public  abstract  class    context  return    the  window  that  is  being  evaluated  public  abstract  w  window    state  accessor  for  per  key  and  per  window  state  p  b  note  b    if  you  use  per  window  state  you  have  to  ensure  that  you  clean  it  up  by  implementing  link    process  window  function  clear    process  window  function    context  public  abstract    keyed  state  store  window  state    state  accessor  for  per  key  global  state  public  abstract    keyed  state  store  global  state    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag  the  code    output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  public  abstract  x  void  output    output  tag  x  output  tag  x  value  
public  evolving  public  abstract  class    process  window  function  in  out  key  w  extends    window  extends    abstract  rich  function  private  static  final  long  serial  version  u  i  d  1  l    evaluates  the  window  and  outputs  none  or  several  elements  param  key    the  key  for  which  this  window  is  evaluated  param  context    the  context  in  which  the  window  is  being  evaluated  param  elements    the  elements  in  the  window  being  evaluated  param  out  a  collector  for  emitting  elements  throws    exception    the  function  may  throw  exceptions  to  fail  the  program  and  trigger  recovery  public  abstract  void  process  key  key    context  context    iterable  in  elements    collector  out  out  throws    exception    deletes  any  state  in  the  code    context  when  the    window  expires  the  watermark  passes  its  code  max  timestamp  code  allowed  lateness  param  context    the  context  to  which  the  window  is  being  evaluated  throws    exception    the  function  may  throw  exceptions  to  fail  the  program  and  trigger  recovery  public  void  clear    context  context  throws    exception    the  context  holding  window  metadata  public  abstract  class    context  implements  java  io    serializable    returns  the  window  that  is  being  evaluated  public  abstract  w  window    returns  the  current  processing  time  public  abstract  long  current  processing  time    returns  the  current  event  time  watermark  public  abstract  long  current  watermark    state  accessor  for  per  key  and  per  window  state  p  b  note  b    if  you  use  per  window  state  you  have  to  ensure  that  you  clean  it  up  by  implementing  link    process  window  function  clear    context  public  abstract    keyed  state  store  window  state    state  accessor  for  per  key  global  state  public  abstract    keyed  state  store  global  state    emits  a  record  to  the  side  output  identified  by  the  link    output  tag  param  output  tag  the  code    output  tag  that  identifies  the  side  output  to  emit  to  param  value    the  record  to  emit  public  abstract  x  void  output    output  tag  x  output  tag  x  value  
public  evolving    deprecated  public  abstract  class    rich  process  all  window  function  in  out  w  extends    window  extends    process  all  window  function  in  out  w  private  static  final  long  serial  version  u  i  d  1  l  
public  evolving    deprecated  public  abstract  class    rich  process  window  function  in  out  key  w  extends    window  extends    process  window  function  in  out  key  w  private  static  final  long  serial  version  u  i  d  1  l  
public  evolving  public  abstract  class    abstract  stream  operator  out  implements    stream  operator  out    setupable  stream  operator  out    checkpointed  stream  operator    serializable  private  static  final  long  serial  version  u  i  d  1  l    the  logger  used  by  the  operator  class  and  its  subclasses  protected  static  final    logger  log    logger  factory  get  logger    abstract  stream  operator  class  configuration  properties  a  sane  default  for  most  operators  protected    chaining  strategy  chaining  strategy    chaining  strategy  head  runtime  fields    the  task  that  contains  this  operator  and  other  operators  in  the  same  chain  private  transient    stream  task  container  protected  transient    stream  config  config  protected  transient    output    stream  record  out  output    the  runtime  context  for    u  d  fs  private  transient    streaming  runtime  context  runtime  context  key  value  state  code    key  selector  for  extracting  a  key  from  an  element  being  processed    this  is  used  to  scope  keyed  state  to  a  key    this  is  null  if  the  operator  is  not  a  keyed  operator  p    this  is  for  elements  from  the  first  input  private  transient    key  selector  state  key  selector1  code    key  selector  for  extracting  a  key  from  an  element  being  processed    this  is  used  to  scope  keyed  state  to  a  key    this  is  null  if  the  operator  is  not  a  keyed  operator  p    this  is  for  elements  from  the  second  input  private  transient    key  selector  state  key  selector2  private  transient    stream  operator  state  handler  state  handler  private  transient    internal  time  service  manager  time  service  manager    metrics    metric  group  for  the  operator  protected  transient    operator  metric  group  metrics  protected  transient    latency  stats  latency  stats  time  handler  protected  transient    processing  time  service  processing  time  service  two  input  operator  watermarks    we  keep  track  of  watermarks  from  both  inputs  the  combined  input  is  the  minimum    once  the  minimum  advances  we  emit  a  new  watermark  for  downstream  operators  private  long  combined  watermark    long  min  value  private  long  input1  watermark    long  min  value  private  long  input2  watermark    long  min  value    life    cycle    override  public  void  setup    stream  task  containing  task    stream  config  config    output    stream  record  out  output  final    environment  environment  containing  task  get  environment  this  container  containing  task  this  config  config  try    operator  metric  group  operator  metric  group  environment  get  metric  group  get  or  add  operator  config  get  operator  i  d  config  get  operator  name  this  output  new    counting  output  output  operator  metric  group  get  i  o  metric  group  get  num  records  out  counter  if  config  is  chain  start  operator  metric  group  get  i  o  metric  group  reuse  input  metrics  for  task  if  config  is  chain  end  operator  metric  group  get  i  o  metric  group  reuse  output  metrics  for  task  this  metrics  operator  metric  group  catch    exception  e  log  warn    an  error  occurred  while  instantiating  task  metrics  e  this  metrics    unregistered  metric  groups  create  unregistered  operator  metric  group  this  output  output  try    configuration  task  manager  config  environment  get  task  manager  info  get  configuration  int  history  size  task  manager  config  get  integer    metric  options  latency  history  size  if  history  size    log  warn  has  been  set  to  a  value  equal  or  below      using  default    metric  options  latency  history  size  history  size  history  size    metric  options  latency  history  size  default  value  final    string  configured  granularity  task  manager  config  get  string    metric  options  latency  source  granularity    latency  stats    granularity  granularity  try  granularity    latency  stats    granularity  value  of  configured  granularity  to  upper  case    locale  root  catch    illegal  argument  exception  iae  granularity    latency  stats    granularity  operator  log  warn    configured  value  option  for  is  invalid    defaulting  to  configured  granularity    metric  options  latency  source  granularity  key  granularity    task  manager  job  metric  group  job  metric  group  this  metrics  parent  parent  this  latency  stats  new    latency  stats  job  metric  group  add  group  latency  history  size  container  get  index  in  subtask  group  get  operator  i  d  granularity  catch    exception  e  log  warn    an  error  occurred  while  instantiating  latency  metrics  e  this  latency  stats  new    latency  stats    unregistered  metric  groups  create  unregistered  task  manager  job  metric  group  add  group  latency      new    operator  i  d    latency  stats    granularity  single  this  runtime  context  new    streaming  runtime  context  environment  environment  get  accumulator  registry  get  user  map  get  metric  group  get  operator  i  d  get  processing  time  service  null  environment  get  external  resource  info  provider  state  key  selector1  config  get  state  partitioner    get  user  code  classloader  state  key  selector2  config  get  state  partitioner    get  user  code  classloader  deprecated    the  link    processing  time  service  instance  should  be  passed  by  the  operator  constructor  and  this  method  will  be  removed  along  with  link    setupable  stream  operator    deprecated  public  void  set  processing  time  service    processing  time  service  processing  time  service  this  processing  time  service    preconditions  check  not  null  processing  time  service    override  public    metric  group  get  metric  group  return  metrics    override  public  final  void  initialize  state    stream  task  state  initializer  stream  task  state  manager  throws    exception  final    type  serializer  key  serializer  config  get  state  key  serializer  get  user  code  classloader  final    stream  task  containing  task    preconditions  check  not  null  get  containing  task  final    closeable  registry  stream  task  closeable  registry    preconditions  check  not  null  containing  task  get  cancelables  final    stream  operator  state  context  context  stream  task  state  manager  stream  operator  state  context  get  operator  i  d  get  class  get  simple  name  get  processing  time  service  this  key  serializer  stream  task  closeable  registry  metrics  state  handler  new    stream  operator  state  handler  context  get  execution  config  stream  task  closeable  registry  time  service  manager  context  internal  timer  service  manager  state  handler  initialize  operator  state  this  runtime  context  set  keyed  state  store  state  handler  get  keyed  state  store  or  else  null    this  method  is  called  immediately  before  any  elements  are  processed  it  should  contain  the  operator  s  initialization  logic  e  g  state  initialization  p    the  default  implementation  does  nothing  throws    exception    an  exception  in  this  method  causes  the  operator  to  fail    override  public  void  open  throws    exception    this  method  is  called  after  all  records  have  been  added  to  the  operators  via  the  methods  link    one  input  stream  operator  process  element    stream  record  or  link    two  input  stream  operator  process  element1    stream  record  and  link    two  input  stream  operator  process  element2    stream  record  p    the  method  is  expected  to  flush  all  remaining  buffered  data    exceptions  during  this  flushing  of  buffered  should  be  propagated  in  order  to  cause  the  operation  to  be  recognized  asa  failed  because  the  last  data  items  are  not  processed  properly  throws    exception    an  exception  in  this  method  causes  the  operator  to  fail    override  public  void  close  throws    exception    this  method  is  called  at  the  very  end  of  the  operator  s  life  both  in  the  case  of  a  successful  completion  of  the  operation  and  in  the  case  of  a  failure  and  canceling  p    this  method  is  expected  to  make  a  thorough  effort  to  release  all  resources  that  the  operator  has  acquired    override  public  void  dispose  throws    exception  if  state  handler  null  state  handler  dispose    override  public  void  prepare  snapshot  pre  barrier  long  checkpoint  id  throws    exception  the  default  implementation  does  nothing  and  accepts  the  checkpoint  this  is  purely  for  subclasses  to  override    override  public  final    operator  snapshot  futures  snapshot  state  long  checkpoint  id  long  timestamp    checkpoint  options  checkpoint  options    checkpoint  stream  factory  factory  throws    exception  return  state  handler  snapshot  state  this    optional  of  nullable  time  service  manager  get  operator  name  checkpoint  id  timestamp  checkpoint  options  factory    stream  operators  with  state  which  want  to  participate  in  a  snapshot  need  to  override  this  hook  method  param  context  context  that  provides  information  and  means  required  for  taking  a  snapshot    override  public  void  snapshot  state    state  snapshot  context  context  throws    exception    stream  operators  with  state  which  can  be  restored  need  to  override  this  hook  method  param  context  context  that  allows  to  register  different  states    override  public  void  initialize  state    state  initialization  context  context  throws    exception    override  public  void  notify  checkpoint  complete  long  checkpoint  id  throws    exception  state  handler  notify  checkpoint  complete  checkpoint  id    override  public  void  notify  checkpoint  aborted  long  checkpoint  id  throws    exception  state  handler  notify  checkpoint  aborted  checkpoint  id    properties  and    services    gets  the  execution  config  defined  on  the  execution  environment  of  the  job  to  which  this  operator  belongs  return    the  job  s  execution  config  public    execution  config  get  execution  config  return  container  get  execution  config  public    stream  config  get  operator  config  return  config  public    stream  task  get  containing  task  return  container  public    class  loader  get  user  code  classloader  return  container  get  user  code  class  loader    return  the  operator  name    if  the  runtime  context  has  been  set  then  the  task  name  with  subtask  index  is  returned    otherwise  the  simple  class  name  is  returned  return    if  runtime  context  is  set  then  return  task  name  with  subtask  index    otherwise  return  simple  class  name  protected    string  get  operator  name  if  runtime  context  null  return  runtime  context  get  task  name  with  subtasks  else  return  get  class  get  simple  name    returns  a  context  that  allows  the  operator  to  query  information  about  the  execution  and  also  to  interact  with  systems  such  as  broadcast  variables  and  managed  state    this  also  allows  to  register  timers    visible  for  testing  public    streaming  runtime  context  get  runtime  context  return  runtime  context    visible  for  testing  public  k    keyed  state  backend  k  get  keyed  state  backend  return  state  handler  get  keyed  state  backend    visible  for  testing  public    operator  state  backend  get  operator  state  backend  return  state  handler  get  operator  state  backend    returns  the  link    processing  time  service  responsible  for  getting  the  current  processing  time  and  registering  timers    visible  for  testing  public    processing  time  service  get  processing  time  service  return  processing  time  service    creates  a  partitioned  state  handle  using  the  state  backend  configured  for  this  task  throws    illegal  state  exception    thrown  if  the  key  value  state  was  already  initialized  throws    exception    thrown  if  the  state  backend  cannot  create  the  key  value  state  protected  s  extends    state  s  get  partitioned  state    state  descriptor  s  state  descriptor  throws    exception  return  get  partitioned  state    void  namespace  instance    void  namespace  serializer  instance  state  descriptor  protected  n  s  extends    state  t  s  get  or  create  keyed  state    type  serializer  n  namespace  serializer    state  descriptor  s  t  state  descriptor  throws    exception  return  state  handler  get  or  create  keyed  state  namespace  serializer  state  descriptor    creates  a  partitioned  state  handle  using  the  state  backend  configured  for  this  task  throws    illegal  state  exception    thrown  if  the  key  value  state  was  already  initialized  throws    exception    thrown  if  the  state  backend  cannot  create  the  key  value  state  protected  s  extends    state  n  s  get  partitioned  state  n  namespace    type  serializer  n  namespace  serializer    state  descriptor  s  state  descriptor  throws    exception  return  state  handler  get  partitioned  state  namespace  namespace  serializer  state  descriptor    override    suppress  warnings  unchecked  rawtypes  public  void  set  key  context  element1    stream  record  record  throws    exception  set  key  context  element  record  state  key  selector1    override    suppress  warnings  unchecked  rawtypes  public  void  set  key  context  element2    stream  record  record  throws    exception  set  key  context  element  record  state  key  selector2  private  t  void  set  key  context  element    stream  record  t  record    key  selector  t  selector  throws    exception  if  selector  null    object  key  selector  get  key  record  get  value  set  current  key  key  public  void  set  current  key    object  key  state  handler  set  current  key  key  public    object  get  current  key  return  state  handler  get  current  key  public    keyed  state  store  get  keyed  state  store  if  state  handler  null  return  null  return  state  handler  get  keyed  state  store  or  else  null    context  and  chaining  properties    override  public  final  void  set  chaining  strategy    chaining  strategy  strategy  this  chaining  strategy  strategy    override  public  final    chaining  strategy  get  chaining  strategy  return  chaining  strategy    metrics    one  input  stream  public  void  process  latency  marker    latency  marker  latency  marker  throws    exception  report  or  forward  latency  marker  latency  marker    two  input  stream  public  void  process  latency  marker1    latency  marker  latency  marker  throws    exception  report  or  forward  latency  marker  latency  marker  public  void  process  latency  marker2    latency  marker  latency  marker  throws    exception  report  or  forward  latency  marker  latency  marker  protected  void  report  or  forward  latency  marker    latency  marker  marker  all  operators  are  tracking  latencies  this  latency  stats  report  latency  marker  everything  except  sinks  forwards  latency  markers  this  output  emit  latency  marker  marker    watermark  handling    returns  a  link    internal  timer  service  that  can  be  used  to  query  current  processing  time  and  event  time  and  to  set  timers    an  operator  can  have  several  timer  services  where  each  has  its  own  namespace  serializer    timer  services  are  differentiated  by  the  string  key  that  is  given  when  requesting  them  if  you  call  this  method  with  the  same  key  multiple  times  you  will  get  the  same  timer  service  instance  in  subsequent  requests  p    timers  are  always  scoped  to  a  key  the  currently  active  key  of  a  keyed  stream  operation    when  a  timer  fires  this  key  will  also  be  set  as  the  currently  active  key  p    each  timer  has  attached  metadata  the  namespace    different  timer  services  can  have  a  different  namespace  type    if  you  don  t  need  namespace  differentiation  you  can  use  link    void  namespace  serializer  as  the  namespace  serializer  param  name    the  name  of  the  requested  timer  service    if  no  service  exists  under  the  given  name  a  new  one  will  be  created  and  returned  param  namespace  serializer  code    type  serializer  for  the  timer  namespace  param  triggerable    the  link    triggerable  that  should  be  invoked  when  timers  fire  param  n    the  type  of  the  timer  namespace  public  k  n    internal  timer  service  n  get  internal  timer  service    string  name    type  serializer  n  namespace  serializer    triggerable  k  n  triggerable  if  time  service  manager  null  throw  new    runtime  exception    the  timer  service  has  not  been  initialized    suppress  warnings  unchecked    internal  time  service  manager  k  keyed  time  service  handler    internal  time  service  manager  k  time  service  manager  return  keyed  time  service  handler  get  internal  timer  service  name  namespace  serializer  triggerable  state  handler  get  keyed  state  backend  public  void  process  watermark    watermark  mark  throws    exception  if  time  service  manager  null  time  service  manager  advance  watermark  mark  output  emit  watermark  mark  public  void  process  watermark1    watermark  mark  throws    exception  input1  watermark  mark  get  timestamp  long  new  min    math  min  input1  watermark  input2  watermark  if  new  min  combined  watermark  combined  watermark  new  min  process  watermark  new    watermark  combined  watermark  public  void  process  watermark2    watermark  mark  throws    exception  input2  watermark  mark  get  timestamp  long  new  min    math  min  input1  watermark  input2  watermark  if  new  min  combined  watermark  combined  watermark  new  min  process  watermark  new    watermark  combined  watermark    override  public    operator  i  d  get  operator  i  d  return  config  get  operator  i  d    visible  for  testing  public  int  num  processing  time  timers  return  time  service  manager  null    time  service  manager  num  processing  time  timers    visible  for  testing  public  int  num  event  time  timers  return  time  service  manager  null    time  service  manager  num  event  time  timers  protected    optional    internal  time  service  manager  get  time  service  manager  return    optional  of  nullable  time  service  manager  
public  evolving  public  abstract  class    abstract  udf  stream  operator  out  f  extends    function  extends    abstract  stream  operator  out  implements    output  type  configurable  out  private  static  final  long  serial  version  u  i  d  1  l    the  user  function  protected  final  f  user  function    flag  to  prevent  duplicate  function  close  calls  in  close  and  dispose  private  transient  boolean  functions  closed  false  public    abstract  udf  stream  operator  f  user  function  this  user  function  require  non  null  user  function  check  udf  checkpointing  preconditions    gets  the  user  function  executed  in  this  operator  return    the  user  function  of  this  operator  public  f  get  user  function  return  user  function  operator  life  cycle    override  public  void  setup    stream  task  containing  task    stream  config  config    output    stream  record  out  output  super  setup  containing  task  config  output    function  utils  set  function  runtime  context  user  function  get  runtime  context    override  public  void  snapshot  state    state  snapshot  context  context  throws    exception  super  snapshot  state  context    streaming  function  utils  snapshot  function  state  context  get  operator  state  backend  user  function    override  public  void  initialize  state    state  initialization  context  context  throws    exception  super  initialize  state  context    streaming  function  utils  restore  function  state  context  user  function    override  public  void  open  throws    exception  super  open    function  utils  open  function  user  function  new    configuration    override  public  void  close  throws    exception  super  close  functions  closed  true    function  utils  close  function  user  function    override  public  void  dispose  throws    exception  super  dispose  if  functions  closed  functions  closed  true    function  utils  close  function  user  function  checkpointing  and  recovery    override  public  void  notify  checkpoint  complete  long  checkpoint  id  throws    exception  super  notify  checkpoint  complete  checkpoint  id  if  user  function  instanceof    checkpoint  listener    checkpoint  listener  user  function  notify  checkpoint  complete  checkpoint  id    override  public  void  notify  checkpoint  aborted  long  checkpoint  id  throws    exception  super  notify  checkpoint  aborted  checkpoint  id  if  user  function  instanceof    checkpoint  listener    checkpoint  listener  user  function  notify  checkpoint  aborted  checkpoint  id    output  type  configuration    override  public  void  set  output  type    type  information  out  out  type  info    execution  config  execution  config    streaming  function  utils  set  output  type  user  function  out  type  info  execution  config    utilities    since  the  streaming  api  does  not  implement  any  parametrization  of  functions  via  a  configuration  the  config  returned  here  is  actually  empty  return    the  user  function  parameters  currently  empty  public    configuration  get  user  function  parameters  return  new    configuration  private  void  check  udf  checkpointing  preconditions  if  user  function  instanceof    checkpointed  function  user  function  instanceof    list  checkpointed  throw  new    illegal  state  exception    user  functions  are  not  allowed  to  implement    checkpointed  function  and    list  checkpointed  
public  evolving  public  interface    bounded  multi  input    it  is  notified  that  no  more  data  will  arrive  on  the  input  identified  by  the  code  input  id    the  code  input  id  is  numbered  starting  from    and    indicates  the  first  input  void  end  input  int  input  id  throws    exception  
public  evolving  public  interface    bounded  one  input    it  is  notified  that  no  more  data  will  arrive  on  the  input  void  end  input  throws    exception  
public  evolving  public  enum    chaining  strategy    operators  will  be  eagerly  chained  whenever  possible  p    to  optimize  performance  it  is  generally  a  good  practice  to  allow  maximal  chaining  and  increase  operator  parallelism  always    the  operator  will  not  be  chained  to  the  preceding  or  succeeding  operators  never    the  operator  will  not  be  chained  to  the  predecessor  but  successors  may  chain  to  this  operator  head  
public  evolving  public  interface    input  in    processes  one  element  that  arrived  on  this  input  of  the  link    multiple  input  stream  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  void  process  element    stream  record  in  element  throws    exception    processes  a  link    watermark  that  arrived  on  the  first  input  of  this  two  input  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  see  org  apache  flink  streaming  api  watermark    watermark  void  process  watermark    watermark  mark  throws    exception    processes  a  link    latency  marker  that  arrived  on  the  first  input  of  this  two  input  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  see  org  apache  flink  streaming  runtime  streamrecord    latency  marker  void  process  latency  marker    latency  marker  latency  marker  throws    exception  void  set  key  context  element    stream  record  in  record  throws    exception  
public  evolving  public  interface    input  selectable    returns  the  next  link    input  selection  that  wants  to  get  the  record    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator    input  selection  next  selection  
public  evolving  public  final  class    input  selection  implements    serializable  public  static  final  int  none  available    private  static  final  long  serial  version  u  i  d  1  l    the  code    input  selection  instance  which  indicates  to  select  all  inputs  public  static  final    input  selection  all  new    input  selection      the  code    input  selection  instance  which  indicates  to  select  the  first  input  public  static  final    input  selection  first  new    builder  select    build    the  code    input  selection  instance  which  indicates  to  select  the  second  input  public  static  final    input  selection  second  new    builder  select    build  private  final  long  input  mask  param  input  mask    to  mark  if  all  inputs  are  selected  private    input  selection  long  input  mask  this  input  mask  input  mask  public  long  get  input  mask  return  input  mask    tests  if  the  input  specified  by  code  input  id  is  selected  param  input  id    the  input  id  see  the  description  of  code  input  id  in  link    builder  select  int  return  code  true  if  the  input  is  selected  code  false  otherwise  public  boolean  is  input  selected  int  input  id  return  input  mask  1  l  input  id        tests  if  all  inputs  are  selected  return  code  true  if  the  input  mask  equals    code  false  otherwise  public  boolean  are  all  inputs  selected  return  input  mask  1  l    fairly  select  one  of  the  two  inputs  for  reading    when  code  input  mask  includes  two  inputs  and  both  inputs  are  available  alternately  select  one  of  them    otherwise  select  the  available  one  of  code  input  mask  or  return  link    input  selection  none  available  to  indicate  no  input  is  selected  p    note  that  this  supports  only  two  inputs  for  performance  reasons  param  available  inputs  mask    the  mask  of  all  available  inputs  param  last  read  input  index    the  index  of  last  read  input  return  the  index  of  the  input  for  reading  or  link    input  selection  none  available  if  code  input  mask  is  empty  or  the  inputs  in  code  input  mask  are  unavailable  public  int  fair  select  next  index  out  of2  int  available  inputs  mask  int  last  read  input  index  int  selection  mask  int  input  mask  int  combine  mask  available  inputs  mask  selection  mask  if  combine  mask    return  last  read  input  index        else  if  combine  mask    combine  mask    return  combine  mask    throw  new    unsupported  operation  exception    only  two  inputs  are  supported    fairly  select  one  of  the  available  inputs  for  reading  param  available  inputs  mask    the  mask  of  all  available  inputs    note    for  this  is  interpreted  as  all  of  the    inputs  are  available  param  last  read  input  index    the  index  of  last  read  input  return  the  index  of  the  input  for  reading  or  link    input  selection  none  available  if  code  input  mask  is  empty  or  the  inputs  in  code  input  mask  are  unavailable  public  int  fair  select  next  index  long  available  inputs  mask  int  last  read  input  index  long  combine  mask  available  inputs  mask  input  mask  if  combine  mask    return  none  available  int  next  read  input  index  select  first  bit  right  from  next  combine  mask  last  read  input  index    if  next  read  input  index    return  next  read  input  index  return  select  first  bit  right  from  next  combine  mask    private  int  select  first  bit  right  from  next  long  bits  int  next  if  next    return  none  available  for  bits  next  bits    bits      bits    next  return  bits    next  none  available    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    input  selection  that    input  selection  o  return  input  mask  that  input  mask    override  public    string  to  string  return    string  value  of  input  mask    utility  class  for  creating  link    input  selection  public  static  final  class    builder  private  long  input  mask      returns  a  code    builder  that  uses  the  input  mask  of  the  specified  code  selection  as  the  initial  mask  public  static    builder  from    input  selection  selection    builder  builder  new    builder  builder  input  mask  selection  input  mask  return  builder    selects  an  input  identified  by  the  given  code  input  id  param  input  id  the  input  id  numbered  starting  from    to    and    indicates  the  first  input    specially    indicates  all  inputs  return  a  reference  to  this  object  public    builder  select  int  input  id  if  input  id    input  id    input  mask  1  l  input  id    else  if  input  id  1  l  input  mask  1  l  else  throw  new    illegal  argument  exception    the  input  id  must  be  in  the  range  of    to    or  be    return  this    build  normalized  mask  if  all  inputs  were  manually  selected  input  mask  will  be  normalized  to    public    input  selection  build  int  input  count  long  all  selected  mask  1  l  input  count    if  input  mask  all  selected  mask  input  mask    else  if  input  mask  all  selected  mask  throw  new    illegal  argument  exception    string  format  input  mask  d  selects  more  than  expected  number  of  inputs  d  input  mask  input  count  return  build  public    input  selection  build  return  new    input  selection  input  mask  
public  evolving  public  interface    mailbox  executor  a  constant  for  empty  args  to  save  on  object  allocation    object  empty  args  new    object      executes  the  given  command  at  some  time  in  the  future  in  the  mailbox  thread  p    an  optional  description  can  and  should  be  added  to  ease  debugging  and  error  reporting    the  description  may  contain  placeholder  that  refer  to  the  provided  description  arguments  using  link  java  util    formatter  syntax    the  actual  description  is  only  formatted  on  demand  param  command  the  runnable  task  to  add  to  the  mailbox  for  execution  param  description  the  optional  description  for  the  command  that  is  used  for  debugging  and  error  reporting  throws    rejected  execution  exception  if  this  task  cannot  be  accepted  for  execution  e  g  because  the  mailbox  is  quiesced  or  closed  default  void  execute    throwing  runnable  extends    exception  command    string  description  execute  command  description  empty  args    executes  the  given  command  at  some  time  in  the  future  in  the  mailbox  thread  p    an  optional  description  can  and  should  be  added  to  ease  debugging  and  error  reporting    the  description  may  contain  placeholder  that  refer  to  the  provided  description  arguments  using  link  java  util    formatter  syntax    the  actual  description  is  only  formatted  on  demand  param  command  the  runnable  task  to  add  to  the  mailbox  for  execution  param  description  format  the  optional  description  for  the  command  that  is  used  for  debugging  and  error  reporting  param  description  args  the  parameters  used  to  format  the  final  description  string  throws    rejected  execution  exception  if  this  task  cannot  be  accepted  for  execution  e  g  because  the  mailbox  is  quiesced  or  closed  void  execute    throwing  runnable  extends    exception  command    string  description  format    object  description  args    submits  the  given  command  for  execution  in  the  future  in  the  mailbox  thread  and  returns  a    future  representing  that  command    the    future  s  code  get  method  will  return  code  null  upon  em  successful  em  completion  p    an  optional  description  can  and  should  be  added  to  ease  debugging  and  error  reporting    the  description  may  contain  placeholder  that  refer  to  the  provided  description  arguments  using  link  java  util    formatter  syntax    the  actual  description  is  only  formatted  on  demand  param  command  the  command  to  submit  param  description  format  the  optional  description  for  the  command  that  is  used  for  debugging  and  error  reporting  param  description  args  the  parameters  used  to  format  the  final  description  string  return  a    future  representing  pending  completion  of  the  task  throws    rejected  execution  exception  if  this  task  cannot  be  accepted  for  execution  e  g  because  the  mailbox  is  quiesced  or  closed  default    nonnull    future    void  submit    nonnull    runnable  with  exception  command    string  description  format    object  description  args    future  task  with  exception    void  future  new    future  task  with  exception  command  execute  future  description  format  description  args  return  future    submits  the  given  command  for  execution  in  the  future  in  the  mailbox  thread  and  returns  a    future  representing  that  command    the    future  s  code  get  method  will  return  code  null  upon  em  successful  em  completion  p    an  optional  description  can  and  should  be  added  to  ease  debugging  and  error  reporting    the  description  may  contain  placeholder  that  refer  to  the  provided  description  arguments  using  link  java  util    formatter  syntax    the  actual  description  is  only  formatted  on  demand  param  command  the  command  to  submit  param  description  the  optional  description  for  the  command  that  is  used  for  debugging  and  error  reporting  return  a    future  representing  pending  completion  of  the  task  throws    rejected  execution  exception  if  this  task  cannot  be  accepted  for  execution  e  g  because  the  mailbox  is  quiesced  or  closed  default    nonnull    future    void  submit    nonnull    runnable  with  exception  command    string  description    future  task  with  exception    void  future  new    future  task  with  exception  command  execute  future  description  empty  args  return  future    submits  the  given  command  for  execution  in  the  future  in  the  mailbox  thread  and  returns  a    future  representing  that  command    the    future  s  code  get  method  will  return  code  null  upon  em  successful  em  completion  p    an  optional  description  can  and  should  be  added  to  ease  debugging  and  error  reporting    the  description  may  contain  placeholder  that  refer  to  the  provided  description  arguments  using  link  java  util    formatter  syntax    the  actual  description  is  only  formatted  on  demand  param  command  the  command  to  submit  param  description  format  the  optional  description  for  the  command  that  is  used  for  debugging  and  error  reporting  param  description  args  the  parameters  used  to  format  the  final  description  string  return  a    future  representing  pending  completion  of  the  task  throws    rejected  execution  exception  if  this  task  cannot  be  accepted  for  execution  e  g  because  the  mailbox  is  quiesced  or  closed  default    nonnull  t    future  t  submit    nonnull    callable  t  command    string  description  format    object  description  args    future  task  with  exception  t  future  new    future  task  with  exception  command  execute  future  description  format  description  args  return  future    submits  the  given  command  for  execution  in  the  future  in  the  mailbox  thread  and  returns  a    future  representing  that  command    the    future  s  code  get  method  will  return  code  null  upon  em  successful  em  completion  p    an  optional  description  can  and  should  be  added  to  ease  debugging  and  error  reporting    the  description  may  contain  placeholder  that  refer  to  the  provided  description  arguments  using  link  java  util    formatter  syntax    the  actual  description  is  only  formatted  on  demand  param  command  the  command  to  submit  param  description  the  optional  description  for  the  command  that  is  used  for  debugging  and  error  reporting  return  a    future  representing  pending  completion  of  the  task  throws    rejected  execution  exception  if  this  task  cannot  be  accepted  for  execution  e  g  because  the  mailbox  is  quiesced  or  closed  default    nonnull  t    future  t  submit    nonnull    callable  t  command    string  description    future  task  with  exception  t  future  new    future  task  with  exception  command  execute  future  description  empty  args  return  future    this  methods  starts  running  the  command  at  the  head  of  the  mailbox  and  is  intended  to  be  used  by  the  mailbox  thread  to  yield  from  a  currently  ongoing  action  to  another  command    the  method  blocks  until  another  command  to  run  is  available  in  the  mailbox  and  must  only  be  called  from  the  mailbox  thread    must  only  be  called  from  the  mailbox  thread  to  not  violate  the  single  threaded  execution  model  throws    interrupted  exception  on  interruption  throws    illegal  state  exception  if  the  mailbox  is  closed  and  can  no  longer  supply  runnables  for  yielding  throws    flink  runtime  exception  if  executed  link    runnable  with  exception  thrown  an  exception  void  yield  throws    interrupted  exception    flink  runtime  exception    this  methods  attempts  to  run  the  command  at  the  head  of  the  mailbox    this  is  intended  to  be  used  by  the  mailbox  thread  to  yield  from  a  currently  ongoing  action  to  another  command    the  method  returns  true  if  a  command  was  found  and  executed  or  false  if  the  mailbox  was  empty    must  only  be  called  from  the  mailbox  thread  to  not  violate  the  single  threaded  execution  model  return  true  on  successful  yielding  to  another  command  false  if  there  was  no  command  to  yield  to  throws    illegal  state  exception  if  the  mailbox  is  closed  and  can  no  longer  supply  runnables  for  yielding  throws    runtime  exception  if  executed  link    runnable  with  exception  thrown  an  exception  boolean  try  yield  throws    flink  runtime  exception  
public  evolving  public  interface    multiple  input  stream  operator  out  extends    stream  operator  out    list    input  get  inputs  
public  evolving  public  interface    one  input  stream  operator  in  out  extends    stream  operator  out    processes  one  element  that  arrived  at  this  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  void  process  element    stream  record  in  element  throws    exception    processes  a  link    watermark    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  see  org  apache  flink  streaming  api  watermark    watermark  void  process  watermark    watermark  mark  throws    exception  void  process  latency  marker    latency  marker  latency  marker  throws    exception  
public  evolving  public  interface    output  t  extends    collector  t    emits  a  link    watermark  from  an  operator    this  watermark  is  broadcast  to  all  downstream  operators  p  a  watermark  specifies  that  no  element  with  a  timestamp  lower  or  equal  to  the  watermark  timestamp  will  be  emitted  in  the  future  void  emit  watermark    watermark  mark    emits  a  record  the  side  output  identified  by  the  given  link    output  tag  param  record    the  record  to  collect  x  void  collect    output  tag  x  output  tag    stream  record  x  record  void  emit  latency  marker    latency  marker  latency  marker  
public  evolving  public  interface    output  type  configurable  out    is  called  by  the  link  org  apache  flink  streaming  api  graph    stream  graph  add  operator    integer    string    stream  operator    type  information    type  information    string  method  when  the  link  org  apache  flink  streaming  api  graph    stream  graph  is  generated    the  method  is  called  with  the  output  link    type  information  which  is  also  used  for  the  link  org  apache  flink  streaming  runtime  tasks    stream  task  output  serializer  param  out  type  info    output  type  information  of  the  link  org  apache  flink  streaming  runtime  tasks    stream  task  param  execution  config    execution  configuration  void  set  output  type    type  information  out  out  type  info    execution  config  execution  config  
public  evolving  public  interface    stream  operator  out  extends    checkpoint  listener    key  context    disposable    serializable  life  cycle    this  method  is  called  immediately  before  any  elements  are  processed  it  should  contain  the  operator  s  initialization  logic  impl  spec    in  case  of  recovery  this  method  needs  to  ensure  that  all  recovered  data  is  processed  before  passing  back  control  so  that  the  order  of  elements  is  ensured  during  the  recovery  of  an  operator  chain  operators  are  opened  from  the  tail  operator  to  the  head  operator  throws  java  lang    exception    an  exception  in  this  method  causes  the  operator  to  fail  void  open  throws    exception    this  method  is  called  after  all  records  have  been  added  to  the  operators  via  the  methods  link  org  apache  flink  streaming  api  operators    one  input  stream  operator  process  element    stream  record  or  link  org  apache  flink  streaming  api  operators    two  input  stream  operator  process  element1    stream  record  and  link  org  apache  flink  streaming  api  operators    two  input  stream  operator  process  element2    stream  record  p    the  method  is  expected  to  flush  all  remaining  buffered  data    exceptions  during  this  flushing  of  buffered  should  be  propagated  in  order  to  cause  the  operation  to  be  recognized  as  failed  because  the  last  data  items  are  not  processed  properly  throws  java  lang    exception    an  exception  in  this  method  causes  the  operator  to  fail  void  close  throws    exception    this  method  is  called  at  the  very  end  of  the  operator  s  life  both  in  the  case  of  a  successful  completion  of  the  operation  and  in  the  case  of  a  failure  and  canceling  p    this  method  is  expected  to  make  a  thorough  effort  to  release  all  resources  that  the  operator  has  acquired    override  void  dispose  throws    exception  state  snapshots    this  method  is  called  when  the  operator  should  do  a  snapshot  before  it  emits  its  own  checkpoint  barrier  p    this  method  is  intended  not  for  any  actual  state  persistence  but  only  for  emitting  some  data  before  emitting  the  checkpoint  barrier    operators  that  maintain  some  small  transient  state  that  is  inefficient  to  checkpoint  especially  when  it  would  need  to  be  checkpointed  in  a  re  scalable  way  but  can  simply  be  sent  downstream  before  the  checkpoint    an  example  are  opportunistic  pre  aggregation  operators  which  have  small  the  pre  aggregation  state  that  is  frequently  flushed  downstream  p  b    important  b    this  method  should  not  be  used  for  any  actual  state  snapshot  logic  because  it  will  inherently  be  within  the  synchronous  part  of  the  operator  s  checkpoint    if  heavy  work  is  done  within  this  method  it  will  affect  latency  and  downstream  checkpoint  alignments  param  checkpoint  id    the  id  of  the  checkpoint  throws    exception    throwing  an  exception  here  causes  the  operator  to  fail  and  go  into  recovery  void  prepare  snapshot  pre  barrier  long  checkpoint  id  throws    exception    called  to  draw  a  state  snapshot  from  the  operator  return  a  runnable  future  to  the  state  handle  that  points  to  the  snapshotted  state    for  synchronous  implementations  the  runnable  might  already  be  finished  throws    exception  exception  that  happened  during  snapshotting    operator  snapshot  futures  snapshot  state  long  checkpoint  id  long  timestamp    checkpoint  options  checkpoint  options    checkpoint  stream  factory  storage  location  throws    exception    provides  a  context  to  initialize  all  state  in  the  operator  void  initialize  state    stream  task  state  initializer  stream  task  state  manager  throws    exception  miscellaneous  void  set  key  context  element1    stream  record  record  throws    exception  void  set  key  context  element2    stream  record  record  throws    exception    metric  group  get  metric  group    operator  i  d  get  operator  i  d  
public  evolving  public  interface    two  input  stream  operator    i  n1    i  n2  out  extends    stream  operator  out    processes  one  element  that  arrived  on  the  first  input  of  this  two  input  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  void  process  element1    stream  record    i  n1  element  throws    exception    processes  one  element  that  arrived  on  the  second  input  of  this  two  input  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  void  process  element2    stream  record    i  n2  element  throws    exception    processes  a  link    watermark  that  arrived  on  the  first  input  of  this  two  input  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  see  org  apache  flink  streaming  api  watermark    watermark  void  process  watermark1    watermark  mark  throws    exception    processes  a  link    watermark  that  arrived  on  the  second  input  of  this  two  input  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  see  org  apache  flink  streaming  api  watermark    watermark  void  process  watermark2    watermark  mark  throws    exception    processes  a  link    latency  marker  that  arrived  on  the  first  input  of  this  two  input  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  see  org  apache  flink  streaming  runtime  streamrecord    latency  marker  void  process  latency  marker1    latency  marker  latency  marker  throws    exception    processes  a  link    latency  marker  that  arrived  on  the  second  input  of  this  two  input  operator    this  method  is  guaranteed  to  not  be  called  concurrently  with  other  methods  of  the  operator  see  org  apache  flink  streaming  runtime  streamrecord    latency  marker  void  process  latency  marker2    latency  marker  latency  marker  throws    exception  
public  evolving  public  enum    time  characteristic    processing  time  for  operators  means  that  the  operator  uses  the  system  clock  of  the  machine  to  determine  the  current  time  of  the  data  stream    processing  time  windows  trigger  based  on  wall  clock  time  and  include  whatever  elements  happen  to  have  arrived  at  the  operator  at  that  point  in  time  p    using  processing  time  for  window  operations  results  in  general  in  quite  non  deterministic  results  because  the  contents  of  the  windows  depends  on  the  speed  in  which  elements  arrive    it  is  however  the  cheapest  method  of  forming  windows  and  the  method  that  introduces  the  least  latency    processing  time    ingestion  time  means  that  the  time  of  each  individual  element  in  the  stream  is  determined  when  the  element  enters  the    flink  streaming  data  flow    operations  like  windows  group  the  elements  based  on  that  time  meaning  that  processing  speed  within  the  streaming  dataflow  does  not  affect  windowing  but  only  the  speed  at  which  sources  receive  elements  p    ingestion  time  is  often  a  good  compromise  between  processing  time  and  event  time    it  does  not  need  any  special  manual  form  of  watermark  generation  and  events  are  typically  not  too  much  out  or  order  when  they  arrive  at  operators  in  fact  out  of  orderness  can  only  be  introduced  by  streaming  shuffles  or  split  join  union  operations    the  fact  that  elements  are  not  very  much  out  of  order  means  that  the  latency  increase  is  moderate  compared  to  event  time    ingestion  time    event  time  means  that  the  time  of  each  individual  element  in  the  stream  also  called  event  is  determined  by  the  event  s  individual  custom  timestamp    these  timestamps  either  exist  in  the  elements  from  before  they  entered  the    flink  streaming  dataflow  or  are  user  assigned  at  the  sources    the  big  implication  of  this  is  that  it  allows  for  elements  to  arrive  in  the  sources  and  in  all  operators  out  of  order  meaning  that  elements  with  earlier  timestamps  may  arrive  after  elements  with  later  timestamps  p    operators  that  window  or  order  data  with  respect  to  event  time  must  buffer  data  until  they  can  be  sure  that  all  timestamps  for  a  certain  time  interval  have  been  received    this  is  handled  by  the  so  called  time  watermarks  p    operations  based  on  event  time  are  very  predictable  the  result  of  windowing  operations  is  typically  identical  no  matter  when  the  window  is  executed  and  how  fast  the  streams  operate    at  the  same  time  the  buffering  and  tracking  of  event  time  is  also  costlier  than  operating  with  processing  time  and  typically  also  introduces  more  latency    the  amount  of  extra  cost  depends  mostly  on  how  much  out  of  order  the  elements  arrive  i  e  how  long  the  time  span  between  the  arrival  of  early  and  late  elements  is    with  respect  to  the  time  watermarks  this  means  that  the  cost  typically  depends  on  how  early  or  late  the  watermarks  can  be  generated  for  their  timestamp  p    in  relation  to  link    ingestion  time  the  event  time  is  similar  but  refers  the  the  event  s  original  time  rather  than  the  time  assigned  at  the  data  source    practically  that  means  that  event  time  has  generally  more  meaning  but  also  that  it  takes  longer  to  determine  that  all  elements  for  a  certain  time  have  arrived    event  time  
public  evolving  public  interface    timer  service    error  string  for  link    unsupported  operation  exception  on  registering  timers    string  unsupported  register  timer  msg    setting  timers  is  only  supported  on  a  keyed  streams    error  string  for  link    unsupported  operation  exception  on  deleting  timers    string  unsupported  delete  timer  msg    deleting  timers  is  only  supported  on  a  keyed  streams    returns  the  current  processing  time  long  current  processing  time    returns  the  current  event  time  watermark  long  current  watermark    registers  a  timer  to  be  fired  when  processing  time  passes  the  given  time  p    timers  can  internally  be  scoped  to  keys  and  or  windows    when  you  set  a  timer  in  a  keyed  context  such  as  in  an  operation  on  link  org  apache  flink  streaming  api  datastream    keyed  stream  then  that  context  will  also  be  active  when  you  receive  the  timer  notification  void  register  processing  time  timer  long  time    registers  a  timer  to  be  fired  when  the  event  time  watermark  passes  the  given  time  p    timers  can  internally  be  scoped  to  keys  and  or  windows    when  you  set  a  timer  in  a  keyed  context  such  as  in  an  operation  on  link  org  apache  flink  streaming  api  datastream    keyed  stream  then  that  context  will  also  be  active  when  you  receive  the  timer  notification  void  register  event  time  timer  long  time    deletes  the  processing  time  timer  with  the  given  trigger  time    this  method  has  only  an  effect  if  such  a  timer  was  previously  registered  and  did  not  already  expire  p    timers  can  internally  be  scoped  to  keys  and  or  windows    when  you  delete  a  timer  it  is  removed  from  the  current  keyed  context  void  delete  processing  time  timer  long  time    deletes  the  event  time  timer  with  the  given  trigger  time    this  method  has  only  an  effect  if  such  a  timer  was  previously  registered  and  did  not  already  expire  p    timers  can  internally  be  scoped  to  keys  and  or  windows    when  you  delete  a  timer  it  is  removed  from  the  current  keyed  context  void  delete  event  time  timer  long  time  
public  evolving  public  enum    shuffle  mode    producer  and  consumer  are  online  at  the  same  time    produced  data  is  received  by  consumer  immediately  pipelined    the  producer  first  produces  its  entire  result  and  finishes    after  that  the  consumer  is  started  and  may  consume  the  data  batch    the  shuffle  mode  is  undefined    it  leaves  it  up  to  the  framework  to  decide  the  shuffle  mode    the  framework  will  pick  one  of  link    shuffle  mode  batch  or  link    shuffle  mode  pipelined  in  the  end  undefined  
public  evolving  public  final  class    watermark  extends    stream  element    the  watermark  that  signifies  end  of  event  time  public  static  final    watermark  max  watermark  new    watermark    long  max  value    the  timestamp  of  the  watermark  in  milliseconds  private  final  long  timestamp    creates  a  new  watermark  with  the  given  timestamp  in  milliseconds  public    watermark  long  timestamp  this  timestamp  timestamp    returns  the  timestamp  associated  with  this  link    watermark  in  milliseconds  public  long  get  timestamp  return  timestamp    override  public  boolean  equals    object  o  return  this  o  o  null  o  get  class    watermark  class    watermark  o  timestamp  this  timestamp    override  public  int  hash  code  return  int  timestamp  timestamp      override  public    string  to  string  return    watermark  timestamp  
public  evolving  public  class    dynamic  event  time  session  windows  t  extends    merging  window  assigner  t    time  window  private  static  final  long  serial  version  u  i  d  1  l  protected    session  window  time  gap  extractor  t  session  window  time  gap  extractor  protected    dynamic  event  time  session  windows    session  window  time  gap  extractor  t  session  window  time  gap  extractor  this  session  window  time  gap  extractor  session  window  time  gap  extractor    override  public    collection    time  window  assign  windows  t  element  long  timestamp    window  assigner  context  context  long  session  timeout  session  window  time  gap  extractor  extract  element  if  session  timeout    throw  new    illegal  argument  exception    dynamic  session  time  gap  must  satisfy    gap  return    collections  singleton  list  new    time  window  timestamp  timestamp  session  timeout    suppress  warnings  unchecked    override  public    trigger  t    time  window  get  default  trigger    stream  execution  environment  env  return    trigger  t    time  window    event  time  trigger  create    override  public    string  to  string  return    dynamic  event  time  session  windows    creates  a  new  code    session  windows  link    window  assigner  that  assigns  elements  to  sessions  based  on  the  element  timestamp  param  session  window  time  gap  extractor    the  extractor  to  use  to  extract  the  time  gap  from  the  input  elements  return    the  policy  public  static  t    dynamic  event  time  session  windows  t  with  dynamic  gap    session  window  time  gap  extractor  t  session  window  time  gap  extractor  return  new    dynamic  event  time  session  windows  session  window  time  gap  extractor    override  public    type  serializer    time  window  get  window  serializer    execution  config  execution  config  return  new    time  window    serializer    override  public  boolean  is  event  time  return  true    merge  overlapping  link    time  window  s    override  public  void  merge  windows    collection    time  window  windows    merge  callback    time  window  c    time  window  merge  windows  windows  c  
public  evolving  public  class    dynamic  processing  time  session  windows  t  extends    merging  window  assigner  t    time  window  private  static  final  long  serial  version  u  i  d  1  l  protected    session  window  time  gap  extractor  t  session  window  time  gap  extractor  protected    dynamic  processing  time  session  windows    session  window  time  gap  extractor  t  session  window  time  gap  extractor  this  session  window  time  gap  extractor  session  window  time  gap  extractor    override  public    collection    time  window  assign  windows  t  element  long  timestamp    window  assigner  context  context  long  current  processing  time  context  get  current  processing  time  long  session  timeout  session  window  time  gap  extractor  extract  element  if  session  timeout    throw  new    illegal  argument  exception    dynamic  session  time  gap  must  satisfy    gap  return    collections  singleton  list  new    time  window  current  processing  time  current  processing  time  session  timeout    suppress  warnings  unchecked    override  public    trigger  t    time  window  get  default  trigger    stream  execution  environment  env  return    trigger  t    time  window    processing  time  trigger  create    override  public    string  to  string  return    dynamic  processing  time  session  windows    creates  a  new  code    session  windows  link    window  assigner  that  assigns  elements  to  sessions  based  on  the  element  timestamp  param  session  window  time  gap  extractor    the  extractor  to  use  to  extract  the  time  gap  from  the  input  elements  return    the  policy  public  static  t    dynamic  processing  time  session  windows  t  with  dynamic  gap    session  window  time  gap  extractor  t  session  window  time  gap  extractor  return  new    dynamic  processing  time  session  windows  session  window  time  gap  extractor    override  public    type  serializer    time  window  get  window  serializer    execution  config  execution  config  return  new    time  window    serializer    override  public  boolean  is  event  time  return  false    merge  overlapping  link    time  window  s    override  public  void  merge  windows    collection    time  window  windows    merge  callback    time  window  c    time  window  merge  windows  windows  c  
public  evolving  public  static  t    dynamic  event  time  session  windows  t  with  dynamic  gap    session  window  time  gap  extractor  t  session  window  time  gap  extractor  return  new    dynamic  event  time  session  windows  session  window  time  gap  extractor  
public  evolving  public  class    global  windows  extends    window  assigner    object    global  window  private  static  final  long  serial  version  u  i  d  1  l  private    global  windows    override  public    collection    global  window  assign  windows    object  element  long  timestamp    window  assigner  context  context  return    collections  singleton  list    global  window  get    override  public    trigger    object    global  window  get  default  trigger    stream  execution  environment  env  return  new    never  trigger    override  public    string  to  string  return    global  windows    creates  a  new  code    global  windows  link    window  assigner  that  assigns  all  elements  to  the  same  link    global  window  return    the  global  window  policy  public  static    global  windows  create  return  new    global  windows  a  trigger  that  never  fires  as  default    trigger  for    global  windows    internal  public  static  class    never  trigger  extends    trigger    object    global  window  private  static  final  long  serial  version  u  i  d  1  l    override  public    trigger  result  on  element    object  element  long  timestamp    global  window  window    trigger  context  ctx  return    trigger  result  continue    override  public    trigger  result  on  event  time  long  time    global  window  window    trigger  context  ctx  return    trigger  result  continue    override  public    trigger  result  on  processing  time  long  time    global  window  window    trigger  context  ctx  return    trigger  result  continue    override  public  void  clear    global  window  window    trigger  context  ctx  throws    exception    override  public  void  on  merge    global  window  window    on  merge  context  ctx    override  public    type  serializer    global  window  get  window  serializer    execution  config  execution  config  return  new    global  window    serializer    override  public  boolean  is  event  time  return  false  
public  evolving  public  abstract  class    merging  window  assigner  t  w  extends    window  extends    window  assigner  t  w  private  static  final  long  serial  version  u  i  d  1  l    determines  which  windows  if  any  should  be  merged  param  windows    the  window  candidates  param  callback  a  callback  that  can  be  invoked  to  signal  which  windows  should  be  merged  public  abstract  void  merge  windows    collection  w  windows    merge  callback  w  callback    callback  to  be  used  in  link  merge  windows    collection    merge  callback  for  specifying  which  windows  should  be  merged  public  interface    merge  callback  w    specifies  that  the  given  windows  should  be  merged  into  the  result  window  param  to  be  merged    the  list  of  windows  that  should  be  merged  into  one  window  param  merge  result    the  resulting  merged  window  void  merge    collection  w  to  be  merged  w  merge  result  
public  evolving  public  static  t    dynamic  processing  time  session  windows  t  with  dynamic  gap    session  window  time  gap  extractor  t  session  window  time  gap  extractor  return  new    dynamic  processing  time  session  windows  session  window  time  gap  extractor  
public  evolving  public  interface    session  window  time  gap  extractor  t  extends    serializable    extracts  the  session  time  gap  param  element    the  input  element  return    the  session  time  gap  in  milliseconds  long  extract  t  element  
public  evolving  public  class    sliding  event  time  windows  extends    window  assigner    object    time  window  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  size  private  final  long  slide  private  final  long  offset  protected    sliding  event  time  windows  long  size  long  slide  long  offset  if    math  abs  offset  slide  size    throw  new    illegal  argument  exception    sliding  event  time  windows  parameters  must  satisfy  abs  offset  slide  and  size    this  size  size  this  slide  slide  this  offset  offset    override  public    collection    time  window  assign  windows    object  element  long  timestamp    window  assigner  context  context  if  timestamp    long  min  value    list    time  window  windows  new    array  list  int  size  slide  long  last  start    time  window  get  window  start  with  offset  timestamp  offset  slide  for  long  start  last  start  start  timestamp  size  start  slide  windows  add  new    time  window  start  start  size  return  windows  else  throw  new    runtime  exception    record  has    long  min  value  timestamp  no  timestamp  marker    is  the  time  characteristic  set  to    processing  time  or  did  you  forget  to  call    data  stream  assign  timestamps  and  watermarks  public  long  get  size  return  size  public  long  get  slide  return  slide    override  public    trigger    object    time  window  get  default  trigger    stream  execution  environment  env  return    event  time  trigger  create    override  public    string  to  string  return    sliding  event  time  windows  size  slide    creates  a  new  code    sliding  event  time  windows  link    window  assigner  that  assigns  elements  to  sliding  time  windows  based  on  the  element  timestamp  param  size    the  size  of  the  generated  windows  param  slide    the  slide  interval  of  the  generated  windows  return    the  time  policy  public  static    sliding  event  time  windows  of    time  size    time  slide  return  new    sliding  event  time  windows  size  to  milliseconds  slide  to  milliseconds      creates  a  new  code    sliding  event  time  windows  link    window  assigner  that  assigns  elements  to  time  windows  based  on  the  element  timestamp  and  offset  p    for  example  if  you  want  window  a  stream  by  hour  but  window  begins  at  the  15th  minutes  of  each  hour  you  can  use  code  of    time  hours      time  minutes    then  you  will  get  time  windows  start  at  0:15  00  1  15:00  2:15    etc  p    rather  than  that  if  you  are  living  in  somewhere  which  is  not  using  utc  00:00  time  such  as    china  which  is  using  utc  08:00  and  you  want  a  time  window  with  size  of  one  day  and  window  begins  at  every  00:00    of  local  time  you  may  use  code  of    time  days      time  hours      the  parameter  of  offset  is  code    time  hours    since  utc  08:00  is    hours  earlier  than  utc  time  param  size    the  size  of  the  generated  windows  param  slide    the  slide  interval  of  the  generated  windows  param  offset    the  offset  which  window  start  would  be  shifted  by  return    the  time  policy  public  static    sliding  event  time  windows  of    time  size    time  slide    time  offset  return  new    sliding  event  time  windows  size  to  milliseconds  slide  to  milliseconds  offset  to  milliseconds    override  public    type  serializer    time  window  get  window  serializer    execution  config  execution  config  return  new    time  window    serializer    override  public  boolean  is  event  time  return  true  
public  evolving    deprecated  public  class    sliding  time  windows  extends    sliding  event  time  windows  private  static  final  long  serial  version  u  i  d  1  l  private    sliding  time  windows  long  size  long  slide  super  size  slide      creates  a  new  code    sliding  time  windows  link    window  assigner  that  assigns  elements  to  sliding  time  windows  based  on  the  element  timestamp  deprecated    please  use  link    sliding  event  time  windows  of    time    time  param  size    the  size  of  the  generated  windows  param  slide    the  slide  interval  of  the  generated  windows  return    the  time  policy    deprecated  public  static    sliding  time  windows  of    time  size    time  slide  return  new    sliding  time  windows  size  to  milliseconds  slide  to  milliseconds  
public  evolving  public  class    tumbling  event  time  windows  extends    window  assigner    object    time  window  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  size  private  final  long  offset  protected    tumbling  event  time  windows  long  size  long  offset  if    math  abs  offset  size  throw  new    illegal  argument  exception    tumbling  event  time  windows  parameters  must  satisfy  abs  offset  size  this  size  size  this  offset  offset    override  public    collection    time  window  assign  windows    object  element  long  timestamp    window  assigner  context  context  if  timestamp    long  min  value    long  min  value  is  currently  assigned  when  no  timestamp  is  present  long  start    time  window  get  window  start  with  offset  timestamp  offset  size  return    collections  singleton  list  new    time  window  start  start  size  else  throw  new    runtime  exception    record  has    long  min  value  timestamp  no  timestamp  marker    is  the  time  characteristic  set  to    processing  time  or  did  you  forget  to  call    data  stream  assign  timestamps  and  watermarks    override  public    trigger    object    time  window  get  default  trigger    stream  execution  environment  env  return    event  time  trigger  create    override  public    string  to  string  return    tumbling  event  time  windows  size    creates  a  new  code    tumbling  event  time  windows  link    window  assigner  that  assigns  elements  to  time  windows  based  on  the  element  timestamp  param  size    the  size  of  the  generated  windows  return    the  time  policy  public  static    tumbling  event  time  windows  of    time  size  return  new    tumbling  event  time  windows  size  to  milliseconds      creates  a  new  code    tumbling  event  time  windows  link    window  assigner  that  assigns  elements  to  time  windows  based  on  the  element  timestamp  and  offset  p    for  example  if  you  want  window  a  stream  by  hour  but  window  begins  at  the  15th  minutes  of  each  hour  you  can  use  code  of    time  hours      time  minutes    then  you  will  get  time  windows  start  at  0:15  00  1  15:00  2:15    etc  p    rather  than  that  if  you  are  living  in  somewhere  which  is  not  using  utc  00:00  time  such  as    china  which  is  using  utc  08:00  and  you  want  a  time  window  with  size  of  one  day  and  window  begins  at  every  00:00    of  local  time  you  may  use  code  of    time  days      time  hours      the  parameter  of  offset  is  code    time  hours    since  utc  08:00  is    hours  earlier  than  utc  time  param  size    the  size  of  the  generated  windows  param  offset    the  offset  which  window  start  would  be  shifted  by  return    the  time  policy  public  static    tumbling  event  time  windows  of    time  size    time  offset  return  new    tumbling  event  time  windows  size  to  milliseconds  offset  to  milliseconds    override  public    type  serializer    time  window  get  window  serializer    execution  config  execution  config  return  new    time  window    serializer    override  public  boolean  is  event  time  return  true  
public  evolving  public  static    tumbling  processing  time  windows  of    time  size    time  offset    window  stagger  window  stagger  return  new    tumbling  processing  time  windows  size  to  milliseconds  offset  to  milliseconds  window  stagger  
public  evolving    deprecated  public  class    tumbling  time  windows  extends    tumbling  event  time  windows  private  static  final  long  serial  version  u  i  d  1  l  private    tumbling  time  windows  long  size  super  size      creates  a  new  code    tumbling  time  windows  link    window  assigner  that  assigns  elements  to  time  windows  based  on  the  element  timestamp  deprecated    please  use  link    tumbling  event  time  windows  of    time  param  size    the  size  of  the  generated  windows  return    the  time  policy    deprecated  public  static    tumbling  time  windows  of    time  size  return  new    tumbling  time  windows  size  to  milliseconds  
public  evolving  public  abstract  class    window  assigner  t  w  extends    window  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l    returns  a  code    collection  of  windows  that  should  be  assigned  to  the  element  param  element    the  element  to  which  windows  should  be  assigned  param  timestamp    the  timestamp  of  the  element  param  context    the  link    window  assigner  context  in  which  the  assigner  operates  public  abstract    collection  w  assign  windows  t  element  long  timestamp    window  assigner  context  context    returns  the  default  trigger  associated  with  this  code    window  assigner  public  abstract    trigger  t  w  get  default  trigger    stream  execution  environment  env    returns  a  link    type  serializer  for  serializing  windows  that  are  assigned  by  this  code    window  assigner  public  abstract    type  serializer  w  get  window  serializer    execution  config  execution  config    returns  code  true  if  elements  are  assigned  to  windows  based  on  event  time  code  false  otherwise  public  abstract  boolean  is  event  time  a  context  provided  to  the  link    window  assigner  that  allows  it  to  query  the  current  processing  time  p    this  is  provided  to  the  assigner  by  its  containing  link  org  apache  flink  streaming  runtime  operators  windowing    window  operator  which  in  turn  gets  it  from  the  containing  link  org  apache  flink  streaming  runtime  tasks    stream  task  public  abstract  static  class    window  assigner  context    returns  the  current  processing  time  public  abstract  long  get  current  processing  time  
public  evolving  public  enum    window  stagger    default  mode  all  panes  fire  at  the  same  time  across  all  partitions  aligned    override  public  long  get  stagger  offset  final  long  current  processing  time  final  long  size  return  0  l    stagger  offset  is  sampled  from  uniform  distribution  u      window  size  when  first  event  ingested  in  the  partitioned  operator  random    override  public  long  get  stagger  offset  final  long  current  processing  time  final  long  size  return  long    thread  local  random  current  next  double  size    when  the  first  event  is  received  in  the  window  operator  take  the  difference  between  the  start  of  the  window  and  current  procesing  time  as  the  offset    this  way  windows  are  staggered  based  on  when  each  parallel  operator  receives  the  first  event  natural    override  public  long  get  stagger  offset  final  long  current  processing  time  final  long  size  final  long  current  processing  window  start    time  window  get  window  start  with  offset  current  processing  time    size  return    math  max    current  processing  time  current  processing  window  start  public  abstract  long  get  stagger  offset  final  long  current  processing  time  final  long  size  
public  evolving  public  class    count  evictor  w  extends    window  implements    evictor    object  w  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  max  count  private  final  boolean  do  evict  after  private    count  evictor  long  count  boolean  do  evict  after  this  max  count  count  this  do  evict  after  do  evict  after  private    count  evictor  long  count  this  max  count  count  this  do  evict  after  false    override  public  void  evict  before    iterable    timestamped  value    object  elements  int  size  w  window    evictor  context  ctx  if  do  evict  after  evict  elements  size  ctx    override  public  void  evict  after    iterable    timestamped  value    object  elements  int  size  w  window    evictor  context  ctx  if  do  evict  after  evict  elements  size  ctx  private  void  evict    iterable    timestamped  value    object  elements  int  size    evictor  context  ctx  if  size  max  count  return  else  int  evicted  count    for    iterator    timestamped  value    object  iterator  elements  iterator  iterator  has  next  iterator  next  evicted  count  if  evicted  count  size  max  count  break  else  iterator  remove    creates  a  code    count  evictor  that  keeps  the  given  number  of  elements    eviction  is  done  before  the  window  function  param  max  count    the  number  of  elements  to  keep  in  the  pane  public  static  w  extends    window    count  evictor  w  of  long  max  count  return  new    count  evictor  max  count    creates  a  code    count  evictor  that  keeps  the  given  number  of  elements  in  the  pane    eviction  is  done  before  after  the  window  function  based  on  the  value  of  do  evict  after  param  max  count    the  number  of  elements  to  keep  in  the  pane  param  do  evict  after    whether  to  do  eviction  after  the  window  function  public  static  w  extends    window    count  evictor  w  of  long  max  count  boolean  do  evict  after  return  new    count  evictor  max  count  do  evict  after  
public  evolving  public  class    delta  evictor  t  w  extends    window  implements    evictor  t  w  private  static  final  long  serial  version  u  i  d  1  l    delta  function  t  delta  function  private  double  threshold  private  final  boolean  do  evict  after  private    delta  evictor  double  threshold    delta  function  t  delta  function  this  delta  function  delta  function  this  threshold  threshold  this  do  evict  after  false  private    delta  evictor  double  threshold    delta  function  t  delta  function  boolean  do  evict  after  this  delta  function  delta  function  this  threshold  threshold  this  do  evict  after  do  evict  after    override  public  void  evict  before    iterable    timestamped  value  t  elements  int  size  w  window    evictor  context  ctx  if  do  evict  after  evict  elements  size  ctx    override  public  void  evict  after    iterable    timestamped  value  t  elements  int  size  w  window    evictor  context  ctx  if  do  evict  after  evict  elements  size  ctx  private  void  evict    iterable    timestamped  value  t  elements  int  size    evictor  context  ctx    timestamped  value  t  last  element    iterables  get  last  elements  for    iterator    timestamped  value  t  iterator  elements  iterator  iterator  has  next    timestamped  value  t  element  iterator  next  if  delta  function  get  delta  element  get  value  last  element  get  value  this  threshold  iterator  remove    override  public    string  to  string  return    delta  evictor  delta  function  threshold    creates  a  code    delta  evictor  from  the  given  threshold  and  code    delta  function    eviction  is  done  before  the  window  function  param  threshold    the  threshold  param  delta  function    the  code    delta  function  public  static  t  w  extends    window    delta  evictor  t  w  of  double  threshold    delta  function  t  delta  function  return  new    delta  evictor  threshold  delta  function    creates  a  code    delta  evictor  from  the  given  threshold  code    delta  function    eviction  is  done  before  after  the  window  function  based  on  the  value  of  do  evict  after  param  threshold    the  threshold  param  delta  function    the  code    delta  function  param  do  evict  after    whether  eviction  should  be  done  after  window  function  public  static  t  w  extends    window    delta  evictor  t  w  of  double  threshold    delta  function  t  delta  function  boolean  do  evict  after  return  new    delta  evictor  threshold  delta  function  do  evict  after  
public  evolving  public  interface    evictor  t  w  extends    window  extends    serializable    optionally  evicts  elements    called  before  windowing  function  param  elements    the  elements  currently  in  the  pane  param  size    the  current  number  of  elements  in  the  pane  param  window    the  link    window  param  evictor  context    the  context  for  the    evictor  void  evict  before    iterable    timestamped  value  t  elements  int  size  w  window    evictor  context  evictor  context    optionally  evicts  elements    called  after  windowing  function  param  elements    the  elements  currently  in  the  pane  param  size    the  current  number  of  elements  in  the  pane  param  window    the  link    window  param  evictor  context    the  context  for  the    evictor  void  evict  after    iterable    timestamped  value  t  elements  int  size  w  window    evictor  context  evictor  context  a  context  object  that  is  given  to  link    evictor  methods  interface    evictor  context    returns  the  current  processing  time  long  get  current  processing  time    returns  the  metric  group  for  this  link    evictor    this  is  the  same  metric  group  that  would  be  returned  from  link    runtime  context  get  metric  group  in  a  user  function  p    you  must  not  call  methods  that  create  metric  objects  such  as  link    metric  group  counter  int  multiple  times  but  instead  call  once  and  store  the  metric  object  in  a  field    metric  group  get  metric  group    returns  the  current  watermark  time  long  get  current  watermark  
public  evolving  public  class    time  evictor  w  extends    window  implements    evictor    object  w  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  window  size  private  final  boolean  do  evict  after  public    time  evictor  long  window  size  this  window  size  window  size  this  do  evict  after  false  public    time  evictor  long  window  size  boolean  do  evict  after  this  window  size  window  size  this  do  evict  after  do  evict  after    override  public  void  evict  before    iterable    timestamped  value    object  elements  int  size  w  window    evictor  context  ctx  if  do  evict  after  evict  elements  size  ctx    override  public  void  evict  after    iterable    timestamped  value    object  elements  int  size  w  window    evictor  context  ctx  if  do  evict  after  evict  elements  size  ctx  private  void  evict    iterable    timestamped  value    object  elements  int  size    evictor  context  ctx  if  has  timestamp  elements  return  long  current  time  get  max  timestamp  elements  long  evict  cutoff  current  time  window  size  for    iterator    timestamped  value    object  iterator  elements  iterator  iterator  has  next    timestamped  value    object  record  iterator  next  if  record  get  timestamp  evict  cutoff  iterator  remove    returns  true  if  the  first  element  in  the    iterable  of  link    timestamped  value  has  a  timestamp  private  boolean  has  timestamp    iterable    timestamped  value    object  elements    iterator    timestamped  value    object  it  elements  iterator  if  it  has  next  return  it  next  has  timestamp  return  false  param  elements    the  elements  currently  in  the  pane  return    the  maximum  value  of  timestamp  among  the  elements  private  long  get  max  timestamp    iterable    timestamped  value    object  elements  long  current  time    long  min  value  for    iterator    timestamped  value    object  iterator  elements  iterator  iterator  has  next    timestamped  value    object  record  iterator  next  current  time    math  max  current  time  record  get  timestamp  return  current  time    override  public    string  to  string  return    time  evictor  window  size    visible  for  testing  public  long  get  window  size  return  window  size    creates  a  code    time  evictor  that  keeps  the  given  number  of  elements    eviction  is  done  before  the  window  function  param  window  size    the  amount  of  time  for  which  to  keep  elements  public  static  w  extends    window    time  evictor  w  of    time  window  size  return  new    time  evictor  window  size  to  milliseconds    creates  a  code    time  evictor  that  keeps  the  given  number  of  elements    eviction  is  done  before  after  the  window  function  based  on  the  value  of  do  evict  after  param  window  size    the  amount  of  time  for  which  to  keep  elements  param  do  evict  after    whether  eviction  is  done  after  window  function  public  static  w  extends    window    time  evictor  w  of    time  window  size  boolean  do  evict  after  return  new    time  evictor  window  size  to  milliseconds  do  evict  after  
public  evolving  public  class    continuous  event  time  trigger  w  extends    window  extends    trigger    object  w  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  interval    when  merging  we  take  the  lowest  of  all  fire  timestamps  as  the  new  fire  timestamp  private  final    reducing  state  descriptor    long  state  desc  new    reducing  state  descriptor  fire  time  new    min    long  serializer  instance  private    continuous  event  time  trigger  long  interval  this  interval  interval    override  public    trigger  result  on  element    object  element  long  timestamp  w  window    trigger  context  ctx  throws    exception  if  window  max  timestamp  ctx  get  current  watermark  if  the  watermark  is  already  past  the  window  fire  immediately  return    trigger  result  fire  else  ctx  register  event  time  timer  window  max  timestamp    reducing  state    long  fire  timestamp  ctx  get  partitioned  state  state  desc  if  fire  timestamp  get  null  long  start  timestamp  timestamp  interval  long  next  fire  timestamp  start  interval  ctx  register  event  time  timer  next  fire  timestamp  fire  timestamp  add  next  fire  timestamp  return    trigger  result  continue    override  public    trigger  result  on  event  time  long  time  w  window    trigger  context  ctx  throws    exception  if  time  window  max  timestamp  return    trigger  result  fire    reducing  state    long  fire  timestamp  state  ctx  get  partitioned  state  state  desc    long  fire  timestamp  fire  timestamp  state  get  if  fire  timestamp  null  fire  timestamp  time  fire  timestamp  state  clear  fire  timestamp  state  add  time  interval  ctx  register  event  time  timer  time  interval  return    trigger  result  fire  return    trigger  result  continue    override  public    trigger  result  on  processing  time  long  time  w  window    trigger  context  ctx  throws    exception  return    trigger  result  continue    override  public  void  clear  w  window    trigger  context  ctx  throws    exception    reducing  state    long  fire  timestamp  ctx  get  partitioned  state  state  desc    long  timestamp  fire  timestamp  get  if  timestamp  null  ctx  delete  event  time  timer  timestamp  fire  timestamp  clear    override  public  boolean  can  merge  return  true    override  public  void  on  merge  w  window    on  merge  context  ctx  throws    exception  ctx  merge  partitioned  state  state  desc    long  next  fire  timestamp  ctx  get  partitioned  state  state  desc  get  if  next  fire  timestamp  null  ctx  register  event  time  timer  next  fire  timestamp    override  public    string  to  string  return    continuous  event  time  trigger  interval    visible  for  testing  public  long  get  interval  return  interval    creates  a  trigger  that  continuously  fires  based  on  the  given  interval  param  interval    the  time  interval  at  which  to  fire  param  w    the  type  of  link    window    windows  on  which  this  trigger  can  operate  public  static  w  extends    window    continuous  event  time  trigger  w  of    time  interval  return  new    continuous  event  time  trigger  interval  to  milliseconds  private  static  class    min  implements    reduce  function    long  private  static  final  long  serial  version  u  i  d  1  l    override  public    long  reduce    long  value1    long  value2  throws    exception  return    math  min  value1  value2  
public  evolving  public  class    continuous  processing  time  trigger  w  extends    window  extends    trigger    object  w  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  interval    when  merging  we  take  the  lowest  of  all  fire  timestamps  as  the  new  fire  timestamp  private  final    reducing  state  descriptor    long  state  desc  new    reducing  state  descriptor  fire  time  new    min    long  serializer  instance  private    continuous  processing  time  trigger  long  interval  this  interval  interval    override  public    trigger  result  on  element    object  element  long  timestamp  w  window    trigger  context  ctx  throws    exception    reducing  state    long  fire  timestamp  ctx  get  partitioned  state  state  desc  timestamp  ctx  get  current  processing  time  if  fire  timestamp  get  null  long  start  timestamp  timestamp  interval  long  next  fire  timestamp  start  interval  ctx  register  processing  time  timer  next  fire  timestamp  fire  timestamp  add  next  fire  timestamp  return    trigger  result  continue  return    trigger  result  continue    override  public    trigger  result  on  event  time  long  time  w  window    trigger  context  ctx  throws    exception  return    trigger  result  continue    override  public    trigger  result  on  processing  time  long  time  w  window    trigger  context  ctx  throws    exception    reducing  state    long  fire  timestamp  ctx  get  partitioned  state  state  desc  if  fire  timestamp  get  equals  time  fire  timestamp  clear  fire  timestamp  add  time  interval  ctx  register  processing  time  timer  time  interval  return    trigger  result  fire  return    trigger  result  continue    override  public  void  clear  w  window    trigger  context  ctx  throws    exception    reducing  state    long  fire  timestamp  ctx  get  partitioned  state  state  desc  long  timestamp  fire  timestamp  get  ctx  delete  processing  time  timer  timestamp  fire  timestamp  clear    override  public  boolean  can  merge  return  true    override  public  void  on  merge  w  window    on  merge  context  ctx  ctx  merge  partitioned  state  state  desc    visible  for  testing  public  long  get  interval  return  interval    override  public    string  to  string  return    continuous  processing  time  trigger  interval    creates  a  trigger  that  continuously  fires  based  on  the  given  interval  param  interval    the  time  interval  at  which  to  fire  param  w    the  type  of  link    window    windows  on  which  this  trigger  can  operate  public  static  w  extends    window    continuous  processing  time  trigger  w  of    time  interval  return  new    continuous  processing  time  trigger  interval  to  milliseconds  private  static  class    min  implements    reduce  function    long  private  static  final  long  serial  version  u  i  d  1  l    override  public    long  reduce    long  value1    long  value2  throws    exception  return    math  min  value1  value2  
public  evolving  public  class    count  trigger  w  extends    window  extends    trigger    object  w  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  max  count  private  final    reducing  state  descriptor    long  state  desc  new    reducing  state  descriptor  count  new    sum    long  serializer  instance  private    count  trigger  long  max  count  this  max  count  max  count    override  public    trigger  result  on  element    object  element  long  timestamp  w  window    trigger  context  ctx  throws    exception    reducing  state    long  count  ctx  get  partitioned  state  state  desc  count  add  1  l  if  count  get  max  count  count  clear  return    trigger  result  fire  return    trigger  result  continue    override  public    trigger  result  on  event  time  long  time  w  window    trigger  context  ctx  return    trigger  result  continue    override  public    trigger  result  on  processing  time  long  time  w  window    trigger  context  ctx  throws    exception  return    trigger  result  continue    override  public  void  clear  w  window    trigger  context  ctx  throws    exception  ctx  get  partitioned  state  state  desc  clear    override  public  boolean  can  merge  return  true    override  public  void  on  merge  w  window    on  merge  context  ctx  throws    exception  ctx  merge  partitioned  state  state  desc    override  public    string  to  string  return    count  trigger  max  count    creates  a  trigger  that  fires  once  the  number  of  elements  in  a  pane  reaches  the  given  count  param  max  count    the  count  of  elements  at  which  to  fire  param  w    the  type  of  link    window    windows  on  which  this  trigger  can  operate  public  static  w  extends    window    count  trigger  w  of  long  max  count  return  new    count  trigger  max  count  private  static  class    sum  implements    reduce  function    long  private  static  final  long  serial  version  u  i  d  1  l    override  public    long  reduce    long  value1    long  value2  throws    exception  return  value1  value2  
public  evolving  public  class    delta  trigger  t  w  extends    window  extends    trigger  t  w  private  static  final  long  serial  version  u  i  d  1  l  private  final    delta  function  t  delta  function  private  final  double  threshold  private  final    value  state  descriptor  t  state  desc  private    delta  trigger  double  threshold    delta  function  t  delta  function    type  serializer  t  state  serializer  this  delta  function  delta  function  this  threshold  threshold  state  desc  new    value  state  descriptor  last  element  state  serializer    override  public    trigger  result  on  element  t  element  long  timestamp  w  window    trigger  context  ctx  throws    exception    value  state  t  last  element  state  ctx  get  partitioned  state  state  desc  if  last  element  state  value  null  last  element  state  update  element  return    trigger  result  continue  if  delta  function  get  delta  last  element  state  value  element  this  threshold  last  element  state  update  element  return    trigger  result  fire  return    trigger  result  continue    override  public    trigger  result  on  event  time  long  time  w  window    trigger  context  ctx  return    trigger  result  continue    override  public    trigger  result  on  processing  time  long  time  w  window    trigger  context  ctx  throws    exception  return    trigger  result  continue    override  public  void  clear  w  window    trigger  context  ctx  throws    exception  ctx  get  partitioned  state  state  desc  clear    override  public    string  to  string  return    delta  trigger  delta  function  threshold    creates  a  delta  trigger  from  the  given  threshold  and  code    delta  function  param  threshold    the  threshold  at  which  to  trigger  param  delta  function    the  delta  function  to  use  param  state  serializer    type  serializer  for  the  data  elements  param  t    the  type  of  elements  on  which  this  trigger  can  operate  param  w    the  type  of  link    window    windows  on  which  this  trigger  can  operate  public  static  t  w  extends    window    delta  trigger  t  w  of  double  threshold    delta  function  t  delta  function    type  serializer  t  state  serializer  return  new    delta  trigger  threshold  delta  function  state  serializer  
public  evolving  public  class    event  time  trigger  extends    trigger    object    time  window  private  static  final  long  serial  version  u  i  d  1  l  private    event  time  trigger    override  public    trigger  result  on  element    object  element  long  timestamp    time  window  window    trigger  context  ctx  throws    exception  if  window  max  timestamp  ctx  get  current  watermark  if  the  watermark  is  already  past  the  window  fire  immediately  return    trigger  result  fire  else  ctx  register  event  time  timer  window  max  timestamp  return    trigger  result  continue    override  public    trigger  result  on  event  time  long  time    time  window  window    trigger  context  ctx  return  time  window  max  timestamp    trigger  result  fire    trigger  result  continue    override  public    trigger  result  on  processing  time  long  time    time  window  window    trigger  context  ctx  throws    exception  return    trigger  result  continue    override  public  void  clear    time  window  window    trigger  context  ctx  throws    exception  ctx  delete  event  time  timer  window  max  timestamp    override  public  boolean  can  merge  return  true    override  public  void  on  merge    time  window  window    on  merge  context  ctx  only  register  a  timer  if  the  watermark  is  not  yet  past  the  end  of  the  merged  window  this  is  in  line  with  the  logic  in  on  element    if  the  watermark  is  past  the  end  of  the  window  on  element  will  fire  and  setting  a  timer  here  would  fire  the  window  twice  long  window  max  timestamp  window  max  timestamp  if  window  max  timestamp  ctx  get  current  watermark  ctx  register  event  time  timer  window  max  timestamp    override  public    string  to  string  return    event  time  trigger    creates  an  event  time  trigger  that  fires  once  the  watermark  passes  the  end  of  the  window  p    once  the  trigger  fires  all  elements  are  discarded    elements  that  arrive  late  immediately  trigger  window  evaluation  with  just  this  one  element  public  static    event  time  trigger  create  return  new    event  time  trigger  
public  evolving  public  class    processing  timeout  trigger  t  w  extends    window  extends    trigger  t  w  private  static  final  long  serial  version  u  i  d  1  l  private  final    trigger  t  w  nested  trigger  private  final  long  interval  private  final  boolean  reset  timer  on  new  record  private  final  boolean  should  clear  on  timeout  private  final    value  state  descriptor    long  timeout  state  desc  private    processing  timeout  trigger    trigger  t  w  nested  trigger  long  interval  boolean  reset  timer  on  new  record  boolean  should  clear  on  timeout  this  nested  trigger  nested  trigger  this  interval  interval  this  reset  timer  on  new  record  reset  timer  on  new  record  this  should  clear  on  timeout  should  clear  on  timeout  this  timeout  state  desc  new    value  state  descriptor  timeout    long  serializer  instance    override  public    trigger  result  on  element  t  element  long  timestamp  w  window    trigger  context  ctx  throws    exception    trigger  result  trigger  result  this  nested  trigger  on  element  element  timestamp  window  ctx  if  trigger  result  is  fire  this  clear  window  ctx  return  trigger  result    value  state    long  timeout  state  ctx  get  partitioned  state  this  timeout  state  desc  long  next  fire  timestamp  ctx  get  current  processing  time  this  interval    long  timeout  timestamp  timeout  state  value  if  timeout  timestamp  null  reset  timer  on  new  record  ctx  delete  processing  time  timer  timeout  timestamp  timeout  state  clear  timeout  timestamp  null  if  timeout  timestamp  null  timeout  state  update  next  fire  timestamp  ctx  register  processing  time  timer  next  fire  timestamp  return  trigger  result    override  public    trigger  result  on  processing  time  long  timestamp  w  window    trigger  context  ctx  throws    exception    trigger  result  trigger  result  this  nested  trigger  on  processing  time  timestamp  window  ctx  if  should  clear  on  timeout  this  clear  window  ctx  return  trigger  result  is  purge    trigger  result  fire  and  purge    trigger  result  fire    override  public    trigger  result  on  event  time  long  timestamp  w  window    trigger  context  ctx  throws    exception    trigger  result  trigger  result  this  nested  trigger  on  event  time  timestamp  window  ctx  if  should  clear  on  timeout  this  clear  window  ctx  return  trigger  result  is  purge    trigger  result  fire  and  purge    trigger  result  fire    override  public  void  clear  w  window    trigger  context  ctx  throws    exception    value  state    long  timeout  timestamp  state  ctx  get  partitioned  state  this  timeout  state  desc    long  timeout  timestamp  timeout  timestamp  state  value  if  timeout  timestamp  null  ctx  delete  processing  time  timer  timeout  timestamp  timeout  timestamp  state  clear  this  nested  trigger  clear  window  ctx    override  public    string  to  string  return    timeout  trigger  this  nested  trigger  to  string    creates  a  new  link    processing  timeout  trigger  that  fires  when  the  inner  trigger  is  fired  or  when  the  timeout  timer  fires  p    for  example  code    processing  timeout  trigger  of    count  trigger  of      will  create  a    count  trigger  with  timeout  of    millis    so  if  the  first  record  arrives  at  time  code  t  and  the  second  record  arrives  at  time  code  t    the  trigger  will  fire  when  the  third  record  arrives  or  when  the  time  is  code  t    timeout  param  nested  trigger  the  nested  link    trigger  param  timeout  the  timeout  interval  return  link    processing  timeout  trigger  with  the  above  configuration  public  static  t  w  extends    window    processing  timeout  trigger  t  w  of    trigger  t  w  nested  trigger    duration  timeout  return  new    processing  timeout  trigger  nested  trigger  timeout  to  millis  false  true    creates  a  new  link    processing  timeout  trigger  that  fires  when  the  inner  trigger  is  fired  or  when  the  timeout  timer  fires  p    for  example  code    processing  timeout  trigger  of    count  trigger  of      false  true  will  create  a    count  trigger  with  timeout  of    millis    so  if  the  first  record  arrives  at  time  code  t  and  the  second  record  arrives  at  time  code  t    the  trigger  will  fire  when  the  third  record  arrives  or  when  the  time  is  code  t    timeout  param  nested  trigger  the  nested  link    trigger  param  timeout  the  timeout  interval  param  reset  timer  on  new  record  each  time  a  new  element  arrives  reset  the  timer  and  start  a  new  one  param  should  clear  on  timeout  whether  to  call  link    trigger  clear    window    trigger  context  when  the  processing  time  timer  fires  param  t    the  type  of  the  element  param  w    the  type  of  link    window    windows  on  which  this  trigger  can  operate  return  link    processing  timeout  trigger  with  the  above  configuration  public  static  t  w  extends    window    processing  timeout  trigger  t  w  of    trigger  t  w  nested  trigger    duration  timeout  boolean  reset  timer  on  new  record  boolean  should  clear  on  timeout  return  new    processing  timeout  trigger  nested  trigger  timeout  to  millis  reset  timer  on  new  record  should  clear  on  timeout  
public  evolving  public  class    processing  time  trigger  extends    trigger    object    time  window  private  static  final  long  serial  version  u  i  d  1  l  private    processing  time  trigger    override  public    trigger  result  on  element    object  element  long  timestamp    time  window  window    trigger  context  ctx  ctx  register  processing  time  timer  window  max  timestamp  return    trigger  result  continue    override  public    trigger  result  on  event  time  long  time    time  window  window    trigger  context  ctx  throws    exception  return    trigger  result  continue    override  public    trigger  result  on  processing  time  long  time    time  window  window    trigger  context  ctx  return    trigger  result  fire    override  public  void  clear    time  window  window    trigger  context  ctx  throws    exception  ctx  delete  processing  time  timer  window  max  timestamp    override  public  boolean  can  merge  return  true    override  public  void  on  merge    time  window  window    on  merge  context  ctx  only  register  a  timer  if  the  time  is  not  yet  past  the  end  of  the  merged  window  this  is  in  line  with  the  logic  in  on  element    if  the  time  is  past  the  end  of  the  window  on  element  will  fire  and  setting  a  timer  here  would  fire  the  window  twice  long  window  max  timestamp  window  max  timestamp  if  window  max  timestamp  ctx  get  current  processing  time  ctx  register  processing  time  timer  window  max  timestamp    override  public    string  to  string  return    processing  time  trigger    creates  a  new  trigger  that  fires  once  system  time  passes  the  end  of  the  window  public  static    processing  time  trigger  create  return  new    processing  time  trigger  
public  evolving  public  class    purging  trigger  t  w  extends    window  extends    trigger  t  w  private  static  final  long  serial  version  u  i  d  1  l  private    trigger  t  w  nested  trigger  private    purging  trigger    trigger  t  w  nested  trigger  this  nested  trigger  nested  trigger    override  public    trigger  result  on  element  t  element  long  timestamp  w  window    trigger  context  ctx  throws    exception    trigger  result  trigger  result  nested  trigger  on  element  element  timestamp  window  ctx  return  trigger  result  is  fire    trigger  result  fire  and  purge  trigger  result    override  public    trigger  result  on  event  time  long  time  w  window    trigger  context  ctx  throws    exception    trigger  result  trigger  result  nested  trigger  on  event  time  time  window  ctx  return  trigger  result  is  fire    trigger  result  fire  and  purge  trigger  result    override  public    trigger  result  on  processing  time  long  time  w  window    trigger  context  ctx  throws    exception    trigger  result  trigger  result  nested  trigger  on  processing  time  time  window  ctx  return  trigger  result  is  fire    trigger  result  fire  and  purge  trigger  result    override  public  void  clear  w  window    trigger  context  ctx  throws    exception  nested  trigger  clear  window  ctx    override  public  boolean  can  merge  return  nested  trigger  can  merge    override  public  void  on  merge  w  window    on  merge  context  ctx  throws    exception  nested  trigger  on  merge  window  ctx    override  public    string  to  string  return    purging  trigger  nested  trigger  to  string    creates  a  new  purging  trigger  from  the  given  code    trigger  param  nested  trigger    the  trigger  that  is  wrapped  by  this  purging  trigger  public  static  t  w  extends    window    purging  trigger  t  w  of    trigger  t  w  nested  trigger  return  new    purging  trigger  nested  trigger    visible  for  testing  public    trigger  t  w  get  nested  trigger  return  nested  trigger  
public  evolving  public  abstract  class    trigger  t  w  extends    window  implements    serializable  private  static  final  long  serial  version  u  i  d    l    called  for  every  element  that  gets  added  to  a  pane    the  result  of  this  will  determine  whether  the  pane  is  evaluated  to  emit  results  param  element    the  element  that  arrived  param  timestamp    the  timestamp  of  the  element  that  arrived  param  window    the  window  to  which  the  element  is  being  added  param  ctx  a  context  object  that  can  be  used  to  register  timer  callbacks  public  abstract    trigger  result  on  element  t  element  long  timestamp  w  window    trigger  context  ctx  throws    exception    called  when  a  processing  time  timer  that  was  set  using  the  trigger  context  fires  param  time    the  timestamp  at  which  the  timer  fired  param  window    the  window  for  which  the  timer  fired  param  ctx  a  context  object  that  can  be  used  to  register  timer  callbacks  public  abstract    trigger  result  on  processing  time  long  time  w  window    trigger  context  ctx  throws    exception    called  when  an  event  time  timer  that  was  set  using  the  trigger  context  fires  param  time    the  timestamp  at  which  the  timer  fired  param  window    the  window  for  which  the  timer  fired  param  ctx  a  context  object  that  can  be  used  to  register  timer  callbacks  public  abstract    trigger  result  on  event  time  long  time  w  window    trigger  context  ctx  throws    exception    returns  true  if  this  trigger  supports  merging  of  trigger  state  and  can  therefore  be  used  with  a  link  org  apache  flink  streaming  api  windowing  assigners    merging  window  assigner  p    if  this  returns  code  true  you  must  properly  implement  link  on  merge    window    on  merge  context  public  boolean  can  merge  return  false    called  when  several  windows  have  been  merged  into  one  window  by  the  link  org  apache  flink  streaming  api  windowing  assigners    window  assigner  param  window    the  new  window  that  results  from  the  merge  param  ctx  a  context  object  that  can  be  used  to  register  timer  callbacks  and  access  state  public  void  on  merge  w  window    on  merge  context  ctx  throws    exception  throw  new    unsupported  operation  exception    this  trigger  does  not  support  merging    clears  any  state  that  the  trigger  might  still  hold  for  the  given  window    this  is  called  when  a  window  is  purged    timers  set  using  link    trigger  context  register  event  time  timer  long  and  link    trigger  context  register  processing  time  timer  long  should  be  deleted  here  as  well  as  state  acquired  using  link    trigger  context  get  partitioned  state    state  descriptor  public  abstract  void  clear  w  window    trigger  context  ctx  throws    exception  a  context  object  that  is  given  to  link    trigger  methods  to  allow  them  to  register  timer  callbacks  and  deal  with  state  public  interface    trigger  context    returns  the  current  processing  time  long  get  current  processing  time    returns  the  metric  group  for  this  link    trigger    this  is  the  same  metric  group  that  would  be  returned  from  link    runtime  context  get  metric  group  in  a  user  function  p    you  must  not  call  methods  that  create  metric  objects  such  as  link    metric  group  counter  int  multiple  times  but  instead  call  once  and  store  the  metric  object  in  a  field    metric  group  get  metric  group    returns  the  current  watermark  time  long  get  current  watermark    register  a  system  time  callback    when  the  current  system  time  passes  the  specified  time  link    trigger  on  processing  time  long    window    trigger  context  is  called  with  the  time  specified  here  param  time    the  time  at  which  to  invoke  link    trigger  on  processing  time  long    window    trigger  context  void  register  processing  time  timer  long  time    register  an  event  time  callback    when  the  current  watermark  passes  the  specified  time  link    trigger  on  event  time  long    window    trigger  context  is  called  with  the  time  specified  here  param  time    the  watermark  at  which  to  invoke  link    trigger  on  event  time  long    window    trigger  context  see  org  apache  flink  streaming  api  watermark    watermark  void  register  event  time  timer  long  time    delete  the  processing  time  trigger  for  the  given  time  void  delete  processing  time  timer  long  time    delete  the  event  time  trigger  for  the  given  time  void  delete  event  time  timer  long  time    retrieves  a  link    state  object  that  can  be  used  to  interact  with  fault  tolerant  state  that  is  scoped  to  the  window  and  key  of  the  current  trigger  invocation  param  state  descriptor    the    state  descriptor  that  contains  the  name  and  type  of  the  state  that  is  being  accessed  param  s    the  type  of  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  os  a    keyed  stream  s  extends    state  s  get  partitioned  state    state  descriptor  s  state  descriptor    retrieves  a  link    value  state  object  that  can  be  used  to  interact  with  fault  tolerant  state  that  is  scoped  to  the  window  and  key  of  the  current  trigger  invocation  param  name    the  name  of  the  key  value  state  param  state  type    the  class  of  the  type  that  is  stored  in  the  state    used  to  generate  serializers  for  managed  memory  and  checkpointing  param  default  state    the  default  state  value  returned  when  the  state  is  accessed  and  no  value  has  yet  been  set  for  the  key    may  be  null  param  s    the  type  of  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  os  a    keyed  stream  deprecated    use  link  get  partitioned  state    state  descriptor    deprecated  s  extends    serializable    value  state  s  get  key  value  state    string  name    class  s  state  type  s  default  state    retrieves  a  link    value  state  object  that  can  be  used  to  interact  with  fault  tolerant  state  that  is  scoped  to  the  window  and  key  of  the  current  trigger  invocation  param  name    the  name  of  the  key  value  state  param  state  type    the  type  information  for  the  type  that  is  stored  in  the  state    used  to  create  serializers  for  managed  memory  and  checkpoints  param  default  state    the  default  state  value  returned  when  the  state  is  accessed  and  no  value  has  yet  been  set  for  the  key    may  be  null  param  s    the  type  of  the  state  return    the  partitioned  state  object  throws    unsupported  operation  exception    thrown  if  no  partitioned  state  is  available  for  the  function  function  is  not  part  os  a    keyed  stream  deprecated    use  link  get  partitioned  state    state  descriptor    deprecated  s  extends    serializable    value  state  s  get  key  value  state    string  name    type  information  s  state  type  s  default  state    extension  of  link    trigger  context  that  is  given  to  link    trigger  on  merge    window    on  merge  context  public  interface    on  merge  context  extends    trigger  context  s  extends    merging  state  void  merge  partitioned  state    state  descriptor  s  state  descriptor  
public  evolving  public  class    global  window  extends    window  private  static  final    global  window  instance  new    global  window  private    global  window  public  static    global  window  get  return  instance    override  public  long  max  timestamp  return    long  max  value    override  public  boolean  equals    object  o  return  this  o  o  null  get  class  o  get  class    override  public  int  hash  code  return      override  public    string  to  string  return    global  window  a  link    type  serializer  for  link    global  window  public  static  class    serializer  extends    type  serializer  singleton    global  window  private  static  final  long  serial  version  u  i  d  1  l    override  public  boolean  is  immutable  type  return  true    override  public    global  window  create  instance  return    global  window  instance    override  public    global  window  copy    global  window  from  return  from    override  public    global  window  copy    global  window  from    global  window  reuse  return  from    override  public  int  get  length  return      override  public  void  serialize    global  window  record    data  output  view  target  throws    i  o  exception  target  write  byte      override  public    global  window  deserialize    data  input  view  source  throws    i  o  exception  source  read  byte  return    global  window  instance    override  public    global  window  deserialize    global  window  reuse    data  input  view  source  throws    i  o  exception  source  read  byte  return    global  window  instance    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  source  read  byte  target  write  byte      override  public    type  serializer  snapshot    global  window  snapshot  configuration  return  new    global  window  serializer  snapshot    serializer  configuration  snapshot  for  compatibility  and  format  evolution    suppress  warnings    weaker  access  public  static  final  class    global  window  serializer  snapshot  extends    simple  type  serializer  snapshot    global  window  public    global  window  serializer  snapshot  super    global  window    serializer  new  
public  evolving  public  class    time  window  extends    window  private  final  long  start  private  final  long  end  public    time  window  long  start  long  end  this  start  start  this  end  end    gets  the  starting  timestamp  of  the  window    this  is  the  first  timestamp  that  belongs  to  this  window  return    the  starting  timestamp  of  this  window  public  long  get  start  return  start    gets  the  end  timestamp  of  this  window    the  end  timestamp  is  exclusive  meaning  it  is  the  first  timestamp  that  does  not  belong  to  this  window  any  more  return    the  exclusive  end  timestamp  of  this  window  public  long  get  end  return  end    gets  the  largest  timestamp  that  still  belongs  to  this  window  p    this  timestamp  is  identical  to  code  get  end    return    the  largest  timestamp  that  still  belongs  to  this  window  see  get  end    override  public  long  max  timestamp  return  end      override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    time  window  window    time  window  o  return  end  window  end  start  window  start    override  public  int  hash  code  return    math  utils  long  to  int  with  bit  mixing  start  end    override  public    string  to  string  return    time  window  start  start  end  end    returns  code  true  if  this  window  intersects  the  given  window  or  if  this  window  is  just  after  or  before  the  given  window  public  boolean  intersects    time  window  other  return  this  start  other  end  this  end  other  start    returns  the  minimal  window  covers  both  this  window  and  the  given  window  public    time  window  cover    time  window  other  return  new    time  window    math  min  start  other  start    math  max  end  other  end    serializer    the  serializer  used  to  write  the    time  window  type  public  static  class    serializer  extends    type  serializer  singleton    time  window  private  static  final  long  serial  version  u  i  d  1  l    override  public  boolean  is  immutable  type  return  true    override  public    time  window  create  instance  return  null    override  public    time  window  copy    time  window  from  return  from    override  public    time  window  copy    time  window  from    time  window  reuse  return  from    override  public  int  get  length  return      override  public  void  serialize    time  window  record    data  output  view  target  throws    i  o  exception  target  write  long  record  start  target  write  long  record  end    override  public    time  window  deserialize    data  input  view  source  throws    i  o  exception  long  start  source  read  long  long  end  source  read  long  return  new    time  window  start  end    override  public    time  window  deserialize    time  window  reuse    data  input  view  source  throws    i  o  exception  return  deserialize  source    override  public  void  copy    data  input  view  source    data  output  view  target  throws    i  o  exception  target  write  long  source  read  long  target  write  long  source  read  long    override  public    type  serializer  snapshot    time  window  snapshot  configuration  return  new    time  window  serializer  snapshot    serializer  configuration  snapshot  for  compatibility  and  format  evolution    suppress  warnings    weaker  access  public  static  final  class    time  window  serializer  snapshot  extends    simple  type  serializer  snapshot    time  window  public    time  window  serializer  snapshot  super    serializer  new    utilities    merge  overlapping  link    time  window  s    for  use  by  merging  link  org  apache  flink  streaming  api  windowing  assigners    window  assigner    window  assigners  public  static  void  merge  windows    collection    time  window  windows    merging  window  assigner    merge  callback    time  window  c  sort  the  windows  by  the  start  time  and  then  merge  overlapping  windows    list    time  window  sorted  windows  new    array  list  windows    collections  sort  sorted  windows  new    comparator    time  window    override  public  int  compare    time  window  o1    time  window  o2  return    long  compare  o1  get  start  o2  get  start    list    tuple2    time  window    set    time  window  merged  new    array  list    tuple2    time  window    set    time  window  current  merge  null  for    time  window  candidate  sorted  windows  if  current  merge  null  current  merge  new    tuple2  current  merge  f0  candidate  current  merge  f1  new    hash  set  current  merge  f1  add  candidate  else  if  current  merge  f0  intersects  candidate  current  merge  f0  current  merge  f0  cover  candidate  current  merge  f1  add  candidate  else  merged  add  current  merge  current  merge  new    tuple2  current  merge  f0  candidate  current  merge  f1  new    hash  set  current  merge  f1  add  candidate  if  current  merge  null  merged  add  current  merge  for    tuple2    time  window    set    time  window  m  merged  if  m  f1  size    c  merge  m  f1  m  f0    method  to  get  the  window  start  for  a  timestamp  param  timestamp  epoch  millisecond  to  get  the  window  start  param  offset    the  offset  which  window  start  would  be  shifted  by  param  window  size    the  size  of  the  generated  windows  return  window  start  public  static  long  get  window  start  with  offset  long  timestamp  long  offset  long  window  size  return  timestamp  timestamp  offset  window  size  window  size  
public  evolving  public  abstract  class    window    gets  the  largest  timestamp  that  still  belongs  to  this  window  return    the  largest  timestamp  that  still  belongs  to  this  window  public  abstract  long  max  timestamp  
public  evolving  public  class    timestamped  value  t    the  actual  value  held  by  this  record  private  t  value    the  timestamp  of  the  record  private  long  timestamp    flag  whether  the  timestamp  is  actually  set  private  boolean  has  timestamp    creates  a  new    timestamped  value    the  record  does  not  have  a  timestamp  public    timestamped  value  t  value  this  value  value    creates  a  new    timestamped  value  wrapping  the  given  value    the  timestamp  is  set  to  the  given  timestamp  param  value    the  value  to  wrap  in  this  link    timestamped  value  param  timestamp    the  timestamp  in  milliseconds  public    timestamped  value  t  value  long  timestamp  this  value  value  this  timestamp  timestamp  this  has  timestamp  true  return    the  value  wrapped  in  this  link    timestamped  value  public  t  get  value  return  value  return    the  timestamp  associated  with  this  stream  value  in  milliseconds  public  long  get  timestamp  if  has  timestamp  return  timestamp  else  throw  new    illegal  state  exception    record  has  no  timestamp    is  the  time  characteristic  set  to    processing  time  or  did  you  forget  to  call    data  stream  assign  timestamps  and  watermarks    checks  whether  this  record  has  a  timestamp  return    true  if  the  record  has  a  timestamp  false  if  not  public  boolean  has  timestamp  return  has  timestamp    creates  a  link    stream  record  from  this    timestamped  value  public    stream  record  t  get  stream  record    stream  record  t  stream  record  new    stream  record  value  if  has  timestamp  stream  record  set  timestamp  timestamp  return  stream  record    creates  a    timestamped  value  from  given  link    stream  record  param  stream  record    the    stream  record  object  from  which    timestamped  value  is  to  be  created  public  static  t    timestamped  value  t  from    stream  record  t  stream  record  if  stream  record  has  timestamp  return  new    timestamped  value  stream  record  get  value  stream  record  get  timestamp  else  return  new    timestamped  value  stream  record  get  value  
public  evolving  public  final  class    latency  marker  extends    stream  element    the  time  the  latency  mark  is  denoting  private  final  long  marked  time  private  final    operator  i  d  operator  id  private  final  int  subtask  index    creates  a  latency  mark  with  the  given  timestamp  public    latency  marker  long  marked  time    operator  i  d  operator  id  int  subtask  index  this  marked  time  marked  time  this  operator  id  operator  id  this  subtask  index  subtask  index    returns  the  timestamp  marked  by  the    latency  marker  public  long  get  marked  time  return  marked  time  public    operator  i  d  get  operator  id  return  operator  id  public  int  get  subtask  index  return  subtask  index    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    latency  marker  that    latency  marker  o  if  marked  time  that  marked  time  return  false  if  operator  id  equals  that  operator  id  return  false  return  subtask  index  that  subtask  index    override  public  int  hash  code  int  result  int  marked  time  marked  time    result    result  operator  id  hash  code  result    result  subtask  index  return  result    override  public    string  to  string  return    latency  marker  marked  time  marked  time  operator  id  operator  id  subtask  index  subtask  index  
public  evolving  public  interface    processing  time  service  aware  void  set  processing  time  service    processing  time  service  processing  time  service  
public  evolving    deprecated    suppress  warnings  deprecation  public  class    simple  string  schema  extends  org  apache  flink  api  common  serialization    simple  string  schema  implements    serialization  schema    string    deserialization  schema    string  private  static  final  long  serial  version  u  i  d  1  l  public    simple  string  schema  super    creates  a  new    simple  string  schema  that  uses  the  given  charset  to  convert  between  strings  and  bytes  param  charset    the  charset  to  use  to  convert  between  strings  and  bytes  public    simple  string  schema    charset  charset  super  charset  
public  evolving  public  interface    aggregated  table    performs  a  selection  operation  after  an  aggregate  operation    the  field  expressions  cannot  contain  table  functions  and  aggregations  p    example  pre  code    aggregate  function  agg  func  new    my  aggregate  function  table  env  register  function  agg  func  agg  func  table  group  by  key  aggregate  agg  func  a  b  as  f0  f1  f2  select  key  f0  f1  pre  deprecated  use  link  select    expression    deprecated    table  select    string  fields    performs  a  selection  operation  after  an  aggregate  operation    the  field  expressions  cannot  contain  table  functions  and  aggregations  p    example  pre  code    aggregate  function  agg  func  new    my  aggregate  function  table  env  register  function  agg  func  agg  func  table  group  by  key  aggregate  call  agg  func  a  b  as  f0  f1  f2  select  key  f0  f1  pre  p    scala    example  pre  code  val  agg  func  new    my  aggregate  function  table  group  by  key  aggregate  agg  func  a  b  as  f0  f1  f2  select  key  f0  f1  pre    table  select    expression  fields  
public  evolving  public  class    execution  config  options    source    options    documentation    table  option  exec  mode    documentation    exec  mode  streaming  public  static  final    config  option    string  table  exec  source  idle  timeout  key  table  exec  source  idle  timeout  default  value    ms  with  description    when  a  source  do  not  receive  any  elements  for  the  timeout  time  it  will  be  marked  as  temporarily  idle    this  allows  downstream  tasks  to  advance  their  watermarks  without  the  need  to  wait  for  watermarks  from  this  source  while  it  is  idle    sink    options    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    not  null  enforcer  table  exec  sink  not  null  enforcer  key  table  exec  sink  not  null  enforcer  enum  type    not  null  enforcer  class  default  value    not  null  enforcer  error  with  description    the  not  null  column  constraint  on  a  table  enforces  that  null  values  can  t  be  inserted  into  the  table    flink  supports  error  default  and  drop  enforcement  behavior    by  default    flink  will  check  values  and  throw  runtime  exception  when  null  values  writing  into  not  null  columns    users  can  change  the  behavior  to  drop  to  silently  drop  such  records  without  throwing  exception    sort    options    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    integer  table  exec  sort  default  limit  key  table  exec  sort  default  limit  default  value    with  description    default  limit  when  user  don  t  set  a  limit  after  order  by    indicates  that  this  configuration  is  ignored    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    integer  table  exec  sort  max  num  file  handles  key  table  exec  sort  max  num  file  handles  default  value    with  description    the  maximal  fan  in  for  external  merge  sort    it  limits  the  number  of  file  handles  per  operator    if  it  is  too  small  may  cause  intermediate  merging    but  if  it  is  too  large  it  will  cause  too  many  files  opened  at  the  same  time  consume  memory  and  lead  to  random  reading    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    boolean  table  exec  sort  async  merge  enabled  key  table  exec  sort  async  merge  enabled  default  value  true  with  description    whether  to  asynchronously  merge  sorted  spill  files    spill    options    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    boolean  table  exec  spill  compression  enabled  key  table  exec  spill  compression  enabled  default  value  true  with  description    whether  to  compress  spilled  data    currently  we  only  support  compress  spilled  data  for  sort  and  hash  agg  and  hash  join  operators    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    string  table  exec  spill  compression  block  size  key  table  exec  spill  compression  block  size  default  value    kb  with  description    the  memory  size  used  to  do  compress  when  spilling  data    the  larger  the  memory  the  higher  the  compression  ratio  but  more  memory  resource  will  be  consumed  by  the  job    resource    options    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    integer  table  exec  resource  default  parallelism  key  table  exec  resource  default  parallelism  default  value    with  description    sets  default  parallelism  for  all  operators  such  as  aggregate  join  filter  to  run  with  parallel  instances    this  config  has  a  higher  priority  than  parallelism  of    stream  execution  environment  actually  this  config  overrides  the  parallelism  of    stream  execution  environment  a  value  of    indicates  that  no  default  parallelism  is  set  then  it  will  fallback  to  use  the  parallelism  of    stream  execution  environment    documentation    exclude  from  documentation    beginning  from    flink  1.10  this  is  interpreted  as  a  weight  hint  instead  of  an  absolute  memory  requirement    users  should  not  need  to  change  these  carefully  tuned  weight  hints  public  static  final    config  option    string  table  exec  resource  external  buffer  memory  key  table  exec  resource  external  buffer  memory  default  value    mb  with  description    sets  the  external  buffer  memory  size  that  is  used  in  sort  merge  join  and  nested  join  and  over  window    note  memory  size  is  only  a  weight  hint  it  will  affect  the  weight  of  memory  that  can  be  applied  by  a  single  operator  in  the  task  the  actual  memory  used  depends  on  the  running  environment    documentation    exclude  from  documentation    beginning  from    flink  1.10  this  is  interpreted  as  a  weight  hint  instead  of  an  absolute  memory  requirement    users  should  not  need  to  change  these  carefully  tuned  weight  hints  public  static  final    config  option    string  table  exec  resource  hash  agg  memory  key  table  exec  resource  hash  agg  memory  default  value    mb  with  description    sets  the  managed  memory  size  of  hash  aggregate  operator    note  memory  size  is  only  a  weight  hint  it  will  affect  the  weight  of  memory  that  can  be  applied  by  a  single  operator  in  the  task  the  actual  memory  used  depends  on  the  running  environment    documentation    exclude  from  documentation    beginning  from    flink  1.10  this  is  interpreted  as  a  weight  hint  instead  of  an  absolute  memory  requirement    users  should  not  need  to  change  these  carefully  tuned  weight  hints  public  static  final    config  option    string  table  exec  resource  hash  join  memory  key  table  exec  resource  hash  join  memory  default  value    mb  with  description    sets  the  managed  memory  for  hash  join  operator    it  defines  the  lower  limit    note  memory  size  is  only  a  weight  hint  it  will  affect  the  weight  of  memory  that  can  be  applied  by  a  single  operator  in  the  task  the  actual  memory  used  depends  on  the  running  environment    documentation    exclude  from  documentation    beginning  from    flink  1.10  this  is  interpreted  as  a  weight  hint  instead  of  an  absolute  memory  requirement    users  should  not  need  to  change  these  carefully  tuned  weight  hints  public  static  final    config  option    string  table  exec  resource  sort  memory  key  table  exec  resource  sort  memory  default  value    mb  with  description    sets  the  managed  buffer  memory  size  for  sort  operator    note  memory  size  is  only  a  weight  hint  it  will  affect  the  weight  of  memory  that  can  be  applied  by  a  single  operator  in  the  task  the  actual  memory  used  depends  on  the  running  environment    agg    options    see  code  org  apache  flink  table  runtime  operators  window  grouping    heap  windows  grouping    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    integer  table  exec  window  agg  buffer  size  limit  key  table  exec  window  agg  buffer  size  limit  default  value      with  description    sets  the  window  elements  buffer  size  limit  used  in  group  window  agg  operator    async    lookup    options    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    integer  table  exec  async  lookup  buffer  capacity  key  table  exec  async  lookup  buffer  capacity  default  value    with  description    the  max  number  of  async  i  o  operation  that  the  async  lookup  join  can  trigger    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    string  table  exec  async  lookup  timeout  key  table  exec  async  lookup  timeout  default  value    min  with  description    the  async  timeout  for  the  asynchronous  operation  to  complete    mini  batch    options    documentation    table  option  exec  mode    documentation    exec  mode  streaming  public  static  final    config  option    boolean  table  exec  minibatch  enabled  key  table  exec  mini  batch  enabled  default  value  false  with  description    specifies  whether  to  enable    mini  batch  optimization    mini  batch  is  an  optimization  to  buffer  input  records  to  reduce  state  access    this  is  disabled  by  default    to  enable  this  users  should  set  this  config  to  true  note    if  mini  batch  is  enabled  table  exec  mini  batch  allow  latency  and  table  exec  mini  batch  size  must  be  set    documentation    table  option  exec  mode    documentation    exec  mode  streaming  public  static  final    config  option    string  table  exec  minibatch  allow  latency  key  table  exec  mini  batch  allow  latency  default  value    ms  with  description    the  maximum  latency  can  be  used  for    mini  batch  to  buffer  input  records    mini  batch  is  an  optimization  to  buffer  input  records  to  reduce  state  access    mini  batch  is  triggered  with  the  allowed  latency  interval  and  when  the  maximum  number  of  buffered  records  reached  note    if  table  exec  minibatch  enabled  key  is  set  true  its  value  must  be  greater  than  zero    documentation    table  option  exec  mode    documentation    exec  mode  streaming  public  static  final    config  option    long  table  exec  minibatch  size  key  table  exec  mini  batch  size  default  value  1  l  with  description    the  maximum  number  of  input  records  can  be  buffered  for    mini  batch    mini  batch  is  an  optimization  to  buffer  input  records  to  reduce  state  access    mini  batch  is  triggered  with  the  allowed  latency  interval  and  when  the  maximum  number  of  buffered  records  reached  note    mini  batch  only  works  for  non  windowed  aggregations  currently    if  table  exec  minibatch  enabled  key  is  set  true  its  value  must  be  positive    other    exec    options    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    string  table  exec  disabled  operators  key  table  exec  disabled  operators  no  default  value  with  description    mainly  for  testing  a  comma  separated  list  of  operator  names  each  name  represents  a  kind  of  disabled  operator  n    operators  that  can  be  disabled  include    nested  loop  join    shuffle  hash  join    broadcast  hash  join    sort  merge  join    hash  agg    sort  agg  n    by  default  no  operator  is  disabled    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    string  table  exec  shuffle  mode  key  table  exec  shuffle  mode  string  type  default  value  all  edges  blocking  with  description    description  builder  text    sets  exec  shuffle  mode  linebreak  text    accepted  values  are  list  text  s    all  edges  will  use  blocking  shuffle  code  all  edges  blocking  text  s    forward  edges  will  use  pipelined  shuffle  others  blocking  code  forward  edges  pipelined  text  s    pointwise  edges  will  use  pipelined  shuffle  others  blocking    pointwise  edges  include  forward  and  rescale  edges  code  pointwise  edges  pipelined  text  s    all  edges  will  use  pipelined  shuffle  code  all  edges  pipelined  text  s  the  same  as  s    deprecated  code  batch  code  all  edges  blocking  text  s  the  same  as  s    deprecated  code  pipelined  code  all  edges  pipelined  text    note    blocking  shuffle  means  data  will  be  fully  produced  before  sent  to  consumer  tasks    pipelined  shuffle  means  data  will  be  sent  to  consumer  tasks  once  produced  build    enum  option  types    the  enforcer  to  guarantee  not  null  column  constraint  when  writing  data  into  sink  public  enum    not  null  enforcer    throws  runtime  exception  when  writing  null  values  into  not  null  column  error    drop  records  when  writing  null  values  into  not  null  column  drop  
public  evolving  public  class    optimizer  config  options    optimizer    options    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    string  table  optimizer  agg  phase  strategy  key  table  optimizer  agg  phase  strategy  default  value  auto  with  description    strategy  for  aggregate  phase    only  auto  two  phase  or  one  phase  can  be  set  n  auto    no  special  enforcer  for  aggregate  stage    whether  to  choose  two  stage  aggregate  or  one  stage  aggregate  depends  on  cost  n  two  phase    enforce  to  use  two  stage  aggregate  which  has  local  aggregate  and  global  aggregate    note  that  if  aggregate  call  does  not  support  optimize  into  two  phase  we  will  still  use  one  stage  aggregate  n  one  phase    enforce  to  use  one  stage  aggregate  which  only  has    complete  global  aggregate    documentation    table  option  exec  mode    documentation    exec  mode  batch  public  static  final    config  option    long  table  optimizer  broadcast  join  threshold  key  table  optimizer  join  broadcast  threshold  default  value      l  with  description    configures  the  maximum  size  in  bytes  for  a  table  that  will  be  broadcast  to  all  worker  nodes  when  performing  a  join    by  setting  this  value  to    to  disable  broadcasting    documentation    table  option  exec  mode    documentation    exec  mode  streaming  public  static  final    config  option    boolean  table  optimizer  distinct  agg  split  enabled  key  table  optimizer  distinct  agg  split  enabled  default  value  false  with  description    tells  the  optimizer  whether  to  split  distinct  aggregation  e  g  count  distinct  col  sum  distinct  col  into  two  level    the  first  aggregation  is  shuffled  by  an  additional  key  which  is  calculated  using  the  hashcode  of  distinct  key  and  number  of  buckets    this  optimization  is  very  useful  when  there  is  data  skew  in  distinct  aggregation  and  gives  the  ability  to  scale  up  the  job    default  is  false    documentation    table  option  exec  mode    documentation    exec  mode  streaming  public  static  final    config  option    integer  table  optimizer  distinct  agg  split  bucket  num  key  table  optimizer  distinct  agg  split  bucket  num  default  value    with  description    configure  the  number  of  buckets  when  splitting  distinct  aggregation    the  number  is  used  in  the  first  level  aggregation  to  calculate  a  bucket  key  hash  code  distinct  key  bucket  num  which  is  used  as  an  additional  group  key  after  splitting    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    boolean  table  optimizer  reuse  sub  plan  enabled  key  table  optimizer  reuse  sub  plan  enabled  default  value  true  with  description    when  it  is  true  the  optimizer  will  try  to  find  out  duplicated  sub  plans  and  reuse  them    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    boolean  table  optimizer  reuse  source  enabled  key  table  optimizer  reuse  source  enabled  default  value  true  with  description    when  it  is  true  the  optimizer  will  try  to  find  out  duplicated  table  sources  and  reuse  them    this  works  only  when  table  optimizer  reuse  sub  plan  enabled  key  is  true    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    boolean  table  optimizer  source  predicate  pushdown  enabled  key  table  optimizer  source  predicate  pushdown  enabled  default  value  true  with  description    when  it  is  true  the  optimizer  will  push  down  predicates  into  the    filterable  table  source    default  value  is  true    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    boolean  table  optimizer  join  reorder  enabled  key  table  optimizer  join  reorder  enabled  default  value  false  with  description    enables  join  reorder  in  optimizer    default  is  disabled  
public  evolving  public  class    table  config  options  private    table  config  options    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    boolean  table  dynamic  table  options  enabled  key  table  dynamic  table  options  enabled  boolean  type  default  value  false  with  description    enable  or  disable  the  options  hint  used  to  specify  table  options  dynamically  if  disabled  an  exception  would  be  thrown  if  any  options  hint  is  specified    documentation    table  option  exec  mode    documentation    exec  mode  batch  streaming  public  static  final    config  option    string  table  sql  dialect  key  table  sql  dialect  string  type  default  value    sql  dialect  default  name  to  lower  case  with  description    the  sql  dialect  defines  how  to  parse  a  sql  query  a  different  sql  dialect  may  support  different  sql  grammar    currently  supported  dialects  are  default  and  hive  
public  evolving  public  class    environment  settings  public  static  final    string  streaming  mode  streaming  mode  public  static  final    string  class  name  class  name  public  static  final    string  default  builtin  catalog  default  catalog  public  static  final    string  default  builtin  database  default  database    canonical  name  of  the  link    planner  class  to  use  private  final    string  planner  class    canonical  name  of  the  link    executor  class  to  use  private  final    string  executor  class    specifies  the  name  of  the  initial  catalog  to  be  created  when  instantiating  link    table  environment  private  final    string  built  in  catalog  name    specifies  the  name  of  the  default  database  in  the  initial  catalog  to  be  created  when  instantiating  link    table  environment  private  final    string  built  in  database  name    determines  if  the  table  environment  should  work  in  a  batch  code  false  or  streaming  code  true  mode  private  final  boolean  is  streaming  mode  private    environment  settings    nullable    string  planner  class    nullable    string  executor  class    string  built  in  catalog  name    string  built  in  database  name  boolean  is  streaming  mode  this  planner  class  planner  class  this  executor  class  executor  class  this  built  in  catalog  name  built  in  catalog  name  this  built  in  database  name  built  in  database  name  this  is  streaming  mode  is  streaming  mode    creates  a  builder  for  creating  an  instance  of  link    environment  settings  p    by  default  it  does  not  specify  a  required  planner  and  will  use  the  one  that  is  available  on  the  classpath  via  discovery  public  static    builder  new  instance  return  new    builder    gets  the  specified  name  of  the  initial  catalog  to  be  created  when  instantiating  a  link    table  environment  public    string  get  built  in  catalog  name  return  built  in  catalog  name    gets  the  specified  name  of  the  default  database  in  the  initial  catalog  to  be  created  when  instantiating  a  link    table  environment  public    string  get  built  in  database  name  return  built  in  database  name    tells  if  the  link    table  environment  should  work  in  a  batch  or  streaming  mode  public  boolean  is  streaming  mode  return  is  streaming  mode    internal  public    map    string    string  to  planner  properties    map    string    string  properties  new    hash  map  to  common  properties  if  planner  class  null  properties  put  class  name  planner  class  return  properties    internal  public    map    string    string  to  executor  properties    map    string    string  properties  new    hash  map  to  common  properties  if  executor  class  null  properties  put  class  name  executor  class  return  properties  private    map    string    string  to  common  properties    map    string    string  properties  new    hash  map  properties  put  streaming  mode    boolean  to  string  is  streaming  mode  return  properties  a  builder  for  link    environment  settings  public  static  class    builder  private  static  final    string  old  planner  factory  org  apache  flink  table  planner    stream  planner  factory  private  static  final    string  old  executor  factory  org  apache  flink  table  executor    stream  executor  factory  private  static  final    string  blink  planner  factory  org  apache  flink  table  planner  delegation    blink  planner  factory  private  static  final    string  blink  executor  factory  org  apache  flink  table  planner  delegation    blink  executor  factory  private    string  planner  class  blink  planner  factory  private    string  executor  class  blink  executor  factory  private    string  built  in  catalog  name  default  builtin  catalog  private    string  built  in  database  name  default  builtin  database  private  boolean  is  streaming  mode  true    sets  the  old    flink  planner  as  the  required  module    by  default  link  use  blink  planner  is  enabled  public    builder  use  old  planner  this  planner  class  old  planner  factory  this  executor  class  old  executor  factory  return  this    sets  the    blink  planner  as  the  required  module  p    this  is  the  default  behavior  public    builder  use  blink  planner  this  planner  class  blink  planner  factory  this  executor  class  blink  executor  factory  return  this    does  not  set  a  planner  requirement  explicitly  p  a  planner  will  be  discovered  automatically  if  there  is  only  one  planner  available  p    by  default  link  use  blink  planner  is  enabled  public    builder  use  any  planner  this  planner  class  null  this  executor  class  null  return  this    sets  that  the  components  should  work  in  a  batch  mode    streaming  mode  by  default  public    builder  in  batch  mode  this  is  streaming  mode  false  return  this    sets  that  the  components  should  work  in  a  streaming  mode    enabled  by  default  public    builder  in  streaming  mode  this  is  streaming  mode  true  return  this    specifies  the  name  of  the  initial  catalog  to  be  created  when  instantiating  a  link    table  environment    this  catalog  will  be  used  to  store  all  non  serializable  objects  such  as  tables  and  functions  registered  via  e  g  link    table  environment  register  table  sink    string    table  sink  or  link    table  environment  register  function    string    scalar  function    it  will  also  be  the  initial  value  for  the  current  catalog  which  can  be  altered  via  link    table  environment  use  catalog    string  p    default  default  catalog  public    builder  with  built  in  catalog  name    string  built  in  catalog  name  this  built  in  catalog  name  built  in  catalog  name  return  this    specifies  the  name  of  the  default  database  in  the  initial  catalog  to  be  created  when  instantiating  a  link    table  environment    the  database  will  be  used  to  store  all  non  serializable  objects  such  as  tables  and  functions  registered  via  e  g  link    table  environment  register  table  sink    string    table  sink  or  link    table  environment  register  function    string    scalar  function    it  will  also  be  the  initial  value  for  the  current  database  which  can  be  altered  via  link    table  environment  use  database    string  p    default  default  database  public    builder  with  built  in  database  name    string  built  in  database  name  this  built  in  database  name  built  in  database  name  return  this    returns  an  immutable  instance  of  link    environment  settings  public    environment  settings  build  return  new    environment  settings  planner  class  executor  class  built  in  catalog  name  built  in  database  name  is  streaming  mode  
public  evolving  public  enum    explain  detail    the  cost  information  on  physical  rel  node  estimated  by  optimizer  e  g    table  source  scan  cumulative  cost  1.0    e8  rows  1.0    e8  cpu  2.4    e9  io  0.0  network  0.0  memory  estimated  cost    the  changelog  mode  produced  by  a  physical  rel  node  e  g    group  aggregate  changelog  mode  i  ua  d  changelog  mode  
public  evolving  public  final  class    expressions    creates  an  unresolved  reference  to  a  table  s  field  p    example  pre  code  tab  select  key  value  pre  checkstyle  off    method  name  public  static    api  expression    string  name  return  new    api  expression  unresolved  ref  name  checkstyle  on    method  name    creates  a  sql  literal  p    the  data  type  is  derived  from  the  object  s  class  and  its  value  p    for  example  ul  li  code  lit    leads  to  code  int  li  li  code  lit  abc  leads  to  code  char    li  li  code  lit  new    big  decimal  123.45  leads  to  code  decimal      li  ul  p    see  link    value  data  type  converter  for  a  list  of  supported  literal  values  public  static    api  expression  lit    object  v  return  new    api  expression  value  literal  v    creates  a  sql  literal  of  a  given  link    data  type  p    the  method  link  lit    object  is  preferred  as  it  extracts  the  link    data  type  automatically    use  this  method  only  when  necessary    the  class  of  code  v  must  be  supported  according  to  the  link  org  apache  flink  table  types  logical    logical  type  supports  input  conversion    class  public  static    api  expression  lit    object  v    data  type  data  type  return  new    api  expression  value  literal  v  data  type    indicates  a  range  from  start  to  end  which  can  be  used  in  columns  selection  p    example  pre  code    table  table  table  select  with  columns  range  b  c  pre  see  with  columns    object    object  see  without  columns    object    object  public  static    api  expression  range    string  start    string  end  return  api  call    built  in  function  definitions  range  to  unresolved  ref  start  unresolved  ref  end    indicates  an  index  based  range  which  can  be  used  in  columns  selection  p    example  pre  code    table  table  table  select  with  columns  range      pre  see  with  columns    object    object  see  without  columns    object    object  public  static    api  expression  range  int  start  int  end  return  api  call    built  in  function  definitions  range  to  value  literal  start  value  literal  end    boolean  and  in  three  valued  logic  public  static    api  expression  and    object  predicate0    object  predicate1    object  predicates  return  api  call  at  least  two  argument    built  in  function  definitions  and  predicate0  predicate1  predicates    boolean  or  in  three  valued  logic  public  static    api  expression  or    object  predicate0    object  predicate1    object  predicates  return  api  call  at  least  two  argument    built  in  function  definitions  or  predicate0  predicate1  predicates    offset  constant  to  be  used  in  the  code  preceding  clause  of  unbounded  code    over  windows    use  this  constant  for  a  time  interval    unbounded  over  windows  start  with  the  first  row  of  a  partition  public  static  final    api  expression  unbounded  row  api  call    built  in  function  definitions  unbounded  row    offset  constant  to  be  used  in  the  code  preceding  clause  of  unbounded  link    over  windows    use  this  constant  for  a  row  count  interval    unbounded  over  windows  start  with  the  first  row  of  a  partition  public  static  final    api  expression  unbounded  range  api  call    built  in  function  definitions  unbounded  range    offset  constant  to  be  used  in  the  code  following  clause  of  link    over  windows    use  this  for  setting  the  upper  bound  of  the  window  to  the  current  row  public  static  final    api  expression  current  row  api  call    built  in  function  definitions  current  row    offset  constant  to  be  used  in  the  code  following  clause  of  link    over  windows    use  this  for  setting  the  upper  bound  of  the  window  to  the  sort  key  of  the  current  row  i  e  all  rows  with  the  same  sort  key  as  the  current  row  are  included  in  the  window  public  static  final    api  expression  current  range  api  call    built  in  function  definitions  current  range    returns  the  current  sql  date  in  utc  time  zone  public  static    api  expression  current  date  return  api  call    built  in  function  definitions  current  date    returns  the  current  sql  time  in  utc  time  zone  public  static    api  expression  current  time  return  api  call    built  in  function  definitions  current  time    returns  the  current  sql  timestamp  in  utc  time  zone  public  static    api  expression  current  timestamp  return  api  call    built  in  function  definitions  current  timestamp    returns  the  current  sql  time  in  local  time  zone  public  static    api  expression  local  time  return  api  call    built  in  function  definitions  local  time    returns  the  current  sql  timestamp  in  local  time  zone  public  static    api  expression  local  timestamp  return  api  call    built  in  function  definitions  local  timestamp    determines  whether  two  anchored  time  intervals  overlap    time  point  and  temporal  are  transformed  into  a  range  defined  by  two  time  points  start  end    the  function  evaluates  code  left  end  right  start  right  end  left  start  code  p    it  evaluates  left  end  right  start  right  end  left  start  p  e  g  pre  code  temporal  overlaps  lit  2:55    to  time  lit    hours  lit  3:30    to  time  lit    hours  pre  leads  to  true  public  static    api  expression  temporal  overlaps    object  left  time  point    object  left  temporal    object  right  time  point    object  right  temporal  return  api  call    built  in  function  definitions  temporal  overlaps  left  time  point  left  temporal  right  time  point  right  temporal    formats  a  timestamp  as  a  string  using  a  specified  format    the  format  must  be  compatible  with    my  s  q  l  s  date  formatting  syntax  as  used  by  the  date  parse  function  p    for  example  code  data  format  time  y  d  m  results  in  strings  formatted  as        may  param  timestamp    the  timestamp  to  format  as  string  param  format    the  format  of  the  string  return    the  formatted  timestamp  as  string  public  static    api  expression  date  format    object  timestamp    object  format  return  api  call    built  in  function  definitions  date  format  timestamp  format    returns  the  signed  number  of  link    time  point  unit  between  time  point1  and  time  point2  p    for  example  code  timestamp  diff    time  point  unit  day  lit  2016-06    to  date  lit  2016-06    to  date  leads  to    param  time  point  unit    the  unit  to  compute  diff  param  time  point1    the  first  point  in  time  param  time  point2    the  second  point  in  time  return    the  number  of  intervals  as  integer  value  public  static    api  expression  timestamp  diff    time  point  unit  time  point  unit    object  time  point1    object  time  point2  return  api  call    built  in  function  definitions  timestamp  diff  value  literal  time  point  unit  time  point1  time  point2    creates  an  array  of  literals  public  static    api  expression  array    object  head    object  tail  return  api  call  at  least  one  argument    built  in  function  definitions  array  head  tail    creates  a  row  of  expressions  public  static    api  expression  row    object  head    object  tail  return  api  call  at  least  one  argument    built  in  function  definitions  row  head  tail    creates  a  map  of  expressions  pre  code  table  select  map  key1    key2    key3    pre  p    note  keys  and  values  should  have  the  same  types  for  all  entries  public  static    api  expression  map    object  key    object  value    object  tail  return  api  call  at  least  two  argument    built  in  function  definitions  map  key  value  tail    creates  an  interval  of  rows  see    table  window    group  window  see    table  window    over  window  public  static    api  expression  row  interval    long  rows  return  new    api  expression  value  literal  rows    returns  a  value  that  is  closer  than  any  other  value  to  pi  public  static    api  expression  pi  return  api  call    built  in  function  definitions  pi    returns  a  value  that  is  closer  than  any  other  value  to  e  public  static    api  expression  e  return  api  call    built  in  function  definitions  e    returns  a  pseudorandom  double  value  between  0.0  inclusive  and  1.0  exclusive  public  static    api  expression  rand  return  api  call    built  in  function  definitions  rand    returns  a  pseudorandom  double  value  between  0.0  inclusive  and  1.0  exclusive  with  a  initial  seed    two  rand  functions  will  return  identical  sequences  of  numbers  if  they  have  same  initial  seed  public  static    api  expression  rand    object  seed  return  api  call    built  in  function  definitions  rand  object  to  expression  seed    returns  a  pseudorandom  integer  value  between  0.0  inclusive  and  the  specified  value  exclusive  public  static    api  expression  rand  integer    object  bound  return  api  call    built  in  function  definitions  rand  integer  object  to  expression  bound    returns  a  pseudorandom  integer  value  between  0.0  inclusive  and  the  specified  value  exclusive  with  a  initial  seed    two  rand  integer  functions  will  return  identical  sequences  of  numbers  if  they  have  same  initial  seed  and  same  bound  public  static    api  expression  rand  integer    object  seed    object  bound  return  api  call    built  in  function  definitions  rand  integer  object  to  expression  seed  object  to  expression  bound    returns  the  string  that  results  from  concatenating  the  arguments    returns  null  if  any  argument  is  null  public  static    api  expression  concat    object  string    object  strings  return  api  call  at  least  one  argument    built  in  function  definitions  concat  string  strings    calculates  the  arc  tangent  of  a  given  coordinate  public  static    api  expression  atan2    object  y    object  x  return  api  call  at  least  one  argument    built  in  function  definitions    a  t  a  n2  y  x    returns  negative  numeric  public  static    api  expression  negative    object  v  return  api  call    built  in  function  definitions  minus  prefix  v    returns  the  string  that  results  from  concatenating  the  arguments  and  separator    returns  null    if  the  separator  is  null  p    note  this  function  does  not  skip  empty  strings    however  it  does  skip  any  null  values  after  the  separator  argument  public  static    api  expression  concat  ws    object  separator    object  string    object  strings  return  api  call  at  least  two  argument    built  in  function  definitions  concat  ws  separator  string  strings    returns  an  uuid    universally    unique    identifier  string  e  g  3d3  c68f7  f608    f  b60c  b0c44ad4cc4e  according  to  rfc    type    pseudo  randomly  generated  uuid    the  uuid  is  generated  using  a  cryptographically  strong  pseudo  random  number  generator  public  static    api  expression  uuid  return  api  call    built  in  function  definitions  uuid    returns  a  null  literal  value  of  a  given  data  type  p  e  g  code  null  of    data  types  int  public  static    api  expression  null  of    data  type  data  type  return  new    api  expression  value  literal  null  data  type  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    it  is  recommended  to  use  link  null  of    data  type  instead  which  uses  the  new  type  system  based  on  link    data  types    please  make  sure  to  use  either  the  old  or  the  new  type  system  consistently  to  avoid  unintended  behavior    see  the  website  documentation  for  more  information  public  static    api  expression  null  of    type  information  type  info  return  null  of    type  conversions  from  legacy  info  to  data  type  type  info    calculates  the  logarithm  of  the  given  value  public  static    api  expression  log    object  value  return  api  call    built  in  function  definitions  log  value    calculates  the  logarithm  of  the  given  value  to  the  given  base  public  static    api  expression  log    object  base    object  value  return  api  call    built  in  function  definitions  log  base  value    ternary  conditional  operator  that  decides  which  of  two  other  expressions  should  be  evaluated  based  on  a  evaluated  boolean  condition  p  e  g  if  then  else  f0    a  b  leads  to  a  param  condition  boolean  condition  param  if  true  expression  to  be  evaluated  if  condition  holds  param  if  false  expression  to  be  evaluated  if  condition  does  not  hold  public  static    api  expression  if  then  else    object  condition    object  if  true    object  if  false  return  api  call    built  in  function  definitions  if  condition  if  true  if  false    creates  an  expression  that  selects  a  range  of  columns    it  can  be  used  wherever  an  array  of  expression  is  accepted  such  as  function  calls  projections  or  groupings  p  a  range  can  either  be  index  based  or  name  based    indices  start  at    and  boundaries  are  inclusive  p  e  g  with  columns  range  b  c  or  without  columns  public  static    api  expression  with  columns    object  head    object  tail  return  api  call  at  least  one  argument    built  in  function  definitions  with  columns  head  tail    creates  an  expression  that  selects  all  columns  except  for  the  given  range  of  columns    it  can  be  used  wherever  an  array  of  expression  is  accepted  such  as  function  calls  projections  or  groupings  p  a  range  can  either  be  index  based  or  name  based    indices  start  at    and  boundaries  are  inclusive  p  e  g  without  columns  range  b  c  or  without  columns  c  public  static    api  expression  without  columns    object  head    object  tail  return  api  call  at  least  one  argument    built  in  function  definitions  without  columns  head  tail  a  call  to  a  function  that  will  be  looked  up  in  a  catalog    there  are  two  kinds  of  functions  ul  li    system  functions  which  are  identified  with  one  part  names  li  li    catalog  functions  which  are  identified  always  with  three  parts  names  catalog  database  function  li  ul  p    moreover  each  function  can  either  be  a  temporary  function  or  permanent  one  which  is  stored  in  an  external  catalog  p    based  on  that  two  properties  the  resolution  order  for  looking  up  a  function  based  on  the  provided  code  function  name  is  following  ul  li    temporary  system  function  li  li    system  function  li  li    temporary  catalog  function  li  li    catalog  function  li  ul  see    table  environment  use  catalog    string  see    table  environment  use  database    string  see    table  environment  create  temporary  function  see    table  environment  create  temporary  system  function  public  static    api  expression  call    string  path    object  arguments  return  new    api  expression    api  expression  utils  lookup  call  path    arrays  stream  arguments  map    api  expression  utils  object  to  expression  to  array    expression  new  a  call  to  an  unregistered  inline  function  p    for  functions  that  have  been  registered  before  and  are  identified  by  a  name  use  link  call    string    object  public  static    api  expression  call    user  defined  function  function    object  arguments  return  api  call  function  arguments  a  call  to  an  unregistered  inline  function  p    for  functions  that  have  been  registered  before  and  are  identified  by  a  name  use  link  call    string    object  public  static    api  expression  call    class  extends    user  defined  function  function    object  arguments  final    user  defined  function  function  instance    user  defined  function  helper  instantiate  function  function  return  api  call  function  instance  arguments  private  static    api  expression  api  call    function  definition  function  definition    object  args    list    expression  arguments    stream  of  args  map    api  expression  utils  object  to  expression  collect    collectors  to  list  return  new    api  expression  unresolved  call  function  definition  arguments  private  static    api  expression  api  call  at  least  one  argument    function  definition  function  definition    object  arg0    object  args    list    expression  arguments    stream  concat    stream  of  arg0    stream  of  args  map    api  expression  utils  object  to  expression  collect    collectors  to  list  return  new    api  expression  unresolved  call  function  definition  arguments  private  static    api  expression  api  call  at  least  two  argument    function  definition  function  definition    object  arg0    object  arg1    object  args    list    expression  arguments    stream  concat    stream  of  arg0  arg1    stream  of  args  map    api  expression  utils  object  to  expression  collect    collectors  to  list  return  new    api  expression  unresolved  call  function  definition  arguments  
public  evolving  public  interface    flat  aggregate  table    performs  a  selection  operation  on  a    flat  aggregate  table    similar  to  a  sql  select  statement    the  field  expressions  can  contain  complex  expressions  p  b    note  b    you  have  to  close  the  flat  aggregate  with  a  select  statement    and  the  select  statement  does  not  support  aggregate  functions  p    example  pre  code    table  aggregate  function  table  agg  func  new    my  table  aggregate  function  table  env  register  function  table  agg  func  table  agg  func  tab  group  by  key  flat  aggregate  table  agg  func  a  b  as  x  y  z  select  key  x  y  z  pre  deprecated  use  link  select    expression    deprecated    table  select    string  fields    performs  a  selection  operation  on  a    flat  aggregate  table  table    similar  to  a  sql  select  statement    the  field  expressions  can  contain  complex  expressions  p  b    note  b    you  have  to  close  the  flat  aggregate  with  a  select  statement    and  the  select  statement  does  not  support  aggregate  functions  p    example  pre  code    table  aggregate  function  table  agg  func  new    my  table  aggregate  function  table  env  register  function  table  agg  func  table  agg  func  tab  group  by  key  flat  aggregate  call  table  agg  func  a  b  as  x  y  z  select  key  x  y  z  pre  p    scala    example  pre  code  val  table  agg  func    table  aggregate  function  new    my  table  aggregate  function  tab  group  by  key  flat  aggregate  table  agg  func  a  b  as  x  y  z  select  key  x  y  z  pre    table  select    expression  fields  
public  evolving  public  interface    grouped  table    performs  a  selection  operation  on  a  grouped  table    similar  to  an  sql  select  statement    the  field  expressions  can  contain  complex  expressions  and  aggregations  p    example  pre  code  tab  group  by  key  select  key  value  avg    the  average  as  average  pre  deprecated  use  link  select    expression    deprecated    table  select    string  fields    performs  a  selection  operation  on  a  grouped  table    similar  to  an  sql  select  statement    the  field  expressions  can  contain  complex  expressions  and  aggregations  p    example  pre  code  tab  group  by  key  select  key  value  avg  plus    the  average  as  average  pre  p    scala    example  pre  code  tab  group  by  key  select  key  value  avg    the  average  as  average  pre    table  select    expression  fields    performs  an  aggregate  operation  with  an  aggregate  function    you  have  to  close  the  link  aggregate    string  with  a  select  statement    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code    aggregate  function  agg  func  new    my  aggregate  function  table  env  register  function  agg  func  agg  func  table  group  by  key  aggregate  agg  func  a  b  as  f0  f1  f2  select  key  f0  f1  pre  deprecated  use  link  aggregate    expression    deprecated    aggregated  table  aggregate    string  aggregate  function    performs  an  aggregate  operation  with  an  aggregate  function    you  have  to  close  the  link  aggregate    expression  with  a  select  statement    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code    aggregate  function  agg  func  new    my  aggregate  function  table  env  register  function  agg  func  agg  func  tab  group  by  key  aggregate  call  agg  func  a  b  as  f0  f1  f2  select  key  f0  f1  pre  p    scala    example  pre  code  val  agg  func  new    my  aggregate  function  table  group  by  key  aggregate  agg  func  a  b  as  f0  f1  f2  select  key  f0  f1  pre    aggregated  table  aggregate    expression  aggregate  function    performs  a  flat  aggregate  operation  on  a  grouped  table    flat  aggregate  takes  a    table  aggregate  function  which  returns  multiple  rows    use  a  selection  after  flat  aggregate  p    example  pre  code  val  table  agg  func    table  aggregate  function  new    my  table  aggregate  function  table  env  register  function  table  agg  func  table  agg  func  tab  group  by  key  flat  aggregate  table  agg  func  a  b  as  x  y  z  select  key  x  y  z  pre  deprecated  use  link  flat  aggregate    expression    deprecated    flat  aggregate  table  flat  aggregate    string  table  agg  function    performs  a  flat  aggregate  operation  on  a  grouped  table    flat  aggregate  takes  a    table  aggregate  function  which  returns  multiple  rows    use  a  selection  after  flat  aggregate  p    example  pre  code    table  aggregate  function  table  agg  func  new    my  table  aggregate  function  table  env  register  function  table  agg  func  table  agg  func  tab  group  by  key  flat  aggregate  call  table  agg  func  a  b  as  x  y  z  select  key  x  y  z  pre  p    scala    example  pre  code  val  table  agg  func    table  aggregate  function  new    my  table  aggregate  function  tab  group  by  key  flat  aggregate  table  agg  func  a  b  as  x  y  z  select  key  x  y  z  pre    flat  aggregate  table  flat  aggregate    expression  table  agg  function  
public  evolving  public  abstract  class    group  window    alias  name  for  the  group  window  private  final    expression  alias  private  final    expression  time  field    group  window    expression  alias    expression  time  field  this  alias    api  expression  utils  unwrap  from  api  alias  this  time  field    api  expression  utils  unwrap  from  api  time  field  public    expression  get  alias  return  alias  public    expression  get  time  field  return  time  field  
public  evolving  public  interface    group  windowed  table    groups  the  elements  by  a  mandatory  window  and  one  or  more  optional  grouping  attributes    the  window  is  specified  by  referring  to  its  alias  p    if  no  additional  grouping  attribute  is  specified  and  if  the  input  is  a  streaming  table  the  aggregation  will  be  performed  by  a  single  task  i  e  with  parallelism    p    aggregations  are  performed  per  group  and  defined  by  a  subsequent  code  select  clause  similar  to  sql  select  group  by  query  p    example  pre  code  tab  window  group  window  as  w  group  by  w  key  select  key  value  avg  pre  deprecated  use  link  group  by    expression    deprecated    window  grouped  table  group  by    string  fields    groups  the  elements  by  a  mandatory  window  and  one  or  more  optional  grouping  attributes    the  window  is  specified  by  referring  to  its  alias  p    if  no  additional  grouping  attribute  is  specified  and  if  the  input  is  a  streaming  table  the  aggregation  will  be  performed  by  a  single  task  i  e  with  parallelism    p    aggregations  are  performed  per  group  and  defined  by  a  subsequent  code  select  clause  similar  to  sql  select  group  by  query  p    example  pre  code  tab  window  group  window  as  w  group  by  w  key  select  key  value  avg  pre  p    scala    example  pre  code  tab  window  group  window  as  w  group  by  w  key  select  key  value  avg  pre    window  grouped  table  group  by    expression  fields  
public  evolving  public  abstract  class    base  expressions    in  type    out  type  protected  abstract    expression  to  expr  protected  abstract    out  type  to  api  specific  expression    expression  expression    specifies  a  name  for  an  expression  i  e  a  field  param  name  name  for  one  field  param  extra  names  additional  names  if  the  expression  expands  to  multiple  fields  public    out  type  as    string  name    string  extra  names  return  to  api  specific  expression    api  expression  utils  unresolved  call    built  in  function  definitions  as    stream  concat    stream  of  to  expr    api  expression  utils  value  literal  name    stream  of  extra  names  map    api  expression  utils  value  literal  to  array    expression  new    boolean  and  in  three  valued  logic    this  is  an  infix  notation    see  also  link    expressions  and    object    object    object  for  prefix  notation  with  multiple  arguments  see    expressions  and    object    object    object  public    out  type  and    in  type  other  return  to  api  specific  expression  unresolved  call  and  to  expr  object  to  expression  other    boolean  or  in  three  valued  logic    this  is  an  infix  notation    see  also  link    expressions  or    object    object    object  for  prefix  notation  with  multiple  arguments  see    expressions  or    object    object    object  public    out  type  or    in  type  other  return  to  api  specific  expression  unresolved  call  or  to  expr  object  to  expression  other    greater  than  public    out  type  is  greater    in  type  other  return  to  api  specific  expression  unresolved  call  greater  than  to  expr  object  to  expression  other    greater  than  or  equal  public    out  type  is  greater  or  equal    in  type  other  return  to  api  specific  expression  unresolved  call  greater  than  or  equal  to  expr  object  to  expression  other    less  than  public    out  type  is  less    in  type  other  return  to  api  specific  expression  unresolved  call  less  than  to  expr  object  to  expression  other    less  than  or  equal  public    out  type  is  less  or  equal    in  type  other  return  to  api  specific  expression  unresolved  call  less  than  or  equal  to  expr  object  to  expression  other    equals  public    out  type  is  equal    in  type  other  return  to  api  specific  expression  unresolved  call  equals  to  expr  object  to  expression  other    not  equal  public    out  type  is  not  equal    in  type  other  return  to  api  specific  expression  unresolved  call  not  equals  to  expr  object  to  expression  other    returns  left  plus  right  public    out  type  plus    in  type  other  return  to  api  specific  expression  unresolved  call  plus  to  expr  object  to  expression  other    returns  left  minus  right  public    out  type  minus    in  type  other  return  to  api  specific  expression  unresolved  call  minus  to  expr  object  to  expression  other    returns  left  divided  by  right  public    out  type  divided  by    in  type  other  return  to  api  specific  expression  unresolved  call  divide  to  expr  object  to  expression  other    returns  left  multiplied  by  right  public    out  type  times    in  type  other  return  to  api  specific  expression  unresolved  call  times  to  expr  object  to  expression  other    returns  true  if  the  given  expression  is  between  lower  bound  and  upper  bound  both  inclusive    false  otherwise    the  parameters  must  be  numeric  types  or  identical  comparable  types  param  lower  bound  numeric  or  comparable  expression  param  upper  bound  numeric  or  comparable  expression  public    out  type  between    in  type  lower  bound    in  type  upper  bound  return  to  api  specific  expression  unresolved  call  between  to  expr  object  to  expression  lower  bound  object  to  expression  upper  bound    returns  true  if  the  given  expression  is  not  between  lower  bound  and  upper  bound  both  inclusive    false  otherwise    the  parameters  must  be  numeric  types  or  identical  comparable  types  param  lower  bound  numeric  or  comparable  expression  param  upper  bound  numeric  or  comparable  expression  public    out  type  not  between    in  type  lower  bound    in  type  upper  bound  return  to  api  specific  expression  unresolved  call  not  between  to  expr  object  to  expression  lower  bound  object  to  expression  upper  bound    ternary  conditional  operator  that  decides  which  of  two  other  expressions  should  be  evaluated  based  on  a  evaluated  boolean  condition  p  e  g  lit    is  greater    then  a  b  leads  to  a  param  if  true  expression  to  be  evaluated  if  condition  holds  param  if  false  expression  to  be  evaluated  if  condition  does  not  hold  public    out  type  then    in  type  if  true    in  type  if  false  return  to  api  specific  expression  unresolved  call  if  to  expr  object  to  expression  if  true  object  to  expression  if  false    returns  true  if  the  given  expression  is  null  public    out  type  is  null  return  to  api  specific  expression  unresolved  call  is  null  to  expr    returns  true  if  the  given  expression  is  not  null  public    out  type  is  not  null  return  to  api  specific  expression  unresolved  call  is  not  null  to  expr    returns  true  if  given  boolean  expression  is  true    false  otherwise  for  null  and  false  public    out  type  is  true  return  to  api  specific  expression  unresolved  call  is  true  to  expr    returns  true  if  given  boolean  expression  is  false    false  otherwise  for  null  and  true  public    out  type  is  false  return  to  api  specific  expression  unresolved  call  is  false  to  expr    returns  true  if  given  boolean  expression  is  not  true  for  null  and  false    false  otherwise  public    out  type  is  not  true  return  to  api  specific  expression  unresolved  call  is  not  true  to  expr    returns  true  if  given  boolean  expression  is  not  false  for  null  and  true    false  otherwise  public    out  type  is  not  false  return  to  api  specific  expression  unresolved  call  is  not  false  to  expr    similar  to  a  sql  distinct  aggregation  clause  such  as  count  distinct  a  declares  that  an  aggregation  function  is  only  applied  on  distinct  input  values  p    for  example  pre  code  orders  group  by  a  select  a  b  sum  distinct  as  d  pre  public    out  type  distinct  return  to  api  specific  expression  unresolved  call  distinct  to  expr    returns  the  sum  of  the  numeric  field  across  all  input  values    if  all  values  are  null  null  is  returned  public    out  type  sum  return  to  api  specific  expression  unresolved  call  sum  to  expr    returns  the  sum  of  the  numeric  field  across  all  input  values    if  all  values  are  null    is  returned  public    out  type  sum0  return  to  api  specific  expression  unresolved  call    s  u  m0  to  expr    returns  the  minimum  value  of  field  across  all  input  values  public    out  type  min  return  to  api  specific  expression  unresolved  call  min  to  expr    returns  the  maximum  value  of  field  across  all  input  values  public    out  type  max  return  to  api  specific  expression  unresolved  call  max  to  expr    returns  the  number  of  input  rows  for  which  the  field  is  not  null  public    out  type  count  return  to  api  specific  expression  unresolved  call  count  to  expr    returns  the  average  arithmetic  mean  of  the  numeric  field  across  all  input  values  public    out  type  avg  return  to  api  specific  expression  unresolved  call  avg  to  expr    returns  the  population  standard  deviation  of  an  expression  the  square  root  of  var  pop  public    out  type  stddev  pop  return  to  api  specific  expression  unresolved  call  stddev  pop  to  expr    returns  the  sample  standard  deviation  of  an  expression  the  square  root  of  var  samp  public    out  type  stddev  samp  return  to  api  specific  expression  unresolved  call  stddev  samp  to  expr    returns  the  population  standard  variance  of  an  expression  public    out  type  var  pop  return  to  api  specific  expression  unresolved  call  var  pop  to  expr    returns  the  sample  variance  of  a  given  expression  public    out  type  var  samp  return  to  api  specific  expression  unresolved  call  var  samp  to  expr    returns  multiset  aggregate  of  a  given  expression  public    out  type  collect  return  to  api  specific  expression  unresolved  call  collect  to  expr    converts  a  value  to  a  given  data  type  p  e  g    cast    data  types  int  leads  to    public    out  type  cast    data  type  to  type  return  to  api  specific  expression  unresolved  call  cast  to  expr  type  literal  to  type  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    it  is  recommended  to  use  link  cast    data  type  instead  which  uses  the  new  type  system  based  on  link  org  apache  flink  table  api    data  types    please  make  sure  to  use  either  the  old  or  the  new  type  system  consistently  to  avoid  unintended  behavior    see  the  website  documentation  for  more  information    deprecated  public    out  type  cast    type  information  to  type  return  to  api  specific  expression  unresolved  call  cast  to  expr  type  literal  from  legacy  info  to  data  type  to  type    specifies  ascending  order  of  an  expression  i  e  a  field  for  order  by  unresolved  call  public    out  type  asc  return  to  api  specific  expression  unresolved  call  order  asc  to  expr    specifies  descending  order  of  an  expression  i  e  a  field  for  order  by  unresolved  call  public    out  type  desc  return  to  api  specific  expression  unresolved  call  order  desc  to  expr    returns  true  if  an  expression  exists  in  a  given  list  of  expressions    this  is  a  shorthand  for  multiple  or  conditions  p    if  the  testing  set  contains  null  the  result  will  be  null  if  the  element  can  not  be  found  and  true  if  it  can  be  found    if  the  element  is  null  the  result  is  always  null  p  e  g  lit    in        leads  to  false    safe  varargs  public  final    out  type  in    in  type  elements    expression  args    stream  concat    stream  of  to  expr    arrays  stream  elements  map    api  expression  utils  object  to  expression  to  array    expression  new  return  to  api  specific  expression  unresolved  call  in  args    returns  true  if  an  expression  exists  in  a  given  table  sub  query    the  sub  query  table  must  consist  of  one  column    this  column  must  have  the  same  data  type  as  the  expression  p    note    this  operation  is  not  supported  in  a  streaming  environment  yet  public    out  type  in    table  table  return  to  api  specific  expression  unresolved  call  in  to  expr  table  ref  table  to  string  table    returns  the  start  time  inclusive  of  a  window  when  applied  on  a  window  reference  public    out  type  start  return  to  api  specific  expression  unresolved  call  window  start  to  expr    returns  the  end  time  exclusive  of  a  window  when  applied  on  a  window  reference  p  e  g  if  a  window  ends  at  10:59  59.999  this  property  will  return  11:00  00.000  public    out  type  end  return  to  api  specific  expression  unresolved  call  window  end  to  expr    calculates  the  remainder  of  division  the  given  number  by  another  one  public    out  type  mod    in  type  other  return  to  api  specific  expression  unresolved  call  mod  to  expr  object  to  expression  other    calculates  the    euler  s  number  raised  to  the  given  power  public    out  type  exp  return  to  api  specific  expression  unresolved  call  exp  to  expr    calculates  the  base    logarithm  of  the  given  value  public    out  type  log10  return  to  api  specific  expression  unresolved  call    l  o  g10  to  expr    calculates  the  base    logarithm  of  the  given  value  public    out  type  log2  return  to  api  specific  expression  unresolved  call    l  o  g2  to  expr    calculates  the  natural  logarithm  of  the  given  value  public    out  type  ln  return  to  api  specific  expression  unresolved  call  ln  to  expr    calculates  the  natural  logarithm  of  the  given  value  public    out  type  log  return  to  api  specific  expression  unresolved  call  log  to  expr    calculates  the  logarithm  of  the  given  value  to  the  given  base  public    out  type  log    in  type  base  return  to  api  specific  expression  unresolved  call  log  object  to  expression  base  to  expr    calculates  the  given  number  raised  to  the  power  of  the  other  value  public    out  type  power    in  type  other  return  to  api  specific  expression  unresolved  call  power  to  expr  object  to  expression  other    calculates  the  hyperbolic  cosine  of  a  given  value  public    out  type  cosh  return  to  api  specific  expression  unresolved  call  cosh  to  expr    calculates  the  square  root  of  a  given  value  public    out  type  sqrt  return  to  api  specific  expression  unresolved  call  sqrt  to  expr    calculates  the  absolute  value  of  given  value  public    out  type  abs  return  to  api  specific  expression  unresolved  call  abs  to  expr    calculates  the  largest  integer  less  than  or  equal  to  a  given  number  public    out  type  floor  return  to  api  specific  expression  unresolved  call  floor  to  expr    calculates  the  hyperbolic  sine  of  a  given  value  public    out  type  sinh  return  to  api  specific  expression  unresolved  call  sinh  to  expr    calculates  the  smallest  integer  greater  than  or  equal  to  a  given  number  public    out  type  ceil  return  to  api  specific  expression  unresolved  call  ceil  to  expr    calculates  the  sine  of  a  given  number  public    out  type  sin  return  to  api  specific  expression  unresolved  call  sin  to  expr    calculates  the  cosine  of  a  given  number  public    out  type  cos  return  to  api  specific  expression  unresolved  call  cos  to  expr    calculates  the  tangent  of  a  given  number  public    out  type  tan  return  to  api  specific  expression  unresolved  call  tan  to  expr    calculates  the  cotangent  of  a  given  number  public    out  type  cot  return  to  api  specific  expression  unresolved  call  cot  to  expr    calculates  the  arc  sine  of  a  given  number  public    out  type  asin  return  to  api  specific  expression  unresolved  call  asin  to  expr    calculates  the  arc  cosine  of  a  given  number  public    out  type  acos  return  to  api  specific  expression  unresolved  call  acos  to  expr    calculates  the  arc  tangent  of  a  given  number  public    out  type  atan  return  to  api  specific  expression  unresolved  call  atan  to  expr    calculates  the  hyperbolic  tangent  of  a  given  number  public    out  type  tanh  return  to  api  specific  expression  unresolved  call  tanh  to  expr    converts  numeric  from  radians  to  degrees  public    out  type  degrees  return  to  api  specific  expression  unresolved  call  degrees  to  expr    converts  numeric  from  degrees  to  radians  public    out  type  radians  return  to  api  specific  expression  unresolved  call  radians  to  expr    calculates  the  signum  of  a  given  number  public    out  type  sign  return  to  api  specific  expression  unresolved  call  sign  to  expr    rounds  the  given  number  to  integer  places  right  to  the  decimal  point  public    out  type  round    in  type  places  return  to  api  specific  expression  unresolved  call  round  to  expr  object  to  expression  places    returns  a  string  representation  of  an  integer  numeric  value  in  binary  format    returns  null  if  numeric  is  null  e  g    leads  to      leads  to    public    out  type  bin  return  to  api  specific  expression  unresolved  call  bin  to  expr    returns  a  string  representation  of  an  integer  numeric  value  or  a  string  in  hex  format    returns  null  if  numeric  or  string  is  null  p  e  g  a  numeric    leads  to    a  numeric    leads  to    and  a  string  hello  world  leads  to  68656c6  c6f2c776f726c64  public    out  type  hex  return  to  api  specific  expression  unresolved  call  hex  to  expr    returns  a  number  of  truncated  to  n  decimal  places    if  n  is    the  result  has  no  decimal  point  or  fractional  part  n  can  be  negative  to  cause  n  digits  left  of  the  decimal  point  of  the  value  to  become  zero  e  g  truncate  42.345    to  42.34  public    out  type  truncate    in  type  n  return  to  api  specific  expression  unresolved  call  truncate  to  expr  object  to  expression  n    returns  a  number  of  truncated  to    decimal  places  e  g  truncate  42.345  to  42.0  public    out  type  truncate  return  to  api  specific  expression  unresolved  call  truncate  to  expr    string  operations    creates  a  substring  of  the  given  string  at  given  index  for  a  given  length  param  begin  index  first  character  of  the  substring  starting  at    inclusive  param  length  number  of  characters  of  the  substring  public    out  type  substring    in  type  begin  index    in  type  length  return  to  api  specific  expression  unresolved  call  substring  to  expr  object  to  expression  begin  index  object  to  expression  length    creates  a  substring  of  the  given  string  beginning  at  the  given  index  to  the  end  param  begin  index  first  character  of  the  substring  starting  at    inclusive  public    out  type  substring    in  type  begin  index  return  to  api  specific  expression  unresolved  call  substring  to  expr  object  to  expression  begin  index    removes  leading  space  characters  from  the  given  string  public    out  type  trim  leading  return  to  api  specific  expression  unresolved  call  trim  value  literal  true  value  literal  false  value  literal  to  expr    removes  leading  characters  from  the  given  string  param  character  string  containing  the  character  public    out  type  trim  leading    in  type  character  return  to  api  specific  expression  unresolved  call  trim  value  literal  true  value  literal  false  object  to  expression  character  to  expr    removes  trailing  space  characters  from  the  given  string  public    out  type  trim  trailing  return  to  api  specific  expression  unresolved  call  trim  value  literal  false  value  literal  true  value  literal  to  expr    removes  trailing  characters  from  the  given  string  param  character  string  containing  the  character  public    out  type  trim  trailing    in  type  character  return  to  api  specific  expression  unresolved  call  trim  value  literal  false  value  literal  true  object  to  expression  character  to  expr    removes  leading  and  trailing  space  characters  from  the  given  string  public    out  type  trim  return  to  api  specific  expression  unresolved  call  trim  value  literal  true  value  literal  true  value  literal  to  expr    removes  leading  and  trailing  characters  from  the  given  string  param  character  string  containing  the  character  public    out  type  trim    in  type  character  return  to  api  specific  expression  unresolved  call  trim  value  literal  true  value  literal  true  object  to  expression  character  to  expr    returns  a  new  string  which  replaces  all  the  occurrences  of  the  search  target  with  the  replacement  string  non  overlapping  public    out  type  replace    in  type  search    in  type  replacement  return  to  api  specific  expression  unresolved  call  replace  to  expr  object  to  expression  search  object  to  expression  replacement    returns  the  length  of  a  string  public    out  type  char  length  return  to  api  specific  expression  unresolved  call  char  length  to  expr    returns  all  of  the  characters  in  a  string  in  upper  case  using  the  rules  of  the  default  locale  public    out  type  upper  case  return  to  api  specific  expression  unresolved  call  upper  to  expr    returns  all  of  the  characters  in  a  string  in  lower  case  using  the  rules  of  the  default  locale  public    out  type  lower  case  return  to  api  specific  expression  unresolved  call  lower  to  expr    converts  the  initial  letter  of  each  word  in  a  string  to  uppercase    assumes  a  string  containing  only  a    za  z0    everything  else  is  treated  as  whitespace  public    out  type  init  cap  return  to  api  specific  expression  unresolved  call  init  cap  to  expr    returns  true  if  a  string  matches  the  specified  like  pattern  p  e  g    jo  n  matches  all  strings  that  start  with    jo  arbitrary  letter  n  public    out  type  like    in  type  pattern  return  to  api  specific  expression  unresolved  call  like  to  expr  object  to  expression  pattern    returns  true  if  a  string  matches  the  specified  sql  regex  pattern  p  e  g  a  matches  all  strings  that  consist  of  at  least  one  a  public    out  type  similar    in  type  pattern  return  to  api  specific  expression  unresolved  call  similar  to  expr  object  to  expression  pattern    returns  the  position  of  string  in  an  other  string  starting  at      returns    if  string  could  not  be  found  p  e  g  lit  a  position  bbbbba  leads  to    public    out  type  position    in  type  haystack  return  to  api  specific  expression  unresolved  call  position  to  expr  object  to  expression  haystack    returns  a  string  left  padded  with  the  given  pad  string  to  a  length  of  len  characters    if  the  string  is  longer  than  len  the  return  value  is  shortened  to  len  characters  p  e  g  lit  hi  lpad    returns  hi  lit  hi  lpad    returns  h  public    out  type  lpad    in  type  len    in  type  pad  return  to  api  specific  expression  unresolved  call  lpad  to  expr  object  to  expression  len  object  to  expression  pad    returns  a  string  right  padded  with  the  given  pad  string  to  a  length  of  len  characters    if  the  string  is  longer  than  len  the  return  value  is  shortened  to  len  characters  p  e  g  lit  hi  rpad    returns  hi  lit  hi  rpad    returns  h  public    out  type  rpad    in  type  len    in  type  pad  return  to  api  specific  expression  unresolved  call  rpad  to  expr  object  to  expression  len  object  to  expression  pad    defines  an  aggregation  to  be  used  for  a  previously  specified  over  window  p    for  example  pre  code  table  window    over  partition  by  c  order  by  rowtime  preceding    rows  following  current  row  as  w  select  c  a  a  count  over  w  a  sum  over  w  pre  public    out  type  over    in  type  alias  return  to  api  specific  expression  unresolved  call  over  to  expr  object  to  expression  alias    replaces  a  substring  of  string  with  a  string  starting  at  a  position  starting  at    p  e  g  lit  xxxxxtest  overlay  xxxx    leads  to  xxxxxxxxx  public    out  type  overlay    in  type  new  string    in  type  starting  return  to  api  specific  expression  unresolved  call  overlay  to  expr  object  to  expression  new  string  object  to  expression  starting    replaces  a  substring  of  string  with  a  string  starting  at  a  position  starting  at      the  length  specifies  how  many  characters  should  be  removed  p  e  g  lit  xxxxxtest  overlay  xxxx      leads  to  xxxxxxxxxst  public    out  type  overlay    in  type  new  string    in  type  starting    in  type  length  return  to  api  specific  expression  unresolved  call  overlay  to  expr  object  to  expression  new  string  object  to  expression  starting  object  to  expression  length    returns  a  string  with  all  substrings  that  match  the  regular  expression  consecutively  being  replaced  public    out  type  regexp  replace    in  type  regex    in  type  replacement  return  to  api  specific  expression  unresolved  call  regexp  replace  to  expr  object  to  expression  regex  object  to  expression  replacement    returns  a  string  extracted  with  a  specified  regular  expression  and  a  regex  match  group  index  public    out  type  regexp  extract    in  type  regex    in  type  extract  index  return  to  api  specific  expression  unresolved  call  regexp  extract  to  expr  object  to  expression  regex  object  to  expression  extract  index    returns  a  string  extracted  with  a  specified  regular  expression  public    out  type  regexp  extract    in  type  regex  return  to  api  specific  expression  unresolved  call  regexp  extract  to  expr  object  to  expression  regex    returns  the  base  string  decoded  with  base64  public    out  type  from  base64  return  to  api  specific  expression  unresolved  call  from    b  a  s  e64  to  expr    returns  the  base64  encoded  result  of  the  input  string  public    out  type  to  base64  return  to  api  specific  expression  unresolved  call  to    b  a  s  e64  to  expr    returns  a  string  that  removes  the  left  whitespaces  from  the  given  string  public    out  type  ltrim  return  to  api  specific  expression  unresolved  call  ltrim  to  expr    returns  a  string  that  removes  the  right  whitespaces  from  the  given  string  public    out  type  rtrim  return  to  api  specific  expression  unresolved  call  rtrim  to  expr    returns  a  string  that  repeats  the  base  string  n  times  public    out  type  repeat    in  type  n  return  to  api  specific  expression  unresolved  call  repeat  to  expr  object  to  expression  n    temporal  operations    parses  a  date  string  in  the  form  yyyy  mm  dd  to  a  sql    date  public    out  type  to  date  return  to  api  specific  expression  unresolved  call  cast  to  expr  type  literal  from  legacy  info  to  data  type    sql  time  type  info  date    parses  a  time  string  in  the  form  hh  mm  ss  to  a  sql    time  public    out  type  to  time  return  to  api  specific  expression  unresolved  call  cast  to  expr  type  literal  from  legacy  info  to  data  type    sql  time  type  info  time    parses  a  timestamp  string  in  the  form  yyyy  mm  dd  hh  mm  ss  sss  to  a  sql    timestamp  public    out  type  to  timestamp  return  to  api  specific  expression  unresolved  call  cast  to  expr  type  literal  from  legacy  info  to  data  type    sql  time  type  info  timestamp    extracts  parts  of  a  time  point  or  time  interval    returns  the  part  as  a  long  value  p  e  g  lit  2006-06    to  date  extract  day  leads  to    public    out  type  extract    time  interval  unit  time  interval  unit  return  to  api  specific  expression  unresolved  call  extract  value  literal  time  interval  unit  to  expr    rounds  down  a  time  point  to  the  given  unit  p  e  g  lit  12:44    to  date  floor  minute  leads  to  12:44    public    out  type  floor    time  interval  unit  time  interval  unit  return  to  api  specific  expression  unresolved  call  floor  to  expr  value  literal  time  interval  unit    rounds  up  a  time  point  to  the  given  unit  p  e  g  lit  12:44    to  date  ceil  minute  leads  to  12:45    public    out  type  ceil    time  interval  unit  time  interval  unit  return  to  api  specific  expression  unresolved  call  ceil  to  expr  value  literal  time  interval  unit    advanced  type  helper  functions    accesses  the  field  of  a    flink  composite  type  such  as    tuple  pojo  etc  by  name  and  returns  it  s  value  param  name  name  of  the  field  similar  to    flink  s  field  expressions  public    out  type  get    string  name  return  to  api  specific  expression  unresolved  call  get  to  expr  value  literal  name    accesses  the  field  of  a    flink  composite  type  such  as    tuple  pojo  etc  by  index  and  returns  it  s  value  param  index  position  of  the  field  public    out  type  get  int  index  return  to  api  specific  expression  unresolved  call  get  to  expr  value  literal  index    converts  a    flink  composite  type  such  as    tuple  pojo  etc  and  all  of  its  direct  subtypes  into  a  flat  representation  where  every  subtype  is  a  separate  field  public    out  type  flatten  return  to  api  specific  expression  unresolved  call  flatten  to  expr    accesses  the  element  of  an  array  or  map  based  on  a  key  or  an  index  starting  at    param  index  key  or  position  of  the  element  array  index  starting  at    public    out  type  at    in  type  index  return  to  api  specific  expression  unresolved  call  at  to  expr  object  to  expression  index    returns  the  number  of  elements  of  an  array  or  number  of  entries  of  a  map  public    out  type  cardinality  return  to  api  specific  expression  unresolved  call  cardinality  to  expr    returns  the  sole  element  of  an  array  with  a  single  element    returns  null  if  the  array  is  empty    throws  an  exception  if  the  array  has  more  than  one  element  public    out  type  element  return  to  api  specific  expression  unresolved  call  array  element  to  expr    time  definition    declares  a  field  as  the  rowtime  attribute  for  indicating  accessing  and  working  in    flink  s  event  time  public    out  type  rowtime  return  to  api  specific  expression  unresolved  call  rowtime  to  expr    declares  a  field  as  the  proctime  attribute  for  indicating  accessing  and  working  in    flink  s  processing  time  public    out  type  proctime  return  to  api  specific  expression  unresolved  call  proctime  to  expr    creates  an  interval  of  the  given  number  of  years  p    the  produced  expression  is  of  type  code    data  types  interval  public    out  type  year  return  to  api  specific  expression  to  month  interval  to  expr      creates  an  interval  of  the  given  number  of  years  public    out  type  years  return  year    creates  an  interval  of  the  given  number  of  quarters  public    out  type  quarter  return  to  api  specific  expression  to  month  interval  to  expr      creates  an  interval  of  the  given  number  of  quarters  public    out  type  quarters  return  quarter    creates  an  interval  of  the  given  number  of  months  public    out  type  month  return  to  api  specific  expression  to  month  interval  to  expr      creates  an  interval  of  the  given  number  of  months  public    out  type  months  return  month    creates  an  interval  of  the  given  number  of  weeks  public    out  type  week  return  to  api  specific  expression  to  milli  interval  to  expr    millis  per  day    creates  an  interval  of  the  given  number  of  weeks  public    out  type  weeks  return  week    creates  an  interval  of  the  given  number  of  days  public    out  type  day  return  to  api  specific  expression  to  milli  interval  to  expr  millis  per  day    creates  an  interval  of  the  given  number  of  days  public    out  type  days  return  day    creates  an  interval  of  the  given  number  of  hours  public    out  type  hour  return  to  api  specific  expression  to  milli  interval  to  expr  millis  per  hour    creates  an  interval  of  the  given  number  of  hours  public    out  type  hours  return  hour    creates  an  interval  of  the  given  number  of  minutes  public    out  type  minute  return  to  api  specific  expression  to  milli  interval  to  expr  millis  per  minute    creates  an  interval  of  the  given  number  of  minutes  public    out  type  minutes  return  minute    creates  an  interval  of  the  given  number  of  seconds  public    out  type  second  return  to  api  specific  expression  to  milli  interval  to  expr  millis  per  second    creates  an  interval  of  the  given  number  of  seconds  public    out  type  seconds  return  second    creates  an  interval  of  the  given  number  of  milliseconds  public    out  type  milli  return  to  api  specific  expression  to  milli  interval  to  expr      creates  an  interval  of  the  given  number  of  milliseconds  public    out  type  millis  return  milli    hash  functions    returns  the    m  d5  hash  of  the  string  argument  null  if  string  is  null  return  string  of    hexadecimal  digits  or  null  public    out  type  md5  return  to  api  specific  expression  unresolved  call    m  d5  to  expr    returns  the  sha    hash  of  the  string  argument  null  if  string  is  null  return  string  of    hexadecimal  digits  or  null  public    out  type  sha1  return  to  api  specific  expression  unresolved  call    s  h  a1  to  expr    returns  the  sha    hash  of  the  string  argument  null  if  string  is  null  return  string  of    hexadecimal  digits  or  null  public    out  type  sha224  return  to  api  specific  expression  unresolved  call    s  h  a224  to  expr    returns  the  sha    hash  of  the  string  argument  null  if  string  is  null  return  string  of    hexadecimal  digits  or  null  public    out  type  sha256  return  to  api  specific  expression  unresolved  call    s  h  a256  to  expr    returns  the  sha    hash  of  the  string  argument  null  if  string  is  null  return  string  of    hexadecimal  digits  or  null  public    out  type  sha384  return  to  api  specific  expression  unresolved  call    s  h  a384  to  expr    returns  the  sha    hash  of  the  string  argument  null  if  string  is  null  return  string  of    hexadecimal  digits  or  null  public    out  type  sha512  return  to  api  specific  expression  unresolved  call    s  h  a512  to  expr    returns  the  hash  for  the  given  string  expression  using  the  sha    family  of  hash  functions  sha    sha    sha    or  sha    param  hash  length  bit  length  of  the  result  either        or    return  string  or  null  if  one  of  the  arguments  is  null  public    out  type  sha2    in  type  hash  length  return  to  api  specific  expression  unresolved  call    s  h  a2  to  expr  object  to  expression  hash  length  
public  evolving  public  final  class    over    partitions  the  elements  on  some  partition  keys  p    each  partition  is  individually  sorted  and  aggregate  functions  are  applied  to  each  partition  separately  param  partition  by  list  of  field  references  return  an  over  window  with  defined  partitioning  public  static    over  window  partitioned  partition  by    string  partition  by  return  new    over  window  partitioned    expression  parser  parse  expression  list  partition  by    partitions  the  elements  on  some  partition  keys  p    each  partition  is  individually  sorted  and  aggregate  functions  are  applied  to  each  partition  separately  param  partition  by  list  of  field  references  return  an  over  window  with  defined  partitioning  public  static    over  window  partitioned  partition  by    expression  partition  by  return  new    over  window  partitioned    arrays  as  list  partition  by    specifies  the  time  attribute  on  which  rows  are  ordered  p    for  streaming  tables  reference  a  rowtime  or  proctime  time  attribute  here  to  specify  the  time  mode  p    for  batch  tables  refer  to  a  timestamp  or  long  attribute  param  order  by  field  reference  return  an  over  window  with  defined  order  deprecated  use  link  order  by    expression    deprecated  public  static    over  window  partitioned  ordered  order  by    string  order  by  return  partition  by  order  by  order  by    specifies  the  time  attribute  on  which  rows  are  ordered  p    for  streaming  tables  reference  a  rowtime  or  proctime  time  attribute  here  to  specify  the  time  mode  p    for  batch  tables  refer  to  a  timestamp  or  long  attribute  param  order  by  field  reference  return  an  over  window  with  defined  order  public  static    over  window  partitioned  ordered  order  by    expression  order  by  return  partition  by  order  by  order  by  
public  evolving  public  final  class    over  window  private  final    expression  alias  private  final    list    expression  partitioning  private  final    expression  order  private  final    expression  preceding  private  final    optional    expression  following    over  window    expression  alias    list    expression  partition  by    expression  order  by    expression  preceding    optional    expression  following  this  alias  alias  this  partitioning  partition  by  this  order  order  by  this  preceding  preceding  this  following  following  public    expression  get  alias  return  alias  public    list    expression  get  partitioning  return  partitioning  public    expression  get  order  return  order  public    expression  get  preceding  return  preceding  public    optional    expression  get  following  return  following  
public  evolving  public  interface    over  windowed  table    performs  a  selection  operation  on  a  over  windowed  table    similar  to  an  sql  select  statement    the  field  expressions  can  contain  complex  expressions  and  aggregations  p    example  pre  code  over  windowed  table  select  c  b  count  over  ow  e  sum  over  ow  pre  deprecated  use  link  select    expression    deprecated    table  select    string  fields    performs  a  selection  operation  on  a  over  windowed  table    similar  to  an  sql  select  statement    the  field  expressions  can  contain  complex  expressions  and  aggregations  p    example  pre  code  over  windowed  table  select  c  b  count  over  ow  e  sum  over  ow  pre  p    scala    example  pre  code  over  windowed  table  select  c  b  count  over  ow  e  sum  over  ow  pre    table  select    expression  fields  
public  evolving  public  final  class    over  window  partitioned    defines  a  partitioning  of  the  input  on  one  or  more  attributes  private  final    list    expression  partition  by    over  window  partitioned    list    expression  partition  by  this  partition  by  partition  by    specifies  the  time  attribute  on  which  rows  are  ordered  p    for  streaming  tables  reference  a  rowtime  or  proctime  time  attribute  here  to  specify  the  time  mode  p    for  batch  tables  refer  to  a  timestamp  or  long  attribute  param  order  by  field  reference  return  an  over  window  with  defined  order  deprecated  use  link  order  by    expression    deprecated  public    over  window  partitioned  ordered  order  by    string  order  by  return  this  order  by    expression  parser  parse  expression  order  by    specifies  the  time  attribute  on  which  rows  are  ordered  p    for  streaming  tables  reference  a  rowtime  or  proctime  time  attribute  here  to  specify  the  time  mode  p    for  batch  tables  refer  to  a  timestamp  or  long  attribute  param  order  by  field  reference  return  an  over  window  with  defined  order  public    over  window  partitioned  ordered  order  by    expression  order  by  return  new    over  window  partitioned  ordered  partition  by  order  by  
public  evolving  public  final  class    over  window  partitioned  ordered  private  final    list    expression  partition  by  private  final    expression  order  by    over  window  partitioned  ordered    list    expression  partition  by    expression  order  by  this  partition  by  partition  by  this  order  by  order  by    set  the  preceding  offset  based  on  time  or  row  count  intervals  for  over  window  param  preceding  preceding  offset  relative  to  the  current  row  return  an  over  window  with  defined  preceding  deprecated  use  link  preceding    expression    deprecated  public    over  window  partitioned  ordered  preceding  preceding    string  preceding  return  this  preceding    expression  parser  parse  expression  preceding    set  the  preceding  offset  based  on  time  or  row  count  intervals  for  over  window  param  preceding  preceding  offset  relative  to  the  current  row  return  an  over  window  with  defined  preceding  public    over  window  partitioned  ordered  preceding  preceding    expression  preceding  return  new    over  window  partitioned  ordered  preceding  partition  by  order  by  preceding    assigns  an  alias  for  this  window  that  the  following  code  select  clause  can  refer  to  param  alias  alias  for  this  over  window  return  the  fully  defined  over  window  public    over  window  as    string  alias  return  as    expression  parser  parse  expression  alias    assigns  an  alias  for  this  window  that  the  following  code  select  clause  can  refer  to  param  alias  alias  for  this  over  window  return  the  fully  defined  over  window  public    over  window  as    expression  alias  return  new    over  window  alias  partition  by  order  by  unresolved  call    built  in  function  definitions  unbounded  range    optional  empty  
public  evolving  public  final  class    over  window  partitioned  ordered  preceding  private  final    list    expression  partition  by  private  final    expression  order  by  private  final    expression  preceding  private    optional    expression  optional  following    optional  empty    over  window  partitioned  ordered  preceding    list    expression  partition  by    expression  order  by    expression  preceding  this  partition  by  partition  by  this  order  by  order  by  this  preceding  preceding    assigns  an  alias  for  this  window  that  the  following  code  select  clause  can  refer  to  param  alias  alias  for  this  over  window  return  the  fully  defined  over  window  public    over  window  as    string  alias  return  as    expression  parser  parse  expression  alias    assigns  an  alias  for  this  window  that  the  following  code  select  clause  can  refer  to  param  alias  alias  for  this  over  window  return  the  fully  defined  over  window  public    over  window  as    expression  alias  return  new    over  window  alias  partition  by  order  by  preceding  optional  following    set  the  following  offset  based  on  time  or  row  count  intervals  for  over  window  param  following  following  offset  that  relative  to  the  current  row  return  an  over  window  with  defined  following  deprecated  use  link  following    expression    deprecated  public    over  window  partitioned  ordered  preceding  following    string  following  return  this  following    expression  parser  parse  expression  following    set  the  following  offset  based  on  time  or  row  count  intervals  for  over  window  param  following  following  offset  that  relative  to  the  current  row  return  an  over  window  with  defined  following  public    over  window  partitioned  ordered  preceding  following    expression  following  optional  following    optional  of  following  return  this  
public  evolving  public  interface    planner  config    planner  config  empty  config  new    planner  config    suppress  warnings  unchecked  default  t  extends    planner  config    optional  t  unwrap    class  t  type  if  type  is  instance  this  return    optional  of  t  this  else  return    optional  empty  
public  evolving  public  enum    result  kind    the  statement  e  g  ddl  use  executes  successfully  and  the  result  only  contains  a  simple  ok  success    the  statement  e  g  dml  dql  show  executes  successfully  and  the  result  contains  important  content  success  with  content  
public  evolving  public  final  class    session    creates  a  session  window    the  boundary  of  session  windows  are  defined  by  intervals  of  inactivity  i  e  a  session  window  is  closes  if  no  event  appears  for  a  defined  gap  period  param  gap  specifies  how  long  as  interval  of  milliseconds  to  wait  for  new  data  before  closing  the  session  window  return  a  partially  defined  session  window  deprecated  use  link  with  gap    expression    deprecated  public  static    session  with  gap  with  gap    string  gap  return  with  gap    expression  parser  parse  expression  gap    creates  a  session  window    the  boundary  of  session  windows  are  defined  by  intervals  of  inactivity  i  e  a  session  window  is  closes  if  no  event  appears  for  a  defined  gap  period  param  gap  specifies  how  long  as  interval  of  milliseconds  to  wait  for  new  data  before  closing  the  session  window  return  a  partially  defined  session  window  public  static    session  with  gap  with  gap    expression  gap  return  new    session  with  gap  gap  
public  evolving  public  final  class    session  with  gap    the  time  interval  of  inactivity  before  a  window  is  closed  private  final    expression  gap    session  with  gap    expression  gap  this  gap    api  expression  utils  unwrap  from  api  gap    specifies  the  time  attribute  on  which  rows  are  grouped  p    for  streaming  tables  you  can  specify  grouping  by  a  event  time  or  processing  time  attribute  p    for  batch  tables  you  can  specify  grouping  on  a  timestamp  or  long  attribute  param  time  field  time  attribute  for  streaming  and  batch  tables  return  a  tumbling  window  on  event  time  deprecated  use  link  on    expression    deprecated  public    session  with  gap  on  time  on    string  time  field  return  on    expression  parser  parse  expression  time  field    specifies  the  time  attribute  on  which  rows  are  grouped  p    for  streaming  tables  you  can  specify  grouping  by  a  event  time  or  processing  time  attribute  p    for  batch  tables  you  can  specify  grouping  on  a  timestamp  or  long  attribute  param  time  field  time  attribute  for  streaming  and  batch  tables  return  a  tumbling  window  on  event  time  public    session  with  gap  on  time  on    expression  time  field  return  new    session  with  gap  on  time  time  field  gap  
public  evolving  public  final  class    session  with  gap  on  time  private  final    expression  time  field  private  final    expression  gap    session  with  gap  on  time    expression  time  field    expression  gap  this  time  field    api  expression  utils  unwrap  from  api  time  field  this  gap    api  expression  utils  unwrap  from  api  gap    assigns  an  alias  for  this  window  that  the  following  code  group  by  and  code  select  clause  can  refer  to  code  select  statement  can  access  window  properties  such  as  window  start  or  end  time  param  alias  alias  for  this  window  return  this  window  public    session  with  gap  on  time  with  alias  as    string  alias  return  as    expression  parser  parse  expression  alias    assigns  an  alias  for  this  window  that  the  following  code  group  by  and  code  select  clause  can  refer  to  code  select  statement  can  access  window  properties  such  as  window  start  or  end  time  param  alias  alias  for  this  window  return  this  window  public    session  with  gap  on  time  with  alias  as    expression  alias  return  new    session  with  gap  on  time  with  alias  alias  time  field  gap  
public  evolving  public  final  class    session  with  gap  on  time  with  alias  extends    group  window  private  final    expression  gap    session  with  gap  on  time  with  alias    expression  alias    expression  time  field    expression  gap  super  alias  time  field  this  gap    api  expression  utils  unwrap  from  api  gap  public    expression  get  gap  return  gap  
public  evolving  public  final  class    slide    creates  a  sliding  window    sliding  windows  have  a  fixed  size  and  slide  by  a  specified  slide  interval    if  the  slide  interval  is  smaller  than  the  window  size  sliding  windows  are  overlapping    thus  an  element  can  be  assigned  to  multiple  windows  p    for  example  a  sliding  window  of  size    minutes  with    minutes  sliding  interval  groups  elements  of    minutes  and  evaluates  every  five  minutes    each  element  is  contained  in  three  consecutive  window  evaluations  param  size  the  size  of  the  window  as  time  or  row  count  interval  return  a  partially  specified  sliding  window  deprecated  use  link  over    expression    deprecated  public  static    slide  with  size  over    string  size  return  over    expression  parser  parse  expression  size    creates  a  sliding  window    sliding  windows  have  a  fixed  size  and  slide  by  a  specified  slide  interval    if  the  slide  interval  is  smaller  than  the  window  size  sliding  windows  are  overlapping    thus  an  element  can  be  assigned  to  multiple  windows  p    for  example  a  sliding  window  of  size    minutes  with    minutes  sliding  interval  groups  elements  of    minutes  and  evaluates  every  five  minutes    each  element  is  contained  in  three  consecutive  param  size  the  size  of  the  window  as  time  or  row  count  interval  return  a  partially  specified  sliding  window  public  static    slide  with  size  over    expression  size  return  new    slide  with  size  size  
public  evolving  public  final  class    slide  with  size    the  size  of  the  window  either  as  time  or  row  count  interval  private  final    expression  size    slide  with  size    expression  size  this  size    api  expression  utils  unwrap  from  api  size    specifies  the  window  s  slide  as  time  or  row  count  interval  p    the  slide  determines  the  interval  in  which  windows  are  started    hence  sliding  windows  can  overlap  if  the  slide  is  smaller  than  the  size  of  the  window  p    for  example  you  could  have  windows  of  size    minutes  that  slide  by    minutes    with  this    minutes  worth  of  elements  are  grouped  every    minutes  and  each  row  contributes  to    windows  param  slide  the  slide  of  the  window  either  as  time  or  row  count  interval  return  a  sliding  window  deprecated  use  link  every    expression    deprecated  public    slide  with  size  and  slide  every    string  slide  return  every    expression  parser  parse  expression  slide    specifies  the  window  s  slide  as  time  or  row  count  interval  p    the  slide  determines  the  interval  in  which  windows  are  started    hence  sliding  windows  can  overlap  if  the  slide  is  smaller  than  the  size  of  the  window  p    for  example  you  could  have  windows  of  size    minutes  that  slide  by    minutes    with  this    minutes  worth  of  elements  are  grouped  every    minutes  and  each  row  contributes  to    windows  param  slide  the  slide  of  the  window  either  as  time  or  row  count  interval  return  a  sliding  window  public    slide  with  size  and  slide  every    expression  slide  return  new    slide  with  size  and  slide  size  slide  
public  evolving  public  final  class    slide  with  size  and  slide    the  size  of  the  window  either  as  time  or  row  count  interval  private  final    expression  size  private  final    expression  slide    slide  with  size  and  slide    expression  size    expression  slide  this  size    api  expression  utils  unwrap  from  api  size  this  slide    api  expression  utils  unwrap  from  api  slide    specifies  the  time  attribute  on  which  rows  are  grouped  p    for  streaming  tables  you  can  specify  grouping  by  a  event  time  or  processing  time  attribute  p    for  batch  tables  you  can  specify  grouping  on  a  timestamp  or  long  attribute  param  time  field  time  attribute  for  streaming  and  batch  tables  return  a  tumbling  window  on  event  time  deprecated  use  link  on    expression    deprecated  public    slide  with  size  and  slide  on  time  on    string  time  field  return  on    expression  parser  parse  expression  time  field    specifies  the  time  attribute  on  which  rows  are  grouped  p    for  streaming  tables  you  can  specify  grouping  by  a  event  time  or  processing  time  attribute  p    for  batch  tables  you  can  specify  grouping  on  a  timestamp  or  long  attribute  param  time  field  time  attribute  for  streaming  and  batch  tables  return  a  tumbling  window  on  event  time  public    slide  with  size  and  slide  on  time  on    expression  time  field  return  new    slide  with  size  and  slide  on  time  time  field  size  slide  
public  evolving  public  final  class    slide  with  size  and  slide  on  time  private  final    expression  time  field  private  final    expression  size  private  final    expression  slide    slide  with  size  and  slide  on  time    expression  time  field    expression  size    expression  slide  this  time  field    api  expression  utils  unwrap  from  api  time  field  this  size    api  expression  utils  unwrap  from  api  size  this  slide    api  expression  utils  unwrap  from  api  slide    assigns  an  alias  for  this  window  that  the  following  code  group  by  and  code  select  clause  can  refer  to  code  select  statement  can  access  window  properties  such  as  window  start  or  end  time  param  alias  alias  for  this  window  return  this  window  public    slide  with  size  and  slide  on  time  with  alias  as    string  alias  return  as    expression  parser  parse  expression  alias    assigns  an  alias  for  this  window  that  the  following  code  group  by  and  code  select  clause  can  refer  to  code  select  statement  can  access  window  properties  such  as  window  start  or  end  time  param  alias  alias  for  this  window  return  this  window  public    slide  with  size  and  slide  on  time  with  alias  as    expression  alias  return  new    slide  with  size  and  slide  on  time  with  alias  alias  time  field  size  slide  
public  evolving  public  final  class    slide  with  size  and  slide  on  time  with  alias  extends    group  window  private  final    expression  size  private  final    expression  slide    slide  with  size  and  slide  on  time  with  alias    expression  alias    expression  time  field    expression  size    expression  slide  super  alias  time  field  this  size    api  expression  utils  unwrap  from  api  size  this  slide    api  expression  utils  unwrap  from  api  slide  public    expression  get  size  return  size  public    expression  get  slide  return  slide  
public  evolving  public  enum    sql  dialect    flink  s  default  sql  behavior  default  sql  dialect  that  allows  some    apache    hive  specific  grammar  p    note    we  might  never  support  all  of  the    hive  grammar    see  the  documentation  for  supported  features  hive  
public  evolving  public  class    sql  parser  exception  extends    runtime  exception  public    sql  parser  exception    string  message    throwable  cause  super  message  cause  public    sql  parser  exception    string  message  super  message  
public  evolving  public  interface    statement  set  add  insert  statement  to  the  set    statement  set  add  insert  sql    string  statement  add    table  with  the  given  sink  table  name  to  the  set    statement  set  add  insert    string  target  path    table  table  add  link    table  with  the  given  sink  table  name  to  the  set    statement  set  add  insert    string  target  path    table  table  boolean  overwrite  returns  the  ast  and  the  execution  plan  to  compute  the  result  of  the  all  statements  and    tables  param  extra  details    the  extra  explain  details  which  the  explain  result  should  include  e  g  estimated  cost  changelog  mode  for  streaming  return  ast  and  the  execution  plan    string  explain    explain  detail  extra  details  execute  all  statements  and    tables  as  a  batch  p    the  added  statements  and    tables  will  be  cleared  when  executing  this  method    table  result  execute  
public  evolving  public  interface    table    returns  the  schema  of  this  table    table  schema  get  schema    prints  the  schema  of  this  table  to  the  console  in  a  tree  format  void  print  schema    returns  underlying  logical  representation  of  this  table    query  operation  get  query  operation    performs  a  selection  operation    similar  to  a  sql  select  statement    the  field  expressions  can  contain  complex  expressions  and  aggregations  p    example  pre  code  tab  select  key  value  avg    the  average  as  average  pre  deprecated  use  link  select    expression    deprecated    table  select    string  fields    performs  a  selection  operation    similar  to  a  sql  select  statement    the  field  expressions  can  contain  complex  expressions  and  aggregations  p    scala    example  pre  code  tab  select  key  value  avg  plus    the  average  as  average  pre  p    scala    example  pre  code  tab  select  key  value  avg    the  average  as  average  pre    table  select    expression  fields    creates  link    temporal  table  function  backed  up  by  this  table  as  a  history  table    temporal    tables  represent  a  concept  of  a  table  that  changes  over  time  and  for  which    flink  keeps  track  of  those  changes  link    temporal  table  function  provides  a  way  how  to  access  those  data  p    for  more  information  please  check    flink  s  documentation  on    temporal    tables  p    currently  link    temporal  table  function  s  are  only  supported  in  streaming  param  time  attribute    must  points  to  a  time  attribute    provides  a  way  to  compare  which  records  are  a  newer  or  older  version  param  primary  key    defines  the  primary  key    with  primary  key  it  is  possible  to  update  a  row  or  to  delete  it  return  link    temporal  table  function  which  is  an  instance  of  link    table  function    it  takes  one  single  argument  the  code  time  attribute  for  which  it  returns  matching  version  of  the  link    table  from  which  link    temporal  table  function  was  created  deprecated  use  link  create  temporal  table  function    expression    expression    deprecated    temporal  table  function  create  temporal  table  function    string  time  attribute    string  primary  key    creates  link    temporal  table  function  backed  up  by  this  table  as  a  history  table    temporal    tables  represent  a  concept  of  a  table  that  changes  over  time  and  for  which    flink  keeps  track  of  those  changes  link    temporal  table  function  provides  a  way  how  to  access  those  data  p    for  more  information  please  check    flink  s  documentation  on    temporal    tables  p    currently  link    temporal  table  function  s  are  only  supported  in  streaming  param  time  attribute    must  points  to  a  time  indicator    provides  a  way  to  compare  which  records  are  a  newer  or  older  version  param  primary  key    defines  the  primary  key    with  primary  key  it  is  possible  to  update  a  row  or  to  delete  it  return  link    temporal  table  function  which  is  an  instance  of  link    table  function    it  takes  one  single  argument  the  code  time  attribute  for  which  it  returns  matching  version  of  the  link    table  from  which  link    temporal  table  function  was  created    temporal  table  function  create  temporal  table  function    expression  time  attribute    expression  primary  key    renames  the  fields  of  the  expression  result    use  this  to  disambiguate  fields  before  joining  to  operations  p    example  pre  code  tab  as  a  b  pre    table  as    string  field    string  fields    renames  the  fields  of  the  expression  result    use  this  to  disambiguate  fields  before  joining  to  operations  p    example  pre  code  tab  as  a  b  pre  p    scala    example  pre  code  tab  as  a  b  pre  deprecated  use  link  as    string    string    deprecated    table  as    expression  fields    filters  out  elements  that  don  t  pass  the  filter  predicate    similar  to  a  sql  where  clause  p    example  pre  code  tab  filter  name    fred  pre  deprecated  use  link  filter    expression    deprecated    table  filter    string  predicate    filters  out  elements  that  don  t  pass  the  filter  predicate    similar  to  a  sql  where  clause  p    example  pre  code  tab  filter  name  is  equal    fred  pre  p    scala    example  pre  code  tab  filter  name    fred  pre    table  filter    expression  predicate    filters  out  elements  that  don  t  pass  the  filter  predicate    similar  to  a  sql  where  clause  p    example  pre  code  tab  where  name    fred  pre  deprecated  use  link  where    expression    deprecated    table  where    string  predicate    filters  out  elements  that  don  t  pass  the  filter  predicate    similar  to  a  sql  where  clause  p    example  pre  code  tab  where  name  is  equal    fred  pre  p    scala    example  pre  code  tab  where  name    fred  pre    table  where    expression  predicate    groups  the  elements  on  some  grouping  keys    use  this  before  a  selection  with  aggregations  to  perform  the  aggregation  on  a  per  group  basis    similar  to  a  sql  group  by  statement  p    example  pre  code  tab  group  by  key  select  key  value  avg  pre  deprecated  use  link  group  by    expression    deprecated    grouped  table  group  by    string  fields    groups  the  elements  on  some  grouping  keys    use  this  before  a  selection  with  aggregations  to  perform  the  aggregation  on  a  per  group  basis    similar  to  a  sql  group  by  statement  p    scala    example  pre  code  tab  group  by  key  select  key  value  avg  pre  p    scala    example  pre  code  tab  group  by  key  select  key  value  avg  pre    grouped  table  group  by    expression  fields    removes  duplicate  values  and  returns  only  distinct  different  values  p    example  pre  code  tab  select  key  value  distinct  pre    table  distinct    joins  two  link    table  s    similar  to  a  sql  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary    you  can  use  where  and  select  clauses  after  a  join  to  further  specify  the  behaviour  of  the  join  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  join  right  where  a  is  equal  b  and  c  is  greater    select  a  b  d  pre    table  join    table  right    joins  two  link    table  s    similar  to  a  sql  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  join  right  a  b  pre  deprecated  use  link  join    table    expression    deprecated    table  join    table  right    string  join  predicate    joins  two  link    table  s    similar  to  a  sql  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  join  right  a  is  equal  b  select  a  b  d  pre  p    scala    example  pre  code  left  join  right  a  b  select  a  b  d  pre    table  join    table  right    expression  join  predicate    joins  two  link    table  s    similar  to  a  sql  left  outer  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  and  its  code    table  config  must  have  null  check  enabled  default  p    example  pre  code  left  left  outer  join  right  select  a  b  d  pre    table  left  outer  join    table  right    joins  two  link    table  s    similar  to  a  sql  left  outer  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  and  its  code    table  config  must  have  null  check  enabled  default  p    example  pre  code  left  left  outer  join  right  a  b  select  a  b  d  pre  deprecated  use  link  left  outer  join    table    expression    deprecated    table  left  outer  join    table  right    string  join  predicate    joins  two  link    table  s    similar  to  a  sql  left  outer  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  and  its  code    table  config  must  have  null  check  enabled  default  p    example  pre  code  left  left  outer  join  right  a  is  equal  b  select  a  b  d  pre  p    scala    example  pre  code  left  left  outer  join  right  a  b  select  a  b  d  pre    table  left  outer  join    table  right    expression  join  predicate    joins  two  link    table  s    similar  to  a  sql  right  outer  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  and  its  code    table  config  must  have  null  check  enabled  default  p    example  pre  code  left  right  outer  join  right  a  b  select  a  b  d  pre  deprecated  use  link  right  outer  join    table    expression    deprecated    table  right  outer  join    table  right    string  join  predicate    joins  two  link    table  s    similar  to  a  sql  right  outer  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  and  its  code    table  config  must  have  null  check  enabled  default  p    example  pre  code  left  right  outer  join  right  a  is  equal  b  select  a  b  d  pre  p    scala    example  pre  code  left  right  outer  join  right  a  b  select  a  b  d  pre    table  right  outer  join    table  right    expression  join  predicate    joins  two  link    table  s    similar  to  a  sql  full  outer  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  and  its  code    table  config  must  have  null  check  enabled  default  p    example  pre  code  left  full  outer  join  right  a  b  select  a  b  d  pre  deprecated  use  link  full  outer  join    table    expression    deprecated    table  full  outer  join    table  right    string  join  predicate    joins  two  link    table  s    similar  to  a  sql  full  outer  join    the  fields  of  the  two  joined  operations  must  not  overlap  use  code  as  to  rename  fields  if  necessary  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  and  its  code    table  config  must  have  null  check  enabled  default  p    example  pre  code  left  full  outer  join  right  a  is  equal  b  select  a  b  d  pre  p    scala    example  pre  code  left  full  outer  join  right  a  b  select  a  b  d  pre    table  full  outer  join    table  right    expression  join  predicate    joins  this  link    table  with  an  user  defined  link    table  function    this  join  is  similar  to  a  sql  inner  join  with  on  true  predicate  but  works  with  a  table  function    each  row  of  the  table  is  joined  with  all  rows  produced  by  the  table  function  p    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  public  void  eval    string  str  str  split  for  each  this  collect    table  function    string  split  new    my  split  u  d  t  f  table  env  register  function  split  split  table  join  lateral  split  c  as  s  select  a  b  c  s  pre  deprecated  use  link  join  lateral    expression    deprecated    table  join  lateral    string  table  function  call    joins  this  link    table  with  an  user  defined  link    table  function    this  join  is  similar  to  a  sql  inner  join  with  on  true  predicate  but  works  with  a  table  function    each  row  of  the  table  is  joined  with  all  rows  produced  by  the  table  function  p    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  public  void  eval    string  str  str  split  for  each  this  collect  table  join  lateral  call    my  split  u  d  t  f  class  c  as  s  select  a  b  c  s  pre  p    scala    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  def  eval  str    string    unit  str  split  foreach  collect  val  split  new    my  split  u  d  t  f  table  join  lateral  split  c  as  s  select  a  b  c  s  pre    table  join  lateral    expression  table  function  call    joins  this  link    table  with  an  user  defined  link    table  function    this  join  is  similar  to  a  sql  inner  join  but  works  with  a  table  function    each  row  of  the  table  is  joined  with  all  rows  produced  by  the  table  function  p    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  public  void  eval    string  str  str  split  for  each  this  collect    table  function    string  split  new    my  split  u  d  t  f  table  env  register  function  split  split  table  join  lateral  split  c  as  s  a  s  select  a  b  c  s  pre  deprecated  use  link  join  lateral    expression    expression    deprecated    table  join  lateral    string  table  function  call    string  join  predicate    joins  this  link    table  with  an  user  defined  link    table  function    this  join  is  similar  to  a  sql  inner  join  but  works  with  a  table  function    each  row  of  the  table  is  joined  with  all  rows  produced  by  the  table  function  p    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  public  void  eval    string  str  str  split  for  each  this  collect  table  join  lateral  call    my  split  u  d  t  f  class  c  as  s  a  is  equal  s  select  a  b  c  s  pre  p    scala    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  def  eval  str    string    unit  str  split  foreach  collect  val  split  new    my  split  u  d  t  f  table  join  lateral  split  c  as  s  a  s  select  a  b  c  s  pre    table  join  lateral    expression  table  function  call    expression  join  predicate    joins  this  link    table  with  an  user  defined  link    table  function    this  join  is  similar  to  a  sql  left  outer  join  with  on  true  predicate  but  works  with  a  table  function    each  row  of  the  table  is  joined  with  all  rows  produced  by  the  table  function    if  the  table  function  does  not  produce  any  row  the  outer  row  is  padded  with  nulls  p    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  public  void  eval    string  str  str  split  for  each  this  collect    table  function    string  split  new    my  split  u  d  t  f  table  env  register  function  split  split  table  left  outer  join  lateral  split  c  as  s  select  a  b  c  s  pre  deprecated  use  link  left  outer  join  lateral    expression    deprecated    table  left  outer  join  lateral    string  table  function  call    joins  this  link    table  with  an  user  defined  link    table  function    this  join  is  similar  to  a  sql  left  outer  join  with  on  true  predicate  but  works  with  a  table  function    each  row  of  the  table  is  joined  with  all  rows  produced  by  the  table  function    if  the  table  function  does  not  produce  any  row  the  outer  row  is  padded  with  nulls  p    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  public  void  eval    string  str  str  split  for  each  this  collect  table  left  outer  join  lateral  call    my  split  u  d  t  f  class  c  as  s  select  a  b  c  s  pre  p    scala    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  def  eval  str    string    unit  str  split  foreach  collect  val  split  new    my  split  u  d  t  f  table  left  outer  join  lateral  split  c  as  s  select  a  b  c  s  pre    table  left  outer  join  lateral    expression  table  function  call    joins  this  link    table  with  an  user  defined  link    table  function    this  join  is  similar  to  a  sql  left  outer  join  with  on  true  predicate  but  works  with  a  table  function    each  row  of  the  table  is  joined  with  all  rows  produced  by  the  table  function    if  the  table  function  does  not  produce  any  row  the  outer  row  is  padded  with  nulls  p    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  public  void  eval    string  str  str  split  for  each  this  collect    table  function    string  split  new    my  split  u  d  t  f  table  env  register  function  split  split  table  left  outer  join  lateral  split  c  as  s  a  s  select  a  b  c  s  pre  deprecated  use  link  left  outer  join  lateral    expression    expression    deprecated    table  left  outer  join  lateral    string  table  function  call    string  join  predicate    joins  this  link    table  with  an  user  defined  link    table  function    this  join  is  similar  to  a  sql  left  outer  join  with  on  true  predicate  but  works  with  a  table  function    each  row  of  the  table  is  joined  with  all  rows  produced  by  the  table  function    if  the  table  function  does  not  produce  any  row  the  outer  row  is  padded  with  nulls  p    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  public  void  eval    string  str  str  split  for  each  this  collect  table  left  outer  join  lateral  call    my  split  u  d  t  f  class  c  as  s  a  is  equal  s  select  a  b  c  s  pre  p    scala    example  pre  code  class    my  split  u  d  t  f  extends    table  function    string  def  eval  str    string    unit  str  split  foreach  collect  val  split  new    my  split  u  d  t  f  table  left  outer  join  lateral  split  c  as  s  a  s  select  a  b  c  s  pre    table  left  outer  join  lateral    expression  table  function  call    expression  join  predicate    minus  of  two  link    table  s  with  duplicate  records  removed    similar  to  a  sql  except  clause    minus  returns  records  from  the  left  table  that  do  not  exist  in  the  right  table    duplicate  records  in  the  left  table  are  returned  exactly  once  i  e  duplicates  are  removed    both  tables  must  have  identical  field  types  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  minus  right  pre    table  minus    table  right    minus  of  two  link    table  s    similar  to  a  sql  except  all    similar  to  a  sql  except  all  clause    minus  all  returns  the  records  that  do  not  exist  in  the  right  table  a  record  that  is  present  n  times  in  the  left  table  and  m  times  in  the  right  table  is  returned  n  m  times  i  e  as  many  duplicates  as  are  present  in  the  right  table  are  removed    both  tables  must  have  identical  field  types  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  minus  all  right  pre    table  minus  all    table  right    unions  two  link    table  s  with  duplicate  records  removed    similar  to  a  sql  union    the  fields  of  the  two  union  operations  must  fully  overlap  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  union  right  pre    table  union    table  right    unions  two  link    table  s    similar  to  a  sql  union  all    the  fields  of  the  two  union  operations  must  fully  overlap  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  union  all  right  pre    table  union  all    table  right    intersects  two  link    table  s  with  duplicate  records  removed    intersect  returns  records  that  exist  in  both  tables    if  a  record  is  present  in  one  or  both  tables  more  than  once  it  is  returned  just  once  i  e  the  resulting  table  has  no  duplicate  records    similar  to  a  sql  intersect    the  fields  of  the  two  intersect  operations  must  fully  overlap  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  intersect  right  pre    table  intersect    table  right    intersects  two  link    table  s    intersect  all  returns  records  that  exist  in  both  tables    if  a  record  is  present  in  both  tables  more  than  once  it  is  returned  as  many  times  as  it  is  present  in  both  tables  i  e  the  resulting  table  might  have  duplicate  records    similar  to  an  sql  intersect  all    the  fields  of  the  two  intersect  operations  must  fully  overlap  p    note    both  tables  must  be  bound  to  the  same  code    table  environment  p    example  pre  code  left  intersect  all  right  pre    table  intersect  all    table  right    sorts  the  given  link    table    similar  to  sql  order  by    the  resulting    table  is  sorted  globally  sorted  across  all  parallel  partitions  p    example  pre  code  tab  order  by  name  desc  pre  deprecated  use  link  order  by    expression    deprecated    table  order  by    string  fields    sorts  the  given  link    table    similar  to  sql  code  order  by  p    the  resulting    table  is  globally  sorted  across  all  parallel  partitions  p    java    example  pre  code  tab  order  by  name  desc  pre  p    scala    example  pre  code  tab  order  by  name  desc  pre  p    for  unbounded  tables  this  operation  requires  a  sorting  on  a  time  attribute  or  a  subsequent  fetch  operation    table  order  by    expression  fields    limits  a  possibly  sorted  result  from  an  offset  position  p    this  method  can  be  combined  with  a  preceding  link  order  by    expression  call  for  a  deterministic  order  and  a  subsequent  link  fetch  int  call  to  return  n  rows  after  skipping  the  first  o  rows  pre  code  skips  the  first    rows  and  returns  all  following  rows  tab  order  by  name  desc  offset    skips  the  first    rows  and  returns  the  next    rows  tab  order  by  name  desc  offset    fetch    pre  p    for  unbounded  tables  this  operation  requires  a  subsequent  fetch  operation  param  offset  number  of  records  to  skip    table  offset  int  offset    limits  a  possibly  sorted  result  to  the  first  n  rows  p    this  method  can  be  combined  with  a  preceding  link  order  by    expression  call  for  a  deterministic  order  and  link  offset  int  call  to  return  n  rows  after  skipping  the  first  o  rows  pre  code  returns  the  first    records  tab  order  by  name  desc  fetch    skips  the  first    rows  and  returns  the  next    rows  tab  order  by  name  desc  offset    fetch    pre  param  fetch  the  number  of  records  to  return    fetch  must  be      table  fetch  int  fetch    limits  a  possibly  sorted  result  to  the  first  n  rows  p    this  method  is  a  synonym  for  link  fetch  int  default    table  limit  int  fetch  return  fetch  fetch    limits  a  possibly  sorted  result  to  the  first  n  rows  from  an  offset  position  p    this  method  is  a  synonym  for  link  offset  int  followed  by  link  fetch  int  default    table  limit  int  offset  int  fetch  return  offset  offset  fetch  fetch    writes  the  link    table  to  a  link    table  sink  that  was  registered  under  the  specified  path    for  the  path  resolution  algorithm  see  link    table  environment  use  database    string  p  a  batch  link    table  can  only  be  written  to  a  code  org  apache  flink  table  sinks    batch  table  sink  a  streaming  link    table  requires  a  code  org  apache  flink  table  sinks    append  stream  table  sink  a  code  org  apache  flink  table  sinks    retract  stream  table  sink  or  an  code  org  apache  flink  table  sinks    upsert  stream  table  sink  param  table  path    the  path  of  the  registered  link    table  sink  to  which  the  link    table  is  written  deprecated  use  link  execute  insert    string  for  single  sink  use  link    table  environment  create  statement  set  for  multiple  sinks    deprecated  void  insert  into    string  table  path    groups  the  records  of  a  table  by  assigning  them  to  windows  defined  by  a  time  or  row  interval  p    for  streaming  tables  of  infinite  size  grouping  into  windows  is  required  to  define  finite  groups  on  which  group  based  aggregates  can  be  computed  p    for  batch  tables  of  finite  size  windowing  essentially  provides  shortcuts  for  time  based  group  by  p  b    note  b    computing  windowed  aggregates  on  a  streaming  table  is  only  a  parallel  operation  if  additional  grouping  attributes  are  added  to  the  code  group  by  clause    if  the  code  group  by  only  references  a    group  window  alias  the  streamed  table  will  be  processed  by  a  single  task  i  e  with  parallelism    param  group  window  group  window  that  specifies  how  elements  are  grouped  return  a  windowed  table    group  windowed  table  window    group  window  group  window    defines  over  windows  on  the  records  of  a  table  p    an  over  window  defines  for  each  record  an  interval  of  records  over  which  aggregation  functions  can  be  computed  p    example  pre  code  table  window    over  partition  by  c  order  by  row  time  preceding  lit    seconds  as  ow  select  c  b  count  over  ow  e  sum  over  ow  pre  p    scala    example  pre  code  table  window    over  partition  by  c  order  by  row  time  preceding    seconds  as  ow  select  c  b  count  over  ow  e  sum  over  ow  pre  p  b    note  b    computing  over  window  aggregates  on  a  streaming  table  is  only  a  parallel  operation  if  the  window  is  partitioned    otherwise  the  whole  stream  will  be  processed  by  a  single  task  i  e  with  parallelism    p  b    note  b    over  windows  for  batch  tables  are  currently  not  supported  param  over  windows  windows  that  specify  the  record  interval  over  which  aggregations  are  computed  return    an    over  windowed  table  to  specify  the  aggregations    over  windowed  table  window    over  window  over  windows    adds  additional  columns    similar  to  a  sql  select  statement    the  field  expressions  can  contain  complex  expressions  but  can  not  contain  aggregations    it  will  throw  an  exception  if  the  added  fields  already  exist  p    example  pre  code  tab  add  columns  a    as  a1  concat  b  sunny  as  b1  pre  deprecated  use  link  add  columns    expression    deprecated    table  add  columns    string  fields    adds  additional  columns    similar  to  a  sql  select  statement    the  field  expressions  can  contain  complex  expressions  but  can  not  contain  aggregations    it  will  throw  an  exception  if  the  added  fields  already  exist  p    example  pre  code  tab  add  columns  a  plus    as  a1  concat  b  sunny  as  b1  pre  p    scala    example  pre  code  tab  add  columns  a    as  a1  concat  b  sunny  as  b1  pre    table  add  columns    expression  fields    adds  additional  columns    similar  to  a  sql  select  statement    the  field  expressions  can  contain  complex  expressions  but  can  not  contain  aggregations    existing  fields  will  be  replaced  if  add  columns  name  is  the  same  as  the  existing  column  name    moreover  if  the  added  fields  have  duplicate  field  name  then  the  last  one  is  used  p    example  pre  code  tab  add  or  replace  columns  a    as  a1  concat  b  sunny  as  b1  pre  deprecated  use  link  add  or  replace  columns    expression    deprecated    table  add  or  replace  columns    string  fields    adds  additional  columns    similar  to  a  sql  select  statement    the  field  expressions  can  contain  complex  expressions  but  can  not  contain  aggregations    existing  fields  will  be  replaced    if  the  added  fields  have  duplicate  field  name  then  the  last  one  is  used  p    example  pre  code  tab  add  or  replace  columns  a  plus    as  a1  concat  b  sunny  as  b1  pre  p    scala    example  pre  code  tab  add  or  replace  columns  a    as  a1  concat  b  sunny  as  b1  pre    table  add  or  replace  columns    expression  fields    renames  existing  columns    similar  to  a  field  alias  statement    the  field  expressions  should  be  alias  expressions  and  only  the  existing  fields  can  be  renamed  p    example  pre  code  tab  rename  columns  a  as  a1  b  as  b1  pre  deprecated  use  link  rename  columns    expression    deprecated    table  rename  columns    string  fields    renames  existing  columns    similar  to  a  field  alias  statement    the  field  expressions  should  be  alias  expressions  and  only  the  existing  fields  can  be  renamed  p    example  pre  code  tab  rename  columns  a  as  a1  b  as  b1  pre  p    scala    example  pre  code  tab  rename  columns  a  as  a1  b  as  b1  pre    table  rename  columns    expression  fields    drops  existing  columns    the  field  expressions  should  be  field  reference  expressions  p    example  pre  code  tab  drop  columns  a  b  pre  deprecated  use  link  drop  columns    expression    deprecated    table  drop  columns    string  fields    drops  existing  columns    the  field  expressions  should  be  field  reference  expressions  p    example  pre  code  tab  drop  columns  a  b  pre  p    scala    example  pre  code  tab  drop  columns  a  b  pre    table  drop  columns    expression  fields    performs  a  map  operation  with  an  user  defined  scalar  function  or  a  built  in  scalar  function    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code    scalar  function  func  new    my  map  function  table  env  register  function  func  func  tab  map  func  c  pre  deprecated  use  link  map    expression    deprecated    table  map    string  map  function    performs  a  map  operation  with  an  user  defined  scalar  function  or  built  in  scalar  function    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code  tab  map  call    my  map  function  class  c  pre  p    scala    example  pre  code  val  func  new    my  map  function  tab  map  func  c  pre    table  map    expression  map  function    performs  a  flat  map  operation  with  an  user  defined  table  function  or  built  in  table  function    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code    table  function  func  new    my  flat  map  function  table  env  register  function  func  func  table  flat  map  func  c  pre  deprecated  use  link  flat  map    expression    deprecated    table  flat  map    string  table  function    performs  a  flat  map  operation  with  an  user  defined  table  function  or  built  in  table  function    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code  tab  flat  map  call    my  flat  map  function  class  c  pre  p    scala    example  pre  code  val  func  new    my  flat  map  function  tab  flat  map  func  c  pre    table  flat  map    expression  table  function    performs  a  global  aggregate  operation  with  an  aggregate  function    you  have  to  close  the  link  aggregate    string  with  a  select  statement    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code    aggregate  function  agg  func  new    my  aggregate  function  table  env  register  function  agg  func  agg  func  table  aggregate  agg  func  a  b  as  f0  f1  f2  select  f0  f1  pre  deprecated  use  link  aggregate    expression    deprecated    aggregated  table  aggregate    string  aggregate  function    performs  a  global  aggregate  operation  with  an  aggregate  function    you  have  to  close  the  link  aggregate    expression  with  a  select  statement    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code  tab  aggregate  call    my  aggregate  function  class  a  b  as  f0  f1  f2  select  f0  f1  pre  p    scala    example  pre  code  val  agg  func  new    my  aggregate  function  table  aggregate  agg  func  a  b  as  f0  f1  f2  select  f0  f1  pre    aggregated  table  aggregate    expression  aggregate  function    perform  a  global  flat  aggregate  without  group  by    flat  aggregate  takes  a    table  aggregate  function  which  returns  multiple  rows    use  a  selection  after  the  flat  aggregate  p    example  pre  code    table  aggregate  function  table  agg  func  new    my  table  aggregate  function  table  env  register  function  table  agg  func  table  agg  func  tab  flat  aggregate  table  agg  func  a  b  as  x  y  z  select  x  y  z  pre  deprecated  use  link  flat  aggregate    expression    deprecated    flat  aggregate  table  flat  aggregate    string  table  aggregate  function    perform  a  global  flat  aggregate  without  group  by    flat  aggregate  takes  a    table  aggregate  function  which  returns  multiple  rows    use  a  selection  after  the  flat  aggregate  p    example  pre  code  tab  flat  aggregate  call    my  table  aggregate  function  class  a  b  as  x  y  z  select  x  y  z  pre  p    scala    example  pre  code  val  table  agg  func    table  aggregate  function  new    my  table  aggregate  function  tab  flat  aggregate  table  agg  func  a  b  as  x  y  z  select  x  y  z  pre    flat  aggregate  table  flat  aggregate    expression  table  aggregate  function    writes  the  link    table  to  a  link    table  sink  that  was  registered  under  the  specified  path  and  then  execute  the  insert  operation  p    see  the  documentation  of  link    table  environment  use  database    string  or  link    table  environment  use  catalog    string  for  the  rules  on  the  path  resolution  p  a  batch  link    table  can  only  be  written  to  a  code  org  apache  flink  table  sinks    batch  table  sink  a  streaming  link    table  requires  a  code  org  apache  flink  table  sinks    append  stream  table  sink  a  code  org  apache  flink  table  sinks    retract  stream  table  sink  or  an  code  org  apache  flink  table  sinks    upsert  stream  table  sink  p    example  pre  code    table  table  table  env  from  query  select  from    my  table    table  result  table  result  table  execute  insert    my  sink  table  result  pre  param  table  path    the  path  of  the  registered    table  sink  to  which  the    table  is  written  return    the  insert  operation  execution  result    table  result  execute  insert    string  table  path    writes  the  link    table  to  a  link    table  sink  that  was  registered  under  the  specified  path  and  then  execute  the  insert  operation  p    see  the  documentation  of  link    table  environment  use  database    string  or  link    table  environment  use  catalog    string  for  the  rules  on  the  path  resolution  p  a  batch  link    table  can  only  be  written  to  a  code  org  apache  flink  table  sinks    batch  table  sink  a  streaming  link    table  requires  a  code  org  apache  flink  table  sinks    append  stream  table  sink  a  code  org  apache  flink  table  sinks    retract  stream  table  sink  or  an  code  org  apache  flink  table  sinks    upsert  stream  table  sink  p    example  pre  code    table  table  table  env  from  query  select  from    my  table    table  result  table  result  table  execute  insert    my  sink  true  table  result  pre  param  table  path    the  path  of  the  registered    table  sink  to  which  the    table  is  written  param  overwrite    the  flag  that  indicates  whether  the  insert  should  overwrite  existing  data  or  not  return    the  insert  operation  execution  result    table  result  execute  insert    string  table  path  boolean  overwrite    collects  the  contents  of  the  current  table  local  client  pre  code    table  table  table  env  from  query  select  from    my  table    table  result  table  result  table  execute  table  result  pre    table  result  execute    returns  the  ast  of  this  table  and  the  execution  plan  to  compute  the  result  of  this  table  param  extra  details    the  extra  explain  details  which  the  explain  result  should  include  e  g  estimated  cost  changelog  mode  for  streaming  return  ast  and  the  execution  plan    string  explain    explain  detail  extra  details  
public  evolving  public  class    table  config    defines  the  zone  id  for  timestamp  with  local  time  zone  private    zone  id  local  zone  id    zone  id  system  default    defines  if  all  fields  need  to  be  checked  for  null  first  private    boolean  null  check  true    defines  the  configuration  of    planner  for    table  api  and  sql  queries  private    planner  config  planner  config    planner  config  empty  config    defines  the  default  context  for  decimal  division  calculation    we  use    scala  s  default    math  context    d  e  c  i  m  a  l128  private    math  context  decimal  context    math  context    d  e  c  i  m  a  l128    specifies  a  threshold  where  generated  code  will  be  split  into  sub  function  calls    java  has  a  maximum  method  length  of    kb    this  setting  allows  for  finer  granularity  if  necessary  private    integer  max  generated  code  length    just  an  estimate    the  minimum  time  until  state  which  was  not  updated  will  be  retained    state  might  be  cleared  and  removed  if  it  was  not  updated  for  the  defined  period  of  time  private  long  min  idle  state  retention  time  0  l    the  maximum  time  until  state  which  was  not  updated  will  be  retained    state  will  be  cleared  and  removed  if  it  was  not  updated  for  the  defined  period  of  time  private  long  max  idle  state  retention  time  0  l  a  configuration  object  to  hold  all  key  value  configuration  private    configuration  configuration  new    configuration    gives  direct  access  to  the  underlying  key  value  map  for  advanced  configuration  public    configuration  get  configuration  return  configuration    adds  the  given  key  value  configuration  to  the  underlying  configuration    it  overwrites  existing  keys  param  configuration  key  value  configuration  to  be  added  public  void  add  configuration    configuration  configuration    preconditions  check  not  null  configuration  this  configuration  add  all  configuration    returns  the  current  sql  dialect  public    sql  dialect  get  sql  dialect  return    sql  dialect  value  of  get  configuration  get  string    table  config  options  table  sql  dialect  to  upper  case    sets  the  current  sql  dialect  to  parse  a  sql  query    flink  s  sql  behavior  by  default  public  void  set  sql  dialect    sql  dialect  sql  dialect  get  configuration  set  string    table  config  options  table  sql  dialect  sql  dialect  name  to  lower  case    returns  the  current  session  time  zone  id    it  is  used  when  converting  to  from  code  timestamp  with  local  time  zone    see  link  set  local  time  zone    zone  id  for  more  details  see  org  apache  flink  table  types  logical    local  zoned  timestamp  type  public    zone  id  get  local  time  zone  return  local  zone  id    sets  the  current  session  time  zone  id    it  is  used  when  converting  to  from  link    data  types  timestamp  with  local  time  zone    internally  timestamps  with  local  time  zone  are  always  represented  in  the  utc  time  zone    however  when  converting  to  data  types  that  don  t  include  a  time  zone  e  g  timestamp  time  or  simply  string  the  session  time  zone  is  used  during  conversion  p    example  pre  code    table  environment  t  env    table  config  config  t  env  get  config  config  set  local  time  zone    zone  offset  of  hours    t  env  create  table  test  table  id  bigint  tmstmp  timestamp  with  local  time  zone  t  env  insert  into  test  table  values    2000-01  01 2  00:00    timestamp  2000-01  01 2  00:00  t  env  select  from  test  table  query  with  local  time  zone  set  to  utc    pre  should  produce  pre  id  tmstmp    2000-01  01 2  00:00    2000-01  01 2  00:00  pre    if  we  change  the  local  time  zone  and  query  the  same  table  pre  code  config  set  local  time  zone    zone  offset  of  hours    t  env  select  from  test  table  query  with  local  time  zone  set  to  utc    pre  we  should  get  pre  id  tmstmp    2000-01  01 0  00:00    2000-01  01 0  00:00  pre  see  org  apache  flink  table  types  logical    local  zoned  timestamp  type  public  void  set  local  time  zone    zone  id  zone  id  this  local  zone  id    preconditions  check  not  null  zone  id    returns  the  null  check    if  enabled  all  fields  need  to  be  checked  for  null  first  public    boolean  get  null  check  return  null  check    sets  the  null  check    if  enabled  all  fields  need  to  be  checked  for  null  first  public  void  set  null  check    boolean  null  check  this  null  check    preconditions  check  not  null  null  check    returns  the  current  configuration  of    planner  for    table  api  and  sql  queries  public    planner  config  get  planner  config  return  planner  config    sets  the  configuration  of    planner  for    table  api  and  sql  queries    changing  the  configuration  has  no  effect  after  the  first  query  has  been  defined  public  void  set  planner  config    planner  config  planner  config  this  planner  config    preconditions  check  not  null  planner  config    returns  the  default  context  for  decimal  division  calculation  link  java  math    math  context    d  e  c  i  m  a  l128  by  default  public    math  context  get  decimal  context  return  decimal  context    sets  the  default  context  for  decimal  division  calculation  link  java  math    math  context    d  e  c  i  m  a  l128  by  default  public  void  set  decimal  context    math  context  decimal  context  this  decimal  context    preconditions  check  not  null  decimal  context    returns  the  current  threshold  where  generated  code  will  be  split  into  sub  function  calls    java  has  a  maximum  method  length  of    kb    this  setting  allows  for  finer  granularity  if  necessary    default  is    public    integer  get  max  generated  code  length  return  max  generated  code  length    returns  the  current  threshold  where  generated  code  will  be  split  into  sub  function  calls    java  has  a  maximum  method  length  of    kb    this  setting  allows  for  finer  granularity  if  necessary    default  is    public  void  set  max  generated  code  length    integer  max  generated  code  length  this  max  generated  code  length    preconditions  check  not  null  max  generated  code  length    specifies  a  minimum  and  a  maximum  time  interval  for  how  long  idle  state  i  e  state  which  was  not  updated  will  be  retained    state  will  never  be  cleared  until  it  was  idle  for  less  than  the  minimum  time  and  will  never  be  kept  if  it  was  idle  for  more  than  the  maximum  time  p    when  new  data  arrives  for  previously  cleaned  up  state  the  new  data  will  be  handled  as  if  it  was  the  first  data    this  can  result  in  previous  results  being  overwritten  p    set  to    zero  to  never  clean  up  the  state  p  note    cleaning  up  state  requires  additional  bookkeeping  which  becomes  less  expensive  for  larger  differences  of  min  time  and  max  time    the  difference  between  min  time  and  max  time  must  be  at  least    minutes  param  min  time    the  minimum  time  interval  for  which  idle  state  is  retained    set  to    zero  to  never  clean  up  the  state  param  max  time    the  maximum  time  interval  for  which  idle  state  is  retained    must  be  at  least    minutes  greater  than  min  time    set  to    zero  to  never  clean  up  the  state  public  void  set  idle  state  retention  time    time  min  time    time  max  time  if  max  time  to  milliseconds  min  time  to  milliseconds    max  time  to  milliseconds    min  time  to  milliseconds    throw  new    illegal  argument  exception    difference  between  min  time  min  time  to  string  and  max  time  max  time  to  string  should  be  at  least    minutes  min  idle  state  retention  time  min  time  to  milliseconds  max  idle  state  retention  time  max  time  to  milliseconds  return    the  minimum  time  until  state  which  was  not  updated  will  be  retained  public  long  get  min  idle  state  retention  time  return  min  idle  state  retention  time  return    the  maximum  time  until  state  which  was  not  updated  will  be  retained  public  long  get  max  idle  state  retention  time  return  max  idle  state  retention  time    sets  a  custom  user  parameter  that  can  be  accessed  via  link  org  apache  flink  table  functions    function  context  get  job  parameter    string    string  p    this  will  add  an  entry  to  the  current  value  of  link    pipeline  options  global  job  parameters  p    it  is  also  possible  to  set  multiple  parameters  at  once  which  will  override  any  previously  set  parameters  pre  code    map    string    string  params    table  config  config  t  env  get  config  config  get  configuration  set    pipeline  options  global  job  parameters  params  pre    experimental  public  void  add  job  parameter    string  key    string  value    map    string    string  params  get  configuration  get  optional    pipeline  options  global  job  parameters  map    hash  map  new  or  else  get    hash  map  new  params  put  key  value  get  configuration  set    pipeline  options  global  job  parameters  params  public  static    table  config  get  default  return  new    table  config  
public  evolving  public  interface    table  environment    creates  a  table  environment  that  is  the  entry  point  and  central  context  for  creating    table  and  sql  api  programs  p    it  is  unified  both  on  a  language  level  for  all  jvm  based  languages  i  e  there  is  no  distinction  between    scala  and    java  api  and  for  bounded  and  unbounded  data  processing  p  a  table  environment  is  responsible  for  ul  li    connecting  to  external  systems  li  li    registering  and  retrieving  link    table  s  and  other  meta  objects  from  a  catalog  li  li    executing  sql  statements  li  li    offering  further  configuration  options  li  ul  p    note    this  environment  is  meant  for  pure  table  programs    if  you  would  like  to  convert  from  or  to  other    flink    a  p  is  it  might  be  necessary  to  use  one  of  the  available  language  specific  table  environments  in  the  corresponding  bridging  modules  param  settings    the  environment  settings  used  to  instantiate  the  link    table  environment  static    table  environment  create    environment  settings  settings  return    table  environment  impl  create  settings    creates  a    table  from  given  values  p    examples  p    you  can  use  a  code  row  expression  to  create  a  composite  rows  pre  code  t  env  from  values  row    abc  row  2  l  abcde  pre  will  produce  a    table  with  a  schema  as  follows  pre  code  root  f0  bigint  not  null  original  types  int  and  bigint  are  generalized  to  bigint  f1  varchar    not  null  original  types  char    and  char    are  generalized  to  varchar    it  uses  varchar  instead  of  char  so  that  no  padding  is  applied  pre  p    the  method  will  derive  the  types  automatically  from  the  input  expressions    if  types  at  a  certain  position  differ  the  method  will  try  to  find  a  common  super  type  for  all  types    if  a  common  super  type  does  not  exist  an  exception  will  be  thrown    if  you  want  to  specify  the  requested  type  explicitly  see  link  from  values    abstract  data  type    object  p    it  is  also  possible  to  use  link  org  apache  flink  types    row  object  instead  of  code  row  expressions  p    r  o  ws  that  are  a  result  of  e  g  a  function  call  are  not  flattened  pre  code  public  class    row  function  extends    scalar  function  literal    data  type  hint  row  f0  bigint  f1  varchar      row  eval  t  env  from  values  call  new    row  function  call  new    row  function  pre  will  produce  a    table  with  a  schema  as  follows  pre  code  root  f0  row  f0  bigint  f1  varchar    pre  p    the  row  constructor  can  be  dropped  to  create  a  table  with  a  single  column  p    r  o  ws  that  are  a  result  of  e  g  a  function  call  are  not  flattened  pre  code  t  env  from  values    2  l    pre  will  produce  a    table  with  a  schema  as  follows  pre  code  root  f0  bigint  not  null  pre  param  values    expressions  for  constructing  rows  of  the  values  table  default    table  from  values    object  values    it  is  necessary  here  to  implement    table  environment  from  values    object  for    batch  table  env  impl    in  scala  varargs  are  translated  to    seq    due  to  the  type  erasure    seq    expression  and    seq    object  are  the  same    it  is  not  a  problem  in  java  as  varargs  in  java  are  translated  to  an  array  return  from  values    arrays  as  list  values    creates  a    table  from  given  collection  of  objects  with  a  given  row  type  p    the  difference  between  this  method  and  link  from  values    object  is  that  the  schema  can  be  manually  adjusted    it  might  be  helpful  for  assigning  more  generic  types  like  e  g  decimal  or  naming  the  columns  p    examples  pre  code  t  env  from  values    data  types  row    data  types  field  id    data  types  decimal        data  types  field  name    data  types  string  row    abc  row  2  l  abcde  pre  will  produce  a    table  with  a  schema  as  follows  pre  code  root  id  decimal      f1  string  pre  p    for  more  examples  see  link  from  values    object  param  row  type    expected  row  type  for  the  values  param  values    expressions  for  constructing  rows  of  the  values  table  see  from  values    object  default    table  from  values    abstract  data  type  row  type    object  values    it  is  necessary  here  to  implement    table  environment  from  values    object  for    batch  table  env  impl    in  scala  varargs  are  translated  to    seq    due  to  the  type  erasure    seq    expression  and    seq    object  are  the  same    it  is  not  a  problem  in  java  as  varargs  in  java  are  translated  to  an  array  return  from  values  row  type    arrays  as  list  values    creates  a    table  from  given  values  p    examples  p    you  can  use  a  code  row  expression  to  create  a  composite  rows  pre  code  t  env  from  values  row    abc  row  2  l  abcde  pre  will  produce  a    table  with  a  schema  as  follows  pre  code  root  f0  bigint  not  null  original  types  int  and  bigint  are  generalized  to  bigint  f1  varchar    not  null  original  types  char    and  char    are  generalized  to  varchar    it  uses  varchar  instead  of  char  so  that  no  padding  is  applied  pre  p    the  method  will  derive  the  types  automatically  from  the  input  expressions    if  types  at  a  certain  position  differ  the  method  will  try  to  find  a  common  super  type  for  all  types    if  a  common  super  type  does  not  exist  an  exception  will  be  thrown    if  you  want  to  specify  the  requested  type  explicitly  see  link  from  values    data  type    expression  p    it  is  also  possible  to  use  link  org  apache  flink  types    row  object  instead  of  code  row  expressions  p    r  o  ws  that  are  a  result  of  e  g  a  function  call  are  not  flattened  pre  code  public  class    row  function  extends    scalar  function  literal    data  type  hint  row  f0  bigint  f1  varchar      row  eval  t  env  from  values  call  new    row  function  call  new    row  function  pre  will  produce  a    table  with  a  schema  as  follows  pre  code  root  f0  row  f0  bigint  f1  varchar    pre  p    the  row  constructor  can  be  dropped  to  create  a  table  with  a  single  column  p    r  o  ws  that  are  a  result  of  e  g  a  function  call  are  not  flattened  pre  code  t  env  from  values  lit    plus    lit  2  l  lit    pre  will  produce  a    table  with  a  schema  as  follows  pre  code  root  f0  bigint  not  null  pre  param  values    expressions  for  constructing  rows  of  the  values  table    table  from  values    expression  values    creates  a    table  from  given  collection  of  objects  with  a  given  row  type  p    the  difference  between  this  method  and  link  from  values    expression  is  that  the  schema  can  be  manually  adjusted    it  might  be  helpful  for  assigning  more  generic  types  like  e  g  decimal  or  naming  the  columns  p    examples  pre  code  t  env  from  values    data  types  row    data  types  field  id    data  types  decimal        data  types  field  name    data  types  string  row    abc  row  2  l  abcde  pre  will  produce  a    table  with  a  schema  as  follows  pre  code  root  id  decimal      name  string  pre  p    for  more  examples  see  link  from  values    expression  param  row  type    expected  row  type  for  the  values  param  values    expressions  for  constructing  rows  of  the  values  table  see  from  values    expression    table  from  values    abstract  data  type  row  type    expression  values    creates  a    table  from  given  collection  of  objects  p    see  link  from  values    object  for  more  explanation  param  values    expressions  for  constructing  rows  of  the  values  table  see  from  values    object    table  from  values    iterable  values    creates  a    table  from  given  collection  of  objects  with  a  given  row  type  p    see  link  from  values    abstract  data  type    object  for  more  explanation  param  row  type    expected  row  type  for  the  values  param  values    expressions  for  constructing  rows  of  the  values  table  see  from  values    abstract  data  type    object    table  from  values    abstract  data  type  row  type    iterable  values    creates  a  table  from  a  table  source  param  source  table  source  used  as  table    deprecated    table  from  table  source    table  source  source    registers  a  link    catalog  under  a  unique  name    all  tables  registered  in  the  link    catalog  can  be  accessed  param  catalog  name    the  name  under  which  the  catalog  will  be  registered  param  catalog    the  catalog  to  register  void  register  catalog    string  catalog  name    catalog  catalog    gets  a  registered  link    catalog  by  name  param  catalog  name    the  name  to  look  up  the  link    catalog  return    the  requested  catalog  empty  if  there  is  no  registered  catalog  with  given  name    optional    catalog  get  catalog    string  catalog  name    loads  a  link    module  under  a  unique  name    modules  will  be  kept  in  the  loaded  order    validation  exception  is  thrown  when  there  is  already  a  module  with  the  same  name  param  module  name  name  of  the  link    module  param  module  the  module  instance  void  load  module    string  module  name    module  module    unloads  a  link    module  with  given  name    validation  exception  is  thrown  when  there  is  no  module  with  the  given  name  param  module  name  name  of  the  link    module  void  unload  module    string  module  name    registers  a  link    scalar  function  under  a  unique  name    replaces  already  existing  user  defined  functions  under  this  name  deprecated    use  link  create  temporary  system  function    string    user  defined  function  instead    please  note  that  the  new  method  also  uses  the  new  type  system  and  reflective  extraction  logic    it  might  be  necessary  to  update  the  function  implementation  as  well    see  the  documentation  of  link    scalar  function  for  more  information  on  the  new  function  design    deprecated  void  register  function    string  name    scalar  function  function    registers  a  link    user  defined  function  class  as  a  temporary  system  function  p    compared  to  link  create  temporary  function    string    class  system  functions  are  identified  by  a  global  name  that  is  independent  of  the  current  catalog  and  current  database    thus  this  method  allows  to  extend  the  set  of  built  in  system  functions  like  code  trim  code  abs  etc  p    temporary  functions  can  shadow  permanent  ones    if  a  permanent  function  under  a  given  name  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  function  available  again  one  can  drop  the  corresponding  temporary  system  function  param  name    the  name  under  which  the  function  will  be  registered  globally  param  function  class    the  function  class  containing  the  implementation    experimental  void  create  temporary  system  function    string  name    class  extends    user  defined  function  function  class    registers  a  link    user  defined  function  instance  as  a  temporary  system  function  p    compared  to  link  create  temporary  system  function    string    class  this  method  takes  a  function  instance  that  might  have  been  parameterized  before  e  g  through  its  constructor    this  might  be  useful  for  more  interactive  sessions    make  sure  that  the  instance  is  link    serializable  p    compared  to  link  create  temporary  function    string    user  defined  function  system  functions  are  identified  by  a  global  name  that  is  independent  of  the  current  catalog  and  current  database    thus  this  method  allows  to  extend  the  set  of  built  in  system  functions  like  code  trim  code  abs  etc  p    temporary  functions  can  shadow  permanent  ones    if  a  permanent  function  under  a  given  name  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  function  available  again  one  can  drop  the  corresponding  temporary  system  function  param  name    the  name  under  which  the  function  will  be  registered  globally  param  function  instance    the  possibly  pre  configured  function  instance  containing  the  implementation    experimental  void  create  temporary  system  function    string  name    user  defined  function  function  instance    drops  a  temporary  system  function  registered  under  the  given  name  p    if  a  permanent  function  with  the  given  name  exists  it  will  be  used  from  now  on  for  any  queries  that  reference  this  name  param  name    the  name  under  which  the  function  has  been  registered  globally  return  true  if  a  function  existed  under  the  given  name  and  was  removed    experimental  boolean  drop  temporary  system  function    string  name    registers  a  link    user  defined  function  class  as  a  catalog  function  in  the  given  path  p    compared  to  system  functions  with  a  globally  defined  name  catalog  functions  are  always  implicitly  or  explicitly  identified  by  a  catalog  and  database  p    there  must  not  be  another  function  temporary  or  permanent  registered  under  the  same  path  param  path    the  path  under  which  the  function  will  be  registered    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  function  class    the  function  class  containing  the  implementation    experimental  void  create  function    string  path    class  extends    user  defined  function  function  class    registers  a  link    user  defined  function  class  as  a  catalog  function  in  the  given  path  p    compared  to  system  functions  with  a  globally  defined  name  catalog  functions  are  always  implicitly  or  explicitly  identified  by  a  catalog  and  database  param  path    the  path  under  which  the  function  will  be  registered    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  function  class    the  function  class  containing  the  implementation  param  ignore  if  exists    if  a  function  exists  under  the  given  path  and  this  flag  is  set  no  operation  is  executed    an  exception  is  thrown  otherwise    experimental  void  create  function    string  path    class  extends    user  defined  function  function  class  boolean  ignore  if  exists    drops  a  catalog  function  registered  in  the  given  path  param  path    the  path  under  which  the  function  has  been  registered    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  return  true  if  a  function  existed  in  the  given  path  and  was  removed    experimental  boolean  drop  function    string  path    registers  a  link    user  defined  function  class  as  a  temporary  catalog  function  p    compared  to  link  create  temporary  system  function    string    class  with  a  globally  defined  name  catalog  functions  are  always  implicitly  or  explicitly  identified  by  a  catalog  and  database  p    temporary  functions  can  shadow  permanent  ones    if  a  permanent  function  under  a  given  name  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  function  available  again  one  can  drop  the  corresponding  temporary  function  param  path    the  path  under  which  the  function  will  be  registered    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  function  class    the  function  class  containing  the  implementation    experimental  void  create  temporary  function    string  path    class  extends    user  defined  function  function  class    registers  a  link    user  defined  function  instance  as  a  temporary  catalog  function  p    compared  to  link  create  temporary  function    string    class  this  method  takes  a  function  instance  that  might  have  been  parameterized  before  e  g  through  its  constructor    this  might  be  useful  for  more  interactive  sessions    make  sure  that  the  instance  is  link    serializable  p    compared  to  link  create  temporary  system  function    string    user  defined  function  with  a  globally  defined  name  catalog  functions  are  always  implicitly  or  explicitly  identified  by  a  catalog  and  database  p    temporary  functions  can  shadow  permanent  ones    if  a  permanent  function  under  a  given  name  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  function  available  again  one  can  drop  the  corresponding  temporary  function  param  path    the  path  under  which  the  function  will  be  registered    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  function  instance    the  possibly  pre  configured  function  instance  containing  the  implementation    experimental  void  create  temporary  function    string  path    user  defined  function  function  instance    drops  a  temporary  catalog  function  registered  in  the  given  path  p    if  a  permanent  function  with  the  given  path  exists  it  will  be  used  from  now  on  for  any  queries  that  reference  this  path  param  path    the  path  under  which  the  function  will  be  registered    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  return  true  if  a  function  existed  in  the  given  path  and  was  removed    experimental  boolean  drop  temporary  function    string  path    registers  a  link    table  under  a  unique  name  in  the    table  environment  s  catalog    registered  tables  can  be  referenced  in  sql  queries  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  one  can  drop  the  corresponding  temporary  object  param  name    the  name  under  which  the  table  will  be  registered  param  table    the  table  to  register  deprecated  use  link  create  temporary  view    string    table    deprecated  void  register  table    string  name    table  table    registers  a  link    table  api  object  as  a  temporary  view  similar  to  sql  temporary  views  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  one  can  drop  the  corresponding  temporary  object  param  path    the  path  under  which  the  view  will  be  registered    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  view    the  view  to  register  void  create  temporary  view    string  path    table  view    scans  a  registered  table  and  returns  the  resulting  link    table  p  a  table  to  scan  must  be  registered  in  the  link    table  environment    it  can  be  either  directly  registered  or  be  an  external  member  of  a  link    catalog  p    see  the  documentation  of  link    table  environment  use  database    string  or  link    table  environment  use  catalog    string  for  the  rules  on  the  path  resolution  p    examples  p    scanning  a  directly  registered  table  pre  code    table  tab  table  env  scan  table  name  pre  p    scanning  a  table  from  a  registered  catalog  pre  code    table  tab  table  env  scan  catalog  name  db  name  table  name  pre  param  table  path    the  path  of  the  table  to  scan  return    the  resulting  link    table  see    table  environment  use  catalog    string  see    table  environment  use  database    string  deprecated  use  link  from    string    deprecated    table  scan    string  table  path    reads  a  registered  table  and  returns  the  resulting  link    table  p  a  table  to  scan  must  be  registered  in  the  link    table  environment  p    see  the  documentation  of  link    table  environment  use  database    string  or  link    table  environment  use  catalog    string  for  the  rules  on  the  path  resolution  p    examples  p    reading  a  table  from  default  catalog  and  database  pre  code    table  tab  table  env  from  table  name  pre  p    reading  a  table  from  a  registered  catalog  pre  code    table  tab  table  env  from  catalog  name  db  name  table  name  pre  p    reading  a  table  from  a  registered  catalog  with  escaping    dots  in  e  g  a  database  name  must  be  escaped  pre  code    table  tab  table  env  from  catalog  name  db    name    table  pre  param  path    the  path  of  a  table  api  object  to  scan  return    either  a  table  or  virtual  table  view  see    table  environment  use  catalog    string  see    table  environment  use  database    string    table  from    string  path    writes  the  link    table  to  a  link    table  sink  that  was  registered  under  the  specified  name  p    see  the  documentation  of  link    table  environment  use  database    string  or  link    table  environment  use  catalog    string  for  the  rules  on  the  path  resolution  param  table    the    table  to  write  to  the  sink  param  sink  path    the  first  part  of  the  path  of  the  registered  link    table  sink  to  which  the  link    table  is  written    this  is  to  ensure  at  least  the  name  of  the  link    table  sink  is  provided  param  sink  path  continued    the  remaining  part  of  the  path  of  the  registered  link    table  sink  to  which  the  link    table  is  written  deprecated  use  link    table  execute  insert    string  for  single  sink  use  link    table  environment  create  statement  set  for  multiple  sinks    deprecated  void  insert  into    table  table    string  sink  path    string  sink  path  continued    instructs  to  write  the  content  of  a  link    table  api  object  into  a  table  p    see  the  documentation  of  link    table  environment  use  database    string  or  link    table  environment  use  catalog    string  for  the  rules  on  the  path  resolution  param  target  path    the  path  of  the  registered  link    table  sink  to  which  the  link    table  is  written  param  table    the    table  to  write  to  the  sink  deprecated  use  link    table  execute  insert    string  for  single  sink  use  link    table  environment  create  statement  set  for  multiple  sinks    deprecated  void  insert  into    string  target  path    table  table    creates  a  temporary  table  from  a  descriptor  p    descriptors  allow  for  declaring  the  communication  to  external  systems  in  an  implementation  agnostic  way    the  classpath  is  scanned  for  suitable  table  factories  that  match  the  desired  configuration  p    the  following  example  shows  how  to  read  from  a  connector  using  a  json  format  and  register  a  temporary  table  as    my  table  pre  code  table  env  connect  new    external  system  x  y  z  version  0.11  with  format  new    json  json  schema  fail  on  missing  field  false  with  schema  new    schema  field  user  name  varchar  from  u  name  field  count  decimal  create  temporary  table    my  table  pre  param  connector  descriptor  connector  descriptor  describing  the  external  system  deprecated    the  sql  code  create  table  ddl  is  richer  than  this  part  of  the  api    this  method  might  be  refactored  in  the  next  versions    please  use  link  execute  sql    string  execute  sql  ddl  to  register  a  table  instead    deprecated    connect  table  descriptor  connect    connector  descriptor  connector  descriptor    gets  the  names  of  all  catalogs  registered  in  this  environment  return  a  list  of  the  names  of  all  registered  catalogs    string  list  catalogs    gets  an  array  of  names  of  all  modules  in  this  environment  in  the  loaded  order  return  a  list  of  the  names  of  all  modules  in  the  loaded  order    string  list  modules    gets  the  names  of  all  databases  registered  in  the  current  catalog  return  a  list  of  the  names  of  all  registered  databases  in  the  current  catalog    string  list  databases    gets  the  names  of  all  tables  available  in  the  current  namespace  the  current  database  of  the  current  catalog    it  returns  both  temporary  and  permanent  tables  and  views  return  a  list  of  the  names  of  all  registered  tables  in  the  current  database  of  the  current  catalog  see  list  temporary  tables  see  list  temporary  views    string  list  tables    gets  the  names  of  all  views  available  in  the  current  namespace  the  current  database  of  the  current  catalog    it  returns  both  temporary  and  permanent  views  return  a  list  of  the  names  of  all  registered  views  in  the  current  database  of  the  current  catalog  see  list  temporary  views    string  list  views    gets  the  names  of  all  temporary  tables  and  views  available  in  the  current  namespace  the  current  database  of  the  current  catalog  return  a  list  of  the  names  of  all  registered  temporary  tables  and  views  in  the  current  database  of  the  current  catalog  see  list  tables    string  list  temporary  tables    gets  the  names  of  all  temporary  views  available  in  the  current  namespace  the  current  database  of  the  current  catalog  return  a  list  of  the  names  of  all  registered  temporary  views  in  the  current  database  of  the  current  catalog  see  list  tables    string  list  temporary  views    gets  the  names  of  all  user  defined  functions  registered  in  this  environment    string  list  user  defined  functions    gets  the  names  of  all  functions  in  this  environment    string  list  functions    drops  a  temporary  table  registered  in  the  given  path  p    if  a  permanent  table  with  a  given  path  exists  it  will  be  used  from  now  on  for  any  queries  that  reference  this  path  return  true  if  a  table  existed  in  the  given  path  and  was  removed  boolean  drop  temporary  table    string  path    drops  a  temporary  view  registered  in  the  given  path  p    if  a  permanent  table  or  view  with  a  given  path  exists  it  will  be  used  from  now  on  for  any  queries  that  reference  this  path  return  true  if  a  view  existed  in  the  given  path  and  was  removed  boolean  drop  temporary  view    string  path    returns  the  ast  of  the  specified    table  api  and  sql  queries  and  the  execution  plan  to  compute  the  result  of  the  given  link    table  param  table    the  table  for  which  the  ast  and  execution  plan  will  be  returned  deprecated  use  link    table  explain    explain  detail    deprecated    string  explain    table  table    returns  the  ast  of  the  specified    table  api  and  sql  queries  and  the  execution  plan  to  compute  the  result  of  the  given  link    table  param  table    the  table  for  which  the  ast  and  execution  plan  will  be  returned  param  extended  if  the  plan  should  contain  additional  properties  e  g  estimated  cost  traits  deprecated  use  link    table  explain    explain  detail    deprecated    string  explain    table  table  boolean  extended    returns  the  ast  of  the  specified    table  api  and  sql  queries  and  the  execution  plan  to  compute  the  result  of  multiple  sinks  plan  param  extended  if  the  plan  should  contain  additional  properties  e  g  estimated  cost  traits  deprecated  use  link    statement  set  explain    explain  detail    deprecated    string  explain  boolean  extended    returns  the  ast  of  the  specified  statement  and  the  execution  plan  to  compute  the  result  of  the  given  statement  param  statement    the  statement  for  which  the  ast  and  execution  plan  will  be  returned  param  extra  details    the  extra  explain  details  which  the  explain  result  should  include  e  g  estimated  cost  changelog  mode  for  streaming  return  ast  and  the  execution  plan    string  explain  sql    string  statement    explain  detail  extra  details    returns  completion  hints  for  the  given  statement  at  the  given  cursor  position    the  completion  happens  case  insensitively  param  statement    partial  or  slightly  incorrect  sql  statement  param  position  cursor  position  return  completion  hints  that  fit  at  the  current  cursor  position  deprecated    will  be  removed  in  the  next  release    deprecated    string  get  completion  hints    string  statement  int  position    evaluates  a  sql  query  on  registered  tables  and  retrieves  the  result  as  a  link    table  p    all  tables  referenced  by  the  query  must  be  registered  in  the    table  environment  a  link    table  is  automatically  registered  when  its  link    table  to  string  method  is  called  for  example  when  it  is  embedded  into  a    string    hence  sql  queries  can  directly  reference  a  link    table  as  follows  pre  code    table  table    string  table  name  table  to  string  the  table  is  not  registered  to  the  table  environment  t  env  sql  query  select  from  table  name  pre  param  query    the  sql  query  to  evaluate  return    the  result  of  the  query  as    table    table  sql  query    string  query    execute  the  given  single  statement  and  return  the  execution  result  p    the  statement  can  be  ddl  dml  dql  show  describe  explain  use    for  dml  and  dql  this  method  returns    table  result  once  the  job  has  been  submitted    for  ddl  and  dcl  statements    table  result  is  returned  once  the  operation  has  finished  return  content  for  dql  show  describe  explain  the  affected  row  count  for  dml    means  unknown  or  a  string  message  ok  for  other  statements    table  result  execute  sql    string  statement    evaluates  a  sql  statement  such  as  insert  update  or  delete  or  a  ddl  statement  note    currently  only  sql  insert  statements  and  create  table  statements  are  supported  p    all  tables  referenced  by  the  query  must  be  registered  in  the    table  environment  a  link    table  is  automatically  registered  when  its  link    table  to  string  method  is  called  for  example  when  it  is  embedded  into  a    string    hence  sql  queries  can  directly  reference  a  link    table  as  follows  pre  code  register  the  configured  table  sink  into  which  the  result  is  inserted  t  env  register  table  sink  internal  sink  table  configured  sink    table  source  table    string  table  name  source  table  to  string  source  table  is  not  registered  to  the  table  environment  t  env  sql  update  s  insert  into  sink  table  select  from  table  name  pre  p  a  ddl  statement  can  also  be  executed  to  create  a  table    for  example  the  below  ddl  statement  would  create  a  csv  table  named  tbl1  into  the  current  catalog  blockquote  pre  create  table  tbl1  a  int  b  bigint  c  varchar  with  connector  type  filesystem  format  type  csv  connector  path  xxx  pre  blockquote  p  sql  queries  can  directly  execute  as  follows  blockquote  pre    string  sink  d  d  l  create  table  sink  table  a  int  b  varchar  with  connector  type  filesystem  format  type  csv  connector  path  xxx    string  source  d  d  l  create  table  source  table  a  int  b  varchar  with  connector  type  kafka  update  mode  append  connector  topic  xxx  connector  properties  bootstrap  servers  localhost      string  query  insert  into  sink  table  select  from  source  table  t  env  sql  update  source  d  d  l  t  env  sql  update  sink  d  d  l  t  env  sql  update  query  t  env  execute    my  job  pre  blockquote    this  code  snippet  creates  a  job  to  read  data  from    kafka  source  into  a  csv  sink  param  stmt    the  sql  statement  to  evaluate  deprecated  use  link  execute  sql    string  for  single  statement  use  link    table  environment  create  statement  set  for  multiple  dml  statements    deprecated  void  sql  update    string  stmt    gets  the  current  default  catalog  name  of  the  current  session  return    the  current  default  catalog  name  that  is  used  for  the  path  resolution  see    table  environment  use  catalog    string    string  get  current  catalog    sets  the  current  catalog  to  the  given  value    it  also  sets  the  default  database  to  the  catalog  s  default  one    see  also  link    table  environment  use  database    string  p    this  is  used  during  the  resolution  of  object  paths    both  the  catalog  and  database  are  optional  when  referencing  catalog  objects  such  as  tables  views  etc    the  algorithm  looks  for  requested  objects  in  following  paths  in  that  order  ol  li  code  current  catalog  current  database  requested  path  li  li  code  current  catalog  requested  path  li  li  code  requested  path  li  ol  p    example  p    given  structure  with  default  catalog  set  to  code  default  catalog  and  default  database  set  to  code  default  database  pre  root  default  catalog  default  database  tab1  db1  tab1  cat1  db1  tab1  pre  p    the  following  table  describes  resolved  paths  table  thead  tr  th    requested  path  th  th    resolved  path  th  tr  thead  tbody  tr  td  tab1  td  td  default  catalog  default  database  tab1  td  tr  tr  td  db1  tab1  td  td  default  catalog  db1  tab1  td  tr  tr  td  cat1  db1  tab1  td  td  cat1  db1  tab1  td  tr  tbody  table  param  catalog  name    the  name  of  the  catalog  to  set  as  the  current  default  catalog  see    table  environment  use  database    string  void  use  catalog    string  catalog  name    gets  the  current  default  database  name  of  the  running  session  return    the  name  of  the  current  database  of  the  current  catalog  see    table  environment  use  database    string    string  get  current  database    sets  the  current  default  database    it  has  to  exist  in  the  current  catalog    that  path  will  be  used  as  the  default  one  when  looking  for  unqualified  object  names  p    this  is  used  during  the  resolution  of  object  paths    both  the  catalog  and  database  are  optional  when  referencing  catalog  objects  such  as  tables  views  etc    the  algorithm  looks  for  requested  objects  in  following  paths  in  that  order  ol  li  code  current  catalog  current  database  requested  path  li  li  code  current  catalog  requested  path  li  li  code  requested  path  li  ol  p    example  p    given  structure  with  default  catalog  set  to  code  default  catalog  and  default  database  set  to  code  default  database  pre  root  default  catalog  default  database  tab1  db1  tab1  cat1  db1  tab1  pre  p  p    the  following  table  describes  resolved  paths  table  thead  tr  th    requested  path  th  th    resolved  path  th  tr  thead  tbody  tr  td  tab1  td  td  default  catalog  default  database  tab1  td  tr  tr  td  db1  tab1  td  td  default  catalog  db1  tab1  td  tr  tr  td  cat1  db1  tab1  td  td  cat1  db1  tab1  td  tr  tbody  table  param  database  name    the  name  of  the  database  to  set  as  the  current  database  see    table  environment  use  catalog    string  void  use  database    string  database  name    returns  the  table  config  that  defines  the  runtime  behavior  of  the    table  api    table  config  get  config    triggers  the  program  execution    the  environment  will  execute  all  parts  of  the  program  p    the  program  execution  will  be  logged  and  displayed  with  the  provided  name  p  b  note  b    it  is  highly  advised  to  set  all  parameters  in  the  link    table  config  on  the  very  beginning  of  the  program    it  is  undefined  what  configurations  values  will  be  used  for  the  execution  if  queries  are  mixed  with  config  changes    it  depends  on  the  characteristic  of  the  particular  parameter    for  some  of  them  the  value  from  the  point  in  time  of  query  construction  e  g  the  current  catalog  will  be  used    on  the  other  hand  some  values  might  be  evaluated  according  to  the  state  from  the  time  when  this  method  is  called  e  g  time  zone  p    once  the  execution  finishes  any  previously  defined    d  m  ls  will  be  cleared  no  matter  whether  the  execution  succeeds  or  not    therefore  if  you  want  to  retry  in  case  of  failures  you  have  to  re  define  the    d  m  ls  i  e  by  calling  link  sql  update    string  before  you  call  this  method  again  param  job  name    desired  name  of  the  job  return    the  result  of  the  job  execution  containing  elapsed  time  and  accumulators  throws    exception  which  occurs  during  job  execution  deprecated  use  link  execute  sql    string  or  link    table  execute  insert    string  for  single  sink  use  link  create  statement  set  for  multiple  sinks    deprecated    job  execution  result  execute    string  job  name  throws    exception    create  a  link    statement  set  instance  which  accepts  dml  statements  or    tables  the  planner  can  optimize  all  added  statements  and    tables  together  and  then  submit  as  one  job    statement  set  create  statement  set  
public  evolving  public  interface    table  result    for  dml  and  dql  statement  return  the  link    job  client  which  associates  the  submitted    flink  job    for  other  statements  e  g  ddl  dcl  return  empty    optional    job  client  get  job  client    get  the  schema  of  result  p    the  schema  of  ddl  use  explain  pre  column  name  column  type  comments  result  string  pre  p    the  schema  of  show  pre  column  name  column  type  comments  lt  object  name  gt  string    the  column  name  of  show  catalogs  is  catalog  name  the  column  name  of  show  databases  is  database  name  the  column  name  of  show  tables  is  table  name  the  column  name  of  show  views  is  view  name  the  column  name  of  show  functions  is  function  name  pre  p    the  schema  of  describe  pre  column  name  column  type  comments  name  string  field  name  type  string  field  type  expressed  as  a    string  null  boolean  field  nullability  true  if  a  field  is  nullable  else  false  key  boolean  key  constraint  pri  for  primary  keys  unq  for  unique  keys  else  null  computed  column  string  computed  column  string  expression  if  a  field  is  computed  column  else  null  watermark  string  watermark  string  expression  if  a  field  is  watermark  else  null  pre  p    the  schema  of  insert  one  column  per  one  sink  pre  column  name  column  type  comments  name  of  the  insert  table  bigint  the  insert  table  name  pre  p    the  schema  of  select  is  the  selected  field  names  and  types    table  schema  get  table  schema    return  the  link    result  kind  which  represents  the  result  type  p    for  ddl  operation  and  use  operation  the  result  kind  is  always  link    result  kind  success    for  other  operations  the  result  kind  is  always  link    result  kind  success  with  content    result  kind  get  result  kind    get  the  result  contents  as  a  closeable  row  iterator  p  strong  note  strong  ul  li    for  select  operation  the  job  will  not  be  finished  unless  all  result  data  has  been  collected    so  we  should  actively  close  the  job  to  avoid  resource  leak  through    closeable  iterator  close  method    calling    closeable  iterator  close  method  will  cancel  the  job  and  release  related  resources  li  li    for  dml  operation    flink  does  not  support  getting  the  real  affected  row  count  now    so  the  affected  row  count  is  always    unknown  for  every  sink  and  them  will  be  returned  after  the  job  is  submitted    calling    closeable  iterator  close  method  does  not  bind  to  the  job    therefore  the    closeable  iterator  close  will  not  cancel  the  job  as  in  the  case  of  select    if  you  need  to  cancel  the  job  you  can  use  the  link  get  job  client  li  li    for  other  operations  no  flink  job  will  be  submitted  link  get  job  client  is  always  empty  and  the  result  is  bounded    do  nothing  when  calling    closeable  iterator  close  method  li  ul  p    recommended  code  to  call    closeable  iterator  close  method  looks  like  pre  code    table  result  result  t  env  execute  select  using  try  with  resources  statement  try    closeable  iterator    row  it  result  collect  it  collect  same  data  pre  p    for  streaming  mode  this  method  guarantees  end  to  end  exactly  once  record  delivery  which  requires  the  checkpointing  mechanism  to  be  enabled    by  default  checkpointing  is  disabled    to  enable  checkpointing  set  checkpointing  properties  see    execution  checkpointing  options  through  link    table  config  get  configuration  p    in  order  to  fetch  result  to  local  you  can  call  either  link  collect  and  link  print    but  they  can  t  be  called  both  on  the  same  link    table  result  instance  because  the  result  can  only  be  accessed  once    closeable  iterator    row  collect    print  the  result  contents  as  tableau  form  to  client  console  p    for  streaming  mode  this  method  guarantees  end  to  end  exactly  once  record  delivery  which  requires  the  checkpointing  mechanism  to  be  enabled    by  default  checkpointing  is  disabled    to  enable  checkpointing  set  checkpointing  properties  see    execution  checkpointing  options  through  link    table  config  get  configuration  p    in  order  to  fetch  result  to  local  you  can  call  either  link  collect  and  link  print    but  they  can  t  be  called  both  on  the  same  link    table  result  instance  because  the  result  can  only  be  accessed  once  void  print  
public  evolving  public  final  class    tumble    creates  a  tumbling  window    tumbling  windows  are  fixed  size  consecutive  non  overlapping  windows  of  a  specified  fixed  length    for  example  a  tumbling  window  of    minutes  size  groups  elements  in    minutes  intervals  param  size  the  size  of  the  window  as  time  or  row  count  interval  return  a  partially  defined  tumbling  window  deprecated  use  link  over    expression    deprecated  public  static    tumble  with  size  over    string  size  return  over    expression  parser  parse  expression  size    creates  a  tumbling  window    tumbling  windows  are  fixed  size  consecutive  non  overlapping  windows  of  a  specified  fixed  length    for  example  a  tumbling  window  of    minutes  size  groups  elements  in    minutes  intervals  param  size  the  size  of  the  window  as  time  or  row  count  interval  return  a  partially  defined  tumbling  window  public  static    tumble  with  size  over    expression  size  return  new    tumble  with  size  size  
public  evolving  public  final  class    tumble  with  size    the  size  of  the  window  either  as  time  or  row  count  interval  private    expression  size    tumble  with  size    expression  size  this  size    api  expression  utils  unwrap  from  api  size    specifies  the  time  attribute  on  which  rows  are  grouped  p    for  streaming  tables  you  can  specify  grouping  by  a  event  time  or  processing  time  attribute  p    for  batch  tables  you  can  specify  grouping  on  a  timestamp  or  long  attribute  param  time  field  time  attribute  for  streaming  and  batch  tables  return  a  tumbling  window  on  event  time  public    tumble  with  size  on  time  on    expression  time  field  return  new    tumble  with  size  on  time  time  field  size    specifies  the  time  attribute  on  which  rows  are  grouped  p    for  streaming  tables  you  can  specify  grouping  by  a  event  time  or  processing  time  attribute  p    for  batch  tables  you  can  specify  grouping  on  a  timestamp  or  long  attribute  param  time  field  time  attribute  for  streaming  and  batch  tables  return  a  tumbling  window  on  event  time  deprecated  use  link  on    expression    deprecated  public    tumble  with  size  on  time  on    string  time  field  return  on    expression  parser  parse  expression  time  field  
public  evolving  public  final  class    tumble  with  size  on  time  private  final    expression  time  private  final    expression  size    tumble  with  size  on  time    expression  time    expression  size  this  time    api  expression  utils  unwrap  from  api  time  this  size    api  expression  utils  unwrap  from  api  size    assigns  an  alias  for  this  window  that  the  following  code  group  by  and  code  select  clause  can  refer  to  code  select  statement  can  access  window  properties  such  as  window  start  or  end  time  param  alias  alias  for  this  window  return  this  window  public    tumble  with  size  on  time  with  alias  as    expression  alias  return  new    tumble  with  size  on  time  with  alias  alias  time  size    assigns  an  alias  for  this  window  that  the  following  code  group  by  and  code  select  clause  can  refer  to  code  select  statement  can  access  window  properties  such  as  window  start  or  end  time  param  alias  alias  for  this  window  return  this  window  public    tumble  with  size  on  time  with  alias  as    string  alias  return  as    expression  parser  parse  expression  alias  
public  evolving  public  final  class    tumble  with  size  on  time  with  alias  extends    group  window  private  final    expression  size    tumble  with  size  on  time  with  alias    expression  alias    expression  time  field    expression  size  super  alias  time  field  this  size    api  expression  utils  unwrap  from  api  size  public    expression  get  size  return  size  
public  evolving  public  interface    window  grouped  table    performs  a  selection  operation  on  a  window  grouped  table    similar  to  an  sql  select  statement    the  field  expressions  can  contain  complex  expressions  and  aggregations  p    example  pre  code  window  grouped  table  select  key  window  start  value  avg  as  valavg  pre  deprecated  use  link  select    expression    deprecated    table  select    string  fields    performs  a  selection  operation  on  a  window  grouped  table    similar  to  an  sql  select  statement    the  field  expressions  can  contain  complex  expressions  and  aggregations  p    example  pre  code  window  grouped  table  select  key  window  start  value  avg  as  valavg  pre  p    scala    example  pre  code  window  grouped  table  select  key  window  start  value  avg  as  valavg  pre    table  select    expression  fields    performs  an  aggregate  operation  on  a  window  grouped  table    you  have  to  close  the  link  aggregate    string  with  a  select  statement    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code    aggregate  function  agg  func  new    my  aggregate  function  table  env  register  function  agg  func  agg  func  window  grouped  table  aggregate  agg  func  a  b  as  x  y  z  select  key  window  start  x  y  z  pre  deprecated  use  link  aggregate    expression    deprecated    aggregated  table  aggregate    string  aggregate  function    performs  an  aggregate  operation  on  a  window  grouped  table    you  have  to  close  the  link  aggregate    expression  with  a  select  statement    the  output  will  be  flattened  if  the  output  type  is  a  composite  type  p    example  pre  code  window  grouped  table  aggregate  call    my  aggregate  function  class  a  b  as  x  y  z  select  key  window  start  x  y  z  pre  p    scala    example  pre  code  val  agg  func  new    my  aggregate  function  window  grouped  table  aggregate  agg  func  a  b  as  x  y  z  select  key  window  start  x  y  z  pre    aggregated  table  aggregate    expression  aggregate  function    performs  a  flat  aggregate  operation  on  a  window  grouped  table    flat  aggregate  takes  a    table  aggregate  function  which  returns  multiple  rows    use  a  selection  after  flat  aggregate  p    example  pre  code    table  aggregate  function  table  agg  func  new    my  table  aggregate  function  table  env  register  function  table  agg  func  table  agg  func  window  grouped  table  flat  aggregate  table  agg  func  a  b  as  x  y  z  select  key  window  start  x  y  z  pre  deprecated  use  link  flat  aggregate    expression    deprecated    flat  aggregate  table  flat  aggregate    string  table  aggregate  function    performs  a  flat  aggregate  operation  on  a  window  grouped  table    flat  aggregate  takes  a    table  aggregate  function  which  returns  multiple  rows    use  a  selection  after  flat  aggregate  p    example  pre  code  window  grouped  table  flat  aggregate  call    my  table  aggregate  function  class  a  b  as  x  y  z  select  key  window  start  x  y  z  pre  p    scala    example  pre  code  val  table  agg  func  new    my  table  aggregate  function  window  grouped  table  flat  aggregate  table  agg  func  a  b  as  x  y  z  select  key  window  start  x  y  z  pre    flat  aggregate  table  flat  aggregate    expression  table  aggregate  function  
public  evolving  public  final  class    catalog  table  builder  extends    table  descriptor    catalog  table  builder  private  final    table  schema  table  schema  private    string  comment  private  final  boolean  is  generic  private    list    string  partition  keys  new    array  list  private    map    string    string  properties    collections  empty  map  public    catalog  table  builder    connector  descriptor  connector  descriptor    table  schema  table  schema  super  connector  descriptor  this  table  schema    preconditions  check  not  null  table  schema    we  don  t  support  non  generic  table  currently  this  is  generic  true  public    catalog  table  builder  with  comment    string  comment  this  comment    preconditions  check  not  null  comment    comment  must  not  be  null  return  this  public    catalog  table  builder  with  properties    map    string    string  properties  this  properties    preconditions  check  not  null  properties    properties  must  not  be  null  return  this  public    catalog  table  builder  with  partition  keys    list    string  partition  keys  this  partition  keys    preconditions  check  not  null  partition  keys    partition  keys  must  not  be  null  return  this    builds  a  link    catalog  table  public    catalog  table  build  return  new    catalog  table  impl  table  schema  partition  keys  to  properties  comment    override  protected    map    string    string  additional  properties    descriptor  properties  descriptor  properties  new    descriptor  properties  descriptor  properties  put  boolean    catalog  config  is  generic  is  generic  descriptor  properties  put  properties  this  properties  return  descriptor  properties  as  map  
public  evolving  public  final  class    batch  table  descriptor  extends    connect  table  descriptor  public    batch  table  descriptor    registration  registration    connector  descriptor  connector  descriptor  super  registration  connector  descriptor    override  public    batch  table  descriptor  with  schema    schema  schema  return    batch  table  descriptor  super  with  schema  schema  
public  evolving  public  abstract  class    connect  table  descriptor  extends    table  descriptor    connect  table  descriptor  private  final    registration  registration  private    nullable    schema  schema  descriptor  private    list    string  partition  keys  new    array  list  public    connect  table  descriptor    registration  registration    connector  descriptor  connector  descriptor  super  connector  descriptor  this  registration  registration    specifies  the  resulting  table  schema  public    connect  table  descriptor  with  schema    schema  schema  schema  descriptor    preconditions  check  not  null  schema    schema  must  not  be  null  return  this    specifies  the  partition  keys  of  this  table  public    connect  table  descriptor  with  partition  keys    list    string  partition  keys  this  partition  keys    preconditions  check  not  null  partition  keys    partition  keys  must  not  be  null  return  this    registers  the  table  described  by  underlying  properties  in  a  given  path  p    there  is  no  distinction  between  source  and  sink  at  the  descriptor  level  anymore  as  this  method  does  not  perform  actual  class  lookup    it  only  stores  the  underlying  properties    the  actual  source  sink  lookup  is  performed  when  the  table  is  used  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  p  b  note  b    the  schema  must  be  explicitly  defined  param  path  path  where  to  register  the  temporary  table  public  void  create  temporary  table    string  path  if  schema  descriptor  null  throw  new    table  exception    table  schema  must  be  explicitly  defined    to  derive  schema  from  the  underlying  connector  use  register  table  source  internal  register  table  sink  internal  register  table  source  and  sink  registration  create  temporary  table  path    catalog  table  impl  from  properties  to  properties    override  protected    map    string    string  additional  properties    descriptor  properties  properties  new    descriptor  properties  if  schema  descriptor  null  properties  put  properties  schema  descriptor  to  properties  properties  put  partition  keys  partition  keys  return  properties  as  map  
public  evolving  public  final  class    stream  table  descriptor  extends    connect  table  descriptor  public    stream  table  descriptor    registration  registration    connector  descriptor  connector  descriptor  super  registration  connector  descriptor    override  public    stream  table  descriptor  with  schema    schema  schema  return    stream  table  descriptor  super  with  schema  schema  
public  evolving  public  final  class    lookup  call  expression  implements    expression  private  final    string  unresolved  name  private  final    list    expression  args    lookup  call  expression    string  unresolved  function    list    expression  args  this  unresolved  name    preconditions  check  not  null  unresolved  function  this  args    collections  unmodifiable  list    preconditions  check  not  null  args  public    string  get  unresolved  name  return  unresolved  name    override  public    string  as  summary  string  final    list    string  arg  list  args  stream  map    object  to  string  collect    collectors  to  list  return  unresolved  name    string  join  arg  list    override  public    list    expression  get  children  return  this  args    override  public  r  r  accept    expression  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    lookup  call  expression  that    lookup  call  expression  o  return    objects  equals  unresolved  name  that  unresolved  name    objects  equals  args  that  args    override  public  int  hash  code  return    objects  hash  unresolved  name  args    override  public    string  to  string  return  as  summary  string  
public  evolving  public  final  class    table  reference  expression  implements    resolved  expression  private  final    string  name  private  final    query  operation  query  operation    table  reference  expression    string  name    query  operation  query  operation  this  name    preconditions  check  not  null  name  this  query  operation    preconditions  check  not  null  query  operation  public    string  get  name  return  name  public    query  operation  get  query  operation  return  query  operation    override  public    data  type  get  output  data  type  return  query  operation  get  table  schema  to  row  data  type    override  public    list    resolved  expression  get  resolved  children  return    collections  empty  list    override  public    string  as  summary  string  return  name    override  public    list    expression  get  children  return    collections  empty  list    override  public  r  r  accept    expression  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    table  reference  expression  that    table  reference  expression  o  return    objects  equals  name  that  name    objects  equals  query  operation  that  query  operation    override  public  int  hash  code  return    objects  hash  name  query  operation    override  public    string  to  string  return  as  summary  string  
public  evolving  public  final  class    unresolved  call  expression  implements    expression  private  final    nullable    function  identifier  function  identifier  private  final    function  definition  function  definition  private  final    list    expression  args    unresolved  call  expression    function  identifier  function  identifier    function  definition  function  definition    list    expression  args  this  function  identifier    preconditions  check  not  null  function  identifier    function  identifier  must  not  be  null  this  function  definition    preconditions  check  not  null  function  definition    function  definition  must  not  be  null  this  args    collections  unmodifiable  list    preconditions  check  not  null  args    arguments  must  not  be  null    unresolved  call  expression    function  definition  function  definition    list    expression  args  this  function  identifier  null  this  function  definition    preconditions  check  not  null  function  definition    function  definition  must  not  be  null  this  args    collections  unmodifiable  list    preconditions  check  not  null  args    arguments  must  not  be  null  public    optional    function  identifier  get  function  identifier  return    optional  of  nullable  function  identifier  public    function  definition  get  function  definition  return  function  definition  public    unresolved  call  expression  replace  args    list    expression  args  if  function  identifier  null  return  new    unresolved  call  expression  function  definition  args  return  new    unresolved  call  expression  function  identifier  function  definition  args  public    call  expression  resolve    list    resolved  expression  args    data  type  data  type  if  function  identifier  null  return  new    call  expression  function  definition  args  data  type  return  new    call  expression  function  identifier  function  definition  args  data  type    override  public    string  as  summary  string  final    string  function  name  if  function  identifier  null  function  name  function  definition  to  string  else  function  name  function  identifier  as  summary  string  final    string  arg  list  args  stream  map    expression  as  summary  string  collect    collectors  joining  return  function  name  arg  list    override  public    list    expression  get  children  return  this  args    override  public  r  r  accept    expression  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    unresolved  call  expression  that    unresolved  call  expression  o  return    objects  equals  function  identifier  that  function  identifier  function  definition  equals  that  function  definition  args  equals  that  args    override  public  int  hash  code  return    objects  hash  function  identifier  function  definition  args    override  public    string  to  string  return  as  summary  string  
public  evolving  public  final  class    unresolved  reference  expression  implements    expression  private  final    string  name    unresolved  reference  expression    string  name  this  name    preconditions  check  not  null  name  public    string  get  name  return  name    override  public    string  as  summary  string  return  name    override  public    list    expression  get  children  return    collections  empty  list    override  public  r  r  accept    expression  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    unresolved  reference  expression  that    unresolved  reference  expression  o  return    objects  equals  name  that  name    override  public  int  hash  code  return    objects  hash  name    override  public    string  to  string  return  as  summary  string  
public  evolving  public  interface    component  factory  extends    table  factory    specifies  a  context  of  optional  parameters  that  if  exist  should  have  the  given  values    this  enables  further  disambiguating  if  there  are  multiple  factories  that  meet  the  link  required  context  and  link  supported  properties  p  b  note  b    all  the  property  keys  should  be  included  in  link  supported  properties  return  optional  properties  to  disambiguate  factories    map    string    string  optional  context    override    map    string    string  required  context  inherit  doc  p  b  note  b    all  the  property  keys  from  link  optional  context  should  also  be  included    override    list    string  supported  properties  
public  evolving  public  interface    operation    returns  a  string  that  summarizes  this  operation  for  printing  to  a  console    an  implementation  might  skip  very  specific  properties  return  summary  string  of  this  operation  for  debugging  purposes    string  as  summary  string  
public  evolving  public  final  class    existing  field  extends    timestamp  extractor  private  static  final  long  serial  version  u  i  d  1  l  private    string  field  param  field    the  field  to  convert  into  a  rowtime  attribute  public    existing  field    string  field  this  field  check  not  null  field    override  public    string  get  argument  fields  return  new    string  field    override  public  void  validate  argument  fields    type  information  argument  field  types    data  type  field  type  from  legacy  info  to  data  type  argument  field  types    switch  field  type  get  logical  type  get  type  root  case  bigint  case  timestamp  without  time  zone  case  varchar  break  default  throw  new    validation  exception    string  format    field  s  must  be  of  type    long  or    timestamp  or    string  but  is  of  type  s  field  field  type    returns  an  link    expression  that  casts  a  link    long  link    timestamp  or  timestamp  formatted  link    string  field  e  g  2018-05  28 12  34:56    into  a  rowtime  attribute    override  public    expression  get  expression    resolved  field  reference  field  accesses    resolved  field  reference  field  access  field  accesses      data  type  type  from  legacy  info  to  data  type  field  access  result  type    field  reference  expression  field  reference  expr  new    field  reference  expression  field  access  name  type    field  access  field  index  switch  type  get  logical  type  get  type  root  case  bigint  case  timestamp  without  time  zone  return  field  reference  expr  case  varchar    data  type  output  type  timestamp    bridged  to    timestamp  class  return  new    call  expression  cast    arrays  as  list  field  reference  expr  type  literal  output  type  output  type  default  throw  new    runtime  exception    unsupport  type  type    override  public    map    string    string  to  properties    map    string    string  map  new    hash  map  map  put    rowtime  rowtime  timestamps  type    rowtime  rowtime  timestamps  type  value  from  field  map  put    rowtime  rowtime  timestamps  from  field  return  map    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    existing  field  that    existing  field  o  return  field  equals  that  field    override  public  int  hash  code  return  field  hash  code  
public  evolving  public  final  class    stream  record  timestamp  extends    timestamp  extractor  private  static  final  long  serial  version  u  i  d  1  l  public  static  final    stream  record  timestamp  instance  new    stream  record  timestamp    override  public    string  get  argument  fields  return  new    string      override  public  void  validate  argument  fields    type  information  argument  field  types    override  public    expression  get  expression    resolved  field  reference  field  accesses  return    api  expression  utils  unresolved  call  stream  record  timestamp    override  public    map    string    string  to  properties    map    string    string  map  new    hash  map  map  put    rowtime  rowtime  timestamps  type    rowtime  rowtime  timestamps  type  value  from  source  return  map    override  public  boolean  equals    object  o  return  this  o  o  null  get  class  o  get  class    override  public  int  hash  code  return  this  get  class  hash  code  
public  evolving  public  interface    batch  table  environment  extends    table  environment    registers  a  link    table  function  under  a  unique  name  in  the    table  environment  s  catalog    registered  functions  can  be  referenced  in    table  api  and  sql  queries  param  name    the  name  under  which  the  function  is  registered  param  table  function    the    table  function  to  register  param  t    the  type  of  the  output  row  t  void  register  function    string  name    table  function  t  table  function    registers  an  link    aggregate  function  under  a  unique  name  in  the    table  environment  s  catalog    registered  functions  can  be  referenced  in    table  api  and  sql  queries  param  name    the  name  under  which  the  function  is  registered  param  aggregate  function    the    aggregate  function  to  register  param  t    the  type  of  the  output  value  param  acc    the  type  of  aggregate  accumulator  t  acc  void  register  function    string  name    aggregate  function  t  acc  aggregate  function    converts  the  given  link    data  set  into  a  link    table  p    the  field  names  of  the  link    table  are  automatically  derived  from  the  type  of  the  link    data  set  param  data  set    the  link    data  set  to  be  converted  param  t    the  type  of  the  link    data  set  return    the  converted  link    table  t    table  from  data  set    data  set  t  data  set    converts  the  given  link    data  set  into  a  link    table  with  specified  field  names  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the  link    table  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  set    tuple2    string    long  set  use  the  original  f0  field  and  give  a  better  name  to  the  f1  field    table  table  table  env  from  table  set  f0  f1  as  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  set    tuple2    string    long  set  renames  the  original  fields  as  a  and  b    table  table  table  env  from  data  set  set  a  b  pre  param  data  set    the  link    data  set  to  be  converted  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  set  to  the  fields  of  the  link    table  param  t    the  type  of  the  link    data  set  return    the  converted  link    table  deprecated  use  link  from  data  set    data  set    expression    deprecated  t    table  from  data  set    data  set  t  data  set    string  fields    converts  the  given  link    data  set  into  a  link    table  with  specified  field  names  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the  link    table  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  set    tuple2    string    long  set    table  table  table  env  from  data  set  set  f1  reorder  and  use  the  original  field  f0  as  name  reorder  and  give  the  original  field  a  better  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  set    tuple2    string    long  set    table  table  table  env  from  data  set  set  a  renames  the  first  field  to  a  b  renames  the  second  field  to  b  pre  param  data  set    the  link    data  set  to  be  converted  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  set  to  the  fields  of  the  link    table  param  t    the  type  of  the  link    data  set  return    the  converted  link    table  t    table  from  data  set    data  set  t  data  set    expression  fields    creates  a  view  from  the  given  link    data  set    registered  views  can  be  referenced  in  sql  queries  p    the  field  names  of  the  link    table  are  automatically  derived  from  the  type  of  the  link    data  set  p    the  view  is  registered  in  the  namespace  of  the  current  catalog  and  database    to  register  the  view  in  a  different  catalog  use  link  create  temporary  view    string    data  set  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  name    the  name  under  which  the  link    data  set  is  registered  in  the  catalog  param  data  set    the  link    data  set  to  register  param  t    the  type  of  the  link    data  set  to  register  deprecated  use  link  create  temporary  view    string    data  set    deprecated  t  void  register  data  set    string  name    data  set  t  data  set    creates  a  view  from  the  given  link    data  set  in  a  given  path    registered  views  can  be  referenced  in  sql  queries  p    the  field  names  of  the  link    table  are  automatically  derived  from  the  type  of  the  link    data  set  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  path    the  path  under  which  the  view  is  created    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  data  set    the  link    data  set  out  of  which  to  create  the  view  param  t    the  type  of  the  link    data  set  t  void  create  temporary  view    string  path    data  set  t  data  set    creates  a  view  from  the  given  link    data  set  in  a  given  path  with  specified  field  names    registered  views  can  be  referenced  in  sql  queries  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the    view  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  set    tuple2    string    long  set  use  the  original  f0  field  and  give  a  better  name  to  the  f1  field  table  env  register  data  set  my  table  set  f0  f1  as  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  set    tuple2    string    long  set  renames  the  original  fields  as  a  and  b  table  env  register  data  set  my  table  set  a  b  pre  p    the  view  is  registered  in  the  namespace  of  the  current  catalog  and  database    to  register  the  view  in  a  different  catalog  use  link  create  temporary  view    string    data  set  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  name    the  name  under  which  the  link    data  set  is  registered  in  the  catalog  param  data  set    the  link    data  set  to  register  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  set  to  the  fields  of  the    view  param  t    the  type  of  the  link    data  set  to  register  deprecated  use  link  create  temporary  view    string    data  set    string    deprecated  t  void  register  data  set    string  name    data  set  t  data  set    string  fields    creates  a  view  from  the  given  link    data  set  in  a  given  path  with  specified  field  names    registered  views  can  be  referenced  in  sql  queries  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the    view  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  set    tuple2    string    long  set  use  the  original  f0  field  and  give  a  better  name  to  the  f1  field  table  env  create  temporary  view  cat  db  my  table  set  f0  f1  as  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  set    tuple2    string    long  set  renames  the  original  fields  as  a  and  b  table  env  create  temporary  view  cat  db  my  table  set  a  b  pre  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  path    the  path  under  which  the  view  is  created    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  data  set    the  link    data  set  out  of  which  to  create  the  view  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  set  to  the  fields  of  the    view  param  t    the  type  of  the  link    data  set  deprecated  use  link  create  temporary  view    string    data  set    expression    deprecated  t  void  create  temporary  view    string  path    data  set  t  data  set    string  fields    creates  a  view  from  the  given  link    data  set  in  a  given  path  with  specified  field  names    registered  views  can  be  referenced  in  sql  queries  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the    view  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  set    tuple2    string    long  set  table  env  create  temporary  view  cat  db  my  table  set  f1  reorder  and  use  the  original  field  f0  as  name  reorder  and  give  the  original  field  a  better  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  set    tuple2    string    long  set  table  env  create  temporary  view  cat  db  my  table  set  a  renames  the  first  field  to  a  b  renames  the  second  field  to  b  pre  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  path    the  path  under  which  the  view  is  created    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  data  set    the  link    data  set  out  of  which  to  create  the  view  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  set  to  the  fields  of  the    view  param  t    the  type  of  the  link    data  set  t  void  create  temporary  view    string  path    data  set  t  data  set    expression  fields    converts  the  given  link    table  into  a  link    data  set  of  a  specified  type  p    the  fields  of  the  link    table  are  mapped  to  link    data  set  fields  as  follows  ul  li  link  org  apache  flink  types    row  and  link  org  apache  flink  api  java  tuple    tuple  types    fields  are  mapped  by  position  field  types  must  match  li  li  pojo  link    data  set  types    fields  are  mapped  by  field  name  field  types  must  match  li  ul  param  table    the  link    table  to  convert  param  clazz    the  class  of  the  type  of  the  resulting  link    data  set  param  t    the  type  of  the  resulting  link    data  set  return    the  converted  link    data  set  t    data  set  t  to  data  set    table  table    class  t  clazz    converts  the  given  link    table  into  a  link    data  set  of  a  specified  type  p    the  fields  of  the  link    table  are  mapped  to  link    data  set  fields  as  follows  ul  li  link  org  apache  flink  types    row  and  link  org  apache  flink  api  java  tuple    tuple  types    fields  are  mapped  by  position  field  types  must  match  li  li  pojo  link    data  set  types    fields  are  mapped  by  field  name  field  types  must  match  li  ul  param  table    the  link    table  to  convert  param  type  info    the  link    type  information  that  specifies  the  type  of  the  resulting  link    data  set  param  t    the  type  of  the  resulting  link    data  set  return    the  converted  link    data  set  t    data  set  t  to  data  set    table  table    type  information  t  type  info    creates  a  temporary  table  from  a  descriptor  p    descriptors  allow  for  declaring  the  communication  to  external  systems  in  an  implementation  agnostic  way    the  classpath  is  scanned  for  suitable  table  factories  that  match  the  desired  configuration  p    the  following  example  shows  how  to  read  from  a  connector  using  a  json  format  and  registering  a  temporary  table  as    my  table  pre  code  table  env  connect  new    external  system  x  y  z  version  0.11  with  format  new    json  json  schema  fail  on  missing  field  false  with  schema  new    schema  field  user  name  varchar  from  u  name  field  count  decimal  create  temporary  table    my  table  pre  param  connector  descriptor  connector  descriptor  describing  the  external  system  deprecated    the  sql  code  create  table  ddl  is  richer  than  this  part  of  the  api    this  method  might  be  refactored  in  the  next  versions    please  use  link  execute  sql    string  execute  sql  ddl  to  register  a  table  instead    override    deprecated    batch  table  descriptor  connect    connector  descriptor  connector  descriptor    returns  a  link    table  environment  for  a    java  batch  link    execution  environment  that  works  with  link    data  set  s  p  a    table  environment  can  be  used  to  ul  li  convert  a  link    data  set  to  a  link    table  li  li  register  a  link    data  set  in  the  link    table  environment  s  catalog  li  li  register  a  link    table  in  the  link    table  environment  s  catalog  li  li  scan  a  registered  table  to  obtain  a  link    table  li  li  specify  a  sql  query  on  registered  tables  to  obtain  a  link    table  li  li  convert  a  link    table  into  a  link    data  set  li  li  explain  the  ast  and  execution  plan  of  a  link    table  li  ul  param  execution  environment    the    java  batch  link    execution  environment  of  the    table  environment  static    batch  table  environment  create    execution  environment  execution  environment  return  create  execution  environment  new    table  config    returns  a  link    table  environment  for  a    java  batch  link    execution  environment  that  works  with  link    data  set  s  p  a    table  environment  can  be  used  to  ul  li  convert  a  link    data  set  to  a  link    table  li  li  register  a  link    data  set  in  the  link    table  environment  s  catalog  li  li  register  a  link    table  in  the  link    table  environment  s  catalog  li  li  scan  a  registered  table  to  obtain  a  link    table  li  li  specify  a  sql  query  on  registered  tables  to  obtain  a  link    table  li  li  convert  a  link    table  into  a  link    data  set  li  li  explain  the  ast  and  execution  plan  of  a  link    table  li  ul  param  execution  environment    the    java  batch  link    execution  environment  of  the    table  environment  param  table  config    the  configuration  of  the    table  environment  static    batch  table  environment  create    execution  environment  execution  environment    table  config  table  config  try  temporary  solution  until  flink    is  fixed    class  loader  class  loader    thread  current  thread  get  context  class  loader    module  manager  module  manager  new    module  manager    string  default  catalog  default  catalog    catalog  manager  catalog  manager    catalog  manager  new  builder  class  loader  class  loader  config  table  config  get  configuration  default  catalog  default  catalog  new    generic  in  memory  catalog  default  catalog  default  database  execution  config  execution  environment  get  config  build    class  clazz    class  for  name  org  apache  flink  table  api  bridge  java  internal    batch  table  environment  impl    constructor  con  clazz  get  constructor    execution  environment  class    table  config  class    catalog  manager  class    module  manager  class  return    batch  table  environment  con  new  instance  execution  environment  table  config  catalog  manager  module  manager  catch    throwable  t  throw  new    table  exception    create    batch  table  environment  failed  t  
public  evolving  public  interface    stream  table  environment  extends    table  environment    creates  a  table  environment  that  is  the  entry  point  and  central  context  for  creating    table  and  sql  api  programs  that  integrate  with  the    java  specific  link    data  stream  api  p    it  is  unified  for  bounded  and  unbounded  data  processing  p  a  stream  table  environment  is  responsible  for  ul  li    convert  a  link    data  stream  into  link    table  and  vice  versa  li  li    connecting  to  external  systems  li  li    registering  and  retrieving  link    table  s  and  other  meta  objects  from  a  catalog  li  li    executing  sql  statements  li  li    offering  further  configuration  options  li  ul  p    note    if  you  don  t  intend  to  use  the  link    data  stream  api  link    table  environment  is  meant  for  pure  table  programs  param  execution  environment    the    java  link    stream  execution  environment  of  the  link    table  environment  static    stream  table  environment  create    stream  execution  environment  execution  environment  return  create  execution  environment    environment  settings  new  instance  build    creates  a  table  environment  that  is  the  entry  point  and  central  context  for  creating    table  and  sql  api  programs  that  integrate  with  the    java  specific  link    data  stream  api  p    it  is  unified  for  bounded  and  unbounded  data  processing  p  a  stream  table  environment  is  responsible  for  ul  li    convert  a  link    data  stream  into  link    table  and  vice  versa  li  li    connecting  to  external  systems  li  li    registering  and  retrieving  link    table  s  and  other  meta  objects  from  a  catalog  li  li    executing  sql  statements  li  li    offering  further  configuration  options  li  ul  p    note    if  you  don  t  intend  to  use  the  link    data  stream  api  link    table  environment  is  meant  for  pure  table  programs  param  execution  environment    the    java  link    stream  execution  environment  of  the  link    table  environment  param  settings    the  environment  settings  used  to  instantiate  the  link    table  environment  static    stream  table  environment  create    stream  execution  environment  execution  environment    environment  settings  settings  return    stream  table  environment  impl  create  execution  environment  settings  new    table  config    creates  a  table  environment  that  is  the  entry  point  and  central  context  for  creating    table  and  sql  api  programs  that  integrate  with  the    java  specific  link    data  stream  api  p    it  is  unified  for  bounded  and  unbounded  data  processing  p  a  stream  table  environment  is  responsible  for  ul  li    convert  a  link    data  stream  into  link    table  and  vice  versa  li  li    connecting  to  external  systems  li  li    registering  and  retrieving  link    table  s  and  other  meta  objects  from  a  catalog  li  li    executing  sql  statements  li  li    offering  further  configuration  options  li  ul  p    note    if  you  don  t  intend  to  use  the  link    data  stream  api  link    table  environment  is  meant  for  pure  table  programs  param  execution  environment    the    java  link    stream  execution  environment  of  the  link    table  environment  param  table  config    the  configuration  of  the  link    table  environment  deprecated    use  link  create    stream  execution  environment  and  link  get  config  for  manipulating  link    table  config    deprecated  static    stream  table  environment  create    stream  execution  environment  execution  environment    table  config  table  config  return    stream  table  environment  impl  create  execution  environment    environment  settings  new  instance  build  table  config    registers  a  link    table  function  under  a  unique  name  in  the    table  environment  s  catalog    registered  functions  can  be  referenced  in    table  api  and  sql  queries  param  name    the  name  under  which  the  function  is  registered  param  table  function    the    table  function  to  register  param  t    the  type  of  the  output  row  deprecated    use  link  create  temporary  system  function    string    user  defined  function  instead    please  note  that  the  new  method  also  uses  the  new  type  system  and  reflective  extraction  logic    it  might  be  necessary  to  update  the  function  implementation  as  well    see  the  documentation  of  link    table  function  for  more  information  on  the  new  function  design    deprecated  t  void  register  function    string  name    table  function  t  table  function    registers  an  link    aggregate  function  under  a  unique  name  in  the    table  environment  s  catalog    registered  functions  can  be  referenced  in    table  api  and  sql  queries  param  name    the  name  under  which  the  function  is  registered  param  aggregate  function    the    aggregate  function  to  register  param  t    the  type  of  the  output  value  param  acc    the  type  of  aggregate  accumulator  t  acc  void  register  function    string  name    aggregate  function  t  acc  aggregate  function    registers  an  link    table  aggregate  function  under  a  unique  name  in  the    table  environment  s  catalog    registered  functions  can  only  be  referenced  in    table  api  param  name    the  name  under  which  the  function  is  registered  param  table  aggregate  function    the    table  aggregate  function  to  register  param  t    the  type  of  the  output  value  param  acc    the  type  of  aggregate  accumulator  t  acc  void  register  function    string  name    table  aggregate  function  t  acc  table  aggregate  function    converts  the  given  link    data  stream  into  a  link    table    the  field  names  of  the  link    table  are  automatically  derived  from  the  type  of  the  link    data  stream  param  data  stream    the  link    data  stream  to  be  converted  param  t    the  type  of  the  link    data  stream  return    the  converted  link    table  t    table  from  data  stream    data  stream  t  data  stream    converts  the  given  link    data  stream  into  a  link    table  with  specified  field  names  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the  link    table  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    moreover  we  can  define  proctime  and  rowtime  attributes  at  arbitrary  positions  using  arbitrary  names  except  those  that  exist  in  the  result  schema    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  stream    tuple2    string    long  stream  reorder  the  fields  rename  the  original  f0  field  to  name  and  add  event  time  attribute  named  rowtime    table  table  table  env  from  data  stream  stream  f1  rowtime  rowtime  f0  as  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    event  time  attributes  can  replace  the  field  on  their  position  in  the  input  data  if  it  is  of  correct  type  or  be  appended  at  the  end    proctime  attributes  must  be  appended  at  the  end    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  stream    tuple2    string    long  stream  rename  the  original  fields  to  a  and  b  and  extract  the  internally  attached  timestamp  into  an  event  time  attribute  named  rowtime    table  table  table  env  from  data  stream  stream  a  b  rowtime  rowtime  pre  param  data  stream    the  link    data  stream  to  be  converted  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  stream  to  the  fields  of  the  link    table  param  t    the  type  of  the  link    data  stream  return    the  converted  link    table  deprecated  use  link  from  data  stream    data  stream    expression    deprecated  t    table  from  data  stream    data  stream  t  data  stream    string  fields    converts  the  given  link    data  stream  into  a  link    table  with  specified  field  names  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the  link    table  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    moreover  we  can  define  proctime  and  rowtime  attributes  at  arbitrary  positions  using  arbitrary  names  except  those  that  exist  in  the  result  schema    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  stream    tuple2    string    long  stream    table  table  table  env  from  data  stream  stream  f1  reorder  and  use  the  original  field  rowtime  rowtime  extract  the  internally  attached  timestamp  into  an  event  time  attribute  named  rowtime  f0  as  name  reorder  and  give  the  original  field  a  better  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    event  time  attributes  can  replace  the  field  on  their  position  in  the  input  data  if  it  is  of  correct  type  or  be  appended  at  the  end    proctime  attributes  must  be  appended  at  the  end    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  stream    tuple2    string    long  stream    table  table  table  env  from  data  stream  stream  a  rename  the  first  field  to  a  b  rename  the  second  field  to  b  rowtime  rowtime  extract  the  internally  attached  timestamp  into  an  event  time  attribute  named  rowtime  pre  param  data  stream    the  link    data  stream  to  be  converted  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  stream  to  the  fields  of  the  code    table  param  t    the  type  of  the  link    data  stream  return    the  converted  link    table  t    table  from  data  stream    data  stream  t  data  stream    expression  fields    creates  a  view  from  the  given  link    data  stream    registered  views  can  be  referenced  in  sql  queries  p    the  field  names  of  the  link    table  are  automatically  derived  from  the  type  of  the  link    data  stream  p    the  view  is  registered  in  the  namespace  of  the  current  catalog  and  database    to  register  the  view  in  a  different  catalog  use  link  create  temporary  view    string    data  stream  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  name    the  name  under  which  the  link    data  stream  is  registered  in  the  catalog  param  data  stream    the  link    data  stream  to  register  param  t    the  type  of  the  link    data  stream  to  register  deprecated  use  link  create  temporary  view    string    data  stream    deprecated  t  void  register  data  stream    string  name    data  stream  t  data  stream    creates  a  view  from  the  given  link    data  stream  in  a  given  path    registered  views  can  be  referenced  in  sql  queries  p    the  field  names  of  the  link    table  are  automatically  derived  from  the  type  of  the  link    data  stream  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  path    the  path  under  which  the  link    data  stream  is  created    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  data  stream    the  link    data  stream  out  of  which  to  create  the  view  param  t    the  type  of  the  link    data  stream  t  void  create  temporary  view    string  path    data  stream  t  data  stream    creates  a  view  from  the  given  link    data  stream  in  a  given  path  with  specified  field  names    registered  views  can  be  referenced  in  sql  queries  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the    view  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    moreover  we  can  define  proctime  and  rowtime  attributes  at  arbitrary  positions  using  arbitrary  names  except  those  that  exist  in  the  result  schema    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  stream    tuple2    string    long  stream  reorder  the  fields  rename  the  original  f0  field  to  name  and  add  event  time  attribute  named  rowtime  table  env  register  data  stream  my  table  stream  f1  rowtime  rowtime  f0  as  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    event  time  attributes  can  replace  the  field  on  their  position  in  the  input  data  if  it  is  of  correct  type  or  be  appended  at  the  end    proctime  attributes  must  be  appended  at  the  end    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  stream    tuple2    string    long  stream  rename  the  original  fields  to  a  and  b  and  extract  the  internally  attached  timestamp  into  an  event  time  attribute  named  rowtime  table  env  register  data  stream  my  table  stream  a  b  rowtime  rowtime  pre  p    the  view  is  registered  in  the  namespace  of  the  current  catalog  and  database    to  register  the  view  in  a  different  catalog  use  link  create  temporary  view    string    data  stream  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  name    the  name  under  which  the  link    data  stream  is  registered  in  the  catalog  param  data  stream    the  link    data  stream  to  register  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  stream  to  the  fields  of  the    view  param  t    the  type  of  the  link    data  stream  to  register  deprecated  use  link  create  temporary  view    string    data  stream    expression    deprecated  t  void  register  data  stream    string  name    data  stream  t  data  stream    string  fields    creates  a  view  from  the  given  link    data  stream  in  a  given  path  with  specified  field  names    registered  views  can  be  referenced  in  sql  queries  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the    view  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    moreover  we  can  define  proctime  and  rowtime  attributes  at  arbitrary  positions  using  arbitrary  names  except  those  that  exist  in  the  result  schema    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  stream    tuple2    string    long  stream  reorder  the  fields  rename  the  original  f0  field  to  name  and  add  event  time  attribute  named  rowtime  table  env  create  temporary  view  cat  db  my  table  stream  f1  rowtime  rowtime  f0  as  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    event  time  attributes  can  replace  the  field  on  their  position  in  the  input  data  if  it  is  of  correct  type  or  be  appended  at  the  end    proctime  attributes  must  be  appended  at  the  end    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  stream    tuple2    string    long  stream  rename  the  original  fields  to  a  and  b  and  extract  the  internally  attached  timestamp  into  an  event  time  attribute  named  rowtime  table  env  create  temporary  view  cat  db  my  table  stream  a  b  rowtime  rowtime  pre  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  path    the  path  under  which  the  link    data  stream  is  created    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  data  stream    the  link    data  stream  out  of  which  to  create  the  view  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  stream  to  the  fields  of  the    view  param  t    the  type  of  the  link    data  stream  deprecated  use  link  create  temporary  view    string    data  stream    expression    deprecated  t  void  create  temporary  view    string  path    data  stream  t  data  stream    string  fields    creates  a  view  from  the  given  link    data  stream  in  a  given  path  with  specified  field  names    registered  views  can  be  referenced  in  sql  queries  p    there  are  two  modes  for  mapping  original  fields  to  the  fields  of  the    view  p      reference  input  fields  by  name    all  fields  in  the  schema  definition  are  referenced  by  name  and  possibly  renamed  using  an  alias  as    moreover  we  can  define  proctime  and  rowtime  attributes  at  arbitrary  positions  using  arbitrary  names  except  those  that  exist  in  the  result  schema    in  this  mode  fields  can  be  reordered  and  projected  out    this  mode  can  be  used  for  any  input  type  including    p  o  j  os  p    example  pre  code    data  stream    tuple2    string    long  stream  table  env  create  temporary  view  cat  db  my  table  stream  f1  reorder  and  use  the  original  field  rowtime  rowtime  extract  the  internally  attached  timestamp  into  an  event  time  attribute  named  rowtime  f0  as  name  reorder  and  give  the  original  field  a  better  name  pre  p      reference  input  fields  by  position    in  this  mode  fields  are  simply  renamed    event  time  attributes  can  replace  the  field  on  their  position  in  the  input  data  if  it  is  of  correct  type  or  be  appended  at  the  end    proctime  attributes  must  be  appended  at  the  end    this  mode  can  only  be  used  if  the  input  type  has  a  defined  field  order  tuple  case  class    row  and  none  of  the  code  fields  references  a  field  of  the  input  type  p    example  pre  code    data  stream    tuple2    string    long  stream  table  env  create  temporary  view  cat  db  my  table  stream  a  rename  the  first  field  to  a  b  rename  the  second  field  to  b  rowtime  rowtime  adds  an  event  time  attribute  named  rowtime  pre  p    temporary  objects  can  shadow  permanent  ones    if  a  permanent  object  in  a  given  path  exists  it  will  be  inaccessible  in  the  current  session    to  make  the  permanent  object  available  again  you  can  drop  the  corresponding  temporary  object  param  path    the  path  under  which  the  link    data  stream  is  created    see  also  the  link    table  environment  class  description  for  the  format  of  the  path  param  data  stream    the  link    data  stream  out  of  which  to  create  the  view  param  fields    the  fields  expressions  to  map  original  fields  of  the    data  stream  to  the  fields  of  the    view  param  t    the  type  of  the  link    data  stream  t  void  create  temporary  view    string  path    data  stream  t  data  stream    expression  fields    converts  the  given  link    table  into  an  append  link    data  stream  of  a  specified  type  p    the  link    table  must  only  have  insert  append  changes    if  the  link    table  is  also  modified  by  update  or  delete  changes  the  conversion  will  fail  p    the  fields  of  the  link    table  are  mapped  to  link    data  stream  fields  as  follows  ul  li  link  org  apache  flink  types    row  and  link  org  apache  flink  api  java  tuple    tuple  types    fields  are  mapped  by  position  field  types  must  match  li  li  pojo  link    data  stream  types    fields  are  mapped  by  field  name  field  types  must  match  li  ul  param  table    the  link    table  to  convert  param  clazz    the  class  of  the  type  of  the  resulting  link    data  stream  param  t    the  type  of  the  resulting  link    data  stream  return    the  converted  link    data  stream  t    data  stream  t  to  append  stream    table  table    class  t  clazz    converts  the  given  link    table  into  an  append  link    data  stream  of  a  specified  type  p    the  link    table  must  only  have  insert  append  changes    if  the  link    table  is  also  modified  by  update  or  delete  changes  the  conversion  will  fail  p    the  fields  of  the  link    table  are  mapped  to  link    data  stream  fields  as  follows  ul  li  link  org  apache  flink  types    row  and  link  org  apache  flink  api  java  tuple    tuple  types    fields  are  mapped  by  position  field  types  must  match  li  li  pojo  link    data  stream  types    fields  are  mapped  by  field  name  field  types  must  match  li  ul  param  table    the  link    table  to  convert  param  type  info    the  link    type  information  that  specifies  the  type  of  the  link    data  stream  param  t    the  type  of  the  resulting  link    data  stream  return    the  converted  link    data  stream  t    data  stream  t  to  append  stream    table  table    type  information  t  type  info    converts  the  given  link    table  into  a  link    data  stream  of  add  and  retract  messages    the  message  will  be  encoded  as  link    tuple2    the  first  field  is  a  link    boolean  flag  the  second  field  holds  the  record  of  the  specified  type  link  t  a  true  link    boolean  flag  indicates  an  add  message  a  false  flag  indicates  a  retract  message  p    the  fields  of  the  link    table  are  mapped  to  link    data  stream  fields  as  follows  ul  li  link  org  apache  flink  types    row  and  link  org  apache  flink  api  java  tuple    tuple  types    fields  are  mapped  by  position  field  types  must  match  li  li  pojo  link    data  stream  types    fields  are  mapped  by  field  name  field  types  must  match  li  ul  param  table    the  link    table  to  convert  param  clazz    the  class  of  the  requested  record  type  param  t    the  type  of  the  requested  record  type  return    the  converted  link    data  stream  t    data  stream    tuple2    boolean  t  to  retract  stream    table  table    class  t  clazz    converts  the  given  link    table  into  a  link    data  stream  of  add  and  retract  messages    the  message  will  be  encoded  as  link    tuple2    the  first  field  is  a  link    boolean  flag  the  second  field  holds  the  record  of  the  specified  type  link  t  a  true  link    boolean  flag  indicates  an  add  message  a  false  flag  indicates  a  retract  message  p    the  fields  of  the  link    table  are  mapped  to  link    data  stream  fields  as  follows  ul  li  link  org  apache  flink  types    row  and  link  org  apache  flink  api  java  tuple    tuple  types    fields  are  mapped  by  position  field  types  must  match  li  li  pojo  link    data  stream  types    fields  are  mapped  by  field  name  field  types  must  match  li  ul  param  table    the  link    table  to  convert  param  type  info    the  link    type  information  of  the  requested  record  type  param  t    the  type  of  the  requested  record  type  return    the  converted  link    data  stream  t    data  stream    tuple2    boolean  t  to  retract  stream    table  table    type  information  t  type  info    creates  a  table  source  and  or  table  sink  from  a  descriptor  p    descriptors  allow  for  declaring  the  communication  to  external  systems  in  an  implementation  agnostic  way    the  classpath  is  scanned  for  suitable  table  factories  that  match  the  desired  configuration  p    the  following  example  shows  how  to  read  from  a    kafka  connector  using  a  json  format  and  registering  a  table  source    my  table  in  append  mode  pre  code  table  env  connect  new    kafka  version  0.11  topic  clicks  property  group  id  click  group  start  from  earliest  with  format  new    json  json  schema  fail  on  missing  field  false  with  schema  new    schema  field  user  name  varchar  from  u  name  field  count  decimal  field  proc  time  timestamp  proctime  in  append  mode  create  temporary  table    my  table  pre  param  connector  descriptor  connector  descriptor  describing  the  external  system  deprecated    the  sql  code  create  table  ddl  is  richer  than  this  part  of  the  api    this  method  might  be  refactored  in  the  next  versions    please  use  link  execute  sql    string  execute  sql  ddl  to  register  a  table  instead    override    deprecated    stream  table  descriptor  connect    connector  descriptor  connector  descriptor    triggers  the  program  execution    the  environment  will  execute  all  parts  of  the  program  p    the  program  execution  will  be  logged  and  displayed  with  the  provided  name  p    it  calls  the  link    stream  execution  environment  execute    string  on  the  underlying  link    stream  execution  environment    in  contrast  to  the  link    table  environment  this  environment  translates  queries  eagerly  param  job  name    desired  name  of  the  job  return    the  result  of  the  job  execution  containing  elapsed  time  and  accumulators  throws    exception  which  occurs  during  job  execution    override    job  execution  result  execute    string  job  name  throws    exception  
public  evolving  public  interface    sink  function  provider  extends    dynamic  table  sink    sink  runtime  provider    helper  method  for  creating  a  static  provider  static    sink  function  provider  of    sink  function    row  data  sink  function  return  sink  function    creates  a  link    sink  function  instance    sink  function    row  data  create  sink  function  
public  evolving  public  interface    periodic  watermark  assigner  provider  extends    supports  watermark  push  down    watermark  provider    assigner  with  periodic  watermarks    row  data  get  periodic  watermark  assigner  
public  evolving  public  interface    punctuated  watermark  assigner  provider  extends    supports  watermark  push  down    watermark  provider    assigner  with  punctuated  watermarks    row  data  get  punctuated  watermark  assigner  
public  evolving  public  interface    source  function  provider  extends    scan  table  source    scan  runtime  provider    helper  method  for  creating  a  static  provider  static    source  function  provider  of    source  function    row  data  source  function  boolean  is  bounded  return  new    source  function  provider    override  public    source  function    row  data  create  source  function  return  source  function    override  public  boolean  is  bounded  return  is  bounded    creates  a  link    source  function  instance    source  function    row  data  create  source  function  
public  evolving  public  class    rowtime  validator  implements    descriptor  validator  private  final  boolean  supports  source  timestamps  private  final  boolean  supports  source  watermarks  private  final    string  prefix  public    rowtime  validator  boolean  supports  source  timestamps  boolean  supports  source  watermarks  this  supports  source  timestamps  supports  source  watermarks  public    rowtime  validator  boolean  supports  source  timestamps  boolean  supports  source  watermarks    string  prefix  this  supports  source  timestamps  supports  source  timestamps  this  supports  source  watermarks  supports  source  watermarks  this  prefix  prefix    override  public  void  validate    descriptor  properties  properties    consumer    string  timestamp  existing  field  s  properties  validate  string  prefix  rowtime  timestamps  from  false      consumer    string  timestamp  custom  s  properties  validate  string  prefix  rowtime  timestamps  class  false    properties  validate  string  prefix  rowtime  timestamps  serialized  false      map    string    consumer    string  timestamps  validation  new    hash  map  if  supports  source  timestamps  timestamps  validation  put  rowtime  timestamps  type  value  from  field  timestamp  existing  field  timestamps  validation  put  rowtime  timestamps  type  value  from  source    descriptor  properties  no  validation  timestamps  validation  put  rowtime  timestamps  type  value  custom  timestamp  custom  else  timestamps  validation  put  rowtime  timestamps  type  value  from  field  timestamp  existing  field  timestamps  validation  put  rowtime  timestamps  type  value  custom  timestamp  custom  properties  validate  enum  prefix  rowtime  timestamps  type  false  timestamps  validation    consumer    string  watermark  periodic  bounded  s  properties  validate  long  prefix  rowtime  watermarks  delay  false      consumer    string  watermark  custom  s  properties  validate  string  prefix  rowtime  watermarks  class  false    properties  validate  string  prefix  rowtime  watermarks  serialized  false      map    string    consumer    string  watermarks  validation  new    hash  map  if  supports  source  watermarks  watermarks  validation  put  rowtime  watermarks  type  value  periodic  ascending    descriptor  properties  no  validation  watermarks  validation  put  rowtime  watermarks  type  value  periodic  bounded  watermark  periodic  bounded  watermarks  validation  put  rowtime  watermarks  type  value  from  source    descriptor  properties  no  validation  watermarks  validation  put  rowtime  watermarks  type  value  custom  watermark  custom  else  watermarks  validation  put  rowtime  watermarks  type  value  periodic  ascending    descriptor  properties  no  validation  watermarks  validation  put  rowtime  watermarks  type  value  periodic  bounded  watermark  periodic  bounded  watermarks  validation  put  rowtime  watermarks  type  value  custom  watermark  custom  properties  validate  enum  prefix  rowtime  watermarks  type  false  watermarks  validation  utilities  public  static    optional    tuple2    timestamp  extractor    watermark  strategy  get  rowtime  components    descriptor  properties  properties    string  prefix  create  timestamp  extractor    timestamp  extractor  extractor    optional    string  t  properties  get  optional  string  prefix  rowtime  timestamps  type  if  t  is  present  return    optional  empty  switch  t  get  case  rowtime  timestamps  type  value  from  field    string  field  properties  get  string  prefix  rowtime  timestamps  from  extractor  new    existing  field  field  break  case  rowtime  timestamps  type  value  from  source  extractor    stream  record  timestamp  instance  break  case  rowtime  timestamps  type  value  custom    class    timestamp  extractor  clazz  properties  get  class  prefix  rowtime  timestamps  class    timestamp  extractor  class  extractor    encoding  utils  decode  string  to  object  properties  get  string  prefix  rowtime  timestamps  serialized  clazz  break  default  throw  new    validation  exception    unsupported  rowtime  timestamps  type  t  get  create  watermark  strategy    watermark  strategy  strategy    string  s  properties  get  string  prefix  rowtime  watermarks  type  switch  s  case  rowtime  watermarks  type  value  periodic  ascending  strategy  new    ascending  timestamps  break  case  rowtime  watermarks  type  value  periodic  bounded  long  delay  properties  get  long  prefix  rowtime  watermarks  delay  strategy  new    bounded  out  of  order  timestamps  delay  break  case  rowtime  watermarks  type  value  from  source  strategy    preserve  watermarks  instance  break  case  rowtime  watermarks  type  value  custom    class    watermark  strategy  clazz  properties  get  class  prefix  rowtime  watermarks  class    watermark  strategy  class  strategy    encoding  utils  decode  string  to  object  properties  get  string  prefix  rowtime  watermarks  serialized  clazz  break  default  throw  new    runtime  exception    unsupported  rowtime  timestamps  type  s  return    optional  of  new    tuple2  extractor  strategy  
public  evolving  public  class    schema  validator  implements    descriptor  validator  private  final  boolean  is  stream  environment  private  final  boolean  supports  source  timestamps  private  final  boolean  supports  source  watermarks  public    schema  validator  boolean  is  stream  environment  boolean  supports  source  timestamps  boolean  supports  source  watermarks  this  is  stream  environment  is  stream  environment  this  supports  source  timestamps  supports  source  timestamps  this  supports  source  watermarks  supports  source  watermarks    override  public  void  validate    descriptor  properties  properties    map    string    string  names  properties  get  indexed  property  schema  schema  name    map    string    string  legacy  types  properties  get  indexed  property  schema  schema  type    map    string    string  data  types  properties  get  indexed  property  schema  schema  data  type  if  names  is  empty  legacy  types  is  empty  data  types  is  empty  throw  new    validation  exception  format    could  not  find  the  required  schema  in  property  s  schema  boolean  proctime  found  false  for  int  i    i    math  max  names  size  legacy  types  size  i  properties  validate  string  schema  i  schema  name  false    properties  validate  data  type  schema  i  schema  data  type  schema  i  schema  type  false  properties  validate  string  schema  i  schema  from  true    either  proctime  or  rowtime    string  proctime  schema  i  schema  proctime    string  rowtime  schema  i  rowtime  if  properties  contains  key  proctime  check  the  environment  if  is  stream  environment  throw  new    validation  exception  format    property  s  is  not  allowed  in  a  batch  environment  proctime  check  for  only  one  proctime  attribute  else  if  proctime  found  throw  new    validation  exception  a  proctime  attribute  must  only  be  defined  once  check  proctime  properties  validate  boolean  proctime  false  proctime  found  properties  get  boolean  proctime  no  rowtime  properties  validate  prefix  exclusion  rowtime  else  if  properties  has  prefix  rowtime  check  rowtime    rowtime  validator  rowtime  validator  new    rowtime  validator  supports  source  timestamps  supports  source  watermarks  schema  i  rowtime  validator  validate  properties  no  proctime  properties  validate  exclusion  proctime    returns  keys  for  a  link    table  format  factory  supported  properties  method  that  are  accepted  for  schema  derivation  using  code  derive  format  fields    descriptor  properties  public  static    list    string  get  schema  derivation  keys    list    string  keys  new    array  list  schema  keys  add  schema  schema  data  type  keys  add  schema  schema  type  keys  add  schema  schema  name  keys  add  schema  schema  from  computed  column  keys  add  schema  expr  time  attributes  keys  add  schema  schema  proctime  keys  add  schema  rowtime  timestamps  type  keys  add  schema  rowtime  timestamps  from  keys  add  schema  rowtime  timestamps  class  keys  add  schema  rowtime  timestamps  serialized  keys  add  schema  rowtime  watermarks  type  keys  add  schema  rowtime  watermarks  class  keys  add  schema  rowtime  watermarks  serialized  keys  add  schema  rowtime  watermarks  delay  watermark  keys  add  schema  watermark  watermark  rowtime  keys  add  schema  watermark  watermark  strategy  expr  keys  add  schema  watermark  watermark  strategy  data  type  table  constraint  keys  add  schema    descriptor  properties  primary  key  name  keys  add  schema    descriptor  properties  primary  key  columns  return  keys    finds  the  proctime  attribute  if  defined  public  static    optional    string  derive  proctime  attribute    descriptor  properties  properties    map    string    string  names  properties  get  indexed  property  schema  schema  name  for  int  i    i  names  size  i    optional    boolean  is  proctime  properties  get  optional  boolean  schema  i  schema  proctime  if  is  proctime  is  present  is  proctime  get  return    optional  of  names  get  schema  i  schema  name  return    optional  empty    finds  the  rowtime  attributes  if  defined  public  static    list    rowtime  attribute  descriptor  derive  rowtime  attributes    descriptor  properties  properties    map    string    string  names  properties  get  indexed  property  schema  schema  name    list    rowtime  attribute  descriptor  attributes  new    array  list  check  for  rowtime  in  every  field  for  int  i    i  names  size  i    optional    tuple2    timestamp  extractor    watermark  strategy  rowtime  components    rowtime  validator  get  rowtime  components  properties  schema  i  int  index  i  create  descriptor  rowtime  components  if  present  tuple2  attributes  add  new    rowtime  attribute  descriptor  properties  get  string  schema  index  schema  name  tuple2  f0  tuple2  f1  return  attributes    derives  the  table  schema  for  a  table  sink  a  sink  ignores  a  proctime  attribute  and  needs  to  track  the  origin  of  a  rowtime  field  deprecated    this  method  combines  two  separate  concepts  of  table  schema  and  field  mapping    this  should  be  split  into  two  methods  once  we  have  support  for  the  corresponding  interfaces  see  flink      deprecated  public  static    table  schema  derive  table  sink  schema    descriptor  properties  properties    table  schema    builder  builder    table  schema  builder    table  schema  table  schema  properties  get  table  schema  schema  for  int  i    i  table  schema  get  field  count  i  final    table  column  table  column  table  schema  get  table  columns  get  i  final    string  field  name  table  column  get  name  final    data  type  data  type  table  column  get  type  boolean  is  generated  column  table  column  is  generated  if  is  generated  column  skip  generated  column  continue  boolean  is  proctime  properties  get  optional  boolean  schema  i  schema  proctime  or  else  false    string  ts  type  schema  i  rowtime  timestamps  type  boolean  is  rowtime  properties  contains  key  ts  type  if  is  proctime  is  rowtime  check  for  a  aliasing    string  alias  name  properties  get  optional  string  schema  i  schema  from  or  else  field  name  builder  field  alias  name  data  type  only  use  the  rowtime  attribute  if  it  references  a  field  else  if  is  rowtime  switch  properties  get  string  ts  type  case  rowtime  timestamps  type  value  from  field    string  field  properties  get  string  schema  i  rowtime  timestamps  from  builder  field  field  data  type  break  other  timestamp  strategies  require  a  reverse  timestamp  extractor  to  insert  the  timestamp  into  the  output  default  throw  new    table  exception  format    unsupported  rowtime  type  s  for  sink  table  schema    currently  only  s  is  supported  for  table  sinks  data  type  rowtime  timestamps  type  value  from  field  return  builder  build    finds  a  table  source  field  mapping  param  properties    the  properties  describing  a  schema  param  input  type    the  input  type  that  a  connector  and  or  format  produces    this  parameter  can  be  used  to  resolve  a  rowtime  field  against  an  input  field  public  static    map    string    string  derive  field  mapping    descriptor  properties  properties    optional    type  information  input  type    map    string    string  mapping  new    hash  map    table  schema  schema  properties  get  table  schema  schema    list    string  column  names  new    array  list  input  type  if  present  t  column  names  add  all    arrays  as  list    composite  type  t  get  field  names  add  all  source  fields  first  because  rowtime  might  reference  one  of  them  column  names  for  each  name  mapping  put  name  name  add  all  schema  fields  first  for  implicit  mappings    arrays  stream  schema  get  field  names  for  each  name  mapping  put  name  name    map    string    string  names  properties  get  indexed  property  schema  schema  name  for  int  i    i  names  size  i    string  name  properties  get  string  schema  i  schema  name    optional    string  source  properties  get  optional  string  schema  i  schema  from  if  source  is  present  add  explicit  mapping  mapping  put  name  source  get  else  implicit  mapping  or  time  boolean  is  proctime  properties  get  optional  boolean  schema  i  schema  proctime  or  else  false  boolean  is  rowtime  properties  contains  key  schema  i  rowtime  timestamps  type  boolean  is  generated  column  properties  contains  key  schema  i  expr  remove  proctime  rowtime  from  mapping  if  is  proctime  is  rowtime  is  generated  column  mapping  remove  name  check  for  invalid  fields  else  if  column  names  contains  name  throw  new    validation  exception  format    could  not  map  the  schema  field  s  to  a  field  from  source    please  specify  the  source  field  from  which  it  can  be  derived  name  return  mapping  
public  evolving  public  interface    batch  table  sink  factory  t  extends    table  sink  factory  t    creates  and  configures  a  link    batch  table  sink  using  the  given  properties  param  properties  normalized  properties  describing  a  table  sink  return  the  configured  table  sink    batch  table  sink  t  create  batch  table  sink    map    string    string  properties    only  create  batch  table  sink    override  default    table  sink  t  create  table  sink    map    string    string  properties  return  create  batch  table  sink  properties  
public  evolving  public  interface    batch  table  source  factory  t  extends    table  source  factory  t    creates  and  configures  a  link    batch  table  source  using  the  given  properties  param  properties  normalized  properties  describing  a  batch  table  source  return  the  configured  batch  table  source    batch  table  source  t  create  batch  table  source    map    string    string  properties    only  create  batch  table  source    override  default    table  source  t  create  table  source    map    string    string  properties  return  create  batch  table  source  properties  
public  evolving  public  class    black  hole  table  sink  factory  implements    dynamic  table  sink  factory  public  static  final    string  identifier  blackhole    override  public    string  factory  identifier  return  identifier    override  public    set    config  option  required  options  return  new    hash  set    override  public    set    config  option  optional  options  return  new    hash  set    override  public    dynamic  table  sink  create  dynamic  table  sink    context  context  return  new    black  hole  sink  private  static  class    black  hole  sink  implements    dynamic  table  sink    override  public    changelog  mode  get  changelog  mode    changelog  mode  requested  mode    changelog  mode    builder  builder    changelog  mode  new  builder  for    row  kind  kind  requested  mode  get  contained  kinds  if  kind    row  kind  update  before  builder  add  contained  kind  kind  return  builder  build    override  public    sink  runtime  provider  get  sink  runtime  provider    dynamic  table  sink    context  context  return    sink  function  provider  of  new    discarding  sink    override  public    dynamic  table  sink  copy  return  new    black  hole  sink    override  public    string  as  summary  string  return    black  hole  
public  evolving  public  class    data  gen  table  source  factory  implements    dynamic  table  source  factory  public  static  final    string  identifier  datagen  public  static  final    long  rows  per  second  default  value    l  public  static  final    config  option    long  rows  per  second  key  rows  per  second  long  type  default  value  rows  per  second  default  value  with  description    rows  per  second  to  control  the  emit  rate  public  static  final    string  fields  fields  public  static  final    string  kind  kind  public  static  final    string  start  start  public  static  final    string  end  end  public  static  final    string  min  min  public  static  final    string  max  max  public  static  final    string  length  length  public  static  final    string  sequence  sequence  public  static  final    string  random  random    override  public    string  factory  identifier  return  identifier    override  public    set    config  option  required  options  return  new    hash  set    override  public    set    config  option  optional  options    set    config  option  options  new    hash  set  options  add  rows  per  second  return  options    override  public    dynamic  table  source  create  dynamic  table  source    context  context    configuration  options  new    configuration  context  get  catalog  table  get  options  for  each  options  set  string    table  schema  table  schema    table  schema  utils  get  physical  schema  context  get  catalog  table  get  schema    data  generator  field  generators  new    data  generator  table  schema  get  field  count  for  int  i    i  field  generators  length  i  field  generators  i  create  data  generator  table  schema  get  field  name  i  get  table  schema  get  field  data  type  i  get  options  return  new    data  gen  table  source  field  generators  table  schema  options  get  rows  per  second  private    data  generator  create  data  generator    string  name    data  type  type    readable  config  options    string  gen  type  options  get  key  fields  name  kind  string  type  default  value  random  switch  gen  type  case  random  return  create  random  generator  name  type  options  case  sequence  return  create  sequence  generator  name  type  options  default  throw  new    validation  exception    unsupported  generator  type  gen  type  private    data  generator  create  random  generator    string  name    data  type  type    readable  config  options    config  option    integer  len  key  key  fields  name  length  int  type  default  value      option  builder  min  key  key  fields  name  min    option  builder  max  key  key  fields  name  max  switch  type  get  logical  type  get  type  root  case  boolean  return    random  generator  boolean  generator  case  char  case  varchar  int  length  options  get  len  key  return  get  random  string  generator  length  case  tinyint  return    random  generator  byte  generator  options  get  min  key  int  type  default  value  int    byte  min  value  byte  value  options  get  max  key  int  type  default  value  int    byte  max  value  byte  value  case  smallint  return    random  generator  short  generator  options  get  min  key  int  type  default  value  int    short  min  value  short  value  options  get  max  key  int  type  default  value  int    short  max  value  short  value  case  integer  return    random  generator  int  generator  options  get  min  key  int  type  default  value    integer  min  value  options  get  max  key  int  type  default  value    integer  max  value  case  bigint  return    random  generator  long  generator  options  get  min  key  long  type  default  value    long  min  value  options  get  max  key  long  type  default  value    long  max  value  case  float  return    random  generator  float  generator  options  get  min  key  float  type  default  value    float  min  value  options  get  max  key  float  type  default  value    float  max  value  case  double  return    random  generator  double  generator  options  get  min  key  double  type  default  value    double  min  value  options  get  max  key  double  type  default  value    double  max  value  default  throw  new    validation  exception    unsupported  type  type  private  static    random  generator    string  data  get  random  string  generator  int  length  return  new    random  generator    string  data    override  public    string  data  next  return    string  data  from  string  random  next  hex  string  length  private    data  generator  create  sequence  generator    string  name    data  type  type    readable  config  options    string  start  key  str  fields  name  start    string  end  key  str  fields  name  end    option  builder  start  key  key  start  key  str    option  builder  end  key  key  end  key  str  options  get  optional  start  key  string  type  no  default  value  or  else  throw  new    validation  exception    could  not  find  required  property  start  key  str  for  sequence  generator  options  get  optional  end  key  string  type  no  default  value  or  else  throw  new    validation  exception    could  not  find  required  property  end  key  str  for  sequence  generator  switch  type  get  logical  type  get  type  root  case  char  case  varchar  return  get  sequence  string  generator  options  get  start  key  long  type  no  default  value  options  get  end  key  long  type  no  default  value  case  tinyint  return    sequence  generator  byte  generator  options  get  start  key  int  type  no  default  value  byte  value  options  get  end  key  int  type  no  default  value  byte  value  case  smallint  return    sequence  generator  short  generator  options  get  start  key  int  type  no  default  value  short  value  options  get  end  key  int  type  no  default  value  short  value  case  integer  return    sequence  generator  int  generator  options  get  start  key  int  type  no  default  value  options  get  end  key  int  type  no  default  value  case  bigint  return    sequence  generator  long  generator  options  get  start  key  long  type  no  default  value  options  get  end  key  long  type  no  default  value  case  float  return    sequence  generator  float  generator  options  get  start  key  int  type  no  default  value  short  value  options  get  end  key  int  type  no  default  value  short  value  case  double  return    sequence  generator  double  generator  options  get  start  key  int  type  no  default  value  options  get  end  key  int  type  no  default  value  default  throw  new    validation  exception    unsupported  type  type  private  static    sequence  generator    string  data  get  sequence  string  generator  long  start  long  end  return  new    sequence  generator    string  data  start  end    override  public    string  data  next  return    string  data  from  string  values  to  emit  poll  to  string  a  link    stream  table  source  that  emits  each  number  from  a  given  interval  exactly  once  possibly  in  parallel    see  link    stateful  sequence  source  static  class    data  gen  table  source  implements    scan  table  source  private  final    data  generator  field  generators  private  final    table  schema  schema  private  final  long  rows  per  second  private    data  gen  table  source    data  generator  field  generators    table  schema  schema  long  rows  per  second  this  field  generators  field  generators  this  schema  schema  this  rows  per  second  rows  per  second    override  public    scan  runtime  provider  get  scan  runtime  provider    scan  context  context  return    source  function  provider  of  create  source  false    visible  for  testing    data  generator  source    row  data  create  source  return  new    data  generator  source  new    row  generator  field  generators  schema  get  field  names  rows  per  second    override  public    dynamic  table  source  copy  return  new    data  gen  table  source  field  generators  schema  rows  per  second    override  public    string  as  summary  string  return    data  gen  table  source    override  public    changelog  mode  get  changelog  mode  return    changelog  mode  insert  only  private  static  class    row  generator  implements    data  generator    row  data  private  static  final  long  serial  version  u  i  d  1  l  private  final    data  generator  field  generators  private  final    string  field  names  private    row  generator    data  generator  field  generators    string  field  names  this  field  generators  field  generators  this  field  names  field  names    override  public  void  open    string  name    function  initialization  context  context    runtime  context  runtime  context  throws    exception  for  int  i    i  field  generators  length  i  field  generators  i  open  field  names  i  context  runtime  context    override  public  void  snapshot  state    function  snapshot  context  context  throws    exception  for    data  generator  generator  field  generators  generator  snapshot  state  context    override  public  boolean  has  next  for    data  generator  generator  field  generators  if  generator  has  next  return  false  return  true    override  public    row  data  next    generic  row  data  row  new    generic  row  data  field  names  length  for  int  i    i  field  generators  length  i  row  set  field  i  field  generators  i  next  return  row  
public  evolving  public  class    print  table  sink  factory  implements    dynamic  table  sink  factory  public  static  final    string  identifier  print  public  static  final    config  option    string  print  identifier  key  print  identifier  string  type  no  default  value  with  description    message  that  identify  print  and  is  prefixed  to  the  output  of  the  value  public  static  final    config  option    boolean  standard  error  key  standard  error  boolean  type  default  value  false  with  description    true  if  the  format  should  print  to  standard  error  instead  of  standard  out    override  public    string  factory  identifier  return  identifier    override  public    set    config  option  required  options  return  new    hash  set    override  public    set    config  option  optional  options    set    config  option  options  new    hash  set  options  add  print  identifier  options  add  standard  error  return  options    override  public    dynamic  table  sink  create  dynamic  table  sink    context  context    factory  util    table  factory  helper  helper    factory  util  create  table  factory  helper  this  context  helper  validate    readable  config  options  helper  get  options  return  new    print  sink  context  get  catalog  table  get  schema  to  physical  row  data  type  options  get  print  identifier  options  get  standard  error  private  static  class    print  sink  implements    dynamic  table  sink  private  final    data  type  type  private  final    string  print  identifier  private  final  boolean  std  err  private    print  sink    data  type  type    string  print  identifier  boolean  std  err  this  type  type  this  print  identifier  print  identifier  this  std  err  std  err    override  public    changelog  mode  get  changelog  mode    changelog  mode  requested  mode  return  requested  mode    override  public    sink  runtime  provider  get  sink  runtime  provider    dynamic  table  sink    context  context    data  structure  converter  converter  context  create  data  structure  converter  type  return    sink  function  provider  of  new    row  data  print  function  converter  print  identifier  std  err    override  public    dynamic  table  sink  copy  return  new    print  sink  type  print  identifier  std  err    override  public    string  as  summary  string  return    print  to  std  err    system  err    system  out    implementation  of  the    sink  function  converting  link    row  data  to  string  and  passing  to  link    print  sink  function  private  static  class    row  data  print  function  extends    rich  sink  function    row  data  private  static  final  long  serial  version  u  i  d  1  l  private  final    data  structure  converter  converter  private  final    print  sink  output  writer    string  writer  private    row  data  print  function    data  structure  converter  converter    string  print  identifier  boolean  std  err  this  converter  converter  this  writer  new    print  sink  output  writer  print  identifier  std  err    override  public  void  open    configuration  parameters  throws    exception  super  open  parameters    streaming  runtime  context  context    streaming  runtime  context  get  runtime  context  writer  open  context  get  index  of  this  subtask  context  get  number  of  parallel  subtasks    override  public  void  invoke    row  data  value    context  context    string  row  kind  value  get  row  kind  short  string    object  data  converter  to  external  value  writer  write  row  kind  data  
public  evolving  public  interface    stream  table  sink  factory  t  extends    table  sink  factory  t    creates  and  configures  a  link    stream  table  sink  using  the  given  properties  param  properties  normalized  properties  describing  a  table  sink  return  the  configured  table  sink  deprecated  link    context  contains  more  information  and  already  contains  table  schema  too    please  use  link  create  table  sink    context  instead    deprecated  default    stream  table  sink  t  create  stream  table  sink    map    string    string  properties  return  null    only  create  stream  table  sink    override  default    table  sink  t  create  table  sink    map    string    string  properties    stream  table  sink  t  sink  create  stream  table  sink  properties  if  sink  null  throw  new    validation  exception    please  override  create  table  sink    context  method  return  sink  
public  evolving  public  interface    stream  table  source  factory  t  extends    table  source  factory  t    creates  and  configures  a  link    stream  table  source  using  the  given  properties  param  properties  normalized  properties  describing  a  stream  table  source  return  the  configured  stream  table  source  deprecated  link    context  contains  more  information  and  already  contains  table  schema  too    please  use  link  create  table  source    context  instead    deprecated  default    stream  table  source  t  create  stream  table  source    map    string    string  properties  return  null    only  create  a  stream  table  source    override  default    table  source  t  create  table  source    map    string    string  properties    stream  table  source  t  source  create  stream  table  source  properties  if  source  null  throw  new    validation  exception    please  override  create  table  source    context  method  return  source  
public  evolving  public  interface    append  stream  table  sink  t  extends    stream  table  sink  t  
public  evolving  public  class    csv  append  table  sink  factory  extends    csv  table  sink  factory  base  implements    stream  table  sink  factory    row    override  public    map    string    string  required  context    map    string    string  context  new    hash  map  super  required  context  context  put  update  mode  update  mode  value  append  return  context    override  public    stream  table  sink    row  create  stream  table  sink    map    string    string  properties  return  create  table  sink  true  properties  
public  evolving  public  class    csv  batch  table  sink  factory  extends    csv  table  sink  factory  base  implements    batch  table  sink  factory    row    override  public    batch  table  sink    row  create  batch  table  sink    map    string    string  properties  return  create  table  sink  false  properties  
public  evolving  public  interface    retract  stream  table  sink  t  extends    stream  table  sink    tuple2    boolean  t    returns  the  requested  record  type    type  information  t  get  record  type    override  default    type  information    tuple2    boolean  t  get  output  type  return  new    tuple  type  info    types  boolean  get  record  type  
public  evolving  public  interface    upsert  stream  table  sink  t  extends    stream  table  sink    tuple2    boolean  t    configures  the  unique  key  fields  of  the  link    table  to  write    the  method  is  called  after  link    table  sink  configure    string    type  information  p    the  keys  array  might  be  empty  if  the  table  consists  of  a  single  updated  record    if  the  table  does  not  have  a  key  and  is  append  only  the  keys  attribute  is  null  param  keys  the  field  names  of  the  table  s  keys  an  empty  array  if  the  table  has  a  single  row  and  null  if  the  table  is  append  only  and  has  no  key  void  set  key  fields    string  keys    specifies  whether  the  link    table  to  write  is  append  only  or  not  param  is  append  only  true  if  the  table  is  append  only  false  otherwise  void  set  is  append  only    boolean  is  append  only    returns  the  requested  record  type    type  information  t  get  record  type    override  default    type  information    tuple2    boolean  t  get  output  type  return  new    tuple  type  info    types  boolean  get  record  type  
public  evolving  public  class    csv  append  table  source  factory  extends    csv  table  source  factory  base  implements    stream  table  source  factory    row    override  public    map    string    string  required  context    map    string    string  context  new    hash  map  super  required  context  context  put  update  mode  update  mode  value  append  return  context    override  public    stream  table  source    row  create  stream  table  source    map    string    string  properties  return  create  table  source  true  properties  
public  evolving  public  class    csv  batch  table  source  factory  extends    csv  table  source  factory  base  implements    batch  table  source  factory    row    override  public    batch  table  source    row  create  batch  table  source    map    string    string  properties  return  create  table  source  false  properties  
public  evolving  public  final  class    ascending  timestamps  extends    periodic  watermark  assigner  private  static  final  long  serial  version  u  i  d  1  l  private  long  max  timestamp    long  min  value      override  public  void  next  timestamp  long  timestamp  if  timestamp  max  timestamp  max  timestamp  timestamp    override  public    map    string    string  to  properties    map    string    string  map  new    hash  map  map  put    rowtime  rowtime  watermarks  type    rowtime  rowtime  watermarks  type  value  periodic  ascending  return  map    override  public  int  hash  code  return    ascending  timestamps  class  hash  code    override  public  boolean  equals    object  obj  return  obj  instanceof    ascending  timestamps    override  public    watermark  get  watermark  return  new    watermark  max  timestamp    
public  evolving  public  final  class    bounded  out  of  order  timestamps  extends    periodic  watermark  assigner  private  static  final  long  serial  version  u  i  d  1  l  private  final  long  delay  private  long  max  timestamp  param  delay    the  delay  by  which  watermarks  are  behind  the  maximum  observed  timestamp  public    bounded  out  of  order  timestamps  long  delay  this  delay  delay  max  timestamp    long  min  value  delay    override  public  void  next  timestamp  long  timestamp  if  timestamp  max  timestamp  max  timestamp  timestamp    override  public    watermark  get  watermark  return  new    watermark  max  timestamp  delay    override  public    map    string    string  to  properties    map    string    string  map  new    hash  map  map  put    rowtime  rowtime  watermarks  type    rowtime  rowtime  watermarks  type  value  periodic  bounded  map  put    rowtime  rowtime  watermarks  delay    string  value  of  delay  return  map    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    bounded  out  of  order  timestamps  that    bounded  out  of  order  timestamps  o  return  delay  that  delay    override  public  int  hash  code  return    long  hash  code  delay  
public  evolving  public  abstract  class    periodic  watermark  assigner  extends    watermark  strategy    updates  the  assigner  with  the  next  timestamp  param  timestamp    the  next  timestamp  to  update  the  assigner  public  abstract  void  next  timestamp  long  timestamp    returns  the  current  watermark  return    the  current  watermark  public  abstract    watermark  get  watermark  
public  evolving  public  abstract  class    punctuated  watermark  assigner  extends    watermark  strategy    returns  the  watermark  for  the  current  row  or  null  if  no  watermark  should  be  generated  param  row    the  current  row  param  timestamp    the  value  of  the  timestamp  attribute  for  the  row  return    the  watermark  for  this  row  or  null  if  no  watermark  should  be  generated  public  abstract    watermark  get  watermark    row  row  long  timestamp  
public  evolving    retention    retention  policy  runtime    target    element  type  type    element  type  method    element  type  field    element  type  parameter  public  interface    data  type  hint    note  to  implementers    because  null  is  not  supported  as  an  annotation  value    every  annotation  parameter  must  have  some  representation  for  unknown  values  in  order  to  merge  multi  level  annotations    explicit  data  type  specification    the  explicit  string  representation  of  a  data  type    see  link    data  types  for  a  list  of  supported  data  types    for  example  code  int  for  an  integer  data  type  or  code  decimal      for  decimal  data  type  with  precision    and  scale    p    use  an  unparameterized  code  raw  string  for  explicitly  declaring  an  opaque  data  type  without  entering  a  full  type  string    for    flink  s  default  raw  serializer  use  code    data  type  hint  raw  or  more  specific  code    data  type  hint  value  raw  bridged  to    my  custom  class  class    for  a  custom  raw  serializer  use  code    data  type  hint  value  raw  raw  serializer    my  custom  serializer  class  p    by  default  the  empty  string  represents  an  undefined  data  type  which  means  that  it  will  be  derived  automatically  p    use  link  input  group  for  accepting  a  group  of  similar  data  types  if  this  hint  is  used  to  enrich  input  arguments  see    logical  type  as  serializable  string  see    data  types    string  value  default    adds  a  hint  that  data  should  be  represented  using  the  given  class  when  entering  or  leaving  the  table  ecosystem  p    if  an  explicit  data  type  has  been  defined  via  link  value  a  supported  conversion  class  depends  on  the  logical  type  and  its  nullability  property  p    if  an  explicit  data  type  has  not  been  defined  via  link  value  this  class  is  used  for  reflective  extraction  of  a  data  type  p    please  see  the  implementation  of  link    logical  type  supports  input  conversion    class  link    logical  type  supports  output  conversion    class  or  the  documentation  for  more  information  about  supported  conversions  p    by  default  the  conversion  class  is  reflectively  extracted  see    data  type  bridged  to    class    class  bridged  to  default  void  class    adds  a  hint  that  defines  a  custom  serializer  that  should  be  used  for  serializing  and  deserializing  opaque  raw  types    it  is  used  if  link  value  is  explicitly  defined  as  an  unparameterized  code  raw  string  or  if  possibly  nested  fields  in  a  structured  type  need  to  be  handled  as  an  opaque  type  p    by  default    flink  s  default  raw  serializer  is  used  see    data  types  raw    class    type  serializer    class  extends    type  serializer  raw  serializer  default    unknown  serializer  class    group  of  data  types  specification    this  hint  parameter  influences  the  extraction  of  a  link    type  inference  in  functions    it  adds  a  hint  for  accepting  pre  defined  groups  of  similar  types  i  e  more  than  just  one  explicit  data  type  p    note    this  hint  parameter  is  only  interpreted  when  used  in  function  hints  or  next  to  arguments  of  implementation  methods    it  has  highest  precedence  above  all  other  hint  parameter  p    some  examples  pre  code  expects  an  integer  for  the  first  input  argument  and  allows  any  data  type  for  the  second    function  hint  input    data  type  hint  int    data  type  hint  input  group  any  output    data  type  hint  boolean  expects  an  integer  for  the  first  input  argument  and  allows  any  data  type  for  the  second  eval  int  i    data  type  hint  input  group  any    object  o  pre    input  group  input  group  default    input  group  unknown    parameterization  of  the  reflection  based  extraction    version  that  describes  the  expected  behavior  of  the  reflection  based  data  type  extraction  p    it  is  meant  for  future  backward  compatibility    whenever  the  extraction  logic  is  changed  old  function  and  structured  type  classes  should  still  return  the  same  data  type  as  before  when  versioned  accordingly  p    by  default  the  version  is  always  the  most  recent  one    extraction  version  version  default    extraction  version  unknown    defines  that  a  raw  data  type  may  be  used  for  all  classes  that  cannot  be  mapped  to  any  sql  like  data  type  or  cause  an  error  p    by  default  this  parameter  is  set  to  code  false  which  means  that  an  exception  is  thrown  for  unmapped  types    this  is  helpful  to  identify  and  fix  faulty  implementations    it  is  generally  recommended  to  use  sql  like  types  instead  of  enabling  raw  opaque  types  p    if  raw  types  cannot  be  avoided  they  should  be  enabled  only  in  designated  areas  i  e  within  package  prefixes  using  link  allow  raw  pattern  in  order  to  not  swallow  all  errors    however  this  parameter  globally  enables  raw  types  for  the  annotated  class  and  all  nested  fields  p    this  parameter  has  higher  precedence  than  link  allow  raw  pattern  see    data  types  raw    class    type  serializer    hint  flag  allow  raw  globally  default    hint  flag  unknown    defines  that  a  raw  data  type  may  be  used  for  all  classes  that  cannot  be  mapped  to  any  sql  like  data  type  or  cause  an  error  if  their  class  name  starts  with  or  is  equal  to  one  of  the  given  patterns  p    for  example  if  some    joda  time  classes  cannot  be  mapped  to  any  sql  like  data  type  one  can  define  the  class  prefix  code  org  joda  time    some  classes  might  be  handled  as  structured  types  on  a  best  effort  basis  but  others  will  be  raw  data  types  if  necessary  p    by  default  the  pattern  list  is  empty  which  means  that  an  exception  is  thrown  for  unmapped  types    this  is  helpful  to  identify  and  fix  faulty  implementations    it  is  generally  recommended  to  use  sql  like  types  instead  of  enabling  raw  opaque  types  p    if  raw  types  cannot  be  avoided  this  parameter  should  be  used  to  enabled  them  only  in  designated  areas  i  e  within  package  prefixes  in  order  to  not  swallow  all  errors  p    this  parameter  has  lower  precedence  than  link  allow  raw  globally  which  would  globally  allow  raw  types  in  the  annotated  class  and  all  nested  fields  see    data  types  raw    class    type  serializer    string  allow  raw  pattern  default    defines  that  a  raw  data  type  must  be  used  for  all  classes  if  their  class  name  starts  with  or  is  equal  to  one  of  the  given  patterns  p    for  example  one  can  define  the  class  prefix  code  org  joda  time  java  math    big  decimal  which  means  that  all    joda  time  classes  and    java  s  link  java  math    big  decimal  will  be  handled  as  raw  data  types  regardless  if  they  could  be  mapped  to  a  more  sql  like  data  type  p    by  default  the  pattern  list  is  empty  which  means  that  an  exception  is  thrown  for  unmapped  types    this  is  helpful  to  identify  and  fix  faulty  implementations    it  is  generally  recommended  to  use  sql  like  types  instead  of  enabling  raw  opaque  types  p    if  raw  types  cannot  be  avoided  they  should  be  enabled  only  in  designated  areas  i  e  within  package  prefixes  in  order  to  not  swallow  all  errors    however  compared  to  link  allow  raw  pattern  this  parameter  forces  to  skip  the  extraction  entirely  for  the  given  prefixes  instead  of  trying  to  match  a  class  to  a  more  sql  like  data  type  p    this  parameter  has  the  highest  precedence  of  all  data  type  related  hint  parameters  see    data  types  raw    class    type  serializer    string  force  raw  pattern  default    defines  a  default  precision  for  all  decimal  data  types  that  are  extracted  p    by  default  decimals  are  not  extracted  from  classes  such  as  link  java  math    big  decimal  because  they  don  t  define  a  fixed  precision  and  scale  which  is  required  in  the  sql  type  system  int  default  decimal  precision  default      defines  a  default  scale  for  all  decimal  data  types  that  are  extracted  p    by  default  decimals  are  not  extracted  from  classes  such  as  link  java  math    big  decimal  because  they  don  t  define  a  fixed  precision  and  scale  which  is  required  in  the  sql  type  system  int  default  decimal  scale  default      defines  a  default  year  precision  for  all  year  month  intervals  that  are  extracted    if  set  to  code    an  code  interval  month  data  type  is  extracted  p    by  default  code  interval  year    to  month  data  types  are  extracted  from  classes  such  as  link  java  time    period  int  default  year  precision  default      defines  a  default  fractional  second  precision  for  all  day  time  intervals  and  timestamps  that  are  extracted  p    by  default  those  data  types  are  extracted  with  nano  second  precision  int  default  second  precision  default    
public  evolving  public  enum    extraction  version    default  if  no  version  is  specified  unknown    initial  reflection  based  extraction  logic  according  to  flip      v1  
public  evolving    retention    retention  policy  runtime    target    element  type  type    element  type  method    repeatable    function  hints  class  public  interface    function  hint    note  to  implementers    because  null  is  not  supported  as  an  annotation  value    every  annotation  parameter  must  have  some  representation  for  unknown  values  in  order  to  merge  multi  level  annotations    explicitly  lists  the  argument  types  that  a  function  takes  as  input  p    by  default  explicit  input  types  are  undefined  and  the  reflection  based  extraction  is  used  p    note    specifying  the  input  arguments  manually  disables  the  entire  reflection  based  extraction  around  arguments    this  means  that  also  link  is  var  args  and  link  argument  names  need  to  be  specified  manually  if  required    data  type  hint  input  default    data  type  hint    defines  that  the  last  argument  type  defined  in  link  input  should  be  treated  as  a  variable  length  argument  p    by  default  if  link  input  is  defined  the  last  argument  type  is  not  a  var  arg    if  link  input  is  not  defined  the  reflection  based  extraction  is  used  to  decide  about  the  var  arg  flag  thus  this  parameter  is  ignored  boolean  is  var  args  default  false    explicitly  lists  the  argument  names  that  a  function  takes  as  input  p    by  default  if  link  input  is  defined  explicit  argument  names  are  undefined  and  this  parameter  can  be  used  to  provide  argument  names    if  link  input  is  not  defined  the  reflection  based  extraction  is  used  thus  this  parameter  is  ignored    string  argument  names  default    explicitly  defines  the  intermediate  result  type  that  a  function  uses  as  accumulator  p    by  default  an  explicit  accumulator  type  is  undefined  and  the  reflection  based  extraction  is  used    data  type  hint  accumulator  default    data  type  hint    explicitly  defines  the  result  type  that  a  function  uses  as  output  p    by  default  an  explicit  output  type  is  undefined  and  the  reflection  based  extraction  is  used    data  type  hint  output  default    data  type  hint  
public  evolving    retention    retention  policy  runtime    target    element  type  type    element  type  method  public  interface    function  hints    function  hint  value  
public  evolving  public  enum    hint  flag  true  false  unknown  
public  evolving  public  enum    input  group    default  if  no  group  is  specified  unknown    enables  input  wildcards    any  data  type  can  be  passed    the  behavior  is  equal  to  link    input  type  strategies  any  p    note    the  class  of  the  annotated  element  must  be  link    object  as  this  is  the  super  class  of  all  possibly  passed  data  types  any  
public  evolving  public  class    catalog  not  exist  exception  extends    runtime  exception  public    catalog  not  exist  exception    string  catalog  name  this  catalog  name  null  public    catalog  not  exist  exception    string  catalog  name    throwable  cause  super    catalog  catalog  name  does  not  exist  cause  
public  evolving  public  interface    constraint    string  get  name    constraints  can  either  be  enforced  or  non  enforced    if  a  constraint  is  enforced  it  will  be  checked  whenever  any  sql  statement  is  executed  that  results  in  data  or  schema  changes    if  the  constraint  is  not  enforced  the  owner  of  the  data  is  responsible  for  ensuring  data  integrity    flink  will  rely  the  information  is  valid  and  might  use  it  for  query  optimisations  boolean  is  enforced    tells  what  kind  of  constraint  it  is  e  g  primary  key  unique    constraint  type  get  type    prints  the  constraint  in  a  readable  way    string  as  summary  string    type  of  the  constraint  p    unique  constraints  ul  li  unique  is  satisfied  if  and  only  if  there  do  not  exist  two  rows  that  have  same  non  null  values  in  the  unique  columns  li  li  primary  key  additionally  to  unique  constraint  it  requires  none  of  the  values  in  specified  columns  be  a  null  value    moreover  there  can  be  only  a  single  primary  key  defined  for  a    table  li  ul  enum    constraint  type  primary  key  unique  key  
public  evolving  public  final  class    unique  constraint  extends    abstract  constraint  private  final    list    string  columns  private  final    constraint  type  type    creates  a  non  enforced  link    constraint  type  primary  key  constraint  public  static    unique  constraint  primary  key    string  name    list    string  columns  return  new    unique  constraint  name  false    constraint  type  primary  key  columns  private    unique  constraint    string  name  boolean  enforced    constraint  type  type    list    string  columns  super  name  enforced  this  columns  check  not  null  columns  this  type  check  not  null  type    list  of  column  names  for  which  the  primary  key  was  defined  public    list    string  get  columns  return  columns    override  public    constraint  type  get  type  return  type    returns  constraint  s  summary    all  constraints  summary  will  be  formatted  as  pre  constraint  constraint  name  constraint  type  constraint  definition  e  g  constraint  pk  primary  key  f0  f1  pre    override  public  final    string  as  summary  string  final    string  type  string  switch  get  type  case  primary  key  type  string  primary  key  break  case  unique  key  type  string  unique  break  default  throw  new    illegal  state  exception    unknown  key  type  get  type  return    string  format  constraint  s  s  s  get  name  type  string    string  join  columns    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    unique  constraint  that    unique  constraint  o  return    objects  equals  columns  that  columns  type  that  type    override  public  int  hash  code  return    objects  hash  super  hash  code  columns  type  
public  evolving  public  final  class    data  types    creates  an  unresolved  type  that  will  be  resolved  to  a  link    data  type  by  analyzing  the  given  class  later  p    during  the  resolution    java  reflection  is  used  which  can  be  supported  by  link    data  type  hint  annotations  for  nested  structured  types  p    it  will  throw  an  link    validation  exception  in  cases  where  the  reflective  extraction  needs  more  information  or  simply  fails  p    the  following  examples  show  how  to  use  and  enrich  the  extraction  process  pre  code  returns  int  of    integer  class  returns  timestamp    of  java  time    local  date  time  class  returns  an  anonymous  unregistered  structured  type  that  is  deeply  integrated  into  the  api  compared  to  opaque  raw  types  class    user  extract  fields  automatically  public    string  name  public  int  age  enrich  the  extraction  with  precision  information  public    data  type  hint  decimal  10  2    big  decimal  account  balance  enrich  the  extraction  with  forcing  using  raw  types  public    data  type  hint  force  raw  pattern  scala    address  address  enrich  the  extraction  by  specifying  defaults  public    data  type  hint  default  second  precision      log  log  of    user  class  pre  p    note    in  most  of  the  cases  the  link    unresolved  data  type  will  be  automatically  resolved  by  the  api    at  other  locations  a  link    data  type  factory  is  provided  public  static    unresolved  data  type  of    class  unresolved  class  return  new    unresolved  data  type    string  format  s  unresolved  class  get  name  factory  factory  create  data  type  unresolved  class    creates  an  unresolved  type  that  will  be  resolved  to  a  link    data  type  by  using  a  fully  or  partially  defined  name  p    it  includes  both  built  in  types  e  g  int  as  well  as  user  defined  types  e  g  mycat  mydb    money  p    note    in  most  of  the  cases  the  link    unresolved  data  type  will  be  automatically  resolved  by  the  api    at  other  locations  a  link    data  type  factory  is  provided  public  static    unresolved  data  type  of    string  unresolved  name  return  new    unresolved  data  type  unresolved  name  factory  factory  create  data  type  unresolved  name  we  use  sql  like  naming  for  data  types  and  avoid    java  keyword  clashes  checkstyle  off    method  name    data  type  of  a  fixed  length  character  string  code  char  n  where  code  n  is  the  number  of  code  points  code  n  must  have  a  value  between    and  link    integer  max  value  both  inclusive  see    char  type  public  static    data  type  char  int  n  return  new    atomic  data  type  new    char  type  n    data  type  of  a  variable  length  character  string  code  varchar  n  where  code  n  is  the  maximum  number  of  code  points  code  n  must  have  a  value  between    and  link    integer  max  value  both  inclusive  see    var  char  type  public  static    data  type  varchar  int  n  return  new    atomic  data  type  new    var  char  type  n    data  type  of  a  variable  length  character  string  with  defined  maximum  length    this  is  a  shortcut  for  code  varchar    for  representing  jvm  strings  see    var  char  type  public  static    data  type  string  return  varchar    integer  max  value    data  type  of  a  boolean  with  a  possibly  three  valued  logic  of  code  true  false  unknown  see    boolean  type  public  static    data  type  boolean  return  new    atomic  data  type  new    boolean  type    data  type  of  a  fixed  length  binary  string  a  sequence  of  bytes  code  binary  n  where  code  n  is  the  number  of  bytes  code  n  must  have  a  value  between    and  link    integer  max  value  both  inclusive  see    binary  type  public  static    data  type  binary  int  n  return  new    atomic  data  type  new    binary  type  n    data  type  of  a  variable  length  binary  string  a  sequence  of  bytes  code  varbinary  n  where  code  n  is  the  maximum  number  of  bytes  code  n  must  have  a  value  between    and  link    integer  max  value  both  inclusive  see    var  binary  type  public  static    data  type  varbinary  int  n  return  new    atomic  data  type  new    var  binary  type  n    data  type  of  a  variable  length  binary  string  a  sequence  of  bytes  with  defined  maximum  length    this  is  a  shortcut  for  code  varbinary    for  representing  jvm  byte  arrays  see    var  binary  type  public  static    data  type  bytes  return  varbinary    integer  max  value    data  type  of  a  decimal  number  with  fixed  precision  and  scale  code  decimal  p  s  where  code  p  is  the  number  of  digits  in  a  number  precision  and  code  s  is  the  number  of  digits  to  the  right  of  the  decimal  point  in  a  number  scale  code  p  must  have  a  value  between    and    both  inclusive  code  s  must  have  a  value  between    and  code  p  both  inclusive  see    decimal  type  public  static    data  type  decimal  int  precision  int  scale  return  new    atomic  data  type  new    decimal  type  precision  scale    data  type  of  a    byte  signed  integer  with  values  from    to    see    tiny  int  type  public  static    data  type  tinyint  return  new    atomic  data  type  new    tiny  int  type    data  type  of  a    byte  signed  integer  with  values  from  32  768  to  32  767  see    small  int  type  public  static    data  type  smallint  return  new    atomic  data  type  new    small  int  type    data  type  of  a    byte  signed  integer  with  values  from  2  147  483  648  to  2  147  483  647  see    int  type  public  static    data  type  int  return  new    atomic  data  type  new    int  type    data  type  of  an    byte  signed  integer  with  values  from  9  223  372  036  854  775    to  9  223  372  036  854  775    see    big  int  type  public  static    data  type  bigint  return  new    atomic  data  type  new    big  int  type    data  type  of  a    byte  single  precision  floating  point  number  see    float  type  public  static    data  type  float  return  new    atomic  data  type  new    float  type    data  type  of  an    byte  double  precision  floating  point  number  see    double  type  public  static    data  type  double  return  new    atomic  data  type  new    double  type    data  type  of  a  date  consisting  of  code  year  month  day  with  values  ranging  from  code  0000-01    to  code  9999-12    p    compared  to  the  sql  standard  the  range  starts  at  year  code    see    data  type  public  static    data  type  date  return  new    atomic  data  type  new    date  type    data  type  of  a  time  without  time  zone  code  time  p  where  code  p  is  the  number  of  digits  of  fractional  seconds  precision  code  p  must  have  a  value  between    and    both  inclusive  p    an  instance  consists  of  code  hour  minute  second  fractional  with  up  to  nanosecond  precision  and  values  ranging  from  code  00:00  00.000000000  to  code  23:59  59.999999999  p    compared  to  the  sql  standard  leap  seconds  23:59    and  23:59    are  not  supported  as  the  semantics  are  closer  to  link  java  time    local  time  a  time  with  time  zone  is  not  provided  see  time  see    time  type  public  static    data  type  time  int  precision  return  new    atomic  data  type  new    time  type  precision    data  type  of  a  time  without  time  zone  code  time  with  no  fractional  seconds  by  default  p    an  instance  consists  of  code  hour  minute  second  with  up  to  second  precision  and  values  ranging  from  code  00:00    to  code  23:59    p    compared  to  the  sql  standard  leap  seconds  23:59    and  23:59    are  not  supported  as  the  semantics  are  closer  to  link  java  time    local  time  a  time  with  time  zone  is  not  provided  see  time  int  see    time  type  public  static    data  type  time  return  new    atomic  data  type  new    time  type    data  type  of  a  timestamp  without  time  zone  code  timestamp  p  where  code  p  is  the  number  of  digits  of  fractional  seconds  precision  code  p  must  have  a  value  between    and    both  inclusive  p    an  instance  consists  of  code  year  month  day  hour  minute  second  fractional  with  up  to  nanosecond  precision  and  values  ranging  from  code  0000-01  01 00  00:00    to  code  9999-12  31 23  59:59    p    compared  to  the  sql  standard  leap  seconds  23:59    and  23:59    are  not  supported  as  the  semantics  are  closer  to  link  java  time    local  date  time  see  timestamp  with  time  zone  int  see  timestamp  with  local  time  zone  int  see    timestamp  type  public  static    data  type  timestamp  int  precision  return  new    atomic  data  type  new    timestamp  type  precision    data  type  of  a  timestamp  without  time  zone  code  timestamp  with    digits  of  fractional  seconds  by  default  p    an  instance  consists  of  code  year  month  day  hour  minute  second  fractional  with  up  to  microsecond  precision  and  values  ranging  from  code  0000-01  01 00  00:00    to  code  9999-12  31 23  59:59    p    compared  to  the  sql  standard  leap  seconds  23:59    and  23:59    are  not  supported  as  the  semantics  are  closer  to  link  java  time    local  date  time  see  timestamp  int  see  timestamp  with  time  zone  int  see  timestamp  with  local  time  zone  int  see    timestamp  type  public  static    data  type  timestamp  return  new    atomic  data  type  new    timestamp  type    data  type  of  a  timestamp  with  time  zone  code  timestamp  p  with  time  zone  where  code  p  is  the  number  of  digits  of  fractional  seconds  precision  code  p  must  have  a  value  between    and    both  inclusive  p    an  instance  consists  of  code  year  month  day  hour  minute  second  fractional  zone  with  up  to  nanosecond  precision  and  values  ranging  from  code  0000-01  01 00  00:00    14:59  to  code  9999-12  31 23  59:59    14:59  p    compared  to  the  sql  standard  leap  seconds  23:59    and  23:59    are  not  supported  as  the  semantics  are  closer  to  link  java  time    offset  date  time  see  timestamp  int  see  timestamp  with  local  time  zone  int  see    zoned  timestamp  type  public  static    data  type  timestamp  with  time  zone  int  precision  return  new    atomic  data  type  new    zoned  timestamp  type  precision    data  type  of  a  timestamp  with  time  zone  code  timestamp  with  time  zone  with    digits  of  fractional  seconds  by  default  p    an  instance  consists  of  code  year  month  day  hour  minute  second  fractional  zone  with  up  to  microsecond  precision  and  values  ranging  from  code  0000-01  01 00  00:00    14:59  to  code  9999-12  31 23  59:59    14:59  p    compared  to  the  sql  standard  leap  seconds  23:59    and  23:59    are  not  supported  as  the  semantics  are  closer  to  link  java  time    offset  date  time  see  timestamp  with  time  zone  int  see  timestamp  int  see  timestamp  with  local  time  zone  int  see    zoned  timestamp  type  public  static    data  type  timestamp  with  time  zone  return  new    atomic  data  type  new    zoned  timestamp  type    data  type  of  a  timestamp  with  local  time  zone  code  timestamp  p  with  local  time  zone  where  code  p  is  the  number  of  digits  of  fractional  seconds  precision  code  p  must  have  a  value  between    and    both  inclusive  p    an  instance  consists  of  code  year  month  day  hour  minute  second  fractional  zone  with  up  to  nanosecond  precision  and  values  ranging  from  code  0000-01  01 00  00:00    14:59  to  code  9999-12  31 23  59:59    14:59    leap  seconds  23:59    and  23:59    are  not  supported  as  the  semantics  are  closer  to  link  java  time    offset  date  time  p    compared  to  link    zoned  timestamp  type  the  time  zone  offset  information  is  not  stored  physically  in  every  datum    instead  the  type  assumes  link  java  time    instant  semantics  in  utc  time  zone  at  the  edges  of  the  table  ecosystem    every  datum  is  interpreted  in  the  local  time  zone  configured  in  the  current  session  for  computation  and  visualization  p    this  type  fills  the  gap  between  time  zone  free  and  time  zone  mandatory  timestamp  types  by  allowing  the  interpretation  of  utc  timestamps  according  to  the  configured  session  timezone  see  timestamp  int  see  timestamp  with  time  zone  int  see    local  zoned  timestamp  type  public  static    data  type  timestamp  with  local  time  zone  int  precision  return  new    atomic  data  type  new    local  zoned  timestamp  type  precision    data  type  of  a  timestamp  with  local  time  zone  code  timestamp  with  local  time  zone  with    digits  of  fractional  seconds  by  default  p    an  instance  consists  of  code  year  month  day  hour  minute  second  fractional  zone  with  up  to  microsecond  precision  and  values  ranging  from  code  0000-01  01 00  00:00    14:59  to  code  9999-12  31 23  59:59    14:59    leap  seconds  23:59    and  23:59    are  not  supported  as  the  semantics  are  closer  to  link  java  time    offset  date  time  p    compared  to  link    zoned  timestamp  type  the  time  zone  offset  information  is  not  stored  physically  in  every  datum    instead  the  type  assumes  link  java  time    instant  semantics  in  utc  time  zone  at  the  edges  of  the  table  ecosystem    every  datum  is  interpreted  in  the  local  time  zone  configured  in  the  current  session  for  computation  and  visualization  p    this  type  fills  the  gap  between  time  zone  free  and  time  zone  mandatory  timestamp  types  by  allowing  the  interpretation  of  utc  timestamps  according  to  the  configured  session  timezone  see  timestamp  with  local  time  zone  int  see  timestamp  int  see  timestamp  with  time  zone  int  see    local  zoned  timestamp  type  public  static    data  type  timestamp  with  local  time  zone  return  new    atomic  data  type  new    local  zoned  timestamp  type    data  type  of  a  temporal  interval    there  are  two  types  of  temporal  intervals  day  time  intervals  with  up  to  nanosecond  granularity  or  year  month  intervals  with  up  to  month  granularity  p    an  interval  of  day  time  consists  of  code  days  hours  months  seconds  fractional  with  values  ranging  from  code  999999 23  59:59    to  code  999999 23  59:59      the  type  must  be  parameterized  to  one  of  the  following  resolutions  interval  of  days  interval  of  days  to  hours  interval  of  days  to  minutes  interval  of  days  to  seconds  interval  of  hours  interval  of  hours  to  minutes  interval  of  hours  to  seconds  interval  of  minutes  interval  of  minutes  to  seconds  or  interval  of  seconds    the  value  representation  is  the  same  for  all  types  of  resolutions    for  example  an  interval  of  seconds  of    is  always  represented  in  an  interval  of  days  to  seconds  format  with  default  precisions  code  00 00  01:10    p    an  interval  of  year  month  consists  of  code  years  months  with  values  ranging  from  code  9999-11  to  code  9999-11    the  type  must  be  parameterized  to  one  of  the  following  resolutions  interval  of  years  interval  of  years  to  months  or  interval  of  months    the  value  representation  is  the  same  for  all  types  of  resolutions    for  example  an  interval  of  months  of    is  always  represented  in  an  interval  of  years  to  months  format  with  default  year  precision  code  04-02  p    examples  code  interval  day    for  a  day  time  interval  or  code  interval  year    for  a  year  month  interval  see    day  time  interval  type  see    year  month  interval  type  public  static    data  type  interval    resolution  resolution    preconditions  check  not  null  resolution    interval  resolution  must  not  be  null  return  new    atomic  data  type    resolution  resolve  interval  resolution  null    data  type  of  a  temporal  interval    there  are  two  types  of  temporal  intervals  day  time  intervals  with  up  to  nanosecond  granularity  or  year  month  intervals  with  up  to  month  granularity  p    an  interval  of  day  time  consists  of  code  days  hours  months  seconds  fractional  with  values  ranging  from  code  999999 23  59:59    to  code  999999 23  59:59      the  type  must  be  parameterized  to  one  of  the  following  resolutions  interval  of  days  interval  of  days  to  hours  interval  of  days  to  minutes  interval  of  days  to  seconds  interval  of  hours  interval  of  hours  to  minutes  interval  of  hours  to  seconds  interval  of  minutes  interval  of  minutes  to  seconds  or  interval  of  seconds    the  value  representation  is  the  same  for  all  types  of  resolutions    for  example  an  interval  of  seconds  of    is  always  represented  in  an  interval  of  days  to  seconds  format  with  default  precisions  code  00 00  01:10    p    an  interval  of  year  month  consists  of  code  years  months  with  values  ranging  from  code  9999-11  to  code  9999-11    the  type  must  be  parameterized  to  one  of  the  following  resolutions  interval  of  years  interval  of  years  to  months  or  interval  of  months    the  value  representation  is  the  same  for  all  types  of  resolutions    for  example  an  interval  of  months  of    is  always  represented  in  an  interval  of  years  to  months  format  with  default  year  precision  code  04-02  p    examples  code  interval  day    second    for  a  day  time  interval  or  code  interval  year    month  for  a  year  month  interval  see    day  time  interval  type  see    year  month  interval  type  public  static    data  type  interval    resolution  upper  resolution    resolution  lower  resolution    preconditions  check  not  null  upper  resolution    upper  interval  resolution  must  not  be  null    preconditions  check  not  null  lower  resolution    lower  interval  resolution  must  not  be  null  return  new    atomic  data  type    resolution  resolve  interval  upper  resolution  lower  resolution    data  type  of  an  array  of  elements  with  same  subtype  p    compared  to  the  sql  standard  the  maximum  cardinality  of  an  array  cannot  be  specified  but  is  fixed  at  link    integer  max  value    also  any  valid  type  is  supported  as  a  subtype  see    array  type  public  static    data  type  array    data  type  element  data  type    preconditions  check  not  null  element  data  type    element  data  type  must  not  be  null  return  new    collection  data  type  new    array  type  element  data  type  get  logical  type  element  data  type    unresolved  data  type  of  an  array  of  elements  with  same  subtype  p    compared  to  the  sql  standard  the  maximum  cardinality  of  an  array  cannot  be  specified  but  is  fixed  at  link    integer  max  value    also  any  valid  type  is  supported  as  a  subtype  p    note    compared  to  link  array    data  type  this  method  produces  an  link    unresolved  data  type    in  most  of  the  cases  the  link    unresolved  data  type  will  be  automatically  resolved  by  the  api    at  other  locations  a  link    data  type  factory  is  provided  see    array  type  public  static    unresolved  data  type  array    abstract  data  type  element  data  type    preconditions  check  not  null  element  data  type    element  data  type  must  not  be  null  return  new    unresolved  data  type    string  format    array  type  format  element  data  type  factory  array  factory  create  data  type  element  data  type    data  type  of  a  multiset  bag    unlike  a  set  it  allows  for  multiple  instances  for  each  of  its  elements  with  a  common  subtype    each  unique  value  including  code  null  is  mapped  to  some  multiplicity  p    there  is  no  restriction  of  element  types  it  is  the  responsibility  of  the  user  to  ensure  uniqueness  see    multiset  type  public  static    data  type  multiset    data  type  element  data  type    preconditions  check  not  null  element  data  type    element  data  type  must  not  be  null  return  new    collection  data  type  new    multiset  type  element  data  type  get  logical  type  element  data  type    unresolved  data  type  of  a  multiset  bag    unlike  a  set  it  allows  for  multiple  instances  for  each  of  its  elements  with  a  common  subtype    each  unique  value  including  code  null  is  mapped  to  some  multiplicity  p    there  is  no  restriction  of  element  types  it  is  the  responsibility  of  the  user  to  ensure  uniqueness  p    note    compared  to  link  multiset    data  type  this  method  produces  an  link    unresolved  data  type    in  most  of  the  cases  the  link    unresolved  data  type  will  be  automatically  resolved  by  the  api    at  other  locations  a  link    data  type  factory  is  provided  see    multiset  type  public  static    unresolved  data  type  multiset    abstract  data  type  element  data  type    preconditions  check  not  null  element  data  type    element  data  type  must  not  be  null  return  new    unresolved  data  type    string  format    multiset  type  format  element  data  type  factory  multiset  factory  create  data  type  element  data  type    data  type  of  an  associative  array  that  maps  keys  including  code  null  to  values  including  code  null  a  map  cannot  contain  duplicate  keys  each  key  can  map  to  at  most  one  value  p    there  is  no  restriction  of  key  types  it  is  the  responsibility  of  the  user  to  ensure  uniqueness    the  map  type  is  an  extension  to  the  sql  standard  see    map  type  public  static    data  type  map    data  type  key  data  type    data  type  value  data  type    preconditions  check  not  null  key  data  type    key  data  type  must  not  be  null    preconditions  check  not  null  value  data  type    value  data  type  must  not  be  null  return  new    key  value  data  type  new    map  type  key  data  type  get  logical  type  value  data  type  get  logical  type  key  data  type  value  data  type    unresolved  data  type  of  an  associative  array  that  maps  keys  including  code  null  to  values  including  code  null  a  map  cannot  contain  duplicate  keys  each  key  can  map  to  at  most  one  value  p    there  is  no  restriction  of  key  types  it  is  the  responsibility  of  the  user  to  ensure  uniqueness    the  map  type  is  an  extension  to  the  sql  standard  p    note    compared  to  link  map    data  type    data  type  this  method  produces  an  link    unresolved  data  type    in  most  of  the  cases  the  link    unresolved  data  type  will  be  automatically  resolved  by  the  api    at  other  locations  a  link    data  type  factory  is  provided  see    map  type  public  static    unresolved  data  type  map    abstract  data  type  key  data  type    abstract  data  type  value  data  type    preconditions  check  not  null  key  data  type    key  data  type  must  not  be  null    preconditions  check  not  null  value  data  type    value  data  type  must  not  be  null  return  new    unresolved  data  type    string  format    map  type  format  key  data  type  value  data  type  factory  map  factory  create  data  type  key  data  type  factory  create  data  type  value  data  type    data  type  of  a  sequence  of  fields  a  field  consists  of  a  field  name  field  type  and  an  optional  description    the  most  specific  type  of  a  row  of  a  table  is  a  row  type    in  this  case  each  column  of  the  row  corresponds  to  the  field  of  the  row  type  that  has  the  same  ordinal  position  as  the  column  p    compared  to  the  sql  standard  an  optional  field  description  simplifies  the  handling  with  complex  structures  p    use  link  field    string    data  type  or  link  field    string    data  type    string  to  construct  fields  see    row  type  public  static    data  type  row    field  fields  final    list    row  field  logical  fields    stream  of  fields  map  f    preconditions  check  not  null  f    field  definition  must  not  be  null  map  f  new    row  field  f  name  f  data  type  get  logical  type  f  description  collect    collectors  to  list  final    list    data  type  field  data  types    stream  of  fields  map  f  f  data  type  collect    collectors  to  list  return  new    fields  data  type  new    row  type  logical  fields  field  data  types    data  type  of  a  row  type  with  no  fields    it  only  exists  for  completeness  see  row    field  public  static    data  type  row  return  row  new    field      unresolved  data  type  of  a  sequence  of  fields  a  field  consists  of  a  field  name  field  type  and  an  optional  description    the  most  specific  type  of  a  row  of  a  table  is  a  row  type    in  this  case  each  column  of  the  row  corresponds  to  the  field  of  the  row  type  that  has  the  same  ordinal  position  as  the  column  p    compared  to  the  sql  standard  an  optional  field  description  simplifies  the  handling  with  complex  structures  p    use  link  field    string    abstract  data  type  or  link  field    string    abstract  data  type    string  to  construct  fields  p    note    compared  to  link  row    field  this  method  produces  an  link    unresolved  data  type  with  link    unresolved  field  s    in  most  of  the  cases  the  link    unresolved  data  type  will  be  automatically  resolved  by  the  api    at  other  locations  a  link    data  type  factory  is  provided  see    row  type  public  static    unresolved  data  type  row    abstract  field  fields    stream  of  fields  for  each  f    preconditions  check  not  null  f    field  definition  must  not  be  null  return  new    unresolved  data  type    string  format    row  type  format    stream  of  fields  map    object  to  string  collect    collectors  joining  factory  final    field  fields  array    stream  of  fields  map  f  new    field  f  name  factory  create  data  type  f  get  abstract  data  type  f  description  to  array    field  new  return  row  fields  array    data  type  for  representing  untyped  code  null  values  a  null  type  has  no  other  value  except  code  null  thus  it  can  be  cast  to  any  nullable  type  similar  to  jvm  semantics  p    this  type  helps  in  representing  unknown  types  in  api  calls  that  use  a  code  null  literal  as  well  as  bridging  to  formats  such  as  json  or    avro  that  define  such  a  type  as  well  p    the  null  type  is  an  extension  to  the  sql  standard  p    note    the  runtime  does  not  support  this  type    it  is  a  pure  helper  type  during  translation  and  planning    table  columns  cannot  be  declared  with  this  type    functions  cannot  declare  return  types  of  this  type  see    null  type  public  static    data  type  null  return  new    atomic  data  type  new    null  type    data  type  of  an  arbitrary  serialized  type    this  type  is  a  black  box  within  the  table  ecosystem  and  is  only  deserialized  at  the  edges  p    the  raw  type  is  an  extension  to  the  sql  standard  p    this  method  assumes  that  a  link    type  serializer  instance  is  present    use  link  raw    class  for  automatically  generating  a  serializer  param  clazz  originating  value  class  param  serializer  type  serializer  see    raw  type  public  static  t    data  type  raw    class  t  clazz    type  serializer  t  serializer  return  new    atomic  data  type  new    raw  type  clazz  serializer    unresolved  data  type  of  an  arbitrary  serialized  type    this  type  is  a  black  box  within  the  table  ecosystem  and  is  only  deserialized  at  the  edges  p    the  raw  type  is  an  extension  to  the  sql  standard  p    compared  to  link  raw    class    type  serializer  this  method  produces  an  link    unresolved  data  type  where  no  serializer  is  known  and  a  generic  serializer  should  be  used    during  the  resolution  a  link    data  types  raw    class    type  serializer  with    flink  s  default  raw  serializer  is  created  and  automatically  configured  p    note    in  most  of  the  cases  the  link    unresolved  data  type  will  be  automatically  resolved  by  the  api    at  other  locations  a  link    data  type  factory  is  provided  see    raw  type  public  static  t    unresolved  data  type  raw    class  t  clazz  return  new    unresolved  data  type    string  format    raw  type  format  clazz  get  name  factory  factory  create  raw  data  type  clazz    data  type  of  an  arbitrary  serialized  type  backed  by  link    type  information    this  type  is  a  black  box  within  the  table  ecosystem  and  is  only  deserialized  at  the  edges  p    the  raw  type  is  an  extension  to  the  sql  standard  p    compared  to  an  link  raw    class    type  serializer  this  type  does  not  contain  a  link    type  serializer  yet    the  serializer  will  be  generated  from  the  enclosed  link    type  information  but  needs  access  to  the  link    execution  config  of  the  current  execution  environment    thus  this  type  is  just  a  placeholder  see    type  information  raw  type  public  static  t    data  type  raw    type  information  t  type  information  return  new    atomic  data  type  new    type  information  raw  type  type  information    data  type  of  a  user  defined  object  structured  type    structured  types  contain  zero  one  or  more  attributes    each  attribute  consists  of  a  name  and  a  type  a  type  cannot  be  defined  so  that  one  of  its  attribute  types  transitively  uses  itself  p    there  are  two  kinds  of  structured  types    types  that  are  stored  in  a  catalog  and  are  identified  by  an  link    object  identifier  or  anonymously  defined  unregistered  types  usually  reflectively  extracted  that  are  identified  by  an  implementation  link    class  p    this  method  helps  in  manually  constructing  anonymous  unregistered  types    this  is  useful  in  cases  where  the  reflective  extraction  using  link    data  types  of    class  is  not  applicable    however  link    data  types  of    class  is  the  recommended  way  of  creating  inline  structured  types  as  it  also  considers  link    data  type  hint  s  p    structured  types  are  converted  to  internal  data  structures  by  the  runtime    the  given  implementation  class  is  only  used  at  the  edges  of  the  table  ecosystem  e  g  when  bridging  to  a  function  or  connector    serialization  and  equality  code  hash  code  equals  are  handled  by  the  runtime  based  on  the  logical  type    an  implementation  class  must  offer  a  default  constructor  with  zero  arguments  or  a  full  constructor  that  assigns  all  attributes  p    note  a  caller  of  this  method  must  make  sure  that  the  link    data  type  get  conversion  class  of  the  given  fields  matches  with  the  attributes  of  the  given  implementation  class  otherwise  an  exception  might  be  thrown  during  runtime  see    data  types  of    class  see    structured  type  public  static  t    data  type  structured    class  t  implementation  class    field  fields  some  basic  validation  of  the  class  to  prevent  common  mistakes  validate  structured  class  implementation  class  final    structured  type    builder  builder    structured  type  new  builder  implementation  class  final    list    structured  attribute  attributes    stream  of  fields  map  f  new    structured  attribute  f  get  name  f  get  data  type  get  logical  type  f  get  description  or  else  null  collect    collectors  to  list  builder  attributes  attributes  builder  set  final  true  builder  set  instantiable  true  final    list    data  type  field  data  types    stream  of  fields  map    data  types    field  get  data  type  collect    collectors  to  list  return  new    fields  data  type  builder  build  implementation  class  field  data  types    helper  functions    resolution  in  seconds  with    digits  for  fractional  seconds  by  default  see  second  int  public  static    resolution  second  return  new    resolution    resolution    interval  unit  second    day  time  interval  type  default  fractional  precision    resolution  in  seconds  and  possibly  fractional  seconds    the  precision  is  the  number  of  digits  of  fractional  seconds    it  must  have  a  value  between    and    both  inclusive    if  no  fractional  is  specified  it  is  equal  to    by  default  see  second  public  static    resolution  second  int  precision  return  new    resolution    resolution    interval  unit  second  precision    resolution  in  minutes  public  static    resolution  minute  return  new    resolution    resolution    interval  unit  minute    resolution  in  hours  public  static    resolution  hour  return  new    resolution    resolution    interval  unit  hour    resolution  in  days    the  precision  is  the  number  of  digits  of  days    it  must  have  a  value  between    and    both  inclusive    if  no  precision  is  specified  it  is  equal  to    by  default  see  day  public  static    resolution  day  int  precision  return  new    resolution    resolution    interval  unit  day  precision    resolution  in  days  with    digits  for  the  number  of  days  by  default  see  day  int  public  static    resolution  day  return  new    resolution    resolution    interval  unit  day    day  time  interval  type  default  day  precision    resolution  in  months  public  static    resolution  month  return  new    resolution    resolution    interval  unit  month    resolution  in  years    the  precision  is  the  number  of  digits  of  years    it  must  have  a  value  between    and    both  inclusive    if  no  precision  is  specified  it  is  equal  to    see  year  public  static    resolution  year  int  precision  return  new    resolution    resolution    interval  unit  year  precision    resolution  in  years  with    digits  for  the  number  of  years  by  default  see  year  int  public  static    resolution  year  return  new    resolution    resolution    interval  unit  year    year  month  interval  type  default  precision    field  definition  with  field  name  and  data  type  public  static    field  field    string  name    data  type  data  type  return  new    field    preconditions  check  not  null  name    field  name  must  not  be  null    preconditions  check  not  null  data  type    field  data  type  must  not  be  null  null    field  definition  with  field  name  data  type  and  a  description  public  static    field  field    string  name    data  type  data  type    string  description  return  new    field    preconditions  check  not  null  name    field  name  must  not  be  null    preconditions  check  not  null  data  type    field  data  type  must  not  be  null    preconditions  check  not  null  description    field  description  must  not  be  null    unresolved  field  definition  with  field  name  and  data  type  p    note    compared  to  link  field    string    data  type  this  method  produces  an  link    unresolved  field  that  can  contain  an  link    unresolved  data  type  public  static    unresolved  field  field    string  name    abstract  data  type  field  data  type  return  new    unresolved  field    preconditions  check  not  null  name    field  name  must  not  be  null    preconditions  check  not  null  field  data  type    field  data  type  must  not  be  null  null    unresolved  field  definition  with  field  name  unresolved  data  type  and  a  description  p    note    compared  to  link  field    string    data  type    string  this  method  produces  an  link    unresolved  field  that  can  contain  an  link    unresolved  data  type  public  static    unresolved  field  field    string  name    abstract  data  type  field  data  type    string  description  return  new    unresolved  field    preconditions  check  not  null  name    field  name  must  not  be  null    preconditions  check  not  null  field  data  type    field  data  type  must  not  be  null    preconditions  check  not  null  description    field  description  must  not  be  null    helper  classes    helper  class  for  defining  the  resolution  of  an  interval  see  interval    resolution  public  static  final  class    resolution  private  static  final  int  empty  precision    private  enum    interval  unit  second  minute  hour  day  month  year  private  static  final    map    list    interval  unit    bi  function    integer    integer    logical  type  resolution  mapping  new    hash  map  static  add  resolution  mapping    interval  unit  year  null  p1  p2  new    year  month  interval  type    year  month  resolution  year  p1  add  resolution  mapping    interval  unit  month  null  p1  p2  new    year  month  interval  type    year  month  resolution  month  add  resolution  mapping    interval  unit  year    interval  unit  month  p1  p2  new    year  month  interval  type    year  month  resolution  year  to  month  p1  add  resolution  mapping    interval  unit  day  null  p1  p2  new    day  time  interval  type    day  time  resolution  day  p1    day  time  interval  type  default  fractional  precision  add  resolution  mapping    interval  unit  day    interval  unit  hour  p1  p2  new    day  time  interval  type    day  time  resolution  day  to  hour  p1    day  time  interval  type  default  fractional  precision  add  resolution  mapping    interval  unit  day    interval  unit  minute  p1  p2  new    day  time  interval  type    day  time  resolution  day  to  minute  p1    day  time  interval  type  default  fractional  precision  add  resolution  mapping    interval  unit  day    interval  unit  second  p1  p2  new    day  time  interval  type    day  time  resolution  day  to  second  p1  p2  add  resolution  mapping    interval  unit  hour  null  p1  p2  new    day  time  interval  type    day  time  resolution  hour  add  resolution  mapping    interval  unit  hour    interval  unit  minute  p1  p2  new    day  time  interval  type    day  time  resolution  hour  to  minute  add  resolution  mapping    interval  unit  hour    interval  unit  second  p1  p2  new    day  time  interval  type    day  time  resolution  hour  to  second    day  time  interval  type  default  day  precision  p2  add  resolution  mapping    interval  unit  minute  null  p1  p2  new    day  time  interval  type    day  time  resolution  minute  add  resolution  mapping    interval  unit  minute    interval  unit  second  p1  p2  new    day  time  interval  type    day  time  resolution  minute  to  second    day  time  interval  type  default  day  precision  p2  add  resolution  mapping    interval  unit  second  null  p1  p2  new    day  time  interval  type    day  time  resolution  second    day  time  interval  type  default  day  precision  p1  private  static  void  add  resolution  mapping    interval  unit  left  unit    interval  unit  right  unit    bi  function    integer    integer    logical  type  type  provider  resolution  mapping  put    arrays  as  list  left  unit  right  unit  type  provider  private  static    logical  type  resolve  interval    resolution  from  resolution    nullable    resolution  to  resolution  final    interval  unit  to  boundary  to  resolution  null  null  to  resolution  unit  final  int  to  precision  to  resolution  null  empty  precision  to  resolution  precision  final    bi  function    integer    integer    logical  type  type  provider  resolution  mapping  get    arrays  as  list  from  resolution  unit  to  boundary  if  type  provider  null  throw  new    validation  exception    string  format    unsupported  interval  definition  s  to  s    please  check  the  documentation  for  supported  combinations  for  year  month  and  day  time  intervals  from  resolution  unit  to  boundary  return  type  provider  apply  from  resolution  precision  to  precision  private  final  int  precision  private  final    interval  unit  unit  private    resolution    interval  unit  unit  int  precision  this  unit  unit  this  precision  precision  private    resolution    interval  unit  unit  this  unit  empty  precision    override  public    string  to  string  if  precision  empty  precision  return    string  format  s  d  unit  precision  return  unit  to  string    common  helper  class  for  resolved  and  unresolved  fields  of  a  row  or  structured  type  see  field    string    data  type  see  field    string    data  type    string  see  field    string    abstract  data  type  see  field    string    abstract  data  type    string  public  abstract  static  class    abstract  field  protected  final    string  name  protected  final    nullable    string  description  private    abstract  field    string  name    nullable    string  description  this  name  name  this  description  description  public    string  get  name  return  name  public    optional    string  get  description  return    optional  of  nullable  description  protected  abstract    abstract  data  type  get  abstract  data  type    override  public    string  to  string  if  description  null  return    string  format    row  field  field  format  with  description  name  get  abstract  data  type  description  return    string  format    row  field  field  format  no  description  name  get  abstract  data  type    helper  class  for  defining  the  field  of  a  row  or  structured  type  see  field    string    data  type  see  field    string    data  type    string  public  static  final  class    field  extends    abstract  field  private  final    data  type  data  type  private    field    string  name    data  type  data  type    nullable    string  description  super  name  description  this  data  type  data  type  public    data  type  get  data  type  return  data  type    override  protected    abstract  data  type  get  abstract  data  type  return  data  type    helper  class  for  defining  the  unresolved  field  of  a  row  or  structured  type  p    compared  to  link    field  an  unresolved  field  can  contain  an  link    unresolved  data  type  see  field    string    abstract  data  type  see  field    string    abstract  data  type    string  public  static  final  class    unresolved  field  extends    abstract  field  private  final    abstract  data  type  data  type  private    unresolved  field    string  name    abstract  data  type  data  type    nullable    string  description  super  name  description  this  data  type  data  type    override  protected    abstract  data  type  get  abstract  data  type  return  data  type  private    data  types  no  instances  checkstyle  on    method  name  
public  evolving  public  interface    data  view  extends    serializable    clears  the  link    data  view  and  removes  all  data  void  clear  
public  evolving  public  class    table  column    instance  fields  private  final    string  name  private  final    data  type  type    nullable  private  final    string  expr    constructors    creates  a  link    table  column  instance  param  name    column  name  param  type    column  data  type  param  expr    column  computation  expression  if  it  is  a  computed  column  private    table  column    string  name    data  type  type    nullable    string  expr  this  name  name  this  type  type  this  expr  expr    methods    creates  a  table  column  from  given  name  and  data  type  public  static    table  column  of    string  name    data  type  type    preconditions  check  not  null  name    column  name  can  not  be  null    preconditions  check  not  null  type    column  type  can  not  be  null  return  new    table  column  name  type  null    creates  a  table  column  from  given  name  and  computation  expression  param  name    name  of  the  column  param  expression  sql  style  expression  public  static    table  column  of    string  name    data  type  type    string  expression    preconditions  check  not  null  name    column  name  can  not  be  null    preconditions  check  not  null  type    column  type  can  not  be  null    preconditions  check  not  null  expression    column  expression  can  not  be  null  return  new    table  column  name  type  expression    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    table  column  that    table  column  o  return    objects  equals  this  name  that  name    objects  equals  this  type  that  type    objects  equals  this  expr  that  expr    override  public  int  hash  code  return    objects  hash  this  name  this  type  this  expr    getter    setter    returns  data  type  of  this  column  public    data  type  get  type  return  this  type    returns  name  of  this  column  public    string  get  name  return  name    returns  computation  expression  of  this  column    or  empty  if  this  column  is  not  a  computed  column  public    optional    string  get  expr  return    optional  of  nullable  this  expr    returns  if  this  column  is  a  computed  column  that  is  generated  from  an  expression  return  true  if  this  column  is  generated  public  boolean  is  generated  return  this  expr  null  
public  evolving  public  class    table  exception  extends    runtime  exception  public    table  exception    string  message    throwable  cause  super  message  cause  public    table  exception    string  message  super  message  
public  evolving  public  class    table  not  exist  exception  extends    runtime  exception  public    table  not  exist  exception    string  catalog  name    string  table  name  this  catalog  name  table  name  null  public    table  not  exist  exception    string  catalog    string  table    throwable  cause  super    table  catalog  table  does  not  exist  cause  
public  evolving  public  class    table  schema  private  static  final    string  atomic  type  field  name  f0  private  final    list    table  column  columns  private  final    list    watermark  spec  watermark  specs  private  final    nullable    unique  constraint  primary  key  private    table  schema    list    table  column  columns    list    watermark  spec  watermark  specs    nullable    unique  constraint  primary  key  this  columns    preconditions  check  not  null  columns  this  watermark  specs    preconditions  check  not  null  watermark  specs  this  primary  key  primary  key  deprecated    use  the  link    builder  instead    deprecated  public    table  schema    string  field  names    type  information  field  types    data  type  field  data  types  from  legacy  info  to  data  type  field  types  validate  name  type  number  equal  field  names  field  data  types    list    table  column  columns  new    array  list  for  int  i    i  field  names  length  i  columns  add    table  column  of  field  names  i  field  data  types  i  validate  columns  and  watermark  specs  columns    collections  empty  list  this  columns  columns  this  watermark  specs    collections  empty  list  this  primary  key  null    returns  a  deep  copy  of  the  table  schema  public    table  schema  copy  return  new    table  schema  new    array  list  columns  new    array  list  watermark  specs  primary  key    returns  all  field  data  types  as  an  array  public    data  type  get  field  data  types  return  columns  stream  map    table  column  get  type  to  array    data  type  new  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    it  is  recommended  to  use  link  get  field  data  types  instead  which  uses  the  new  type  system  based  on  link    data  types    please  make  sure  to  use  either  the  old  or  the  new  type  system  consistently  to  avoid  unintended  behavior    see  the  website  documentation  for  more  information    deprecated  public    type  information  get  field  types  return  from  data  type  to  legacy  info  get  field  data  types    returns  the  specified  data  type  for  the  given  field  index  param  field  index  the  index  of  the  field  public    optional    data  type  get  field  data  type  int  field  index  if  field  index    field  index  columns  size  return    optional  empty  return    optional  of  columns  get  field  index  get  type  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    it  is  recommended  to  use  link  get  field  data  type  int  instead  which  uses  the  new  type  system  based  on  link    data  types    please  make  sure  to  use  either  the  old  or  the  new  type  system  consistently  to  avoid  unintended  behavior    see  the  website  documentation  for  more  information    deprecated  public    optional    type  information  get  field  type  int  field  index  return  get  field  data  type  field  index  map    type  conversions  from  data  type  to  legacy  info    returns  the  specified  data  type  for  the  given  field  name  param  field  name  the  name  of  the  field  public    optional    data  type  get  field  data  type    string  field  name  return  this  columns  stream  filter  column  column  get  name  equals  field  name  find  first  map    table  column  get  type  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    it  is  recommended  to  use  link  get  field  data  type    string  instead  which  uses  the  new  type  system  based  on  link    data  types    please  make  sure  to  use  either  the  old  or  the  new  type  system  consistently  to  avoid  unintended  behavior    see  the  website  documentation  for  more  information    deprecated  public    optional    type  information  get  field  type    string  field  name  return  get  field  data  type  field  name  map    type  conversions  from  data  type  to  legacy  info    returns  the  number  of  fields  public  int  get  field  count  return  columns  size    returns  all  field  names  as  an  array  public    string  get  field  names  return  this  columns  stream  map    table  column  get  name  to  array    string  new    returns  the  specified  name  for  the  given  field  index  param  field  index  the  index  of  the  field  public    optional    string  get  field  name  int  field  index  if  field  index    field  index  columns  size  return    optional  empty  return    optional  of  this  columns  get  field  index  get  name    returns  the  link    table  column  instance  for  the  given  field  index  param  field  index  the  index  of  the  field  public    optional    table  column  get  table  column  int  field  index  if  field  index    field  index  columns  size  return    optional  empty  return    optional  of  this  columns  get  field  index    returns  the  link    table  column  instance  for  the  given  field  name  param  field  name  the  name  of  the  field  public    optional    table  column  get  table  column    string  field  name  return  this  columns  stream  filter  column  column  get  name  equals  field  name  find  first    returns  all  the  link    table  column  s  for  this  table  schema  public    list    table  column  get  table  columns  return  new    array  list  this  columns    converts  all  columns  of  this  schema  into  a  possibly  nested  row  data  type  p    note    the  returned  row  data  type  contains  both  physical  and  computed  columns    be  careful  when  using  this  method  in  a  table  source  or  table  sink    in  many  cases  link  to  physical  row  data  type  might  be  more  appropriate  see    data  types  row    field  see  to  physical  row  data  type  public    data  type  to  row  data  type  final    field  fields  columns  stream  map  column  field  column  get  name  column  get  type  to  array    field  new  return  row  fields    converts  all  physical  columns  of  this  schema  into  a  possibly  nested  row  data  type  p    note    the  returned  row  data  type  contains  only  physical  columns    it  does  not  include  computed  columns  see    data  types  row    field  see  to  row  data  type  public    data  type  to  physical  row  data  type  final    field  fields  columns  stream  filter  column  column  is  generated  map  column  field  column  get  name  column  get  type  to  array    field  new  return  row  fields  deprecated    use  link  to  row  data  type  instead    deprecated    suppress  warnings  unchecked  public    type  information    row  to  row  type  return    type  information    row  from  data  type  to  legacy  info  to  row  data  type    returns  a  list  of  the  watermark  specification  which  contains  rowtime  attribute  and  watermark  strategy  expression  p  note    currently  there  is  at  most  one  link    watermark  spec  in  the  list  because  we  don  t  support  multiple  watermarks  definition  yet    but  in  the  future  we  may  support  multiple  watermarks  public    list    watermark  spec  get  watermark  specs  return  watermark  specs  public    optional    unique  constraint  get  primary  key  return    optional  of  nullable  primary  key    override  public    string  to  string  final    string  builder  sb  new    string  builder  sb  append  root  n  for    table  column  column  columns  sb  append  append  column  get  name  append  sb  append  column  get  type  if  column  get  expr  is  present  sb  append  as  append  column  get  expr  get  sb  append  n  if  watermark  specs  is  empty  for    watermark  spec  watermark  watermark  specs  sb  append  append  watermark  for  append  watermark  get  rowtime  attribute  append  as  append  watermark  get  watermark  expr  sb  append  n  if  primary  key  null  sb  append  append  primary  key  as  summary  string  sb  append  n  return  sb  to  string    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    table  schema  that    table  schema  o  return    objects  equals  columns  that  columns    objects  equals  watermark  specs  that  watermark  specs    objects  equals  primary  key  that  primary  key    override  public  int  hash  code  return    objects  hash  columns  watermark  specs  primary  key    creates  a  table  schema  from  a  link    type  information  instance    if  the  type  information  is  a  link    composite  type  the  field  names  and  types  for  the  composite  type  are  used  to  construct  the  link    table  schema  instance    otherwise  a  table  schema  with  a  single  field  is  created    the  field  name  is  f0  and  the  field  type  the  provided  type  param  type  info    the  link    type  information  from  which  the  table  schema  is  generated  return    the  table  schema  that  was  generated  from  the  given  link    type  information  deprecated    this  method  will  be  removed  soon    use  link    data  types  to  declare  types    deprecated  public  static    table  schema  from  type  info    type  information  type  info  if  type  info  instanceof    composite  type  final    composite  type  composite  type    composite  type  type  info  get  field  names  and  types  from  composite  type  final    string  field  names  composite  type  get  field  names  final    type  information  field  types  new    type  information  field  names  length  for  int  i    i  field  types  length  i  field  types  i  composite  type  get  type  at  i  return  new    table  schema  field  names  field  types  else  create  table  schema  with  a  single  field  named  f0  of  the  given  type  return  new    table  schema  new    string  atomic  type  field  name  new    type  information  type  info  public  static    builder  builder  return  new    builder    tools    validate  the  field  names  code  field  names  and  field  types  code  field  types  have  equal  number  param  field  names    field  names  param  field  types    field  data  types  private  static  void  validate  name  type  number  equal    string  field  names    data  type  field  types  if  field  names  length  field  types  length  throw  new    validation  exception    number  of  field  names  and  field  data  types  must  be  equal  n    number  of  names  is  field  names  length  number  of  data  types  is  field  types  length  n    list  of  field  names    arrays  to  string  field  names  n    list  of  field  data  types    arrays  to  string  field  types    table  column  and  watermark  specification  sanity  check  private  static  void  validate  columns  and  watermark  specs    list    table  column  columns    list    watermark  spec  watermark  specs    validate  and  create  name  to  type  mapping    field  name  to  data  type  mapping  we  need  this  because  the  row  time  attribute  field  can  be  nested    this  also  check  duplicate  fields  final    map    string    logical  type  field  name  to  type  new    hash  map  for    table  column  column  columns  validate  and  create  name  to  type  mapping  field  name  to  type  column  get  name  column  get  type  get  logical  type    validate  watermark  and  rowtime  attribute  for    watermark  spec  watermark  watermark  specs    string  rowtime  attribute  watermark  get  rowtime  attribute    logical  type  rowtime  type    optional  of  nullable  field  name  to  type  get  rowtime  attribute  or  else  throw  new    validation  exception    string  format    rowtime  attribute  s  is  not  defined  in  schema  rowtime  attribute  if  rowtime  type  get  type  root  timestamp  without  time  zone  throw  new    validation  exception    string  format    rowtime  attribute  s  must  be  of  type  timestamp  but  is  of  type  s  rowtime  attribute  rowtime  type    logical  type  watermark  output  type  watermark  get  watermark  expr  output  type  get  logical  type  if  watermark  output  type  get  type  root  timestamp  without  time  zone  throw  new    validation  exception    string  format    watermark  strategy  s  must  be  of  type  timestamp  but  is  of  type  s  watermark  get  watermark  expr  watermark  output  type  as  summary  string  private  static  void  validate  primary  key    list    table  column  columns    unique  constraint  primary  key    map    string    table  column  columns  by  name  lookup  columns  stream  collect    collectors  to  map    table  column  get  name    function  identity  for    string  column  name  primary  key  get  columns    table  column  column  columns  by  name  lookup  get  column  name  if  column  null  throw  new    validation  exception    string  format    could  not  create  a  primary  key  s    column  s  does  not  exist  primary  key  get  name  column  name  if  column  is  generated  throw  new    validation  exception    string  format    could  not  create  a  primary  key  s  with  a  generated  column  s  primary  key  get  name  column  name  if  column  get  type  get  logical  type  is  nullable  throw  new    validation  exception    string  format    could  not  create  a  primary  key  s    column  s  is  nullable  primary  key  get  name  column  name    creates  a  mapping  from  field  name  to  data  type  the  field  name  can  be  a  nested  field    this  is  mainly  used  for  validating  whether  the  rowtime  attribute  might  be  nested  exists  in  the  schema    during  creating  it  also  validates  whether  there  is  duplicate  field  names  p    for  example  a  f0  field  of  row  type  has  two  nested  fields  q1  and  q2    then  the  mapping  will  be  f0  row  f0  q1  int  f0  q2  string  pre  code  f0  row  q1  int  q2  string  pre  param  field  name  to  type    field  name  to  type  mapping  that  to  update  param  field  name    name  of  this  field  e  g  q1  or  q2  in  the  above  example  param  field  type    data  type  of  this  field  param  parent  field  name    field  name  of  parent  type  e  g  f0  in  the  above  example  private  static  void  validate  and  create  name  to  type  mapping    map    string    logical  type  field  name  to  type    string  field  name    logical  type  field  type    string  parent  field  name    string  full  field  name  parent  field  name  is  empty  field  name  parent  field  name  field  name    logical  type  old  type  field  name  to  type  put  full  field  name  field  type  if  old  type  null  throw  new    validation  exception    field  names  must  be  unique    duplicate  field  full  field  name  if  is  composite  type  field  type  field  type  instanceof    legacy  type  information  type  final    list    string  field  names    logical  type  checks  get  field  names  field  type  final    list    logical  type  field  types  field  type  get  children    int  stream  range    field  names  size  for  each  i  validate  and  create  name  to  type  mapping  field  name  to  type  field  names  get  i  field  types  get  i  full  field  name    builder  for  creating  a  link    table  schema  public  static  class    builder  private    list    table  column  columns  private  final    list    watermark  spec  watermark  specs  private    unique  constraint  primary  key  public    builder  columns  new    array  list  watermark  specs  new    array  list    add  a  field  with  name  and  data  type  p    the  call  order  of  this  method  determines  the  order  of  fields  in  the  schema  public    builder  field    string  name    data  type  data  type    preconditions  check  not  null  name    preconditions  check  not  null  data  type  columns  add    table  column  of  name  data  type  return  this    add  a  computed  field  which  is  generated  by  the  given  expression    this  also  defines  the  field  name  and  the  data  type  p    the  call  order  of  this  method  determines  the  order  of  fields  in  the  schema  param  name    field  name  param  data  type    field  data  type  param  expression    computed  column  expression  it  should  be  a  sql  style  expression  whose  identifiers  should  be  all  quoted  and  expanded    it  should  be  expanded  because  this  expression  may  be  persisted  then  deserialized  from  the  catalog  an  expanded  identifier  would  avoid  the  ambiguity  if  there  are  same  name  udf  referenced  from  different  paths    for  example  if  there  is  a  udf  named  my  udf  from  path  my  catalog  my  database  you  could  pass  in  an  expression  like  my  catalog  my  database  my  udf  f0      it  should  be  quoted  because  user  could  use  a  reserved  keyword  as  the  identifier  and  we  have  no  idea  if  it  is  quoted  when  deserialize  from  the  catalog  so  we  force  to  use  quoted  identifier  here    but  framework  will  not  check  whether  it  is  qualified  and  quoted  or  not  public    builder  field    string  name    data  type  data  type    string  expression    preconditions  check  not  null  name    preconditions  check  not  null  data  type    preconditions  check  not  null  expression  columns  add    table  column  of  name  data  type  expression  return  this    adds  a  link    table  column  to  this  builder  p    the  call  order  of  this  method  determines  the  order  of  fields  in  the  schema  public    builder  add    table  column  column  columns  add  column  return  this    add  an  array  of  fields  with  names  and  data  types  p    the  call  order  of  this  method  determines  the  order  of  fields  in  the  schema  public    builder  fields    string  names    data  type  data  types    preconditions  check  not  null  names    preconditions  check  not  null  data  types  validate  name  type  number  equal  names  data  types    list    table  column  columns    int  stream  range    names  length  map  to  obj  idx    table  column  of  names  idx  data  types  idx  collect    collectors  to  list  this  columns  add  all  columns  return  this  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    it  is  recommended  to  use  link  field    string    data  type  instead  which  uses  the  new  type  system  based  on  link    data  types    please  make  sure  to  use  either  the  old  or  the  new  type  system  consistently  to  avoid  unintended  behavior    see  the  website  documentation  for  more  information    deprecated  public    builder  field    string  name    type  information  type  info  return  field  name  from  legacy  info  to  data  type  type  info    specifies  the  previously  defined  field  as  an  event  time  attribute  and  specifies  the  watermark  strategy  param  rowtime  attribute  the  field  name  as  a  rowtime  attribute  can  be  a  nested  field  using  dot  separator  param  watermark  expression  string  the  string  representation  of  watermark  generation  expression  e  g  ts  interval    second    the  string  is  a  qualified  sql  expression  string    u  d  fs  are  expanded  but  will  not  be  validated  by  link    table  schema  param  watermark  expr  output  type  the  data  type  of  the  computation  result  of  watermark  generation  expression    whether  the  data  type  equals  to  the  output  type  of  expression  will  also  not  be  validated  by  link    table  schema  public    builder  watermark    string  rowtime  attribute    string  watermark  expression  string    data  type  watermark  expr  output  type    preconditions  check  not  null  rowtime  attribute    preconditions  check  not  null  watermark  expression  string    preconditions  check  not  null  watermark  expr  output  type  if  this  watermark  specs  is  empty  throw  new    illegal  state  exception    multiple  watermark  definition  is  not  supported  yet  this  watermark  specs  add  new    watermark  spec  rowtime  attribute  watermark  expression  string  watermark  expr  output  type  return  this    adds  the  given  link    watermark  spec  to  this  builder  public    builder  watermark    watermark  spec  watermark  spec  if  this  watermark  specs  is  empty  throw  new    illegal  state  exception    multiple  watermark  definition  is  not  supported  yet  this  watermark  specs  add  watermark  spec  return  this    creates  a  primary  key  constraint  for  a  set  of  given  columns    the  primary  key  is  informational  only    it  will  not  be  enforced    it  can  be  used  for  optimizations    it  is  the  owner  s  of  the  data  responsibility  to  ensure  uniqueness  of  the  data  p    the  primary  key  will  be  assigned  a  random  name  param  columns  array  of  columns  that  form  a  unique  primary  key  public    builder  primary  key    string  columns  return  primary  key  uuid  random  u  u  i  d  to  string  columns    creates  a  primary  key  constraint  for  a  set  of  given  columns    the  primary  key  is  informational  only    it  will  not  be  enforced    it  can  be  used  for  optimizations    it  is  the  owner  s  of  the  data  responsibility  to  ensure  param  columns  array  of  columns  that  form  a  unique  primary  key  param  name  name  for  the  primary  key  can  be  used  to  reference  the  constraint  public    builder  primary  key    string  name    string  columns  if  this  primary  key  null  throw  new    validation  exception    can  not  create  multiple  primary  keys  if    string  utils  is  null  or  whitespace  only  name  throw  new    validation  exception  primary  key  s  name  can  not  be  null  or  empty  if  columns  null  columns  length    throw  new    validation  exception  primary  key  constraint  must  be  defined  for  at  least  a  single  column  this  primary  key    unique  constraint  primary  key  name    arrays  as  list  columns  return  this    returns  a  link    table  schema  instance  public    table  schema  build  validate  columns  and  watermark  specs  this  columns  this  watermark  specs  if  primary  key  null  validate  primary  key  this  columns  primary  key  return  new    table  schema  columns  watermark  specs  primary  key  
public  evolving  public  class    validation  exception  extends    runtime  exception  public    validation  exception    string  message    throwable  cause  super  message  cause  public    validation  exception    string  message  super  message  
public  evolving  public  interface    catalog    returns  a  factory  for  creating  instances  from  catalog  objects  p    this  method  enables  bypassing  the  discovery  process    implementers  can  directly  pass  internal  catalog  specific  objects  to  their  own  factory    for  example  a  custom  link    catalog  table  can  be  processed  by  a  custom  link    dynamic  table  factory  p    because  all  factories  are  interfaces  the  returned  link    factory  instance  can  implement  multiple  supported  extension  points    an  code  instanceof  check  is  performed  by  the  caller  that  checks  whether  a  required  factory  is  implemented  otherwise  the  discovery  process  is  used  default    optional    factory  get  factory  return    optional  empty    get  an  optional  link    table  factory  instance  that  s  responsible  for  generating  table  related  instances  stored  in  this  catalog  instances  such  as  source  sink  return  an  optional    table  factory  instance  deprecated    use  link  get  factory  for  the  new  factory  stack    the  new  factory  stack  uses  the  new  table  sources  and  sinks  defined  in  flip    and  a  slightly  different  discovery  mechanism    deprecated  default    optional    table  factory  get  table  factory  return    optional  empty    get  an  optional  link    function  definition  factory  instance  that  s  responsible  for  instantiating  function  definitions  return  an  optional    function  definition  factory  instance  default    optional    function  definition  factory  get  function  definition  factory  return    optional  empty    open  the  catalog    used  for  any  required  preparation  in  initialization  phase  throws    catalog  exception  in  case  of  any  runtime  exception  void  open  throws    catalog  exception    close  the  catalog  when  it  is  no  longer  needed  and  release  any  resource  that  it  might  be  holding  throws    catalog  exception  in  case  of  any  runtime  exception  void  close  throws    catalog  exception  databases    get  the  name  of  the  default  database  for  this  catalog    the  default  database  will  be  the  current  database  for  the  catalog  when  user  s  session  doesn  t  specify  a  current  database    the  value  probably  comes  from  configuration  will  not  change  for  the  life  time  of  the  catalog  instance  return  the  name  of  the  current  database  throws    catalog  exception  in  case  of  any  runtime  exception    string  get  default  database  throws    catalog  exception    get  the  names  of  all  databases  in  this  catalog  return  a  list  of  the  names  of  all  databases  throws    catalog  exception  in  case  of  any  runtime  exception    list    string  list  databases  throws    catalog  exception    get  a  database  from  this  catalog  param  database  name    name  of  the  database  return    the  requested  database  throws    database  not  exist  exception  if  the  database  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception    catalog  database  get  database    string  database  name  throws    database  not  exist  exception    catalog  exception    check  if  a  database  exists  in  this  catalog  param  database  name    name  of  the  database  return  true  if  the  given  database  exists  in  the  catalog  false  otherwise  throws    catalog  exception  in  case  of  any  runtime  exception  boolean  database  exists    string  database  name  throws    catalog  exception    create  a  database  param  name    name  of  the  database  to  be  created  param  database    the  database  definition  param  ignore  if  exists    flag  to  specify  behavior  when  a  database  with  the  given  name  already  exists  if  set  to  false  throw  a    database  already  exist  exception  if  set  to  true  do  nothing  throws    database  already  exist  exception  if  the  given  database  already  exists  and  ignore  if  exists  is  false  throws    catalog  exception  in  case  of  any  runtime  exception  void  create  database    string  name    catalog  database  database  boolean  ignore  if  exists  throws    database  already  exist  exception    catalog  exception    drop  a  database  param  name    name  of  the  database  to  be  dropped  param  ignore  if  not  exists    flag  to  specify  behavior  when  the  database  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  do  nothing  throws    database  not  exist  exception  if  the  given  database  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  default  void  drop  database    string  name  boolean  ignore  if  not  exists  throws    database  not  exist  exception    database  not  empty  exception    catalog  exception  drop  database  name  ignore  if  not  exists  false    drop  a  database  param  name    name  of  the  database  to  be  dropped  param  ignore  if  not  exists    flag  to  specify  behavior  when  the  database  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  do  nothing  param  cascade    flag  to  specify  behavior  when  the  database  contains  table  or  function  if  set  to  true  delete  all  tables  and  functions  in  the  database  and  then  delete  the  database  if  set  to  false  throw  an  exception  throws    database  not  exist  exception  if  the  given  database  does  not  exist  throws    database  not  empty  exception  if  the  given  database  is  not  empty  and  is  restrict  is  true  throws    catalog  exception  in  case  of  any  runtime  exception  void  drop  database    string  name  boolean  ignore  if  not  exists  boolean  cascade  throws    database  not  exist  exception    database  not  empty  exception    catalog  exception    modify  an  existing  database  param  name    name  of  the  database  to  be  modified  param  new  database    the  new  database  definition  param  ignore  if  not  exists    flag  to  specify  behavior  when  the  given  database  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  do  nothing  throws    database  not  exist  exception  if  the  given  database  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  alter  database    string  name    catalog  database  new  database  boolean  ignore  if  not  exists  throws    database  not  exist  exception    catalog  exception  tables  and  views    get  names  of  all  tables  and  views  under  this  database    an  empty  list  is  returned  if  none  exists  return  a  list  of  the  names  of  all  tables  and  views  in  this  database  throws    database  not  exist  exception  if  the  database  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception    list    string  list  tables    string  database  name  throws    database  not  exist  exception    catalog  exception    get  names  of  all  views  under  this  database    an  empty  list  is  returned  if  none  exists  param  database  name  the  name  of  the  given  database  return  a  list  of  the  names  of  all  views  in  the  given  database  throws    database  not  exist  exception  if  the  database  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception    list    string  list  views    string  database  name  throws    database  not  exist  exception    catalog  exception    get  a    catalog  table  or    catalog  view  identified  by  table  path  param  table  path    path  of  the  table  or  view  return    the  requested  table  or  view  throws    table  not  exist  exception  if  the  target  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception    catalog  base  table  get  table    object  path  table  path  throws    table  not  exist  exception    catalog  exception    check  if  a  table  or  view  exists  in  this  catalog  param  table  path    path  of  the  table  or  view  return  true  if  the  given  table  exists  in  the  catalog  false  otherwise  throws    catalog  exception  in  case  of  any  runtime  exception  boolean  table  exists    object  path  table  path  throws    catalog  exception    drop  a  table  or  view  param  table  path    path  of  the  table  or  view  to  be  dropped  param  ignore  if  not  exists    flag  to  specify  behavior  when  the  table  or  view  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  do  nothing  throws    table  not  exist  exception  if  the  table  or  view  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  drop  table    object  path  table  path  boolean  ignore  if  not  exists  throws    table  not  exist  exception    catalog  exception    rename  an  existing  table  or  view  param  table  path    path  of  the  table  or  view  to  be  renamed  param  new  table  name  the  new  name  of  the  table  or  view  param  ignore  if  not  exists    flag  to  specify  behavior  when  the  table  or  view  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  do  nothing  throws    table  not  exist  exception  if  the  table  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  rename  table    object  path  table  path    string  new  table  name  boolean  ignore  if  not  exists  throws    table  not  exist  exception    table  already  exist  exception    catalog  exception    create  a  new  table  or  view  param  table  path  path  of  the  table  or  view  to  be  created  param  table  the  table  definition  param  ignore  if  exists  flag  to  specify  behavior  when  a  table  or  view  already  exists  at  the  given  path  if  set  to  false  it  throws  a    table  already  exist  exception  if  set  to  true  do  nothing  throws    table  already  exist  exception  if  table  already  exists  and  ignore  if  exists  is  false  throws    database  not  exist  exception  if  the  database  in  table  path  doesn  t  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  create  table    object  path  table  path    catalog  base  table  table  boolean  ignore  if  exists  throws    table  already  exist  exception    database  not  exist  exception    catalog  exception    modify  an  existing  table  or  view    note  that  the  new  and  old    catalog  base  table  must  be  of  the  same  type    for  example  this  doesn  t  allow  alter  a  regular  table  to  partitioned  table  or  alter  a  view  to  a  table  and  vice  versa  param  table  path  path  of  the  table  or  view  to  be  modified  param  new  table  the  new  table  definition  param  ignore  if  not  exists  flag  to  specify  behavior  when  the  table  or  view  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  do  nothing  throws    table  not  exist  exception  if  the  table  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  alter  table    object  path  table  path    catalog  base  table  new  table  boolean  ignore  if  not  exists  throws    table  not  exist  exception    catalog  exception  partitions    get    catalog  partition  spec  of  all  partitions  of  the  table  param  table  path  path  of  the  table  return  a  list  of    catalog  partition  spec  of  the  table  throws    table  not  exist  exception  thrown  if  the  table  does  not  exist  in  the  catalog  throws    table  not  partitioned  exception  thrown  if  the  table  is  not  partitioned  throws    catalog  exception  in  case  of  any  runtime  exception    list    catalog  partition  spec  list  partitions    object  path  table  path  throws    table  not  exist  exception    table  not  partitioned  exception    catalog  exception    get    catalog  partition  spec  of  all  partitions  that  is  under  the  given    catalog  partition  spec  in  the  table  param  table  path  path  of  the  table  param  partition  spec  the  partition  spec  to  list  return  a  list  of    catalog  partition  spec  that  is  under  the  given    catalog  partition  spec  in  the  table  throws    table  not  exist  exception  thrown  if  the  table  does  not  exist  in  the  catalog  throws    table  not  partitioned  exception  thrown  if  the  table  is  not  partitioned  throws    catalog  exception  in  case  of  any  runtime  exception    list    catalog  partition  spec  list  partitions    object  path  table  path    catalog  partition  spec  partition  spec  throws    table  not  exist  exception    table  not  partitioned  exception    catalog  exception    get    catalog  partition  spec  of  partitions  by  expression  filters  in  the  table  p  note    for    field  reference  expression  the  field  index  is  based  on  schema  of  this  table  instead  of  partition  columns  only  p    the  passed  in  predicates  have  been  translated  in  conjunctive  form  p    if  catalog  does  not  support  this  interface  at  present  throw  an  link    unsupported  operation  exception  directly    if  the  catalog  does  not  have  a  valid  filter  throw  the  link    unsupported  operation  exception  directly    planner  will  fallback  to  get  all  partitions  and  filter  by  itself  param  table  path  path  of  the  table  param  filters  filters  to  push  down  filter  to  catalog  return  a  list  of    catalog  partition  spec  that  is  under  the  given    catalog  partition  spec  in  the  table  throws    table  not  exist  exception  thrown  if  the  table  does  not  exist  in  the  catalog  throws    table  not  partitioned  exception  thrown  if  the  table  is  not  partitioned  throws    catalog  exception  in  case  of  any  runtime  exception    list    catalog  partition  spec  list  partitions  by  filter    object  path  table  path    list    expression  filters  throws    table  not  exist  exception    table  not  partitioned  exception    catalog  exception    get  a  partition  of  the  given  table    the  given  partition  spec  keys  and  values  need  to  be  matched  exactly  for  a  result  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  partition  to  get  return  the  requested  partition  throws    partition  not  exist  exception  thrown  if  the  partition  doesn  t  exist  throws    catalog  exception  in  case  of  any  runtime  exception    catalog  partition  get  partition    object  path  table  path    catalog  partition  spec  partition  spec  throws    partition  not  exist  exception    catalog  exception    check  whether  a  partition  exists  or  not  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  the  partition  to  check  throws    catalog  exception  in  case  of  any  runtime  exception  boolean  partition  exists    object  path  table  path    catalog  partition  spec  partition  spec  throws    catalog  exception    create  a  partition  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  the  partition  param  partition  the  partition  to  add  param  ignore  if  exists  flag  to  specify  behavior  if  a  table  with  the  given  name  already  exists  if  set  to  false  it  throws  a    table  already  exist  exception  if  set  to  true  nothing  happens  throws    table  not  exist  exception  thrown  if  the  target  table  does  not  exist  throws    table  not  partitioned  exception  thrown  if  the  target  table  is  not  partitioned  throws    partition  spec  invalid  exception  thrown  if  the  given  partition  spec  is  invalid  throws    partition  already  exists  exception  thrown  if  the  target  partition  already  exists  throws    catalog  exception  in  case  of  any  runtime  exception  void  create  partition    object  path  table  path    catalog  partition  spec  partition  spec    catalog  partition  partition  boolean  ignore  if  exists  throws    table  not  exist  exception    table  not  partitioned  exception    partition  spec  invalid  exception    partition  already  exists  exception    catalog  exception    drop  a  partition  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  the  partition  to  drop  param  ignore  if  not  exists  flag  to  specify  behavior  if  the  database  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  nothing  happens  throws    partition  not  exist  exception  thrown  if  the  target  partition  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  drop  partition    object  path  table  path    catalog  partition  spec  partition  spec  boolean  ignore  if  not  exists  throws    partition  not  exist  exception    catalog  exception    alter  a  partition  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  the  partition  param  new  partition  new  partition  to  replace  the  old  one  param  ignore  if  not  exists  flag  to  specify  behavior  if  the  database  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  nothing  happens  throws    partition  not  exist  exception  thrown  if  the  target  partition  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  alter  partition    object  path  table  path    catalog  partition  spec  partition  spec    catalog  partition  new  partition  boolean  ignore  if  not  exists  throws    partition  not  exist  exception    catalog  exception  functions    list  the  names  of  all  functions  in  the  given  database    an  empty  list  is  returned  if  none  is  registered  param  db  name  name  of  the  database  return  a  list  of  the  names  of  the  functions  in  this  database  throws    database  not  exist  exception  if  the  database  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception    list    string  list  functions    string  db  name  throws    database  not  exist  exception    catalog  exception    get  the  function    function  name  should  be  handled  in  a  case  insensitive  way  param  function  path  path  of  the  function  return  the  requested  function  throws    function  not  exist  exception  if  the  function  does  not  exist  in  the  catalog  throws    catalog  exception  in  case  of  any  runtime  exception    catalog  function  get  function    object  path  function  path  throws    function  not  exist  exception    catalog  exception    check  whether  a  function  exists  or  not    function  name  should  be  handled  in  a  case  insensitive  way  param  function  path  path  of  the  function  return  true  if  the  function  exists  in  the  catalog  false  otherwise  throws    catalog  exception  in  case  of  any  runtime  exception  boolean  function  exists    object  path  function  path  throws    catalog  exception    create  a  function    function  name  should  be  handled  in  a  case  insensitive  way  param  function  path  path  of  the  function  param  function  the  function  to  be  created  param  ignore  if  exists  flag  to  specify  behavior  if  a  function  with  the  given  name  already  exists  if  set  to  false  it  throws  a    function  already  exist  exception  if  set  to  true  nothing  happens  throws    function  already  exist  exception  if  the  function  already  exist  throws    database  not  exist  exception  if  the  given  database  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  create  function    object  path  function  path    catalog  function  function  boolean  ignore  if  exists  throws    function  already  exist  exception    database  not  exist  exception    catalog  exception    modify  an  existing  function    function  name  should  be  handled  in  a  case  insensitive  way  param  function  path  path  of  the  function  param  new  function  the  function  to  be  modified  param  ignore  if  not  exists  flag  to  specify  behavior  if  the  function  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  nothing  happens  throws    function  not  exist  exception  if  the  function  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  alter  function    object  path  function  path    catalog  function  new  function  boolean  ignore  if  not  exists  throws    function  not  exist  exception    catalog  exception    drop  a  function    function  name  should  be  handled  in  a  case  insensitive  way  param  function  path  path  of  the  function  to  be  dropped  param  ignore  if  not  exists  plag  to  specify  behavior  if  the  function  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  nothing  happens  throws    function  not  exist  exception  if  the  function  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  drop  function    object  path  function  path  boolean  ignore  if  not  exists  throws    function  not  exist  exception    catalog  exception  statistics    get  the  statistics  of  a  table  param  table  path  path  of  the  table  return  statistics  of  the  given  table  throws    table  not  exist  exception  if  the  table  does  not  exist  in  the  catalog  throws    catalog  exception  in  case  of  any  runtime  exception    catalog  table  statistics  get  table  statistics    object  path  table  path  throws    table  not  exist  exception    catalog  exception    get  the  column  statistics  of  a  table  param  table  path  path  of  the  table  return  column  statistics  of  the  given  table  throws    table  not  exist  exception  if  the  table  does  not  exist  in  the  catalog  throws    catalog  exception  in  case  of  any  runtime  exception    catalog  column  statistics  get  table  column  statistics    object  path  table  path  throws    table  not  exist  exception    catalog  exception    get  the  statistics  of  a  partition  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  the  partition  return  statistics  of  the  given  partition  throws    partition  not  exist  exception  if  the  partition  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception    catalog  table  statistics  get  partition  statistics    object  path  table  path    catalog  partition  spec  partition  spec  throws    partition  not  exist  exception    catalog  exception    get  the  column  statistics  of  a  partition  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  the  partition  return  column  statistics  of  the  given  partition  throws    partition  not  exist  exception  if  the  partition  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception    catalog  column  statistics  get  partition  column  statistics    object  path  table  path    catalog  partition  spec  partition  spec  throws    partition  not  exist  exception    catalog  exception    update  the  statistics  of  a  table  param  table  path  path  of  the  table  param  table  statistics  new  statistics  to  update  param  ignore  if  not  exists  flag  to  specify  behavior  if  the  table  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  nothing  happens  throws    table  not  exist  exception  if  the  table  does  not  exist  in  the  catalog  throws    catalog  exception  in  case  of  any  runtime  exception  void  alter  table  statistics    object  path  table  path    catalog  table  statistics  table  statistics  boolean  ignore  if  not  exists  throws    table  not  exist  exception    catalog  exception    update  the  column  statistics  of  a  table  param  table  path  path  of  the  table  param  column  statistics  new  column  statistics  to  update  param  ignore  if  not  exists  flag  to  specify  behavior  if  the  table  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  nothing  happens  throws    table  not  exist  exception  if  the  table  does  not  exist  in  the  catalog  throws    catalog  exception  in  case  of  any  runtime  exception  void  alter  table  column  statistics    object  path  table  path    catalog  column  statistics  column  statistics  boolean  ignore  if  not  exists  throws    table  not  exist  exception    catalog  exception    table  partitioned  exception    update  the  statistics  of  a  table  partition  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  the  partition  param  partition  statistics  new  statistics  to  update  param  ignore  if  not  exists  flag  to  specify  behavior  if  the  partition  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  nothing  happens  throws    partition  not  exist  exception  if  the  partition  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  alter  partition  statistics    object  path  table  path    catalog  partition  spec  partition  spec    catalog  table  statistics  partition  statistics  boolean  ignore  if  not  exists  throws    partition  not  exist  exception    catalog  exception    update  the  column  statistics  of  a  table  partition  param  table  path  path  of  the  table  param  partition  spec  partition  spec  of  the  partition  param  column  statistics  new  column  statistics  to  update  param  ignore  if  not  exists  flag  to  specify  behavior  if  the  partition  does  not  exist  if  set  to  false  throw  an  exception  if  set  to  true  nothing  happens  throws    partition  not  exist  exception  if  the  partition  does  not  exist  throws    catalog  exception  in  case  of  any  runtime  exception  void  alter  partition  column  statistics    object  path  table  path    catalog  partition  spec  partition  spec    catalog  column  statistics  column  statistics  boolean  ignore  if  not  exists  throws    partition  not  exist  exception    catalog  exception  
public  evolving  public  interface    data  type  factory    creates  a  type  out  of  an  link    abstract  data  type  p    if  the  given  type  is  already  a  link    data  type  the  factory  will  return  it  unmodified    in  case  of  link    unresolved  data  type  the  factory  will  resolve  it  to  a  link    data  type    data  type  create  data  type    abstract  data  type  abstract  data  type    creates  a  type  by  a  fully  or  partially  defined  name  p    the  factory  will  parse  and  resolve  the  name  of  a  type  to  a  link    data  type    this  includes  both  built  in  types  as  well  as  user  defined  types  see  link    distinct  type  and  link    structured  type    data  type  create  data  type    string  name    creates  a  type  by  a  fully  or  partially  defined  identifier  p    the  factory  will  parse  and  resolve  the  name  of  a  type  to  a  link    data  type    this  includes  both  built  in  types  as  well  as  user  defined  types  see  link    distinct  type  and  link    structured  type    data  type  create  data  type    unresolved  identifier  identifier    creates  a  type  by  analyzing  the  given  class  p    it  does  this  by  using    java  reflection  which  can  be  supported  by  link    data  type  hint  annotations  for  nested  structured  types  p    it  will  throw  an  link    validation  exception  in  cases  where  the  reflective  extraction  needs  more  information  or  simply  fails  p    see  link    data  types  of    class  for  further  examples  t    data  type  create  data  type    class  t  clazz    creates  a  raw  type  for  the  given  class  in  cases  where  no  serializer  is  known  and  a  generic  serializer  should  be  used    the  factory  will  create  link    data  types  raw    class    type  serializer  with    flink  s  default  raw  serializer  that  is  automatically  configured  p    note    this  type  is  a  black  box  within  the  table  ecosystem  and  is  only  deserialized  at  the  edges  of  the  api  t    data  type  create  raw  data  type    class  t  clazz  
public  evolving  public  enum    function  language  java  scala  python  
public  evolving  public  final  class    changelog  mode  private  static  final    changelog  mode  insert  only    changelog  mode  new  builder  add  contained  kind    row  kind  insert  build  private  final    set    row  kind  kinds  private    changelog  mode    set    row  kind  kinds    preconditions  check  argument  kinds  size      at  least  one  kind  of  row  should  be  contained  in  a  changelog  this  kinds    collections  unmodifiable  set  kinds    shortcut  for  a  simple  link    row  kind  insert  only  changelog  public  static    changelog  mode  insert  only  return  insert  only    builder  for  configuring  and  creating  instances  of  link    changelog  mode  public  static    builder  new  builder  return  new    builder  public    set    row  kind  get  contained  kinds  return  kinds  public  boolean  contains    row  kind  kind  return  kinds  contains  kind  public  boolean  contains  only    row  kind  kind  return  kinds  size    kinds  contains  kind    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    changelog  mode  that    changelog  mode  o  return  kinds  equals  that  kinds    override  public  int  hash  code  return    objects  hash  kinds    builder  for  configuring  and  creating  instances  of  link    changelog  mode  public  static  class    builder  private  final    set    row  kind  kinds    enum  set  none  of    row  kind  class  private    builder  default  constructor  to  allow  a  fluent  definition  public    builder  add  contained  kind    row  kind  kind  this  kinds  add  kind  return  this  public    changelog  mode  build  return  new    changelog  mode  kinds  
public  evolving  public  interface    decoding  format  i  extends    format    creates  runtime  decoder  implementation  that  is  configured  to  produce  data  of  the  given  data  type  i  create  runtime  decoder    dynamic  table  source    context  context    data  type  produced  data  type  
public  evolving  public  interface    encoding  format  i  extends    format    creates  runtime  encoder  implementation  that  is  configured  to  consume  data  of  the  given  data  type  i  create  runtime  encoder    dynamic  table  sink    context  context    data  type  consumed  data  type  
public  evolving  public  interface    format    returns  the  set  of  changes  that  a  connector  and  transitively  the  planner  can  expect  during  runtime    changelog  mode  get  changelog  mode  
public  evolving  public  interface    runtime  converter  extends    serializable    initializes  the  converter  during  runtime  p    this  should  be  called  in  the  code  open  method  of  a  runtime  class  void  open    context  context    context  for  conversions  during  runtime  interface    context    runtime  classloader  for  loading  user  defined  classes    class  loader  get  class  loader    creates  a  new  instance  of  link    context  param  class  loader  runtime  classloader  for  loading  user  defined  classes  static    context  create    class  loader  class  loader  return  new    context    override  public    class  loader  get  class  loader  return  class  loader  
public  evolving  public  interface    supports  overwrite    provides  whether  existing  data  should  be  overwritten  or  not  void  apply  overwrite  boolean  overwrite  
public  evolving  public  interface    supports  partitioning    provides  the  static  part  of  a  partition  p  a  single  partition  maps  each  partition  key  to  a  partition  value    depending  on  the  user  defined  statement  the  partition  might  not  include  all  partition  keys  p    see  the  documentation  of  link    supports  partitioning  for  more  information  param  partition  user  defined  possibly  partial  static  partition  void  apply  static  partition    map    string    string  partition    returns  whether  data  needs  to  be  grouped  by  partition  before  it  is  consumed  by  the  sink    by  default  this  is  not  required  from  the  runtime  and  records  arrive  in  arbitrary  partition  order  p    if  this  method  returns  true  the  sink  can  expect  that  all  records  will  be  grouped  by  the  partition  keys  before  consumed  by  the  sink    in  other  words    the  sink  will  receive  all  elements  of  one  partition  and  then  all  elements  of  another  partition    elements  of  different  partitions  will  not  be  mixed    for  some  sinks  this  can  be  used  to  reduce  the  number  of  partition  writers  and  improve  writing  performance  by  writing  one  partition  at  a  time  p    the  given  argument  indicates  whether  the  current  execution  mode  supports  grouping  or  not    for  example  depending  on  the  execution  mode  a  sorting  operation  might  not  be  available  during  runtime  param  supports  grouping  whether  the  current  execution  mode  supports  grouping  return  whether  data  need  to  be  grouped  by  partition  before  consumed  by  the  sink    if  code  supports  grouping  is  false  it  should  never  return  true  otherwise  the  planner  will  fail    suppress  warnings  unused  default  boolean  requires  partition  grouping  boolean  supports  grouping  return  false  
public  evolving  public  interface    dynamic  table  sink    returns  the  set  of  changes  that  the  sink  accepts  during  runtime  p    the  planner  can  make  suggestions  but  the  sink  has  the  final  decision  what  it  requires    if  the  planner  does  not  support  this  mode  it  will  throw  an  error    for  example  the  sink  can  return  that  it  only  supports  link    changelog  mode  insert  only  param  requested  mode  expected  set  of  changes  by  the  current  plan    changelog  mode  get  changelog  mode    changelog  mode  requested  mode    returns  a  provider  of  runtime  implementation  for  writing  the  data  p    there  might  exist  different  interfaces  for  runtime  implementation  which  is  why  link    sink  runtime  provider  serves  as  the  base  interface    concrete  link    sink  runtime  provider  interfaces  might  be  located  in  other    flink  modules  p    independent  of  the  provider  interface  the  table  runtime  expects  that  a  sink  implementation  accepts  internal  data  structures  see  link  org  apache  flink  table  data    row  data  for  more  information  p    the  given  link    context  offers  utilities  by  the  planner  for  creating  runtime  implementation  with  minimal  dependencies  to  internal  data  structures  p    see  code  org  apache  flink  table  connector  sink    sink  function  provider  in  code  flink  table  api  java  bridge    sink  runtime  provider  get  sink  runtime  provider    context  context    creates  a  copy  of  this  instance  during  planning    the  copy  should  be  a  deep  copy  of  all  mutable  members    dynamic  table  sink  copy    returns  a  string  that  summarizes  this  sink  for  printing  to  a  console  or  log    string  as  summary  string    helper  interfaces    context  for  creating  runtime  implementation  via  a  link    sink  runtime  provider  p    it  offers  utilities  by  the  planner  for  creating  runtime  implementation  with  minimal  dependencies  to  internal  data  structures  p    methods  should  be  called  in  link  get  sink  runtime  provider    context    the  returned  instances  are  link    serializable  and  can  be  directly  passed  into  the  runtime  implementation  class  interface    context    returns  whether  a  runtime  implementation  can  expect  a  finite  number  of  rows  p    this  information  might  be  derived  from  the  session  s  execution  mode  and  or  kind  of  query  boolean  is  bounded    creates  type  information  describing  the  internal  data  structures  of  the  given  link    data  type  see    table  schema  to  physical  row  data  type    type  information  create  type  information    data  type  consumed  data  type    creates  a  converter  for  mapping  between    flink  s  internal  data  structures  and  objects  specified  by  the  given  link    data  type  that  can  be  passed  into  a  runtime  implementation  p    for  example  link    row  data  and  its  fields  can  be  converted  into  a  link    row  or  the  internal  representation  for  structured  types  can  be  converted  back  into  the  original  possibly  nested  pojo  see    logical  type  supports  output  conversion    class    data  structure  converter  create  data  structure  converter    data  type  consumed  data  type    converter  for  mapping  between    flink  s  internal  data  structures  and  objects  specified  by  the  given  link    data  type  that  can  be  passed  into  a  runtime  implementation  p    for  example  link    row  data  and  its  fields  can  be  converted  into  a  link    row  or  the  internal  representation  for  structured  types  can  be  converted  back  into  the  original  possibly  nested  pojo  see    logical  type  supports  output  conversion    class  interface    data  structure  converter  extends    runtime  converter    converts  the  given  internal  structure  into  an  external  object    nullable    object  to  external    nullable    object  internal  structure    provides  actual  runtime  implementation  for  writing  the  data  p    there  might  exist  different  interfaces  for  runtime  implementation  which  is  why  link    sink  runtime  provider  serves  as  the  base  interface    concrete  link    sink  runtime  provider  interfaces  might  be  located  in  other    flink  modules  p    see  code  org  apache  flink  table  connector  sink    sink  function  provider  in  code  flink  table  api  java  bridge  interface    sink  runtime  provider  marker  interface  
public  evolving  public  interface    output  format  provider  extends    dynamic  table  sink    sink  runtime  provider    helper  method  for  creating  a  static  provider  static    output  format  provider  of    output  format    row  data  output  format  return  output  format    creates  an  link    output  format  instance    output  format    row  data  create  output  format  
public  evolving  public  interface    supports  computed  column  push  down    provides  a  converter  that  converts  the  produced  link    row  data  containing  the  physical  fields  of  the  external  system  into  a  new  link    row  data  with  push  downed  computed  columns  p    note    use  the  passed  data  type  instead  of  link    table  schema  to  physical  row  data  type  for  describing  the  final  output  data  type  when  creating  link    type  information    if  the  source  implements  link    supports  projection  push  down  the  projection  is  already  considered  in  both  the  converter  and  the  given  output  data  type  void  apply  computed  column    computed  column  converter  converter    data  type  output  data  type    generates  and  adds  computed  columns  to  link    row  data  if  necessary  p    instances  of  this  interface  are  link    serializable  and  can  be  directly  passed  into  the  runtime  implementation  class  interface    computed  column  converter  extends    runtime  converter    generates  and  adds  computed  columns  to  link    row  data  if  necessary  param  produced  row  physical  row  read  from  the  external  system  return  row  enriched  with  computed  columns    row  data  convert    row  data  produced  row  
public  evolving  public  interface    supports  filter  push  down    provides  a  list  of  filters  in  conjunctive  form  a  source  can  pick  filters  and  return  the  accepted  and  remaining  filters  p    see  the  documentation  of  link    supports  filter  push  down  for  more  information    result  apply  filters    list    resolved  expression  filters    result  of  a  filter  push  down    it  represents  the  communication  of  the  source  to  the  planner  during  optimization  final  class    result  private  final    list    resolved  expression  accepted  filters  private  final    list    resolved  expression  remaining  filters  private    result    list    resolved  expression  accepted  filters    list    resolved  expression  remaining  filters  this  accepted  filters  accepted  filters  this  remaining  filters  remaining  filters    constructs  a  filter  push  down  result  p    see  the  documentation  of  link    supports  filter  push  down  for  more  information  param  accepted  filters  filters  that  are  consumed  by  the  source  but  may  be  applied  on  a  best  effort  basis  param  remaining  filters  filters  that  a  subsequent  filter  operation  still  needs  to  perform  during  runtime  public  static    result  of    list    resolved  expression  accepted  filters    list    resolved  expression  remaining  filters  return  new    result  accepted  filters  remaining  filters  public    list    resolved  expression  get  accepted  filters  return  accepted  filters  public    list    resolved  expression  get  remaining  filters  return  remaining  filters  
public  evolving  public  interface    supports  limit  push  down    provides  the  expected  maximum  number  of  produced  records  for  limiting  on  a  best  effort  basis  void  apply  limit  long  limit  
public  evolving  public  interface    supports  partition  push  down    returns  a  list  of  all  partitions  that  a  source  can  read  if  available  p  a  single  partition  maps  each  partition  key  to  a  partition  value  p    if  link    optional  empty  is  returned  the  list  of  partitions  is  queried  from  the  catalog    optional    list    map    string    string  list  partitions    provides  a  list  of  remaining  partitions    after  those  partitions  are  applied  a  source  must  not  read  the  data  of  other  partitions  during  runtime  p    see  the  documentation  of  link    supports  partition  push  down  for  more  information  void  apply  partitions    list    map    string    string  remaining  partitions  
public  evolving  public  interface    supports  projection  push  down    returns  whether  this  source  supports  nested  projection  boolean  supports  nested  projection    provides  the  field  index  paths  that  should  be  used  for  a  projection    the  indices  are    based  and  support  fields  within  possibly  nested  structures  if  this  is  enabled  via  link  supports  nested  projection  p    in  the  example  mentioned  in  link    supports  projection  push  down  this  method  would  receive  ul  li  code      which  is  equivalent  to  code  s  r  if  link  supports  nested  projection  returns  false  li  code        which  is  equivalent  to  code  s  r  d  if  link  supports  nested  projection  returns  true  ul  param  projected  fields  field  index  paths  of  all  fields  that  must  be  present  in  the  physically  produced  data  void  apply  projection  int  projected  fields  
public  evolving  public  interface    supports  watermark  push  down    provides  actual  runtime  implementation  for  generating  watermarks  p    there  exist  different  interfaces  for  runtime  implementation  which  is  why  link    watermark  provider  serves  as  the  base  interface    concrete  link    watermark  provider  interfaces  might  be  located  in  other    flink  modules  p    see  code  org  apache  flink  table  connector  source  abilities  in  code  flink  table  api  java  bridge  p    implementations  need  to  perform  an  code  instanceof  check  and  fail  with  an  exception  if  the  given  link    watermark  provider  is  unsupported  void  apply  watermark    watermark  provider  provider    provides  actual  runtime  implementation  for  generating  watermarks  p    there  exist  different  interfaces  for  runtime  implementation  which  is  why  link    watermark  provider  serves  as  the  base  interface  interface    watermark  provider  marker  interface  that  will  be  filled  after  flip      watermark  generator    row  data  get  watermark  generator  
public  evolving  public  interface    async  table  function  provider  t  extends    lookup  table  source    lookup  runtime  provider    helper  method  for  creating  a  static  provider  static  t    async  table  function  provider  t  of    async  table  function  t  async  table  function  return  async  table  function    creates  a  link    async  table  function  instance    async  table  function  t  create  async  table  function  
public  evolving  public  interface    dynamic  table  source    creates  a  copy  of  this  instance  during  planning    the  copy  should  be  a  deep  copy  of  all  mutable  members    dynamic  table  source  copy    returns  a  string  that  summarizes  this  source  for  printing  to  a  console  or  log    string  as  summary  string    helper  interfaces    base  context  for  creating  runtime  implementation  via  a  link    scan  table  source    scan  runtime  provider  and  link    lookup  table  source    lookup  runtime  provider  p    it  offers  utilities  by  the  planner  for  creating  runtime  implementation  with  minimal  dependencies  to  internal  data  structures  p    methods  should  be  called  in  link    scan  table  source  get  scan  runtime  provider    scan  table  source    scan  context  and  link    lookup  table  source  get  lookup  runtime  provider    lookup  table  source    lookup  context    the  returned  instances  are  link    serializable  and  can  be  directly  passed  into  the  runtime  implementation  class  interface    context    creates  type  information  describing  the  internal  data  structures  of  the  given  link    data  type  see    table  schema  to  physical  row  data  type    type  information  create  type  information    data  type  produced  data  type    creates  a  converter  for  mapping  between  objects  specified  by  the  given  link    data  type  and    flink  s  internal  data  structures  that  can  be  passed  into  a  runtime  implementation  p    for  example  a  link    row  and  its  fields  can  be  converted  into  link    row  data  or  a  possibly  nested  pojo  can  be  converted  into  the  internal  representation  for  structured  types  see    logical  type  supports  input  conversion    class  see    table  schema  to  physical  row  data  type    data  structure  converter  create  data  structure  converter    data  type  produced  data  type    converter  for  mapping  between  objects  and    flink  s  internal  data  structures  during  runtime  p    on  request  the  planner  will  provide  a  specialized  possibly  code  generated  converter  that  can  be  passed  into  a  runtime  implementation  p    for  example  a  link    row  and  its  fields  can  be  converted  into  link    row  data  or  a  possibly  nested  pojo  can  be  converted  into  the  internal  representation  for  structured  types  see    logical  type  supports  input  conversion    class  interface    data  structure  converter  extends    runtime  converter    converts  the  given  object  into  an  internal  data  structure    nullable    object  to  internal    nullable    object  external  structure  
public  evolving  public  interface    input  format  provider  extends    scan  table  source    scan  runtime  provider    helper  method  for  creating  a  static  provider  static    input  format  provider  of    input  format    row  data  input  format  return  new    input  format  provider    override  public    input  format    row  data  create  input  format  return  input  format    override  public  boolean  is  bounded  return  true    creates  an  link    input  format  instance    input  format    row  data  create  input  format  
public  evolving  public  interface    scan  table  source  extends    dynamic  table  source    returns  the  set  of  changes  that  the  planner  can  expect  during  runtime  see    row  kind    changelog  mode  get  changelog  mode    returns  a  provider  of  runtime  implementation  for  reading  the  data  p    there  might  exist  different  interfaces  for  runtime  implementation  which  is  why  link    scan  runtime  provider  serves  as  the  base  interface    concrete  link    scan  runtime  provider  interfaces  might  be  located  in  other    flink  modules  p    independent  of  the  provider  interface  the  table  runtime  expects  that  a  source  implementation  emits  internal  data  structures  see  link  org  apache  flink  table  data    row  data  for  more  information  p    the  given  link    scan  context  offers  utilities  by  the  planner  for  creating  runtime  implementation  with  minimal  dependencies  to  internal  data  structures  p    see  code  org  apache  flink  table  connector  source    source  function  provider  in  code  flink  table  api  java  bridge    scan  runtime  provider  get  scan  runtime  provider    scan  context  runtime  provider  context    helper  interfaces    context  for  creating  runtime  implementation  via  a  link    scan  runtime  provider  p    it  offers  utilities  by  the  planner  for  creating  runtime  implementation  with  minimal  dependencies  to  internal  data  structures  p    methods  should  be  called  in  link  get  scan  runtime  provider    scan  context    the  returned  instances  are  link    serializable  and  can  be  directly  passed  into  the  runtime  implementation  class  interface    scan  context  extends    dynamic  table  source    context  may  introduce  scan  specific  methods  in  the  future    provides  actual  runtime  implementation  for  reading  the  data  p    there  might  exist  different  interfaces  for  runtime  implementation  which  is  why  link    scan  runtime  provider  serves  as  the  base  interface    concrete  link    scan  runtime  provider  interfaces  might  be  located  in  other    flink  modules  p    see  code  org  apache  flink  table  connector  source    source  function  provider  in  code  flink  table  api  java  bridge  interface    scan  runtime  provider    returns  whether  the  data  is  bounded  or  not  boolean  is  bounded  
public  evolving  public  interface    table  function  provider  t  extends    lookup  table  source    lookup  runtime  provider    helper  method  for  creating  a  static  provider  static  t    table  function  provider  t  of    table  function  t  table  function  return  table  function    creates  a  link    table  function  instance    table  function  t  create  table  function  
public  evolving  public  interface    array  data    returns  the  number  of  elements  in  this  array  int  size    read  only  accessor  methods    returns  true  if  the  element  is  null  at  the  given  position  boolean  is  null  at  int  pos    returns  the  boolean  value  at  the  given  position  boolean  get  boolean  int  pos    returns  the  byte  value  at  the  given  position  byte  get  byte  int  pos    returns  the  short  value  at  the  given  position  short  get  short  int  pos    returns  the  integer  value  at  the  given  position  int  get  int  int  pos    returns  the  long  value  at  the  given  position  long  get  long  int  pos    returns  the  float  value  at  the  given  position  float  get  float  int  pos    returns  the  double  value  at  the  given  position  double  get  double  int  pos    returns  the  string  value  at  the  given  position    string  data  get  string  int  pos    returns  the  decimal  value  at  the  given  position  p    the  precision  and  scale  are  required  to  determine  whether  the  decimal  value  was  stored  in  a  compact  representation  see  link    decimal  data    decimal  data  get  decimal  int  pos  int  precision  int  scale    returns  the  timestamp  value  at  the  given  position  p    the  precision  is  required  to  determine  whether  the  timestamp  value  was  stored  in  a  compact  representation  see  link    timestamp  data    timestamp  data  get  timestamp  int  pos  int  precision    returns  the  raw  value  at  the  given  position  t    raw  value  data  t  get  raw  value  int  pos    returns  the  binary  value  at  the  given  position  byte  get  binary  int  pos    returns  the  array  value  at  the  given  position    array  data  get  array  int  pos    returns  the  map  value  at  the  given  position    map  data  get  map  int  pos    returns  the  row  value  at  the  given  position  p    the  number  of  fields  is  required  to  correctly  extract  the  row    row  data  get  row  int  pos  int  num  fields    conversion    utilities  boolean  to  boolean  array  byte  to  byte  array  short  to  short  array  int  to  int  array  long  to  long  array  float  to  float  array  double  to  double  array    access    utilities    returns  the  element  object  in  the  internal  array  data  structure  at  the  given  position  param  array  the  internal  array  data  param  pos  position  of  the  element  to  return  param  element  type  the  element  type  of  the  array  return  the  element  object  at  the  specified  position  in  this  array  data  deprecated    use  link  create  element  getter    logical  type  for  avoiding  logical  types  during  runtime    deprecated  static    object  get    array  data  array  int  pos    logical  type  element  type  if  array  is  null  at  pos  return  null  switch  element  type  get  type  root  case  boolean  return  array  get  boolean  pos  case  tinyint  return  array  get  byte  pos  case  smallint  return  array  get  short  pos  case  integer  case  date  case  time  without  time  zone  case  interval  year  month  return  array  get  int  pos  case  bigint  case  interval  day  time  return  array  get  long  pos  case  timestamp  without  time  zone    timestamp  type  timestamp  type    timestamp  type  element  type  return  array  get  timestamp  pos  timestamp  type  get  precision  case  timestamp  with  local  time  zone    local  zoned  timestamp  type  lz  ts    local  zoned  timestamp  type  element  type  return  array  get  timestamp  pos  lz  ts  get  precision  case  float  return  array  get  float  pos  case  double  return  array  get  double  pos  case  char  case  varchar  return  array  get  string  pos  case  decimal    decimal  type  decimal  type    decimal  type  element  type  return  array  get  decimal  pos  decimal  type  get  precision  decimal  type  get  scale  case  array  return  array  get  array  pos  case  map  case  multiset  return  array  get  map  pos  case  row  return  array  get  row  pos    row  type  element  type  get  field  count  case  structured  type  not  the  most  efficient  code  but  ok  for  a  deprecated  method  return  array  get  row  pos  get  field  count  element  type  case  binary  case  varbinary  return  array  get  binary  pos  case  raw  return  array  get  raw  value  pos  default  throw  new    unsupported  operation  exception    unsupported  type  element  type    creates  an  accessor  for  getting  elements  in  an  internal  array  data  structure  at  the  given  position  param  element  type  the  element  type  of  the  array  static    element  getter  create  element  getter    logical  type  element  type  final    element  getter  element  getter  ordered  by  type  root  definition  switch  element  type  get  type  root  case  char  case  varchar  element  getter    array  data  get  string  break  case  boolean  element  getter    array  data  get  boolean  break  case  binary  case  varbinary  element  getter    array  data  get  binary  break  case  decimal  final  int  decimal  precision  get  precision  element  type  final  int  decimal  scale  get  scale  element  type  element  getter  array  pos  array  get  decimal  pos  decimal  precision  decimal  scale  break  case  tinyint  element  getter    array  data  get  byte  break  case  smallint  element  getter    array  data  get  short  break  case  integer  case  date  case  time  without  time  zone  case  interval  year  month  element  getter    array  data  get  int  break  case  bigint  case  interval  day  time  element  getter    array  data  get  long  break  case  float  element  getter    array  data  get  float  break  case  double  element  getter    array  data  get  double  break  case  timestamp  without  time  zone  case  timestamp  with  local  time  zone  final  int  timestamp  precision  get  precision  element  type  element  getter  array  pos  array  get  timestamp  pos  timestamp  precision  break  case  timestamp  with  time  zone  throw  new    unsupported  operation  exception  case  array  element  getter    array  data  get  array  break  case  multiset  case  map  element  getter    array  data  get  map  break  case  row  case  structured  type  final  int  row  field  count  get  field  count  element  type  element  getter  array  pos  array  get  row  pos  row  field  count  break  case  distinct  type  element  getter  create  element  getter    distinct  type  element  type  get  source  type  break  case  raw  element  getter    array  data  get  raw  value  break  case  null  case  symbol  case  unresolved  default  throw  new    illegal  argument  exception  if  element  type  is  nullable  return  element  getter  return  array  pos  if  array  is  null  at  pos  return  null  return  element  getter  get  element  or  null  array  pos    accessor  for  getting  the  elements  of  an  array  during  runtime  see  create  element  getter    logical  type  interface    element  getter  extends    serializable    nullable    object  get  element  or  null    array  data  array  int  pos  
public  evolving  public  final  class    decimal  data  implements    comparable    decimal  data  member  fields  and  static  fields  are  package  visible  in  order  to  be  accessible  for    decimal  data  utils  static  final  int  max  compact  precision      maximum  number  of  decimal  digits  an    int  can  represent  1e9    int    max  value  1e10  static  final  int  max  int  digits      maximum  number  of  decimal  digits  a    long  can  represent  1e18    long    max  value  1e19  static  final  int  max  long  digits    static  final  long    p  o  w10  new  long  max  compact  precision    static    p  o  w10      for  int  i    i    p  o  w10  length  i    p  o  w10  i      p  o  w10  i      the  semantics  of  the  fields  are  as  follows  precision  and  scale  represent  the  precision  and  scale  of  sql  decimal  type    if  decimal  val  is  set  it  represents  the  whole  decimal  value    otherwise  the  decimal  value  is  long  val    scale    note  that  the  precision  scale  must  be  correct  if  precision  max  compact  precision  decimal  val  represents  the  value  long  val  is  undefined  otherwise  long  val  scale  represents  the  value  decimal  val  may  be  set  and  cached  final  int  precision  final  int  scale  final  long  long  val    big  decimal  decimal  val  this  constructor  does  not  perform  any  sanity  check    decimal  data  int  precision  int  scale  long  long  val    big  decimal  decimal  val  this  precision  precision  this  scale  scale  this  long  val  long  val  this  decimal  val  decimal  val    public    interfaces    returns  the  i  precision  i  of  this  link    decimal  data  p    the  precision  is  the  number  of  digits  in  the  unscaled  value  public  int  precision  return  precision    returns  the  i  scale  i  of  this  link    decimal  data  public  int  scale  return  scale    converts  this  link    decimal  data  into  an  instance  of  link    big  decimal  public    big  decimal  to  big  decimal    big  decimal  bd  decimal  val  if  bd  null  decimal  val  bd    big  decimal  value  of  long  val  scale  return  bd    returns  a  long  describing  the  i  unscaled  value  i  of  this  link    decimal  data  throws    arithmetic  exception  if  this  link    decimal  data  does  not  exactly  fit  in  a  long  public  long  to  unscaled  long  if  is  compact  return  long  val  else  return  to  big  decimal  unscaled  value  long  value  exact    returns  a  byte  array  describing  the  i  unscaled  value  i  of  this  link    decimal  data  return  the  unscaled  byte  array  of  this  link    decimal  data  public  byte  to  unscaled  bytes  return  to  big  decimal  unscaled  value  to  byte  array    returns  whether  the  decimal  value  is  small  enough  to  be  stored  in  a  long  public  boolean  is  compact  return  precision  max  compact  precision    returns  a  copy  of  this  link    decimal  data  object  public    decimal  data  copy  return  new    decimal  data  precision  scale  long  val  decimal  val    override  public  int  hash  code  return  to  big  decimal  hash  code    override  public  int  compare  to    nonnull    decimal  data  that  if  this  is  compact  that  is  compact  this  scale  that  scale  return    long  compare  this  long  val  that  long  val  return  this  to  big  decimal  compare  to  that  to  big  decimal    override  public  boolean  equals  final    object  o  if  o  instanceof    decimal  data  return  false    decimal  data  that    decimal  data  o  return  this  compare  to  that      override  public    string  to  string  return  to  big  decimal  to  plain  string    constructor    utilities    creates  an  instance  of  link    decimal  data  from  a  link    big  decimal  and  the  given  precision  and  scale  p    the  returned  decimal  value  may  be  rounded  to  have  the  desired  scale    the  precision  will  be  checked    if  the  precision  overflows  null  will  be  returned  public  static    nullable    decimal  data  from  big  decimal    big  decimal  bd  int  precision  int  scale  bd  bd  set  scale  scale    rounding  mode  half  up  if  bd  precision  precision  return  null  long  long  val    if  precision  max  compact  precision  long  val  bd  move  point  right  scale  long  value  exact  return  new    decimal  data  precision  scale  long  val  bd    creates  an  instance  of  link    decimal  data  from  an  unscaled  long  value  and  the  given  precision  and  scale  public  static    decimal  data  from  unscaled  long  long  unscaled  long  int  precision  int  scale  check  argument  precision    precision  max  long  digits  return  new    decimal  data  precision  scale  unscaled  long  null    creates  an  instance  of  link    decimal  data  from  an  unscaled  byte  array  value  and  the  given  precision  and  scale  public  static    decimal  data  from  unscaled  bytes  byte  unscaled  bytes  int  precision  int  scale    big  decimal  bd  new    big  decimal  new    big  integer  unscaled  bytes  scale  return  from  big  decimal  bd  precision  scale    creates  an  instance  of  link    decimal  data  for  a  zero  value  with  the  given  precision  and  scale  p    the  precision  will  be  checked    if  the  precision  overflows  null  will  be  returned  public  static    nullable    decimal  data  zero  int  precision  int  scale  if  precision  max  compact  precision  return  new    decimal  data  precision  scale    null  else  return  from  big  decimal    big  decimal  zero  precision  scale    utilities    returns  whether  the  decimal  value  is  small  enough  to  be  stored  in  a  long  public  static  boolean  is  compact  int  precision  return  precision  max  compact  precision  
public  evolving  public  final  class    generic  array  data  implements    array  data  private  final    object  array  private  final  int  size  private  final  boolean  is  primitive  array    creates  an  instance  of  link    generic  array  data  using  the  given    java  array  p    note    all  elements  of  the  array  must  be  internal  data  structures  public    generic  array  data    object  array  this  array  array  length  false  public    generic  array  data  int  primitive  array  this  primitive  array  primitive  array  length  true  public    generic  array  data  long  primitive  array  this  primitive  array  primitive  array  length  true  public    generic  array  data  float  primitive  array  this  primitive  array  primitive  array  length  true  public    generic  array  data  double  primitive  array  this  primitive  array  primitive  array  length  true  public    generic  array  data  short  primitive  array  this  primitive  array  primitive  array  length  true  public    generic  array  data  byte  primitive  array  this  primitive  array  primitive  array  length  true  public    generic  array  data  boolean  primitive  array  this  primitive  array  primitive  array  length  true  private    generic  array  data    object  array  int  size  boolean  is  primitive  array  this  array  array  this  size  size  this  is  primitive  array  is  primitive  array    returns  true  if  this  is  a  primitive  array  p  a  primitive  array  is  an  array  whose  elements  are  of  primitive  type  public  boolean  is  primitive  array  return  is  primitive  array    converts  this  link    generic  array  data  into  an  array  of    java  link    object  p    the  method  will  convert  a  primitive  array  into  an  object  array    but  it  will  not  convert  internal  data  structures  into  external  data  structures  e  g  link    string  data  to  link    string  public    object  to  object  array  if  is  primitive  array    class  array  class  array  get  class  if  int  class  equals  array  class  return    array  utils  to  object  int  array  else  if  long  class  equals  array  class  return    array  utils  to  object  long  array  else  if  float  class  equals  array  class  return    array  utils  to  object  float  array  else  if  double  class  equals  array  class  return    array  utils  to  object  double  array  else  if  short  class  equals  array  class  return    array  utils  to  object  short  array  else  if  byte  class  equals  array  class  return    array  utils  to  object  byte  array  else  if  boolean  class  equals  array  class  return    array  utils  to  object  boolean  array  throw  new    runtime  exception    unsupported  primitive  array  array  class  else  return    object  array    override  public  int  size  return  size    override  public  boolean  is  null  at  int  pos  return  is  primitive  array    object  array  pos  null    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    generic  array  data  that    generic  array  data  o  return  size  that  size  is  primitive  array  that  is  primitive  array    objects  deep  equals  array  that  array    override  public  int  hash  code  int  result    objects  hash  size  is  primitive  array  result    result    arrays  deep  hash  code  new    object  array  return  result    read  only  accessor  methods    override  public  boolean  get  boolean  int  pos  return  is  primitive  array  boolean  array  pos  boolean  get  object  pos    override  public  byte  get  byte  int  pos  return  is  primitive  array  byte  array  pos  byte  get  object  pos    override  public  short  get  short  int  pos  return  is  primitive  array  short  array  pos  short  get  object  pos    override  public  int  get  int  int  pos  return  is  primitive  array  int  array  pos  int  get  object  pos    override  public  long  get  long  int  pos  return  is  primitive  array  long  array  pos  long  get  object  pos    override  public  float  get  float  int  pos  return  is  primitive  array  float  array  pos  float  get  object  pos    override  public  double  get  double  int  pos  return  is  primitive  array  double  array  pos  double  get  object  pos    override  public  byte  get  binary  int  pos  return  byte  get  object  pos    override  public    string  data  get  string  int  pos  return    string  data  get  object  pos    override  public    decimal  data  get  decimal  int  pos  int  precision  int  scale  return    decimal  data  get  object  pos    override  public    timestamp  data  get  timestamp  int  pos  int  precision  return    timestamp  data  get  object  pos    suppress  warnings  unchecked    override  public  t    raw  value  data  t  get  raw  value  int  pos  return    raw  value  data  t  get  object  pos    override  public    row  data  get  row  int  pos  int  num  fields  return    row  data  get  object  pos    override  public    array  data  get  array  int  pos  return    array  data  get  object  pos    override  public    map  data  get  map  int  pos  return    map  data  get  object  pos  private    object  get  object  int  pos  return    object  array  pos    conversion    utilities  private  boolean  any  null  for    object  element    object  array  if  element  null  return  true  return  false  private  void  check  no  null  if  any  null  throw  new    runtime  exception    primitive  array  must  not  contain  a  null  value    override  public  boolean  to  boolean  array  if  is  primitive  array  return  boolean  array  check  no  null  return    array  utils  to  primitive    boolean  array    override  public  byte  to  byte  array  if  is  primitive  array  return  byte  array  check  no  null  return    array  utils  to  primitive    byte  array    override  public  short  to  short  array  if  is  primitive  array  return  short  array  check  no  null  return    array  utils  to  primitive    short  array    override  public  int  to  int  array  if  is  primitive  array  return  int  array  check  no  null  return    array  utils  to  primitive    integer  array    override  public  long  to  long  array  if  is  primitive  array  return  long  array  check  no  null  return    array  utils  to  primitive    long  array    override  public  float  to  float  array  if  is  primitive  array  return  float  array  check  no  null  return    array  utils  to  primitive    float  array    override  public  double  to  double  array  if  is  primitive  array  return  double  array  check  no  null  return    array  utils  to  primitive    double  array  
public  evolving  public  final  class    generic  map  data  implements    map  data  private  final    map  map    creates  an  instance  of  link    generic  map  data  using  the  given    java  map  p    note    all  keys  and  values  of  the  map  must  be  internal  data  structures  public    generic  map  data    map  map  this  map  map    returns  the  value  to  which  the  specified  key  is  mapped  or  code  null  if  this  map  contains  no  mapping  for  the  key    the  returned  value  is  in  internal  data  structure  public    object  get    object  key  return  map  get  key    override  public  int  size  return  map  size    override  public    array  data  key  array    object  keys  map  key  set  to  array  return  new    generic  array  data  keys    override  public    array  data  value  array    object  values  map  values  to  array  return  new    generic  array  data  values    override  public  boolean  equals    object  o  if  o  this  return  true  if  o  instanceof    generic  map  data  return  false  deep  equals  for  values  of  byte  return  deep  equals  map    generic  map  data  o  map  private  static  k  v  boolean  deep  equals    map  k  v  m1    map  m2  copied  from    hash  map  equals  but  with  deep  equals  comparision  if  m1  size  m2  size  return  false  try  for    map    entry  k  v  e  m1  entry  set  k  key  e  get  key  v  value  e  get  value  if  value  null  if  m2  get  key  null  m2  contains  key  key  return  false  else  if    objects  deep  equals  value  m2  get  key  return  false  catch    class  cast  exception    null  pointer  exception  unused  return  false  return  true    override  public  int  hash  code  int  result    for    object  key  map  key  set  only  include  key  because  values  can  contain  byte  result    key  hash  code  return  result  
public  evolving  public  final  class    generic  row  data  implements    row  data    the  array  to  store  the  actual  internal  format  values  private  final    object  fields    the  kind  of  change  that  a  row  describes  in  a  changelog  private    row  kind  kind    creates  an  instance  of  link    generic  row  data  with  given  kind  and  number  of  fields  p    initially  all  fields  are  set  to  null  p    note    all  fields  of  the  row  must  be  internal  data  structures  param  kind  kind  of  change  that  this  row  describes  in  a  changelog  param  arity  number  of  fields  public    generic  row  data    row  kind  kind  int  arity  this  fields  new    object  arity  this  kind  kind    creates  an  instance  of  link    generic  row  data  with  given  number  of  fields  p    initially  all  fields  are  set  to  null    by  default  the  row  describes  a  link    row  kind  insert  in  a  changelog  p    note    all  fields  of  the  row  must  be  internal  data  structures  param  arity  number  of  fields  public    generic  row  data  int  arity  this  fields  new    object  arity  this  kind    row  kind  insert  insert  as  default    sets  the  field  value  at  the  given  position  p    note    the  given  field  value  must  be  an  internal  data  structures    otherwise  the  link    generic  row  data  is  corrupted  and  may  throw  exception  when  processing    see  link    row  data  for  more  information  about  internal  data  structures  p    the  field  value  can  be  null  for  representing  nullability  public  void  set  field  int  pos    object  value  this  fields  pos  value    returns  the  field  value  at  the  given  position  p    note    the  returned  value  is  in  internal  data  structure    see  link    row  data  for  more  information  about  internal  data  structures  p    the  returned  field  value  can  be  null  for  representing  nullability  public    object  get  field  int  pos  return  this  fields  pos    override  public  int  get  arity  return  fields  length    override  public    row  kind  get  row  kind  return  kind    override  public  void  set  row  kind    row  kind  kind  check  not  null  kind  this  kind  kind    override  public  boolean  is  null  at  int  pos  return  this  fields  pos  null    override  public  boolean  get  boolean  int  pos  return  boolean  this  fields  pos    override  public  byte  get  byte  int  pos  return  byte  this  fields  pos    override  public  short  get  short  int  pos  return  short  this  fields  pos    override  public  int  get  int  int  pos  return  int  this  fields  pos    override  public  long  get  long  int  pos  return  long  this  fields  pos    override  public  float  get  float  int  pos  return  float  this  fields  pos    override  public  double  get  double  int  pos  return  double  this  fields  pos    override  public    string  data  get  string  int  pos  return    string  data  this  fields  pos    override  public    decimal  data  get  decimal  int  pos  int  precision  int  scale  return    decimal  data  this  fields  pos    override  public    timestamp  data  get  timestamp  int  pos  int  precision  return    timestamp  data  this  fields  pos    suppress  warnings  unchecked    override  public  t    raw  value  data  t  get  raw  value  int  pos  return    raw  value  data  t  this  fields  pos    override  public  byte  get  binary  int  pos  return  byte  this  fields  pos    override  public    array  data  get  array  int  pos  return    array  data  this  fields  pos    override  public    map  data  get  map  int  pos  return    map  data  this  fields  pos    override  public    row  data  get  row  int  pos  int  num  fields  return    row  data  this  fields  pos    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  instanceof    generic  row  data  return  false    generic  row  data  that    generic  row  data  o  return  kind  that  kind    arrays  deep  equals  fields  that  fields    override  public  int  hash  code  int  result    objects  hash  kind  result    result    arrays  deep  hash  code  fields  return  result    override  public    string  to  string    string  builder  sb  new    string  builder  sb  append  kind  short  string  append  for  int  i    i  fields  length  i  if  i    sb  append  sb  append    string  utils  array  aware  to  string  fields  i  sb  append  return  sb  to  string    utilities    creates  an  instance  of  link    generic  row  data  with  given  field  values  p    by  default  the  row  describes  a  link    row  kind  insert  in  a  changelog  p    note    all  fields  of  the  row  must  be  internal  data  structures  public  static    generic  row  data  of    object  values    generic  row  data  row  new    generic  row  data  values  length  for  int  i    i  values  length  i  row  set  field  i  values  i  return  row    creates  an  instance  of  link    generic  row  data  with  given  kind  and  field  values  p    note    all  fields  of  the  row  must  be  internal  data  structures  public  static    generic  row  data  of  kind    row  kind  kind    object  values    generic  row  data  row  new    generic  row  data  kind  values  length  for  int  i    i  values  length  i  row  set  field  i  values  i  return  row  
public  evolving  public  interface    map  data    returns  the  number  of  key  value  mappings  in  this  map  int  size    returns  an  array  view  of  the  keys  contained  in  this  map  p  a  key  value  pair  has  the  same  index  in  the  key  array  and  value  array    array  data  key  array    returns  an  array  view  of  the  values  contained  in  this  map  p  a  key  value  pair  has  the  same  index  in  the  key  array  and  value  array    array  data  value  array  
public  evolving  public  interface    raw  value  data  t    converts  this  link    raw  value  data  into  a    java  object  p    the  given  serializer  is  required  because  the  raw  value  might  be  represented  in  a  binary  format  and  needs  to  be  deserialized  first  p    note    the  returned    java  object  may  be  reused  t  to  object    type  serializer  t  serializer    converts  this  link    raw  value  data  into  a  byte  array  p    the  given  serializer  is  required  because  the  raw  value  might  be  still  be  a    java  object  and  needs  to  be  serialized  first  p    note    the  returned  byte  array  may  be  reused  byte  to  bytes    type  serializer  t  serializer    constructor    utilities    creates  an  instance  of  link    raw  value  data  from  a    java  object  static  t    raw  value  data  t  from  object  t  java  object  return    binary  raw  value  data  from  object  java  object    creates  an  instance  of  link    raw  value  data  from  the  given  byte  array  static  t    raw  value  data  t  from  bytes  byte  bytes  return    binary  raw  value  data  from  bytes  bytes  
public  evolving  public  interface    row  data    returns  the  number  of  fields  in  this  row  p    the  number  does  not  include  link    row  kind    it  is  kept  separately  int  get  arity    returns  the  kind  of  change  that  this  row  describes  in  a  changelog  see    row  kind    row  kind  get  row  kind    sets  the  kind  of  change  that  this  row  describes  in  a  changelog  see    row  kind  void  set  row  kind    row  kind  kind    read  only  accessor  methods    returns  true  if  the  field  is  null  at  the  given  position  boolean  is  null  at  int  pos    returns  the  boolean  value  at  the  given  position  boolean  get  boolean  int  pos    returns  the  byte  value  at  the  given  position  byte  get  byte  int  pos    returns  the  short  value  at  the  given  position  short  get  short  int  pos    returns  the  integer  value  at  the  given  position  int  get  int  int  pos    returns  the  long  value  at  the  given  position  long  get  long  int  pos    returns  the  float  value  at  the  given  position  float  get  float  int  pos    returns  the  double  value  at  the  given  position  double  get  double  int  pos    returns  the  string  value  at  the  given  position    string  data  get  string  int  pos    returns  the  decimal  value  at  the  given  position  p    the  precision  and  scale  are  required  to  determine  whether  the  decimal  value  was  stored  in  a  compact  representation  see  link    decimal  data    decimal  data  get  decimal  int  pos  int  precision  int  scale    returns  the  timestamp  value  at  the  given  position  p    the  precision  is  required  to  determine  whether  the  timestamp  value  was  stored  in  a  compact  representation  see  link    timestamp  data    timestamp  data  get  timestamp  int  pos  int  precision    returns  the  raw  value  at  the  given  position  t    raw  value  data  t  get  raw  value  int  pos    returns  the  binary  value  at  the  given  position  byte  get  binary  int  pos    returns  the  array  value  at  the  given  position    array  data  get  array  int  pos    returns  the  map  value  at  the  given  position    map  data  get  map  int  pos    returns  the  row  value  at  the  given  position  p    the  number  of  fields  is  required  to  correctly  extract  the  row    row  data  get  row  int  pos  int  num  fields    access    utilities    returns  the  field  object  in  the  internal  row  data  structure  at  the  given  position  param  row  the  internal  row  data  param  pos  position  of  the  field  to  return  param  field  type  the  field  type  return  the  field  object  at  the  specified  position  in  this  row  data  deprecated    use  link  create  field  getter    logical  type  int  for  avoiding  logical  types  during  runtime  static    object  get    row  data  row  int  pos    logical  type  field  type  if  row  is  null  at  pos  return  null  switch  field  type  get  type  root  case  boolean  return  row  get  boolean  pos  case  tinyint  return  row  get  byte  pos  case  smallint  return  row  get  short  pos  case  integer  case  date  case  time  without  time  zone  case  interval  year  month  return  row  get  int  pos  case  bigint  case  interval  day  time  return  row  get  long  pos  case  timestamp  without  time  zone    timestamp  type  timestamp  type    timestamp  type  field  type  return  row  get  timestamp  pos  timestamp  type  get  precision  case  timestamp  with  local  time  zone    local  zoned  timestamp  type  lz  ts    local  zoned  timestamp  type  field  type  return  row  get  timestamp  pos  lz  ts  get  precision  case  float  return  row  get  float  pos  case  double  return  row  get  double  pos  case  char  case  varchar  return  row  get  string  pos  case  decimal    decimal  type  decimal  type    decimal  type  field  type  return  row  get  decimal  pos  decimal  type  get  precision  decimal  type  get  scale  case  array  return  row  get  array  pos  case  map  case  multiset  return  row  get  map  pos  case  row  return  row  get  row  pos    row  type  field  type  get  field  count  case  structured  type  not  the  most  efficient  code  but  ok  for  a  deprecated  method  return  row  get  row  pos  get  field  count  field  type  case  binary  case  varbinary  return  row  get  binary  pos  case  raw  return  row  get  raw  value  pos  default  throw  new    unsupported  operation  exception    unsupported  type  field  type    creates  an  accessor  for  getting  elements  in  an  internal  row  data  structure  at  the  given  position  param  field  type  the  element  type  of  the  row  param  field  pos  the  element  type  of  the  row  static    field  getter  create  field  getter    logical  type  field  type  int  field  pos  final    field  getter  field  getter  ordered  by  type  root  definition  switch  field  type  get  type  root  case  char  case  varchar  field  getter  row  row  get  string  field  pos  break  case  boolean  field  getter  row  row  get  boolean  field  pos  break  case  binary  case  varbinary  field  getter  row  row  get  binary  field  pos  break  case  decimal  final  int  decimal  precision  get  precision  field  type  final  int  decimal  scale  get  scale  field  type  field  getter  row  row  get  decimal  field  pos  decimal  precision  decimal  scale  break  case  tinyint  field  getter  row  row  get  byte  field  pos  break  case  smallint  field  getter  row  row  get  short  field  pos  break  case  integer  case  date  case  time  without  time  zone  case  interval  year  month  field  getter  row  row  get  int  field  pos  break  case  bigint  case  interval  day  time  field  getter  row  row  get  long  field  pos  break  case  float  field  getter  row  row  get  float  field  pos  break  case  double  field  getter  row  row  get  double  field  pos  break  case  timestamp  without  time  zone  case  timestamp  with  local  time  zone  final  int  timestamp  precision  get  precision  field  type  field  getter  row  row  get  timestamp  field  pos  timestamp  precision  break  case  timestamp  with  time  zone  throw  new    unsupported  operation  exception  case  array  field  getter  row  row  get  array  field  pos  break  case  multiset  case  map  field  getter  row  row  get  map  field  pos  break  case  row  case  structured  type  final  int  row  field  count  get  field  count  field  type  field  getter  row  row  get  row  field  pos  row  field  count  break  case  distinct  type  field  getter  create  field  getter    distinct  type  field  type  get  source  type  field  pos  break  case  raw  field  getter  row  row  get  raw  value  field  pos  break  case  null  case  symbol  case  unresolved  default  throw  new    illegal  argument  exception  if  field  type  is  nullable  return  field  getter  return  row  if  row  is  null  at  field  pos  return  null  return  field  getter  get  field  or  null  row    accessor  for  getting  the  field  of  a  row  during  runtime  see  create  field  getter    logical  type  int  interface    field  getter  extends    serializable    nullable    object  get  field  or  null    row  data  row  
public  evolving  public  interface    string  data  extends    comparable    string  data    converts  this  link    string  data  object  to  a  utf    byte  array  p    note    the  returned  byte  array  may  be  reused  byte  to  bytes    converts  this  link    string  data  object  to  a  link    string    string  to  string    construction    utilities    creates  an  instance  of  link    string  data  from  the  given  link    string  static    string  data  from  string    string  str  return    binary  string  data  from  string  str    creates  an  instance  of  link    string  data  from  the  given  utf    byte  array  static    string  data  from  bytes  byte  bytes  return    binary  string  data  from  bytes  bytes    creates  an  instance  of  link    string  data  from  the  given  utf    byte  array  with  offset  and  number  of  bytes  static    string  data  from  bytes  byte  bytes  int  offset  int  num  bytes  return    binary  string  data  from  bytes  bytes  offset  num  bytes  
public  evolving  public  final  class    timestamp  data  implements    comparable    timestamp  data  the  number  of  milliseconds  in  a  day  private  static  final  long  millis  per  day            this  field  holds  the  integral  second  and  the  milli  of  second  private  final  long  millisecond  this  field  holds  the  nano  of  millisecond  private  final  int  nano  of  millisecond  private    timestamp  data  long  millisecond  int  nano  of  millisecond    preconditions  check  argument  nano  of  millisecond    nano  of  millisecond      this  millisecond  millisecond  this  nano  of  millisecond  nano  of  millisecond    returns  the  number  of  milliseconds  since  code  1970-01  01 00  00:00  public  long  get  millisecond  return  millisecond    returns  the  number  of  nanoseconds  the  nanoseconds  within  the  milliseconds  p    the  value  range  is  from    to  999  999  public  int  get  nano  of  millisecond  return  nano  of  millisecond    converts  this  link    timestamp  data  object  to  a  link    timestamp  public    timestamp  to  timestamp  return    timestamp  value  of  to  local  date  time    converts  this  link    timestamp  data  object  to  a  link    local  date  time  public    local  date  time  to  local  date  time  int  date  int  millisecond  millis  per  day  int  time  int  millisecond  millis  per  day  if  time    date  time  millis  per  day  long  nano  of  day  time        000  l  nano  of  millisecond    local  date  local  date    local  date  of  epoch  day  date    local  time  local  time    local  time  of  nano  of  day  nano  of  day  return    local  date  time  of  local  date  local  time    converts  this  link    timestamp  data  object  to  a  link    instant  public    instant  to  instant  long  epoch  second  millisecond    int  milli  of  second  int  millisecond    if  milli  of  second    epoch  second  milli  of  second    long  nano  adjustment  milli  of  second          nano  of  millisecond  return    instant  of  epoch  second  epoch  second  nano  adjustment    override  public  int  compare  to    timestamp  data  that  int  cmp    long  compare  this  millisecond  that  millisecond  if  cmp    cmp  this  nano  of  millisecond  that  nano  of  millisecond  return  cmp    override  public  boolean  equals    object  obj  if  obj  instanceof    timestamp  data  return  false    timestamp  data  that    timestamp  data  obj  return  this  millisecond  that  millisecond  this  nano  of  millisecond  that  nano  of  millisecond    override  public    string  to  string  return  to  local  date  time  to  string    override  public  int  hash  code  int  ret  int  millisecond  int  millisecond    return    ret  nano  of  millisecond    constructor    utilities    creates  an  instance  of  link    timestamp  data  from  milliseconds  p    the  nanos  of  millisecond  field  will  be  set  to  zero  param  milliseconds  the  number  of  milliseconds  since  code  1970-01  01 00  00:00  a  negative  number  is  the  number  of  milliseconds  before  code  1970-01  01 00  00:00  public  static    timestamp  data  from  epoch  millis  long  milliseconds  return  new    timestamp  data  milliseconds      creates  an  instance  of  link    timestamp  data  from  milliseconds  and  a  nanos  of  millisecond  param  milliseconds  the  number  of  milliseconds  since  code  1970-01  01 00  00:00  a  negative  number  is  the  number  of  milliseconds  before  code  1970-01  01 00  00:00  param  nanos  of  millisecond  the  nanoseconds  within  the  millisecond  from    to  999  999  public  static    timestamp  data  from  epoch  millis  long  milliseconds  int  nanos  of  millisecond  return  new    timestamp  data  milliseconds  nanos  of  millisecond    creates  an  instance  of  link    timestamp  data  from  an  instance  of  link    local  date  time  param  date  time  an  instance  of  link    local  date  time  public  static    timestamp  data  from  local  date  time    local  date  time  date  time  long  epoch  day  date  time  to  local  date  to  epoch  day  long  nano  of  day  date  time  to  local  time  to  nano  of  day  long  millisecond  epoch  day  millis  per  day  nano  of  day          int  nano  of  millisecond  int  nano  of  day          return  new    timestamp  data  millisecond  nano  of  millisecond    creates  an  instance  of  link    timestamp  data  from  an  instance  of  link    timestamp  param  timestamp  an  instance  of  link    timestamp  public  static    timestamp  data  from  timestamp    timestamp  timestamp  return  from  local  date  time  timestamp  to  local  date  time    creates  an  instance  of  link    timestamp  data  from  an  instance  of  link    instant  param  instant  an  instance  of  link    instant  public  static    timestamp  data  from  instant    instant  instant  long  epoch  second  instant  get  epoch  second  int  nano  second  instant  get  nano  long  millisecond  epoch  second      nano  second          int  nano  of  millisecond  nano  second          return  new    timestamp  data  millisecond  nano  of  millisecond    returns  whether  the  timestamp  data  is  small  enough  to  be  stored  in  a  long  of  milliseconds  public  static  boolean  is  compact  int  precision  return  precision    
public  evolving  public  abstract  class    catalog  descriptor  extends    descriptor  base  private  final    string  type  private  final  int  property  version  private  final    string  default  database    constructs  a  link    catalog  descriptor  param  type  string  that  identifies  this  catalog  param  property  version  property  version  for  backwards  compatibility  public    catalog  descriptor    string  type  int  property  version  this  type  property  version  null    constructs  a  link    catalog  descriptor  param  type  string  that  identifies  this  catalog  param  property  version  property  version  for  backwards  compatibility  param  default  database  default  database  of  the  catalog  public    catalog  descriptor    string  type  int  property  version    string  default  database  check  argument    string  utils  is  null  or  whitespace  only  type  type  cannot  be  null  or  empty  this  type  type  this  property  version  property  version  this  default  database  default  database    override  public  final    map    string    string  to  properties  final    descriptor  properties  properties  new    descriptor  properties  properties  put  string  catalog  type  type  properties  put  long  catalog  property  version  property  version  if  default  database  null  properties  put  string  catalog  default  database  default  database  properties  put  properties  to  catalog  properties  return  properties  as  map  public    string  get  default  database  return  default  database    converts  this  descriptor  into  a  set  of  catalog  properties  protected  abstract    map    string    string  to  catalog  properties  
public  evolving  public  class    class  instance  extends    hierarchy  descriptor  private    string  class  name  the  parameter  is  either  a  literal  value  or  the  instance  of  a  class  private    list    either    literal  value    class  instance  constructor  new    array  list    sets  the  fully  qualified  class  name  for  creating  an  instance  p  e  g  org  example    my  class  or  org  example    my  class    static  inner  class  param  class  name  fully  qualified  class  name  public    class  instance  of    string  class  name  this  class  name  class  name  return  this    adds  a  constructor  parameter  value  of  literal  type    the  type  is  automatically  derived  from  the  value    currently  this  is  supported  for  boolean  int  double  and  varchar    expression  values  are  not  allowed  p    examples  true  false  boolean      int  2.0  1234.222  double  varchar  otherwise  p    for  other  types  and  explicit  type  declaration  use  link  parameter    string    string  or  link  parameter    type  information    string  public    class  instance  parameter  string    string  value  string  constructor  add    left  new    literal  value  value  value  string  return  this    adds  a  constructor  parameter  value  of  literal  type    the  type  is  explicitly  defined  using  a  type  string  such  as  varchar  float  boolean  int  bigint  etc    the  value  is  parsed  accordingly    expression  values  are  not  allowed  param  type  string  the  type  string  that  define  how  to  parse  the  given  value  string  param  value  string  the  literal  value  to  be  parsed  public    class  instance  parameter    string  type  string    string  value  string  constructor  add    left  new    literal  value  of  type  string  value  value  string  return  this    adds  a  constructor  parameter  value  of  literal  type    the  type  is  explicitly  defined  using  type  information    the  value  is  parsed  accordingly    expression  values  are  not  allowed  param  type  info  the  type  that  define  how  to  parse  the  given  value  string  param  value  string  the  literal  value  to  be  parsed  public    class  instance  parameter    type  information  type  info    string  value  string  constructor  add    left  new    literal  value  of  type  info  value  value  string  return  this    adds  a  constructor  parameter  value  of  boolean  type  param  value  boolean  value  public    class  instance  parameter  boolean  value  constructor  add    left  new    literal  value  of    types  boolean  value  value  return  this    adds  a  constructor  parameter  value  of  double  type  param  value  double  value  public    class  instance  parameter  double  value  constructor  add    left  new    literal  value  of    types  double  value  value  return  this    adds  a  constructor  parameter  value  of  float  type  param  value  float  value  public    class  instance  parameter  float  value  constructor  add    left  new    literal  value  of    types  float  value  value  return  this    adds  a  constructor  parameter  value  of  int  type  param  value  int  value  public    class  instance  parameter  int  value  constructor  add    left  new    literal  value  of    types  int  value  value  return  this    adds  a  constructor  parameter  value  of  varchar  type  param  value  varchar  value  public    class  instance  parameter    string  value  constructor  add    left  new    literal  value  of    types  string  value  value  return  this    adds  a  constructor  parameter  value  of  bigint  type  param  value  bigint  value  public    class  instance  parameter  long  value  constructor  add    left  new    literal  value  of    types  long  value  value  return  this    adds  a  constructor  parameter  value  of  tinyint  type  param  value  tinyint  value  public    class  instance  parameter  byte  value  constructor  add    left  new    literal  value  of    types  byte  value  value  return  this    adds  a  constructor  parameter  value  of  smallint  type  param  value  smallint  value  public    class  instance  parameter  short  value  constructor  add    left  new    literal  value  of    types  short  value  value  return  this    adds  a  constructor  parameter  value  of  decimal  type  param  value  decimal  value  public    class  instance  parameter    big  decimal  value  constructor  add    left  new    literal  value  of    types  big  dec  value  value  return  this    adds  a  constructor  parameter  value  of  a  class  instance  i  e  a    java  object  with  a  public  constructor  param  class  instance  description  of  a  class  instance  i  e  a    java  object  with  a  public  constructor  public    class  instance  parameter    class  instance  class  instance  constructor  add    right  class  instance  return  this    converts  this  descriptor  into  a  set  of  properties    override  public    map    string    string  to  properties    descriptor  properties  properties  new    descriptor  properties  add  properties  with  prefix    hierarchy  descriptor  validator  empty  prefix  properties  return  properties  as  map    internal  method  for  properties  conversion    override  public  void  add  properties  with  prefix    string  key  prefix    descriptor  properties  properties  if  class  name  null  properties  put  string  key  prefix    class  instance  validator  class  class  name  for  int  i    i  constructor  size  i    either    literal  value    class  instance  either  constructor  get  i    string  key  prefix  with  idx  key  prefix    class  instance  validator  constructor  i  if  either  is  left  either  left  add  properties  with  prefix  key  prefix  with  idx  properties  else  either  right  add  properties  with  prefix  key  prefix  with  idx  properties  
public  evolving  public  abstract  class    connector  descriptor  extends    descriptor  base  implements    descriptor  private    string  type  private  int  version  private  boolean  format  needed    constructs  a  link    connector  descriptor  param  type  string  that  identifies  this  connector  param  version  property  version  for  backwards  compatibility  param  format  needed  flag  for  basic  validation  of  a  needed  format  descriptor  public    connector  descriptor    string  type  int  version  boolean  format  needed  this  type  type  this  version  version  this  format  needed  format  needed    override  public  final    map    string    string  to  properties  final    descriptor  properties  properties  new    descriptor  properties  properties  put  string  connector  type  type  properties  put  long  connector  property  version  version  properties  put  properties  to  connector  properties  return  properties  as  map    returns  if  this  connector  requires  a  format  descriptor  protected  final  boolean  is  format  needed  return  format  needed    converts  this  descriptor  into  a  set  of  connector  properties    usually  prefixed  with  link    connector  descriptor  validator  connector  protected  abstract    map    string    string  to  connector  properties  
public  evolving  public  interface    descriptor    converts  this  descriptor  into  a  set  of  properties    map    string    string  to  properties  
public  evolving  public  abstract  class    descriptor  base  implements    descriptor    override  public    string  to  string  return    descriptor  properties  to  string  to  properties  
public  evolving  public  class    file  system  extends    connector  descriptor  private    string  path  null  public    file  system  super  connector  type  value    true    sets  the  path  to  a  file  or  directory  in  a  file  system  param  path  the  path  a  file  or  directory  public    file  system  path    string  path  this  path  path  return  this    override  protected    map    string    string  to  connector  properties    descriptor  properties  properties  new    descriptor  properties  if  path  null  properties  put  string  connector  path  path  return  properties  as  map  
public  evolving  public  class    file  system  validator  extends    connector  descriptor  validator  public  static  final    string  connector  type  value  filesystem  public  static  final    string  connector  path  connector  path    override  public  void  validate    descriptor  properties  properties  super  validate  properties  properties  validate  value  connector  type  connector  type  value  false  properties  validate  string  connector  path  false    
public  evolving  public  abstract  class    format  descriptor  extends    descriptor  base  implements    descriptor  private    string  type  private  int  version    constructs  a  link    format  descriptor  param  type  string  that  identifies  this  format  param  version  property  version  for  backwards  compatibility  public    format  descriptor    string  type  int  version  this  type  type  this  version  version    override  public  final    map    string    string  to  properties  final    descriptor  properties  properties  new    descriptor  properties  properties  put  string    format  descriptor  validator  format  type  type  properties  put  int    format  descriptor  validator  format  property  version  version  properties  put  properties  to  format  properties  return  properties  as  map    converts  this  descriptor  into  a  set  of  format  properties    usually  prefixed  with  link    format  descriptor  validator  format  protected  abstract    map    string    string  to  format  properties  
public  evolving  public  class    function  descriptor  implements    descriptor  private    string  from  private    class  instance  class  instance  private    string  fully  qualified  name    creates  a  function  from  a  class  description  public    function  descriptor  from  class    class  instance  class  type  from    function  descriptor  validator  from  value  class  this  class  instance  class  type  this  fully  qualified  name  null  return  this    converts  this  descriptor  into  a  set  of  properties    override  public    map    string    string  to  properties    descriptor  properties  properties  new    descriptor  properties  if  from  null  properties  put  string    function  descriptor  validator  from  from  if  class  instance  null  properties  put  properties  class  instance  to  properties  if  fully  qualified  name  null  properties  put  string    python  function  validator  fully  qualified  name  fully  qualified  name  return  properties  as  map  
public  evolving  public  abstract  class    hierarchy  descriptor  implements    descriptor    internal  method  for  properties  conversion    all  the  property  keys  will  be  prefixed  with  the  given  key  prefix  public  abstract  void  add  properties  with  prefix    string  key  prefix    descriptor  properties  properties  
public  evolving  public  class    literal  value  extends    hierarchy  descriptor  private    string  type  info  private    object  value    type  information  of  the  literal  value  e  g    types  boolean  param  type  info  type  information  describing  the  value  public    literal  value  of    type  information  type  info    preconditions  check  not  null  type  info    type  information  must  not  be  null  this  type  info    type  string  utils  write  type  info  type  info  return  this    type  string  of  the  literal  value  e  g  boolean  param  type  string  type  string  describing  the  value  public    literal  value  of    string  type  string  this  type  info  type  string  return  this    literal  boolean  value  param  value  literal  boolean  value  public    literal  value  value  boolean  value  this  value  value  return  this    literal  int  value  param  value  literal  int  value  public    literal  value  value  int  value  this  value  value  return  this    literal  double  value  param  value  literal  double  value  public    literal  value  value  double  value  this  value  value  return  this    literal  float  value  param  value  literal  float  value  public    literal  value  value  float  value  this  value  value  return  this    literal  value  either  for  an  explicit  varchar  type  or  automatically  derived  type  p    if  no  type  is  set  the  type  is  automatically  derived  from  the  value    currently  this  is  supported  for  boolean  int  double  and  varchar  param  value  literal  value  public    literal  value  value    string  value  this  value  value  return  this    literal  bigint  value  param  value  literal  bigint  value  public    literal  value  value  long  value  this  value  value  return  this    literal  tinyint  value  param  value  literal  tinyint  value  public    literal  value  value  byte  value  this  value  value  return  this    literal  smallint  value  param  value  literal  smallint  value  public    literal  value  value  short  value  this  value  value  return  this    literal  decimal  value  param  value  literal  decimal  value  public    literal  value  value    big  decimal  value  this  value  value  return  this    override  public    map    string    string  to  properties    descriptor  properties  properties  new    descriptor  properties  add  properties  with  prefix    hierarchy  descriptor  validator  empty  prefix  properties  return  properties  as  map    override  public  void  add  properties  with  prefix    string  key  prefix    descriptor  properties  properties  if  type  info  null  properties  put  string  key  prefix  type  type  info  if  value  null  properties  put  string  key  prefix  value    string  value  of  value  else  do  not  allow  values  in  top  level  if  key  prefix  equals    hierarchy  descriptor  validator  empty  prefix  throw  new    validation  exception    literal  values  with  implicit  type  must  not  exist  in  the  top  level  of  a  hierarchy  if  value  null  properties  put  string  key  prefix  substring    key  prefix  length      string  value  of  value  
public  evolving  public  abstract  class    module  descriptor  extends    descriptor  base  private  final    string  type    constructs  a  link    module  descriptor  param  type  string  that  identifies  this  catalog  public    module  descriptor    string  type  check  argument    string  utils  is  null  or  whitespace  only  type  type  cannot  be  null  or  empty  this  type  type    override  public  final    map    string    string  to  properties  final    descriptor  properties  properties  new    descriptor  properties  properties  put  string  module  type  type  properties  put  properties  to  module  properties  return  properties  as  map    converts  this  descriptor  into  a  set  of  module  properties  protected  abstract    map    string    string  to  module  properties  
public  evolving  public  class    rowtime  implements    descriptor  private  final    descriptor  properties  internal  properties  new    descriptor  properties  true  todo    put  these  fields  into    rowtime  validator  once  it  is  also  ported  into  table  common  todo    because  these  fields  have  polluted  this  api  class  public  static  final    string  rowtime  rowtime  public  static  final    string  rowtime  timestamps  type  rowtime  timestamps  type  public  static  final    string  rowtime  timestamps  type  value  from  field  from  field  public  static  final    string  rowtime  timestamps  type  value  from  source  from  source  public  static  final    string  rowtime  timestamps  type  value  custom  custom  public  static  final    string  rowtime  timestamps  from  rowtime  timestamps  from  public  static  final    string  rowtime  timestamps  class  rowtime  timestamps  class  public  static  final    string  rowtime  timestamps  serialized  rowtime  timestamps  serialized  public  static  final    string  rowtime  watermarks  type  rowtime  watermarks  type  public  static  final    string  rowtime  watermarks  type  value  periodic  ascending  periodic  ascending  public  static  final    string  rowtime  watermarks  type  value  periodic  bounded  periodic  bounded  public  static  final    string  rowtime  watermarks  type  value  from  source  from  source  public  static  final    string  rowtime  watermarks  type  value  custom  custom  public  static  final    string  rowtime  watermarks  class  rowtime  watermarks  class  public  static  final    string  rowtime  watermarks  serialized  rowtime  watermarks  serialized  public  static  final    string  rowtime  watermarks  delay  rowtime  watermarks  delay    sets  a  built  in  timestamp  extractor  that  converts  an  existing  link    long  or  link    types  sql  timestamp  field  into  the  rowtime  attribute  param  field  name    the  field  to  convert  into  a  rowtime  attribute  public    rowtime  timestamps  from  field    string  field  name  internal  properties  put  string  rowtime  timestamps  type  rowtime  timestamps  type  value  from  field  internal  properties  put  string  rowtime  timestamps  from  field  name  return  this    sets  a  built  in  timestamp  extractor  that  converts  the  assigned  timestamps  from  a    data  stream  api  record  into  the  rowtime  attribute  and  thus  preserves  the  assigned  timestamps  from  the  source  p    note    this  extractor  only  works  in  streaming  environments  public    rowtime  timestamps  from  source  internal  properties  put  string  rowtime  timestamps  type  rowtime  timestamps  type  value  from  source  return  this    sets  a  custom  timestamp  extractor  to  be  used  for  the  rowtime  attribute  param  extractor    the  link    timestamp  extractor  to  extract  the  rowtime  attribute  from  the  physical  type  public    rowtime  timestamps  from  extractor    timestamp  extractor  extractor  internal  properties  put  properties  extractor  to  properties  return  this    sets  a  built  in  watermark  strategy  for  ascending  rowtime  attributes  p    emits  a  watermark  of  the  maximum  observed  timestamp  so  far  minus      rows  that  have  a  timestamp  equal  to  the  max  timestamp  are  not  late  public    rowtime  watermarks  periodic  ascending  internal  properties  put  string  rowtime  watermarks  type  rowtime  watermarks  type  value  periodic  ascending  return  this    sets  a  built  in  watermark  strategy  for  rowtime  attributes  which  are  out  of  order  by  a  bounded  time  interval  p    emits  watermarks  which  are  the  maximum  observed  timestamp  minus  the  specified  delay  param  delay  delay  in  milliseconds  public    rowtime  watermarks  periodic  bounded  long  delay  internal  properties  put  string  rowtime  watermarks  type  rowtime  watermarks  type  value  periodic  bounded  internal  properties  put  long  rowtime  watermarks  delay  delay  return  this    sets  a  built  in  watermark  strategy  which  indicates  the  watermarks  should  be  preserved  from  the  underlying    data  stream  api  and  thus  preserves  the  assigned  watermarks  from  the  source  public    rowtime  watermarks  from  source  internal  properties  put  string  rowtime  watermarks  type  rowtime  watermarks  type  value  from  source  return  this    sets  a  custom  watermark  strategy  to  be  used  for  the  rowtime  attribute  public    rowtime  watermarks  from  strategy    watermark  strategy  strategy  internal  properties  put  properties  strategy  to  properties  return  this    converts  this  descriptor  into  a  set  of  properties    override  public    map    string    string  to  properties  final    descriptor  properties  properties  new    descriptor  properties  properties  put  properties  internal  properties  return  properties  as  map  
public  evolving  public  class    schema  implements    descriptor  public  static  final    string  schema  schema  public  static  final    string  schema  name  name  deprecated  link    schema  uses  the  legacy  type  key  e  g  schema    type  long  to  store  type  information  in  prior  v1      since  v1    link    schema  uses  data  type  key  e  g  schema    data  type  bigint  to  store  types    deprecated  public  static  final    string  schema  type  type  public  static  final    string  schema  data  type  data  type  public  static  final    string  schema  proctime  proctime  public  static  final    string  schema  from  from  maps  a  field  name  to  a  list  of  properties  that  describe  type  origin  and  the  time  attribute  private  final    map    string    linked  hash  map    string    string  table  schema  new    linked  hash  map  private    string  last  field    sets  the  schema  with  field  names  and  the  types    required  p    this  method  overwrites  existing  fields  added  with  link  field    string    data  type  param  schema  the  table  schema  public    schema  schema    table  schema  schema  table  schema  clear  last  field  null  for  int  i    i  schema  get  field  count  i  field  schema  get  field  name  i  get  schema  get  field  data  type  i  get  return  this    adds  a  field  with  the  field  name  and  the  data  type    required    this  method  can  be  called  multiple  times    the  call  order  of  this  method  defines  also  the  order  of  the  fields  in  a  row  param  field  name  the  field  name  param  field  type  the  type  information  of  the  field  public    schema  field    string  field  name    data  type  field  type  add  field  field  name  field  type  get  logical  type  as  serializable  string  return  this    adds  a  field  with  the  field  name  and  the  type  information    required    this  method  can  be  called  multiple  times    the  call  order  of  this  method  defines  also  the  order  of  the  fields  in  a  row  param  field  name  the  field  name  param  field  type  the  type  information  of  the  field  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    please  use  link  field    string    data  type  instead    deprecated  public    schema  field    string  field  name    type  information  field  type  field  field  name    type  conversions  from  legacy  info  to  data  type  field  type  return  this    adds  a  field  with  the  field  name  and  the  type  string    required    this  method  can  be  called  multiple  times    the  call  order  of  this  method  defines  also  the  order  of  the  fields  in  a  row  p  note  the  field  type  string  should  follow  the  type  string  defined  in  link    logical  type  parser    this  method  also  keeps  compatible  with  old  type  string  defined  in  link    type  string  utils  but  will  be  dropped  in  future  versions  as  it  uses  the  old  type  system  param  field  name  the  field  name  param  field  type  the  type  string  of  the  field  public    schema  field    string  field  name    string  field  type  if  is  legacy  type  string  field  type  fallback  to  legacy  parser    type  information  type  info    type  string  utils  read  type  info  field  type  return  field  field  name    type  conversions  from  legacy  info  to  data  type  type  info  else  return  add  field  field  name  field  type  private    schema  add  field    string  field  name    string  field  type  if  table  schema  contains  key  field  name  throw  new    validation  exception    duplicate  field  name  field  name    linked  hash  map    string    string  field  properties  new    linked  hash  map  field  properties  put  schema  data  type  field  type  table  schema  put  field  name  field  properties  last  field  field  name  return  this  private  static  boolean  is  legacy  type  string    string  field  type  try    logical  type  type    logical  type  parser  parse  field  type  return  type  instanceof    unresolved  user  defined  type  catch    exception  e  if  the  parsing  failed  fallback  to  the  legacy  parser  return  true    specifies  the  origin  of  the  previously  defined  field    the  origin  field  is  defined  by  a  connector  or  format  p  e  g  field  my  string    types  string  from  csv  my  string  p    note    field  names  are  matched  by  the  exact  name  by  default  case  sensitive  public    schema  from    string  origin  field  name  if  last  field  null  throw  new    validation  exception    no  field  previously  defined    use  field  before  table  schema  get  last  field  put  schema  from  origin  field  name  last  field  null  return  this    specifies  the  previously  defined  field  as  a  processing  time  attribute  p  e  g  field  proctime    types  sql  timestamp  proctime  public    schema  proctime  if  last  field  null  throw  new    validation  exception    no  field  defined  previously    use  field  before  table  schema  get  last  field  put  schema  proctime  true  last  field  null  return  this    specifies  the  previously  defined  field  as  an  event  time  attribute  p  e  g  field  rowtime    types  sql  timestamp  rowtime  public    schema  rowtime    rowtime  rowtime  if  last  field  null  throw  new    validation  exception    no  field  defined  previously    use  field  before  table  schema  get  last  field  put  all  rowtime  to  properties  last  field  null  return  this    converts  this  descriptor  into  a  set  of  properties    override  public    map    string    string  to  properties    descriptor  properties  properties  new    descriptor  properties    list    map    string    string  sub  key  values  new    array  list  for    map    entry    string    linked  hash  map    string    string  entry  table  schema  entry  set    string  name  entry  get  key    linked  hash  map    string    string  props  entry  get  value    map    string    string  map  new    hash  map  map  put  schema  name  name  map  put  all  props  sub  key  values  add  map  properties  put  indexed  variable  properties  schema  sub  key  values  return  properties  as  map  
public  evolving  public  abstract  class    table  descriptor  d  extends    table  descriptor  d  extends    descriptor  base  private  final    connector  descriptor  connector  descriptor  private    nullable    format  descriptor  format  descriptor  private    nullable    string  update  mode  protected    table  descriptor    connector  descriptor  connector  descriptor  this  connector  descriptor    preconditions  check  not  null  connector  descriptor    connector  must  not  be  null    specifies  the  format  that  defines  how  to  read  data  from  a  connector    suppress  warnings  unchecked  public  d  with  format    format  descriptor  format  format  descriptor    preconditions  check  not  null  format    format  must  not  be  null  return  d  this    declares  how  to  perform  the  conversion  between  a  dynamic  table  and  an  external  connector  p    in  append  mode  a  dynamic  table  and  an  external  connector  only  exchange  insert  messages  see  in  retract  mode  see  in  upsert  mode    suppress  warnings  unchecked  public  d  in  append  mode  update  mode  update  mode  value  append  return  d  this    declares  how  to  perform  the  conversion  between  a  dynamic  table  and  an  external  connector  p    in  retract  mode  a  dynamic  table  and  an  external  connector  exchange  add  and  retract  messages  p    an  insert  change  is  encoded  as  an  add  message  a  delete  change  as  a  retract  message  and  an  update  change  as  a  retract  message  for  the  updated  previous  row  and  an  add  message  for  the  updating  new  row  p    in  this  mode  a  key  must  not  be  defined  as  opposed  to  upsert  mode    however  every  update  consists  of  two  messages  which  is  less  efficient  see  in  append  mode  see  in  upsert  mode    suppress  warnings  unchecked  public  d  in  retract  mode  update  mode  update  mode  value  retract  return  d  this    declares  how  to  perform  the  conversion  between  a  dynamic  table  and  an  external  connector  p    in  upsert  mode  a  dynamic  table  and  an  external  connector  exchange  upsert  and  delete  messages  p    this  mode  requires  a  possibly  composite  unique  key  by  which  updates  can  be  propagated    the  external  connector  needs  to  be  aware  of  the  unique  key  attribute  in  order  to  apply  messages  correctly  insert  and  update  changes  are  encoded  as  upsert  messages  delete  changes  as  delete  messages  p    the  main  difference  to  a  retract  stream  is  that  update  changes  are  encoded  with  a  single  message  and  are  therefore  more  efficient  see  in  append  mode  see  in  retract  mode    suppress  warnings  unchecked  public  d  in  upsert  mode  update  mode  update  mode  value  upsert  return  d  this    converts  this  descriptor  into  a  set  of  properties    override  public  final    map    string    string  to  properties  final    descriptor  properties  properties  new    descriptor  properties  this  performs  only  basic  validation  more  validation  can  only  happen  within  a  factory  if  connector  descriptor  is  format  needed  format  descriptor  null  throw  new    validation  exception    string  format    the  connector  s  requires  a  format  description  connector  descriptor  get  class  get  name  else  if  connector  descriptor  is  format  needed  format  descriptor  null  throw  new    validation  exception    string  format    the  connector  s  does  not  require  a  format  description  but  s  found  connector  descriptor  get  class  get  name  format  descriptor  get  class  get  name  properties  put  properties  connector  descriptor  to  properties  if  format  descriptor  null  properties  put  properties  format  descriptor  to  properties  if  update  mode  null  properties  put  string  update  mode  update  mode  properties  put  properties  additional  properties  return  properties  as  map    enables  adding  more  specific  properties  to  link  to  properties  protected    map    string    string  additional  properties  return    collections  empty  map  
public  evolving  public  final  class    call  expression  implements    resolved  expression  private  final    nullable    function  identifier  function  identifier  private  final    function  definition  function  definition  private  final    list    resolved  expression  args  private  final    data  type  data  type  public    call  expression    function  identifier  function  identifier    function  definition  function  definition    list    resolved  expression  args    data  type  data  type  this  function  identifier    preconditions  check  not  null  function  identifier    object  identifier  must  not  be  null  this  function  definition    preconditions  check  not  null  function  definition    function  definition  must  not  be  null  this  args  new    array  list    preconditions  check  not  null  args    arguments  must  not  be  null  this  data  type    preconditions  check  not  null  data  type    data  type  must  not  be  null  public    call  expression    function  definition  function  definition    list    resolved  expression  args    data  type  data  type  this  function  identifier  null  this  function  definition    preconditions  check  not  null  function  definition    function  definition  must  not  be  null  this  args  new    array  list    preconditions  check  not  null  args    arguments  must  not  be  null  this  data  type    preconditions  check  not  null  data  type    data  type  must  not  be  null  public    optional    function  identifier  get  function  identifier  return    optional  of  nullable  function  identifier  public    function  definition  get  function  definition  return  function  definition    override  public    data  type  get  output  data  type  return  data  type    override  public    list    resolved  expression  get  resolved  children  return  args    override  public    string  as  summary  string  final    string  function  name  if  function  identifier  null  function  name  function  definition  to  string  else  function  name  function  identifier  as  summary  string  final    string  arg  list  args  stream  map    expression  as  summary  string  collect    collectors  joining  return  function  name  arg  list    override  public    list    expression  get  children  return    collections  unmodifiable  list  this  args    override  public  r  r  accept    expression  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    call  expression  that    call  expression  o  return    objects  equals  function  identifier  that  function  identifier  function  definition  equals  that  function  definition  args  equals  that  args  data  type  equals  that  data  type    override  public  int  hash  code  return    objects  hash  function  identifier  function  definition  args  data  type    override  public    string  to  string  return  as  summary  string  
public  evolving  public  interface    expression    returns  a  string  that  summarizes  this  expression  for  printing  to  a  console    an  implementation  might  skip  very  specific  properties  return  summary  string  of  this  expression  for  debugging  purposes    string  as  summary  string    list    expression  get  children  r  r  accept    expression  visitor  r  visitor  
public  evolving  public  interface    expression  visitor  r  resolved  expressions  r  visit    call  expression  call  r  visit    value  literal  expression  value  literal  r  visit    field  reference  expression  field  reference  r  visit    type  literal  expression  type  literal  other  expressions  r  visit    expression  other  
public  evolving  public  final  class    field  reference  expression  implements    resolved  expression  private  final    string  name  private  final    data  type  data  type  index  of  an  input  the  field  belongs  to  e  g  for  a  join  input  index  of  left  input  is    and  input  index  of  right  input  is    private  final  int  input  index  index  of  a  field  within  the  corresponding  input  private  final  int  field  index  public    field  reference  expression    string  name    data  type  data  type  int  input  index  int  field  index    preconditions  check  argument  input  index      index  of  input  should  be  a  positive  number    preconditions  check  argument  field  index      index  of  field  should  be  a  positive  number  this  name    preconditions  check  not  null  name    field  name  must  not  be  null  this  data  type    preconditions  check  not  null  data  type    field  data  type  must  not  be  null  this  input  index  input  index  this  field  index  field  index  public    string  get  name  return  name  public  int  get  input  index  return  input  index  public  int  get  field  index  return  field  index    override  public    data  type  get  output  data  type  return  data  type    override  public    list    resolved  expression  get  resolved  children  return    collections  empty  list    override  public    string  as  summary  string  return  name    override  public    list    expression  get  children  return    collections  empty  list    override  public  r  r  accept    expression  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    field  reference  expression  that    field  reference  expression  o  return  name  equals  that  name  data  type  equals  that  data  type  input  index  that  input  index  field  index  that  field  index    override  public  int  hash  code  return    objects  hash  name  data  type  input  index  field  index    override  public    string  to  string  return  as  summary  string  
public  evolving  public  interface    resolved  expression  extends    expression    returns  a  string  that  fully  serializes  this  instance    the  serialized  string  can  be  used  for  storing  the  query  in  for  example  a  link  org  apache  flink  table  catalog    catalog  as  a  view  return  detailed  string  for  persisting  in  a  catalog  default    string  as  serializable  string  throw  new    unsupported  operation  exception    expressions  are  not  string  serializable  for  now    returns  the  data  type  of  the  computation  result    data  type  get  output  data  type    list    resolved  expression  get  resolved  children  
public  evolving  public  class    resolved  field  reference  private  final    string  name  private  final    type  information  result  type  private  final  int  field  index  public    resolved  field  reference    string  name    type  information  result  type  int  field  index    preconditions  check  argument  field  index      index  of  field  should  be  a  positive  number  this  name    preconditions  check  not  null  name    field  name  must  not  be  null  this  result  type    preconditions  check  not  null  result  type    field  result  type  must  not  be  null  this  field  index  field  index  public    type  information  result  type  return  result  type  public    string  name  return  name  public  int  field  index  return  field  index  
public  evolving  public  interface    table  symbol  
public  evolving  public  enum    time  interval  unit  implements    table  symbol  year  year  to  month  quarter  month  week  day  day  to  hour  day  to  minute  day  to  second  hour  second  hour  to  minute  hour  to  second  minute  minute  to  second  
public  evolving  public  enum    time  point  unit  implements    table  symbol  year  month  day  hour  minute  second  quarter  week  millisecond  microsecond  
public  evolving  public  final  class    type  literal  expression  implements    resolved  expression  private  final    data  type  data  type  public    type  literal  expression    data  type  data  type  this  data  type    preconditions  check  not  null  data  type    data  type  must  not  be  null    override  public    data  type  get  output  data  type  return  data  type    override  public    list    resolved  expression  get  resolved  children  return    collections  empty  list    override  public    string  as  summary  string  return  data  type  to  string    override  public    list    expression  get  children  return    collections  empty  list    override  public  r  r  accept    expression  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    type  literal  expression  that    type  literal  expression  o  return  data  type  equals  that  data  type    override  public  int  hash  code  return    objects  hash  data  type    override  public    string  to  string  return  as  summary  string  
public  evolving  public  final  class    value  literal  expression  implements    resolved  expression  private  final    nullable    object  value  private  final    data  type  data  type  public    value  literal  expression    nonnull    object  value  this  value  derive  data  type  from  value  value  public    value  literal  expression    nullable    object  value    data  type  data  type  validate  value  data  type  value    preconditions  check  not  null  data  type    data  type  must  not  be  null  this  value  value  can  be  null  this  data  type  data  type  public  boolean  is  null  return  value  null    returns  the  value  excluding  null  as  an  instance  of  the  given  class  p    it  supports  conversions  to  default  conversion  classes  of  link    logical  type    logical  types  and  additionally  to  link    big  decimal  for  all  types  of  link    logical  type  family  numeric    this  method  should  not  be  called  with  other  classes  p    note  to  implementers    whenever  we  add  a  new  class  here  make  sure  to  also  update  the  planner  for  supporting  the  class  via  link    call  context  get  argument  value  int    class    suppress  warnings  unchecked  public  t    optional  t  get  value  as    class  t  clazz    preconditions  check  argument  clazz  is  primitive  if  value  null  return    optional  empty    object  converted  value  null  if  clazz  is  instance  value  converted  value  clazz  cast  value  else    class  value  class  value  get  class  if  clazz    period  class  converted  value  convert  to  period  value  value  class  else  if  clazz    duration  class  converted  value  convert  to  duration  value  value  class  else  if  clazz    local  date  class  converted  value  convert  to  local  date  value  value  class  else  if  clazz    local  time  class  converted  value  convert  to  local  time  value  value  class  else  if  clazz    local  date  time  class  converted  value  convert  to  local  date  time  value  value  class  else  if  clazz    offset  date  time  class  converted  value  convert  to  offset  date  time  value  value  class  else  if  clazz    instant  class  converted  value  convert  to  instant  value  value  class  else  if  clazz    big  decimal  class  converted  value  convert  to  big  decimal  value  return    optional  of  nullable  t  converted  value  private    nullable    local  date  convert  to  local  date    object  value    class  value  class  if  value  class  java  sql    date  class  return    date  value  to  local  date  else  if  value  class    integer  class  return    local  date  of  epoch  day  int  value  return  null  private    nullable    local  time  convert  to  local  time    object  value    class  value  class  if  value  class  java  sql    time  class  return    time  value  to  local  time  else  if  value  class    integer  class  return    local  time  of  nano  of  day  int  value        000  l  else  if  value  class    long  class  return    local  time  of  nano  of  day  long  value  return  null  private    nullable    local  date  time  convert  to  local  date  time    object  value    class  value  class  if  value  class  java  sql    timestamp  class  return    timestamp  value  to  local  date  time  return  null  private    nullable    offset  date  time  convert  to  offset  date  time    object  value    class  value  class  if  value  class    zoned  date  time  class  return    zoned  date  time  value  to  offset  date  time  return  null  private    nullable    instant  convert  to  instant    object  value    class  value  class  if  value  class    integer  class  return    instant  of  epoch  second  int  value  else  if  value  class    long  class  return    instant  of  epoch  milli  long  value  return  null  private    nullable    duration  convert  to  duration    object  value    class  value  class  if  value  class    long  class  final    long  long  value    long  value  return    duration  of  millis  long  value  return  null  private    nullable    period  convert  to  period    object  value    class  value  class  if  value  class    integer  class  final    integer  integer    integer  value  return    period  of  months  integer  return  null  private    nullable    big  decimal  convert  to  big  decimal    object  value  if    number  class  is  assignable  from  value  get  class  return  new    big  decimal    string  value  of  value  return  null    override  public    data  type  get  output  data  type  return  data  type    override  public    list    resolved  expression  get  resolved  children  return    collections  empty  list    override  public    string  as  summary  string  return  stringify  value  value    override  public    list    expression  get  children  return    collections  empty  list    override  public  r  r  accept    expression  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    value  literal  expression  that    value  literal  expression  o  return    objects  deep  equals  value  that  value  data  type  equals  that  data  type    override  public  int  hash  code  return    objects  hash  value  data  type    override  public    string  to  string  return  as  summary  string  private  static    data  type  derive  data  type  from  value    object  value  return    value  data  type  converter  extract  data  type  value  or  else  throw  new    validation  exception    cannot  derive  a  data  type  for  value  value    the  data  type  must  be  specified  explicitly  private  static  void  validate  value  data  type    object  value    data  type  data  type  final    logical  type  logical  type  data  type  get  logical  type  if  value  null  if  logical  type  is  nullable  throw  new    validation  exception    string  format    data  type  s  does  not  support  null  values  data  type  return  if  logical  type  is  nullable  throw  new    validation  exception    literals  that  have  a  non  null  value  must  not  have  a  nullable  data  type  final    class  candidate  value  get  class  ensure  value  and  data  type  match  if  data  type  get  conversion  class  is  assignable  from  candidate  throw  new    validation  exception    string  format    data  type  s  with  conversion  class  s  does  not  support  a  value  literal  of  class  s  data  type  data  type  get  conversion  class  get  name  value  get  class  get  name  check  for  proper  input  as  this  cannot  be  checked  in  data  type  if  logical  type  supports  input  conversion  candidate  throw  new    validation  exception    string  format    data  type  s  does  not  support  a  conversion  from  class  s  data  type  candidate  get  name    supports  nested  arrays  and  makes  string  values  more  explicit  private  static    string  stringify  value    object  value  if  value  instanceof    string  final    string  array    string  value  return    stream  of  array  map    value  literal  expression  stringify  value  collect    collectors  joining  else  if  value  instanceof    object  final    object  array    object  value  return    stream  of  array  map    value  literal  expression  stringify  value  collect    collectors  joining  else  if  value  instanceof    string  return    string  value  replace  return    string  utils  array  aware  to  string  value  
public  evolving  public  interface    catalog  factory  extends    table  factory    creates  and  configures  a  link    catalog  using  the  given  properties  param  properties  normalized  properties  describing  an  external  catalog  return  the  configured  catalog    catalog  create  catalog    string  name    map    string    string  properties  
public  evolving  public  interface    decoding  format  factory  i  extends    factory    creates  a  format  from  the  given  context  and  format  options  p    the  format  options  have  been  projected  to  top  level  options  e  g  from  code  key  format  ignore  errors  to  code  format  ignore  errors    decoding  format  i  create  decoding  format    dynamic  table  factory    context  context    readable  config  format  options  
public  evolving  public  interface    deserialization  format  factory  extends    decoding  format  factory    deserialization  schema    row  data  interface  is  used  for  discovery  but  is  already  fully  specified  by  the  generics  
public  evolving  public  interface    deserialization  schema  factory  t  extends    table  format  factory  t    creates  and  configures  a  link    deserialization  schema  using  the  given  properties  param  properties  normalized  properties  describing  the  format  return  the  configured  serialization  schema  or  null  if  the  factory  cannot  provide  an  instance  of  this  class    deserialization  schema  t  create  deserialization  schema    map    string    string  properties  
public  evolving  public  interface    dynamic  table  factory  extends    factory    provides  catalog  and  session  information  describing  the  dynamic  table  to  be  accessed  interface    context    returns  the  identifier  of  the  table  in  the  link    catalog    object  identifier  get  object  identifier    returns  table  information  received  from  the  link    catalog    catalog  table  get  catalog  table    gives  read  only  access  to  the  configuration  of  the  current  session    readable  config  get  configuration    returns  the  class  loader  of  the  current  session  p    the  class  loader  is  in  particular  useful  for  discovering  further  nested  factories    class  loader  get  class  loader  
public  evolving  public  interface    dynamic  table  sink  factory  extends    dynamic  table  factory    creates  a  link    dynamic  table  sink  instance  from  a  link    catalog  table  and  additional  context  information  p    an  implementation  should  perform  validation  and  the  discovery  of  further  nested  factories  in  this  method    dynamic  table  sink  create  dynamic  table  sink    context  context  
public  evolving  public  interface    dynamic  table  source  factory  extends    dynamic  table  factory    creates  a  link    dynamic  table  source  instance  from  a  link    catalog  table  and  additional  context  information  p    an  implementation  should  perform  validation  and  the  discovery  of  further  nested  factories  in  this  method    dynamic  table  source  create  dynamic  table  source    context  context  
public  evolving  public  interface    encoding  format  factory  i  extends    factory    creates  a  format  from  the  given  context  and  format  options  p    the  format  options  have  been  projected  to  top  level  options  e  g  from  code  key  format  ignore  errors  to  code  format  ignore  errors    encoding  format  i  create  encoding  format    dynamic  table  factory    context  context    readable  config  format  options  
public  evolving  public  interface    factory    returns  a  unique  identifier  among  same  factory  interfaces  p    for  consistency  an  identifier  should  be  declared  as  one  lower  case  word  e  g  code  kafka    if  multiple  factories  exist  for  different  versions  a  version  should  be  appended  using  e  g  code  kafka  0.10    string  factory  identifier    returns  a  set  of  link    config  option  that  an  implementation  of  this  factory  requires  in  addition  to  link  optional  options  p    see  the  documentation  of  link    factory  for  more  information    set    config  option  required  options    returns  a  set  of  link    config  option  that  an  implementation  of  this  factory  consumes  in  addition  to  link  required  options  p    see  the  documentation  of  link    factory  for  more  information    set    config  option  optional  options  
public  evolving  public  final  class    factory  util  private  static  final    logger  log    logger  factory  get  logger    factory  util  class  public  static  final    config  option    integer  property  version    config  options  key  property  version  int  type  default  value    with  description    version  of  the  overall  property  design    this  option  is  meant  for  future  backwards  compatibility  public  static  final    config  option    string  connector    config  options  key  connector  string  type  no  default  value  with  description    uniquely  identifies  the  connector  of  a  dynamic  table  that  is  used  for  accessing  data  in  an  external  system    its  value  is  used  during  table  source  and  table  sink  discovery  public  static  final    config  option    string  key  format    config  options  key  key  format  string  type  no  default  value  with  description    defines  the  format  identifier  for  encoding  key  data    the  identifier  is  used  to  discover  a  suitable  format  factory  public  static  final    config  option    string  value  format    config  options  key  value  format  string  type  no  default  value  with  description    defines  the  format  identifier  for  encoding  value  data    the  identifier  is  used  to  discover  a  suitable  format  factory  public  static  final    config  option    string  format    config  options  key  format  string  type  no  default  value  with  description    defines  the  format  identifier  for  encoding  data    the  identifier  is  used  to  discover  a  suitable  format  factory  private  static  final    string  format  key  format  private  static  final    string  format  suffix  format    creates  a  link    dynamic  table  source  from  a  link    catalog  table  p    it  considers  link    catalog  get  factory  if  provided  public  static    dynamic  table  source  create  table  source    nullable    catalog  catalog    object  identifier  object  identifier    catalog  table  catalog  table    readable  config  configuration    class  loader  class  loader  final    default  dynamic  table  context  context  new    default  dynamic  table  context  object  identifier  catalog  table  configuration  class  loader  try  final    dynamic  table  source  factory  factory  get  dynamic  table  factory    dynamic  table  source  factory  class  catalog  context  return  factory  create  dynamic  table  source  context  catch    throwable  t  throw  new    validation  exception    string  format    unable  to  create  a  source  for  reading  table  s  n  n    table  options  are  n  n  s  object  identifier  as  summary  string  catalog  table  get  options  entry  set  stream  map  e  stringify  option  e  get  key  e  get  value  sorted  collect    collectors  joining  n  t    creates  a  link    dynamic  table  sink  from  a  link    catalog  table  p    it  considers  link    catalog  get  factory  if  provided  public  static    dynamic  table  sink  create  table  sink    nullable    catalog  catalog    object  identifier  object  identifier    catalog  table  catalog  table    readable  config  configuration    class  loader  class  loader  final    default  dynamic  table  context  context  new    default  dynamic  table  context  object  identifier  catalog  table  configuration  class  loader  try  final    dynamic  table  sink  factory  factory  get  dynamic  table  factory    dynamic  table  sink  factory  class  catalog  context  return  factory  create  dynamic  table  sink  context  catch    throwable  t  throw  new    validation  exception    string  format    unable  to  create  a  sink  for  writing  table  s  n  n    table  options  are  n  n  s  object  identifier  as  summary  string  catalog  table  get  options  entry  set  stream  map  e  stringify  option  e  get  key  e  get  value  sorted  collect    collectors  joining  n  t    creates  a  utility  that  helps  in  discovering  formats  and  validating  all  options  for  a  link    dynamic  table  factory  p    the  following  example  sketches  the  usage  pre  code  in  create  dynamic  table  source  helper    factory  util  create  table  factory  helper  this  context  key  format  helper  discover  scan  format    deserialization  format  factory  class  key  format  value  format  helper  discover  scan  format    deserialization  format  factory  class  value  format  helper  validate  construct  connector  with  discovered  formats  pre  p    note    the  format  option  parameter  of  code  helper  discover  scan  format  format  factory  class  format  option  and  code  helper  discover  sink  format  format  factory  class  format  option  must  be  format  or  with  format  suffix  e  g  link  format  link  key  format  and  link  value  format    the  discovery  logic  will  replace  format  with  the  factory  identifier  value  as  the  format  prefix    for  example  assuming  the  identifier  is  json  if  format  option  key  is  format  then  format  prefix  is  json    if  format  option  key  is  value  format  then  format  prefix  is  value  json    the  format  prefix  is  used  to  project  the  options  for  the  format  factory  p    note    this  utility  checks  for  left  over  options  in  the  final  step  public  static    table  factory  helper  create  table  factory  helper    dynamic  table  factory  factory    dynamic  table  factory    context  context  return  new    table  factory  helper  factory  context    discovers  a  factory  using  the  given  factory  base  class  and  identifier  p    this  method  is  meant  for  cases  where  link  create  table  factory  helper    dynamic  table  factory    dynamic  table  factory    context  link  create  table  source    catalog    object  identifier    catalog  table    readable  config    class  loader  and  link  create  table  sink    catalog    object  identifier    catalog  table    readable  config    class  loader  are  not  applicable    suppress  warnings  unchecked  public  static  t  extends    factory  t  discover  factory    class  loader  class  loader    class  t  factory  class    string  factory  identifier  final    list    factory  factories  discover  factories  class  loader  final    list    factory  found  factories  factories  stream  filter  f  factory  class  is  assignable  from  f  get  class  collect    collectors  to  list  if  found  factories  is  empty  throw  new    validation  exception    string  format    could  not  find  any  factories  that  implement  s  in  the  classpath  factory  class  get  name  final    list    factory  matching  factories  found  factories  stream  filter  f  f  factory  identifier  equals  factory  identifier  collect    collectors  to  list  if  matching  factories  is  empty  throw  new    validation  exception    string  format    could  not  find  any  factory  for  identifier  s  that  implements  s  in  the  classpath  n  n    available  factory  identifiers  are  n  n  s  factory  identifier  factory  class  get  name  found  factories  stream  map    factory  factory  identifier  sorted  collect    collectors  joining  n  if  matching  factories  size    throw  new    validation  exception    string  format    multiple  factories  for  identifier  s  that  implement  s  found  in  the  classpath  n  n    ambiguous  factory  classes  are  n  n  s  factory  identifier  factory  class  get  name  found  factories  stream  map  f  factories  get  class  get  name  sorted  collect    collectors  joining  n  return  t  matching  factories  get      validates  the  required  and  optional  link    config  option  s  of  a  factory  p    note    it  does  not  check  for  left  over  options  public  static  void  validate  factory  options    factory  factory    readable  config  options  currently    flink  s  options  have  no  validation  feature  which  is  why  we  access  them  eagerly  to  provoke  a  parsing  error  final    list    string  missing  required  options  factory  required  options  stream  filter  option  read  option  options  option  null  map    config  option  key  sorted  collect    collectors  to  list  if  missing  required  options  is  empty  throw  new    validation  exception    string  format    one  or  more  required  options  are  missing  n  n    missing  required  options  are  n  n  s    string  join  n  missing  required  options  factory  optional  options  for  each  option  read  option  options  option    helper  methods    suppress  warnings  unchecked  private  static  t  extends    dynamic  table  factory  t  get  dynamic  table  factory    class  t  factory  class    nullable    catalog  catalog    default  dynamic  table  context  context  catalog  factory  has  highest  precedence  if  catalog  null  final    factory  factory  catalog  get  factory  filter  f  factory  class  is  assignable  from  f  get  class  or  else  null  if  factory  null  return  t  factory  fallback  to  factory  discovery  final    string  connector  option  context  get  catalog  table  get  options  get  connector  key  if  connector  option  null  throw  new    validation  exception    string  format    table  options  do  not  contain  an  option  key  s  for  discovering  a  connector  connector  key  try  return  discover  factory  context  get  class  loader  factory  class  connector  option  catch    validation  exception  e  throw  new    validation  exception    string  format    cannot  discover  a  connector  using  option  s  stringify  option  connector  key  connector  option  e  private  static    list    factory  discover  factories    class  loader  class  loader  try  final    list    factory  result  new    linked  list    service  loader  load    factory  class  class  loader  iterator  for  each  remaining  result  add  return  result  catch    service  configuration  error  e  log  error    could  not  load  service  provider  for  factories  e  throw  new    table  exception    could  not  load  service  provider  for  factories  e  private  static    string  stringify  option    string  key    string  value  return    string  format  s  s    encoding  utils  escape  single  quotes  key    encoding  utils  escape  single  quotes  value  private  static    configuration  as  configuration    map    string    string  options  final    configuration  configuration  new    configuration  options  for  each  configuration  set  string  return  configuration  private  static  t  t  read  option    readable  config  options    config  option  t  option  try  return  options  get  option  catch    throwable  t  throw  new    validation  exception    string  format    invalid  value  for  option  s  option  key  t    helper  classes    helper  utility  for  discovering  formats  and  validating  all  options  for  a  link    dynamic  table  factory  see  create  table  factory  helper    dynamic  table  factory    dynamic  table  factory    context  public  static  class    table  factory  helper  private  final    dynamic  table  factory  table  factory  private  final    dynamic  table  factory    context  context  private  final    configuration  all  options  private  final    set    string  consumed  option  keys  private    table  factory  helper    dynamic  table  factory  table  factory    dynamic  table  factory    context  context  this  table  factory  table  factory  this  context  context  this  all  options  as  configuration  context  get  catalog  table  get  options  this  consumed  option  keys  new    hash  set  this  consumed  option  keys  add  property  version  key  this  consumed  option  keys  add  connector  key  this  consumed  option  keys  add  all  table  factory  required  options  stream  map    config  option  key  collect    collectors  to  set  this  consumed  option  keys  add  all  table  factory  optional  options  stream  map    config  option  key  collect    collectors  to  set    discovers  a  link    decoding  format  of  the  given  type  using  the  given  option  as  factory  identifier  public  i  f  extends    decoding  format  factory  i    decoding  format  i  discover  decoding  format    class  f  format  factory  class    config  option    string  format  option  return  discover  optional  decoding  format  format  factory  class  format  option  or  else  throw  new    validation  exception    string  format    could  not  find  required  scan  format  s  format  option  key    discovers  a  link    decoding  format  of  the  given  type  using  the  given  option  if  present  as  factory  identifier  public  i  f  extends    decoding  format  factory  i    optional    decoding  format  i  discover  optional  decoding  format    class  f  format  factory  class    config  option    string  format  option  return  discover  optional  format  factory  format  factory  class  format  option  map  format  factory    string  format  prefix  format  prefix  format  factory  format  option  try  return  format  factory  create  decoding  format  context  project  options  format  prefix  catch    throwable  t  throw  new    validation  exception    string  format    error  creating  scan  format  s  in  option  space  s  format  factory  factory  identifier  format  prefix  t    discovers  a  link    encoding  format  of  the  given  type  using  the  given  option  as  factory  identifier  public  i  f  extends    encoding  format  factory  i    encoding  format  i  discover  encoding  format    class  f  format  factory  class    config  option    string  format  option  return  discover  optional  encoding  format  format  factory  class  format  option  or  else  throw  new    validation  exception    string  format    could  not  find  required  sink  format  s  format  option  key    discovers  a  link    encoding  format  of  the  given  type  using  the  given  option  if  present  as  factory  identifier  public  i  f  extends    encoding  format  factory  i    optional    encoding  format  i  discover  optional  encoding  format    class  f  format  factory  class    config  option    string  format  option  return  discover  optional  format  factory  format  factory  class  format  option  map  format  factory    string  format  prefix  format  prefix  format  factory  format  option  try  return  format  factory  create  encoding  format  context  project  options  format  prefix  catch    throwable  t  throw  new    validation  exception    string  format    error  creating  sink  format  s  in  option  space  s  format  factory  factory  identifier  format  prefix  t    validates  the  options  of  the  link    dynamic  table  factory    it  checks  for  unconsumed  option  keys  public  void  validate  validate  factory  options  table  factory  all  options  final    set    string  remaining  option  keys  new    hash  set  all  options  key  set  remaining  option  keys  remove  all  consumed  option  keys  if  remaining  option  keys  size    throw  new    validation  exception    string  format    unsupported  options  found  for  connector  s  n  n    unsupported  options  n  n  s  n  n    supported  options  n  n  s  table  factory  factory  identifier  remaining  option  keys  stream  sorted  collect    collectors  joining  n  consumed  option  keys  stream  sorted  collect    collectors  joining  n    validates  the  options  of  the  link    dynamic  table  factory    it  checks  for  unconsumed  option  keys  while  ignoring  the  options  with  given  prefixes  p    the  option  keys  that  have  given  prefix  code  prefix  to  skip  would  just  be  skipped  for  validation  param  prefixes  to  skip    set  of  option  key  prefixes  to  skip  validation  public  void  validate  except    string  prefixes  to  skip    preconditions  check  argument  prefixes  to  skip  length      prefixes  to  skip  can  not  be  empty  final    list    string  prefixes  list    arrays  as  list  prefixes  to  skip  consumed  option  keys  add  all  all  options  key  set  stream  filter  key  prefixes  list  stream  any  match  key  starts  with  collect    collectors  to  set  validate    returns  all  options  of  the  table  public    readable  config  get  options  return  all  options  private  f  extends    factory    optional  f  discover  optional  format  factory    class  f  format  factory  class    config  option    string  format  option  final    string  identifier  all  options  get  format  option  if  identifier  null  return    optional  empty  final  f  factory  discover  factory  context  get  class  loader  format  factory  class  identifier    string  format  prefix  format  prefix  factory  format  option  log  all  used  options  of  other  factories  consumed  option  keys  add  all  factory  required  options  stream  map    config  option  key  map  k  format  prefix  k  collect    collectors  to  set  consumed  option  keys  add  all  factory  optional  options  stream  map    config  option  key  map  k  format  prefix  k  collect    collectors  to  set  return    optional  of  factory  private    string  format  prefix    factory  format  factory    config  option    string  format  option    string  identifier  format  factory  factory  identifier  if  format  option  key  equals  format  key  return  identifier  else  if  format  option  key  ends  with  format  suffix  extract  the  key  prefix  e  g  extract  key  from  key  format    string  key  prefix  format  option  key  substring    format  option  key  length  format  suffix  length  return  key  prefix  identifier  else  throw  new    validation  exception    format  identifier  key  should  be  format  or  suffix  with  format  don  t  support  format  identifier  key  format  option  key  private    readable  config  project  options    string  format  prefix  return  new    delegating  configuration  all  options  format  prefix  private  static  class    default  dynamic  table  context  implements    dynamic  table  factory    context  private  final    object  identifier  object  identifier  private  final    catalog  table  catalog  table  private  final    readable  config  configuration  private  final    class  loader  class  loader    default  dynamic  table  context    object  identifier  object  identifier    catalog  table  catalog  table    readable  config  configuration    class  loader  class  loader  this  object  identifier  object  identifier  this  catalog  table  catalog  table  this  configuration  configuration  this  class  loader  class  loader    override  public    object  identifier  get  object  identifier  return  object  identifier    override  public    catalog  table  get  catalog  table  return  catalog  table    override  public    readable  config  get  configuration  return  configuration    override  public    class  loader  get  class  loader  return  class  loader  private    factory  util  no  instantiation  
public  evolving  public  interface    module  factory  extends    table  factory    creates  and  configures  a  link    module  using  the  given  properties  param  properties  normalized  properties  describing  a  module  return  the  configured  module    module  create  module    map    string    string  properties  
public  evolving  public  interface    serialization  format  factory  extends    encoding  format  factory    serialization  schema    row  data  interface  is  used  for  discovery  but  is  already  fully  specified  by  the  generics  
public  evolving  public  interface    serialization  schema  factory  t  extends    table  format  factory  t    creates  and  configures  a    serialization  schema  using  the  given  properties  param  properties  normalized  properties  describing  the  format  return  the  configured  serialization  schema  or  null  if  the  factory  cannot  provide  an  instance  of  this  class    serialization  schema  t  create  serialization  schema    map    string    string  properties  
public  evolving  public  interface    table  factory    specifies  the  context  that  this  factory  has  been  implemented  for    the  framework  guarantees  to  only  match  for  this  factory  if  the  specified  set  of  properties  and  values  are  met  p    typical  properties  might  be  connector  type  format  type  p    specified  property  versions  allow  the  framework  to  provide  backwards  compatible  properties  in  case  of  string  format  changes  connector  property  version  format  property  version  p    an  empty  context  means  that  the  factory  matches  for  all  requests    map    string    string  required  context    list  of  property  keys  that  this  factory  can  handle    this  method  will  be  used  for  validation    if  a  property  is  passed  that  this  factory  cannot  handle  an  exception  will  be  thrown    the  list  must  not  contain  the  keys  that  are  specified  by  the  context  p    example  properties  might  be  schema  type  schema  name  connector  topic  format  line  delimiter  format  ignore  parse  errors  format  fields  type  format  fields  name  p    note    use  to  denote  an  array  of  values  where  represents  one  or  more  digits    property  versions  like  format  property  version  must  not  be  part  of  the  supported  properties  p    in  some  cases  it  might  be  useful  to  declare  wildcards    wildcards  can  only  be  declared  at  the  end  of  a  property  key  p    for  example  if  an  arbitrary  format  should  be  supported  format  p    note    wildcards  should  be  used  with  caution  as  they  might  swallow  unsupported  properties  and  thus  might  lead  to  undesired  behavior    list    string  supported  properties  
public  evolving  public  interface    table  format  factory  t  extends    table  factory    flag  to  indicate  if  the  given  format  supports  deriving  information  from  a  schema    if  the  format  can  handle  schema  information  those  properties  must  be  added  to  the  list  of  supported  properties  boolean  supports  schema  derivation    list  of  format  property  keys  that  this  factory  can  handle    this  method  will  be  used  for  validation    if  a  property  is  passed  that  this  factory  cannot  handle  an  exception  will  be  thrown    the  list  must  not  contain  the  keys  that  are  specified  by  the  context  p    example  format  properties  might  be  format  line  delimiter  format  ignore  parse  errors  format  fields  type  format  fields  name  p    if  schema  derivation  is  enabled  the  list  must  include  schema  properties  schema  name  schema  type  p    note    all  supported  format  properties  must  be  prefixed  with  format    if  schema  derivation  is  enabled  also  properties  with  schema  prefix  can  be  used  p    use  to  denote  an  array  of  values  where  represents  one  or  more  digits    property  versions  like  format  property  version  must  not  be  part  of  the  supported  properties  p    see  also  link    table  factory  supported  properties  for  more  information    list    string  supported  properties  
public  evolving  public  abstract  class    table  format  factory  base  t  implements    table  format  factory  t    constants  for  schema  derivation  todo  drop  constants  once    schema  validator  has  been  ported  to  flink  table  common  private  static  final    string  schema  schema  private  static  final    string  schema  name  name  private  static  final    string  schema  data  type  data  type  deprecated  link  schema  type  will  be  removed  in  future  version  as  it  uses  old  type  system    please  use  link  schema  data  type  instead    deprecated  private  static  final    string  schema  type  type  private  static  final    string  schema  proctime  proctime  private  static  final    string  schema  from  from  private  static  final    string  rowtime  timestamps  type  rowtime  timestamps  type  private  static  final    string  rowtime  timestamps  type  value  from  field  from  field  private  static  final    string  rowtime  timestamps  from  rowtime  timestamps  from  private  static  final    string  rowtime  timestamps  class  rowtime  timestamps  class  private  static  final    string  rowtime  timestamps  serialized  rowtime  timestamps  serialized  private  static  final    string  rowtime  watermarks  type  rowtime  watermarks  type  private  static  final    string  rowtime  watermarks  class  rowtime  watermarks  class  private  static  final    string  rowtime  watermarks  serialized  rowtime  watermarks  serialized  private  static  final    string  rowtime  watermarks  delay  rowtime  watermarks  delay  private    string  type  private    string  version  private  boolean  supports  schema  derivation  public    table  format  factory  base    string  type  int  version  boolean  supports  schema  derivation  this  type  type  this  version    integer  to  string  version  this  supports  schema  derivation  supports  schema  derivation    override  public  final    map    string    string  required  context  final    map    string    string  context  new    hash  map  context  put    format  descriptor  validator  format  type  type  context  put    format  descriptor  validator  format  property  version  version  context  put  all  required  format  context  return  context    override  public  final  boolean  supports  schema  derivation  return  supports  schema  derivation    override  public  final    list    string  supported  properties  final    list    string  properties  new    array  list  if  supports  schema  derivation  properties  add    format  descriptor  validator  format  derive  schema  schema  properties  add  schema  schema  data  type  properties  add  schema  schema  type  properties  add  schema  schema  name  properties  add  schema  schema  from  computed  column  properties  add  schema  expr  time  attributes  properties  add  schema  schema  proctime  properties  add  schema  rowtime  timestamps  type  properties  add  schema  rowtime  timestamps  from  properties  add  schema  rowtime  timestamps  class  properties  add  schema  rowtime  timestamps  serialized  properties  add  schema  rowtime  watermarks  type  properties  add  schema  rowtime  watermarks  class  properties  add  schema  rowtime  watermarks  serialized  properties  add  schema  rowtime  watermarks  delay  watermark  properties  add  schema  watermark  watermark  rowtime  properties  add  schema  watermark  watermark  strategy  expr  properties  add  schema  watermark  watermark  strategy  data  type  table  constraint  properties  add  schema    descriptor  properties  primary  key  name  properties  add  schema    descriptor  properties  primary  key  columns  properties  add  all  supported  format  properties  return  properties    format  specific  context  p    this  method  can  be  used  if  format  type  and  a  property  version  is  not  enough  protected    map    string    string  required  format  context  return    collections  empty  map    format  specific  supported  properties  p    this  method  can  be  used  if  schema  derivation  is  not  enough  protected    list    string  supported  format  properties  return    collections  empty  list    finds  the  table  schema  that  can  be  used  for  a  format  schema  without  time  attributes  and  generated  columns  public  static    table  schema  derive  schema    map    string    string  properties  final    descriptor  properties  descriptor  properties  new    descriptor  properties  descriptor  properties  put  properties  properties  final    table  schema    builder  builder    table  schema  builder  final    table  schema  table  schema  descriptor  properties  get  table  schema  schema  for  int  i    i  table  schema  get  field  count  i  final    table  column  table  column  table  schema  get  table  columns  get  i  final    string  field  name  table  column  get  name  final    data  type  data  type  table  column  get  type  final  boolean  is  generated  column  table  column  is  generated  if  is  generated  column  skip  generated  column  continue  final  boolean  is  proctime  descriptor  properties  get  optional  boolean  schema  i  schema  proctime  or  else  false  final    string  timestamp  key  schema  i  rowtime  timestamps  type  final  boolean  is  rowtime  descriptor  properties  contains  key  timestamp  key  if  is  proctime  is  rowtime  check  for  aliasing  final    string  alias  name  descriptor  properties  get  optional  string  schema  i  schema  from  or  else  field  name  builder  field  alias  name  data  type  only  use  the  rowtime  attribute  if  it  references  a  field  else  if  is  rowtime  descriptor  properties  is  value  timestamp  key  rowtime  timestamps  type  value  from  field  final    string  alias  name  descriptor  properties  get  string  schema  i  rowtime  timestamps  from  builder  field  alias  name  data  type  return  builder  build  
public  evolving  public  interface    table  sink  factory  t  extends    table  factory    creates  and  configures  a  link    table  sink  using  the  given  properties  param  properties  normalized  properties  describing  a  table  sink  return  the  configured  table  sink  deprecated  link    context  contains  more  information  and  already  contains  table  schema  too    please  use  link  create  table  sink    context  instead    deprecated  default    table  sink  t  create  table  sink    map    string    string  properties  return  null    creates  and  configures  a  link    table  sink  based  on  the  given  link    catalog  table  instance  param  table  path  path  of  the  given  link    catalog  table  param  table  link    catalog  table  instance  return  the  configured  table  sink  deprecated  link    context  contains  more  information  and  already  contains  table  schema  too    please  use  link  create  table  sink    context  instead    deprecated  default    table  sink  t  create  table  sink    object  path  table  path    catalog  table  table  return  create  table  sink  table  to  properties    creates  and  configures  a  link    table  sink  based  on  the  given  link    context  param  context  context  of  this  table  sink  return  the  configured  table  sink  default    table  sink  t  create  table  sink    context  context  return  create  table  sink  context  get  object  identifier  to  object  path  context  get  table    context  of  table  sink  creation    contains  table  information  and  environment  information  interface    context  return  full  identifier  of  the  given  link    catalog  table    object  identifier  get  object  identifier  return  table  link    catalog  table  instance    catalog  table  get  table  return  readable  config  of  this  table  environment    the  configuration  gives  the  ability  to  access  code    table  config  get  configuration  which  holds  the  current  code    table  environment  session  configurations    readable  config  get  configuration    it  depends  on  whether  the  code    table  environment  execution  mode  is  batch  p    in  the  future  the  new  sink  interface  will  infer  from  input  to  source    see  link    source  get  boundedness  boolean  is  bounded  
public  evolving  public  interface    table  source  factory  t  extends    table  factory    creates  and  configures  a  link    table  source  using  the  given  properties  param  properties  normalized  properties  describing  a  table  source  return  the  configured  table  source  deprecated  link    context  contains  more  information  and  already  contains  table  schema  too    please  use  link  create  table  source    context  instead    deprecated  default    table  source  t  create  table  source    map    string    string  properties  return  null    creates  and  configures  a  link    table  source  based  on  the  given  link    catalog  table  instance  param  table  path  path  of  the  given  link    catalog  table  param  table  link    catalog  table  instance  return  the  configured  table  source  deprecated  link    context  contains  more  information  and  already  contains  table  schema  too    please  use  link  create  table  source    context  instead    deprecated  default    table  source  t  create  table  source    object  path  table  path    catalog  table  table  return  create  table  source  table  to  properties    creates  and  configures  a  link    table  source  based  on  the  given  link    context  param  context  context  of  this  table  source  return  the  configured  table  source  default    table  source  t  create  table  source    context  context  return  create  table  source  context  get  object  identifier  to  object  path  context  get  table    context  of  table  source  creation    contains  table  information  and  environment  information  interface    context  return  full  identifier  of  the  given  link    catalog  table    object  identifier  get  object  identifier  return  table  link    catalog  table  instance    catalog  table  get  table  return  readable  config  of  this  table  environment    the  configuration  gives  the  ability  to  access  code    table  config  get  configuration  which  holds  the  current  code    table  environment  session  configurations    readable  config  get  configuration  
public  evolving  public  abstract  class    aggregate  function  t  acc  extends    user  defined  aggregate  function  t  acc    called  every  time  when  an  aggregation  result  should  be  materialized    the  returned  value  could  be  either  an  early  and  incomplete  result  periodically  emitted  as  data  arrive  or  the  final  result  of  the  aggregation  param  accumulator  the  accumulator  which  contains  the  current  aggregated  results  return  the  aggregation  result  public  abstract  t  get  value  acc  accumulator    returns  code  true  code  if  this  link    aggregate  function  can  only  be  applied  in  an  over  window  return  code  true  code  if  the  link    aggregate  function  requires  an  over  window  code  false  code  otherwise  deprecated    use  link  get  requirements  instead    deprecated  public  boolean  requires  over  return  false    override  public  final    function  kind  get  kind  return    function  kind  aggregate    override  public    type  inference  get  type  inference    data  type  factory  type  factory  throw  new    table  exception    aggregate  functions  are  not  updated  to  the  new  type  system  yet    override  public    set    function  requirement  get  requirements  final    hash  set    function  requirement  requirements  new    hash  set  if  requires  over  requirements  add    function  requirement  over  window  only  return    collections  unmodifiable  set  requirements  
public  evolving  public  final  class    built  in  function  definitions  logic  functions  public  static  final    built  in  function  definition  and  new    built  in  function  definition    builder  name  and  kind  scalar  input  type  strategy  varying  sequence  logical    logical  type  root  boolean  logical    logical  type  root  boolean  logical    logical  type  root  boolean  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  or  new    built  in  function  definition    builder  name  or  kind  scalar  input  type  strategy  varying  sequence  logical    logical  type  root  boolean  logical    logical  type  root  boolean  logical    logical  type  root  boolean  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  not  new    built  in  function  definition    builder  name  not  kind  scalar  input  type  strategy  sequence  logical    logical  type  root  boolean  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  if  new    built  in  function  definition    builder  name  if  then  else  kind  scalar  input  type  strategy  composite  sequence  argument  logical    logical  type  root  boolean  subsequence  common  type    finish  output  type  strategy  argument    build  comparison  functions  public  static  final    built  in  function  definition  equals  new    built  in  function  definition    builder  name  equals  kind  scalar  input  type  strategy  two  equals  comparable  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  greater  than  new    built  in  function  definition    builder  name  greater  than  kind  scalar  input  type  strategy  two  fully  comparable  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  greater  than  or  equal  new    built  in  function  definition    builder  name  greater  than  or  equal  kind  scalar  input  type  strategy  two  fully  comparable  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  less  than  new    built  in  function  definition    builder  name  less  than  kind  scalar  input  type  strategy  two  fully  comparable  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  less  than  or  equal  new    built  in  function  definition    builder  name  less  than  or  equal  kind  scalar  input  type  strategy  two  fully  comparable  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  not  equals  new    built  in  function  definition    builder  name  not  equals  kind  scalar  input  type  strategy  two  equals  comparable  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  is  null  new    built  in  function  definition    builder  name  is  null  kind  scalar  input  type  strategy  wildcard  with  count    constant  argument  count  of    output  type  strategy  explicit    data  types  boolean  not  null  build  public  static  final    built  in  function  definition  is  not  null  new    built  in  function  definition    builder  name  is  not  null  kind  scalar  input  type  strategy  wildcard  with  count    constant  argument  count  of    output  type  strategy  explicit    data  types  boolean  not  null  build  public  static  final    built  in  function  definition  is  true  new    built  in  function  definition    builder  name  is  true  kind  scalar  input  type  strategy  sequence  logical    logical  type  root  boolean  output  type  strategy  explicit    data  types  boolean  not  null  build  public  static  final    built  in  function  definition  is  false  new    built  in  function  definition    builder  name  is  false  kind  scalar  input  type  strategy  sequence  logical    logical  type  root  boolean  output  type  strategy  explicit    data  types  boolean  not  null  build  public  static  final    built  in  function  definition  is  not  true  new    built  in  function  definition    builder  name  is  not  true  kind  scalar  input  type  strategy  sequence  logical    logical  type  root  boolean  output  type  strategy  explicit    data  types  boolean  not  null  build  public  static  final    built  in  function  definition  is  not  false  new    built  in  function  definition    builder  name  is  not  false  kind  scalar  input  type  strategy  sequence  logical    logical  type  root  boolean  output  type  strategy  explicit    data  types  boolean  not  null  build  public  static  final    built  in  function  definition  between  new    built  in  function  definition    builder  name  between  kind  scalar  input  type  strategy  comparable    constant  argument  count  of      structured  comparision  full  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  not  between  new    built  in  function  definition    builder  name  not  between  kind  scalar  input  type  strategy  comparable    constant  argument  count  of      structured  comparision  full  output  type  strategy  nullable  explicit    data  types  boolean  build  aggregate  functions  public  static  final    built  in  function  definition  avg  new    built  in  function  definition    builder  name  avg  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  count  new    built  in  function  definition    builder  name  count  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  max  new    built  in  function  definition    builder  name  max  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  min  new    built  in  function  definition    builder  name  min  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  sum  new    built  in  function  definition    builder  name  sum  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition    s  u  m0  new    built  in  function  definition    builder  name  sum0  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  stddev  pop  new    built  in  function  definition    builder  name  stddev  pop  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  stddev  samp  new    built  in  function  definition    builder  name  stddev  samp  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  var  pop  new    built  in  function  definition    builder  name  var  pop  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  var  samp  new    built  in  function  definition    builder  name  var  samp  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  collect  new    built  in  function  definition    builder  name  collect  kind  aggregate  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  distinct  new    built  in  function  definition    builder  name  distinct  kind  aggregate  output  type  strategy    type  strategies  missing  build  string  functions  public  static  final    built  in  function  definition  char  length  new    built  in  function  definition    builder  name  char  length  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  int  build  public  static  final    built  in  function  definition  init  cap  new    built  in  function  definition    builder  name  init  cap  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  argument    build  public  static  final    built  in  function  definition  like  new    built  in  function  definition    builder  name  like  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  lower  new    built  in  function  definition    builder  name  lower  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  argument    build  we  need  lowercase  here  to  maintain  compatibility  for  the  string  based  expression  dsl  which  exposes  lower  as  lower  case  public  static  final    built  in  function  definition  lowercase  new    built  in  function  definition    builder  name  lower  case  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  argument    build  public  static  final    built  in  function  definition  similar  new    built  in  function  definition    builder  name  similar  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  boolean  build  public  static  final    built  in  function  definition  substring  new    built  in  function  definition    builder  name  substring  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  character  string  logical    logical  type  root  integer  sequence  logical    logical  type  family  character  string  logical    logical  type  root  integer  logical    logical  type  root  integer  output  type  strategy  nullable  varying  string  argument    build  public  static  final    built  in  function  definition  replace  new    built  in  function  definition    builder  name  replace  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  trim  new    built  in  function  definition    builder  name  trim  kind  scalar  input  type  strategy  sequence  logical    logical  type  root  boolean  logical    logical  type  root  boolean  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  varying  string  argument    build  public  static  final    built  in  function  definition  upper  new    built  in  function  definition    builder  name  upper  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  argument    build  we  need  uppercase  here  to  maintain  compatibility  for  the  string  based  expression  dsl  which  exposes  upper  as  upper  case  public  static  final    built  in  function  definition  uppercase  new    built  in  function  definition    builder  name  upper  case  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  argument    build  public  static  final    built  in  function  definition  position  new    built  in  function  definition    builder  name  position  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  int  build  public  static  final    built  in  function  definition  overlay  new    built  in  function  definition    builder  name  overlay  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  logical    logical  type  root  integer  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  logical    logical  type  root  integer  logical    logical  type  root  integer  output  type  strategy  nullable  string  concat  build  public  static  final    built  in  function  definition  concat  new    built  in  function  definition    builder  name  concat  kind  scalar  input  type  strategy  or  varying  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  varying  sequence  logical    logical  type  family  binary  string  logical    logical  type  family  binary  string  output  type  strategy  nullable  string  concat  build  public  static  final    built  in  function  definition  concat  ws  new    built  in  function  definition    builder  name  concat  ws  kind  scalar  input  type  strategy  varying  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  lpad  new    built  in  function  definition    builder  name  lpad  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  root  integer  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  rpad  new    built  in  function  definition    builder  name  rpad  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  root  integer  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  regexp  extract  new    built  in  function  definition    builder  name  regexp  extract  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  logical    logical  type  root  integer  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  from    b  a  s  e64  new    built  in  function  definition    builder  name  from  base64  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  to    b  a  s  e64  new    built  in  function  definition    builder  name  to  base64  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  uuid  new    built  in  function  definition    builder  name  uuid  kind  scalar  not  deterministic  input  type  strategy  no  args  output  type  strategy  explicit    data  types  char    not  null  build  public  static  final    built  in  function  definition  ltrim  new    built  in  function  definition    builder  name  ltrim  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  character  string  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  varying  string  argument    build  public  static  final    built  in  function  definition  rtrim  new    built  in  function  definition    builder  name  rtrim  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  character  string  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  varying  string  argument    build  public  static  final    built  in  function  definition  repeat  new    built  in  function  definition    builder  name  repeat  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  root  integer  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  regexp  replace  new    built  in  function  definition    builder  name  regexp  replace  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  family  character  string  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  string  build  math  functions    combines  numeric  addition  datetime  interval  interval  interval  arithmetic  and  string  concatenation  public  static  final    built  in  function  definition  plus  new    built  in  function  definition    builder  name  plus  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  logical    logical  type  family  numeric  sequence  logical    logical  type  root  interval  day  time  logical    logical  type  root  interval  day  time  sequence  logical    logical  type  root  interval  year  month  logical    logical  type  root  interval  year  month  sequence  logical    logical  type  family  datetime  logical    logical  type  family  interval  sequence  logical    logical  type  family  interval  logical    logical  type  family  datetime  sequence  logical    logical  type  family  character  string  logical    logical  type  family  predefined  output  type  strategy  nullable  first  decimal  plus  common  explicit    data  types  string  build    combines  numeric  subtraction  and  datetime  interval  arithmetic  public  static  final    built  in  function  definition  minus  new    built  in  function  definition    builder  name  minus  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  logical    logical  type  family  numeric  sequence  logical    logical  type  root  interval  day  time  logical    logical  type  root  interval  day  time  sequence  logical    logical  type  root  interval  year  month  logical    logical  type  root  interval  year  month  sequence  logical    logical  type  family  datetime  logical    logical  type  family  interval  output  type  strategy  nullable  first  decimal  plus  common  build  public  static  final    built  in  function  definition  divide  new    built  in  function  definition    builder  name  divide  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  logical    logical  type  family  numeric  sequence  logical    logical  type  family  interval  logical    logical  type  family  numeric  output  type  strategy  nullable  first  decimal  divide  match  family      logical  type  family  interval  common  build  public  static  final    built  in  function  definition  times  new    built  in  function  definition    builder  name  times  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  logical    logical  type  family  numeric  sequence  logical    logical  type  family  interval  logical    logical  type  family  numeric  sequence  logical    logical  type  family  numeric  logical    logical  type  family  interval  output  type  strategy  nullable  first  decimal  times  match  family      logical  type  family  interval  common  build  public  static  final    built  in  function  definition  abs  new    built  in  function  definition    builder  name  abs  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  sequence  logical    logical  type  family  interval  output  type  strategy  argument    build  public  static  final    built  in  function  definition  exp  new    built  in  function  definition    builder  name  exp  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  floor  new    built  in  function  definition    builder  name  floor  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  sequence  logical    logical  type  family  interval  sequence  logical    logical  type  family  datetime  logical    logical  type  root  symbol  output  type  strategy  nullable  first  decimal    s  c  a  l  e0  argument    build  public  static  final    built  in  function  definition  ceil  new    built  in  function  definition    builder  name  ceil  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  sequence  logical    logical  type  family  interval  sequence  logical    logical  type  family  datetime  logical    logical  type  root  symbol  output  type  strategy  nullable  first  decimal    s  c  a  l  e0  argument    build  public  static  final    built  in  function  definition    l  o  g10  new    built  in  function  definition    builder  name  log10  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition    l  o  g2  new    built  in  function  definition    builder  name  log2  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  ln  new    built  in  function  definition    builder  name  ln  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  log  new    built  in  function  definition    builder  name  log  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  sequence  logical    logical  type  family  numeric  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  power  new    built  in  function  definition    builder  name  power  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  mod  new    built  in  function  definition    builder  name  mod  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  exact  numeric  logical    logical  type  family  exact  numeric  output  type  strategy  nullable  first  decimal  mod  argument    build  public  static  final    built  in  function  definition  sqrt  new    built  in  function  definition    builder  name  sqrt  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  minus  prefix  new    built  in  function  definition    builder  name  minus  prefix  kind  scalar  input  type  strategy  or  sequence  output  if  null  sequence  logical    logical  type  family  numeric  sequence  logical    logical  type  family  interval  output  type  strategy  nullable  argument    build  public  static  final    built  in  function  definition  sin  new    built  in  function  definition    builder  name  sin  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  cos  new    built  in  function  definition    builder  name  cos  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  sinh  new    built  in  function  definition    builder  name  sinh  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  tan  new    built  in  function  definition    builder  name  tan  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  tanh  new    built  in  function  definition    builder  name  tanh  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  cot  new    built  in  function  definition    builder  name  cot  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  asin  new    built  in  function  definition    builder  name  asin  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  acos  new    built  in  function  definition    builder  name  acos  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  atan  new    built  in  function  definition    builder  name  atan  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition    a  t  a  n2  new    built  in  function  definition    builder  name  atan2  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  cosh  new    built  in  function  definition    builder  name  cosh  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  degrees  new    built  in  function  definition    builder  name  degrees  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  radians  new    built  in  function  definition    builder  name  radians  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  nullable  explicit    data  types  double  build  public  static  final    built  in  function  definition  sign  new    built  in  function  definition    builder  name  sign  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  numeric  output  type  strategy  argument    build  public  static  final    built  in  function  definition  round  new    built  in  function  definition    builder  name  round  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  sequence  logical    logical  type  family  numeric  logical    logical  type  root  integer  output  type  strategy  nullable    type  strategies  round  build  public  static  final    built  in  function  definition  pi  new    built  in  function  definition    builder  name  pi  kind  scalar  input  type  strategy  no  args  output  type  strategy  explicit    data  types  double  not  null  build  public  static  final    built  in  function  definition  e  new    built  in  function  definition    builder  name  e  kind  scalar  input  type  strategy  no  args  output  type  strategy  explicit    data  types  double  not  null  build  public  static  final    built  in  function  definition  rand  new    built  in  function  definition    builder  name  rand  kind  scalar  not  deterministic  input  type  strategy  or  no  args  sequence  logical    logical  type  root  integer  output  type  strategy  explicit    data  types  double  not  null  build  public  static  final    built  in  function  definition  rand  integer  new    built  in  function  definition    builder  name  rand  integer  kind  scalar  not  deterministic  input  type  strategy  or  sequence  logical    logical  type  root  integer  sequence  logical    logical  type  root  integer  logical    logical  type  root  integer  output  type  strategy  explicit    data  types  int  not  null  build  public  static  final    built  in  function  definition  bin  new    built  in  function  definition    builder  name  bin  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  integer  numeric  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  hex  new    built  in  function  definition    builder  name  hex  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  integer  numeric  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  string  build  public  static  final    built  in  function  definition  truncate  new    built  in  function  definition    builder  name  truncate  kind  scalar  input  type  strategy  or  sequence  logical    logical  type  family  numeric  sequence  logical    logical  type  family  numeric  logical    logical  type  root  integer  output  type  strategy  nullable  argument    build  time  functions  public  static  final    built  in  function  definition  extract  new    built  in  function  definition    builder  name  extract  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  current  date  new    built  in  function  definition    builder  name  current  date  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  current  time  new    built  in  function  definition    builder  name  current  time  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  current  timestamp  new    built  in  function  definition    builder  name  current  timestamp  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  local  time  new    built  in  function  definition    builder  name  local  time  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  local  timestamp  new    built  in  function  definition    builder  name  local  timestamp  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  temporal  overlaps  new    built  in  function  definition    builder  name  temporal  overlaps  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  date  format  new    built  in  function  definition    builder  name  date  format  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  timestamp  diff  new    built  in  function  definition    builder  name  timestamp  diff  kind  scalar  output  type  strategy    type  strategies  missing  build  collection  public  static  final    built  in  function  definition  at  new    built  in  function  definition    builder  name  at  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  cardinality  new    built  in  function  definition    builder  name  cardinality  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  array  new    built  in  function  definition    builder  name  array  kind  scalar  input  type  strategy    input  type  strategies  specific  for  array  output  type  strategy    type  strategies  array  build  public  static  final    built  in  function  definition  array  element  new    built  in  function  definition    builder  name  element  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  map  new    built  in  function  definition    builder  name  map  kind  scalar  input  type  strategy    input  type  strategies  specific  for  map  output  type  strategy    type  strategies  map  build  public  static  final    built  in  function  definition  row  new    built  in  function  definition    builder  name  row  kind  scalar  input  type  strategy    input  type  strategies  wildcard  with  count    constant  argument  count  from    output  type  strategy    type  strategies  row  build  composite  public  static  final    built  in  function  definition  flatten  new    built  in  function  definition    builder  name  flatten  kind  other  input  type  strategy  sequence    input  type  strategies  composite  output  type  strategy  call  context  throw  new    unsupported  operation  exception  flatten  should  be  resolved  to  get  expressions  build  public  static  final    built  in  function  definition  get  new    built  in  function  definition    builder  name  get  kind  other  input  type  strategy  sequence    input  type  strategies  composite  and    input  type  strategies  literal  or  logical    logical  type  root  integer  logical    logical  type  family  character  string  output  type  strategy    type  strategies  get  build  window  properties  public  static  final    built  in  function  definition  window  start  new    built  in  function  definition    builder  name  start  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  window  end  new    built  in  function  definition    builder  name  end  kind  other  output  type  strategy    type  strategies  missing  build  ordering  public  static  final    built  in  function  definition  order  asc  new    built  in  function  definition    builder  name  asc  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  order  desc  new    built  in  function  definition    builder  name  desc  kind  other  output  type  strategy    type  strategies  missing  build  crypto  hash  public  static  final    built  in  function  definition    m  d5  new    built  in  function  definition    builder  name  md5  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  char    build  public  static  final    built  in  function  definition    s  h  a1  new    built  in  function  definition    builder  name  sha1  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  char    build  public  static  final    built  in  function  definition    s  h  a224  new    built  in  function  definition    builder  name  sha224  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  char    build  public  static  final    built  in  function  definition    s  h  a256  new    built  in  function  definition    builder  name  sha256  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  char    build  public  static  final    built  in  function  definition    s  h  a384  new    built  in  function  definition    builder  name  sha384  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  char    build  public  static  final    built  in  function  definition    s  h  a512  new    built  in  function  definition    builder  name  sha512  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  output  type  strategy  nullable  explicit    data  types  char    build  public  static  final    built  in  function  definition    s  h  a2  new    built  in  function  definition    builder  name  sha2  kind  scalar  input  type  strategy  sequence  logical    logical  type  family  character  string  logical    logical  type  root  integer  output  type  strategy  nullable  explicit    data  types  varchar    build  time  attributes  public  static  final    built  in  function  definition  proctime  new    built  in  function  definition    builder  name  proctime  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  rowtime  new    built  in  function  definition    builder  name  rowtime  kind  other  output  type  strategy    type  strategies  missing  build  over  window  public  static  final    built  in  function  definition  over  new    built  in  function  definition    builder  name  over  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  unbounded  range  new    built  in  function  definition    builder  name  unbounded  range  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  unbounded  row  new    built  in  function  definition    builder  name  unbounded  row  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  current  range  new    built  in  function  definition    builder  name  current  range  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  current  row  new    built  in  function  definition    builder  name  current  row  kind  other  output  type  strategy    type  strategies  missing  build  columns  public  static  final    built  in  function  definition  with  columns  new    built  in  function  definition    builder  name  with  columns  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  without  columns  new    built  in  function  definition    builder  name  without  columns  kind  other  output  type  strategy    type  strategies  missing  build  etc  public  static  final    built  in  function  definition  in  new    built  in  function  definition    builder  name  in  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  cast  new    built  in  function  definition    builder  name  cast  kind  scalar  input  type  strategy  specific  for  cast  output  type  strategy  nullable    constant  argument  count  to      type  strategies  argument    build  public  static  final    built  in  function  definition  reinterpret  cast  new    built  in  function  definition    builder  name  reinterpret  cast  kind  scalar  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  as  new    built  in  function  definition    builder  name  as  kind  other  input  type  strategy  varying  sequence  or  output  if  null    input  type  strategies  any  and    input  type  strategies  literal  logical    logical  type  family  character  string  and    input  type  strategies  literal  logical    logical  type  family  character  string  output  type  strategy    type  strategies  argument    build  public  static  final    built  in  function  definition  stream  record  timestamp  new    built  in  function  definition    builder  name  stream  record  timestamp  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    built  in  function  definition  range  to  new    built  in  function  definition    builder  name  range  to  kind  other  output  type  strategy    type  strategies  missing  build  public  static  final    set    function  definition  window  properties  new    hash  set    arrays  as  list  window  start  window  end  proctime  rowtime  public  static  final    set    function  definition  time  attributes  new    hash  set    arrays  as  list  proctime  rowtime  public  static  final    list    function  definition  ordering    arrays  as  list  order  asc  order  desc  public  static    list    built  in  function  definition  get  definitions  final    field  fields    built  in  function  definitions  class  get  fields  final    list    built  in  function  definition  list  new    array  list  fields  length  for    field  field  fields  if    function  definition  class  is  assignable  from  field  get  type  try  final    built  in  function  definition  func  def    built  in  function  definition  field  get    built  in  function  definitions  class  list  add    preconditions  check  not  null  func  def  catch    illegal  access  exception  e  throw  new    table  exception    the  function  definition  for  field  field  get  name  is  not  accessible  e  return  list  
public  evolving  public  class    function  context  private    runtime  context  context    wraps  the  underlying  link    runtime  context  param  context  the  runtime  context  in  which    flink  s  link    function  is  executed  public    function  context    runtime  context  context  this  context  context    returns  the  metric  group  for  this  parallel  subtask  return  metric  group  for  this  parallel  subtask  public    metric  group  get  metric  group  return  context  get  metric  group    gets  the  local  temporary  file  copy  of  a  distributed  cache  files  param  name  distributed  cache  file  name  return  local  temporary  file  copy  of  a  distributed  cache  file  public    file  get  cached  file    string  name  return  context  get  distributed  cache  get  file  name    gets  the  global  job  parameter  value  associated  with  the  given  key  as  a  string  param  key  key  pointing  to  the  associated  value  param  default  value  default  value  which  is  returned  in  case  global  job  parameter  is  null  or  there  is  no  value  associated  with  the  given  key  return  default  value  associated  with  the  given  key  public    string  get  job  parameter    string  key    string  default  value  final    global  job  parameters  conf  context  get  execution  config  get  global  job  parameters  if  conf  null  conf  to  map  contains  key  key  return  conf  to  map  get  key  else  return  default  value    get  the  external  resource  information  public    set    external  resource  info  get  external  resource  infos    string  resource  name  return  context  get  external  resource  infos  resource  name  
public  evolving  public  interface    function  definition    returns  the  kind  of  function  this  definition  describes    function  kind  get  kind    returns  the  logic  for  performing  type  inference  of  a  call  to  this  function  definition  p    the  type  inference  process  is  responsible  for  inferring  unknown  types  of  input  arguments  validating  input  arguments  and  producing  result  types    the  type  inference  process  happens  independent  of  a  function  body    the  output  of  the  type  inference  is  used  to  search  for  a  corresponding  runtime  implementation  p    instances  of  type  inference  can  be  created  by  using  link    type  inference  new  builder  p    see  link    built  in  function  definitions  for  concrete  usage  examples    type  inference  get  type  inference    data  type  factory  type  factory    returns  the  set  of  requirements  this  definition  demands  default    set    function  requirement  get  requirements  return    collections  empty  set    returns  information  about  the  determinism  of  the  function  s  results  p    it  returns  code  true  code  if  and  only  if  a  call  to  this  function  is  guaranteed  to  always  return  the  same  result  given  the  same  parameters  code  true  code  is  assumed  by  default    if  the  function  is  not  pure  functional  like  code  random  date  now  code  this  method  must  return  code  false  code  default  boolean  is  deterministic  return  true  
public  evolving  public  final  class    function  identifier  implements    serializable  private  static  final  long  serial  version  u  i  d  1  l  private  final    nullable    object  identifier  object  identifier  private  final    nullable    string  function  name  public  static    function  identifier  of    object  identifier  oi  return  new    function  identifier  oi  public  static    function  identifier  of    string  function  name  return  new    function  identifier  function  name  private    function  identifier    object  identifier  object  identifier  check  not  null  object  identifier    object  identifier  cannot  be  null  this  object  identifier  object  identifier  this  function  name  null  private    function  identifier    string  function  name  check  argument    string  utils  is  null  or  whitespace  only  function  name  function  name  cannot  be  null  or  empty  string  this  function  name  function  name  this  object  identifier  null    normalize  a  function  name  public  static    string  normalize  name    string  name  return  name  to  lower  case    normalize  an  object  identifier  by  only  normalizing  the  function  name  public  static    object  identifier  normalize  object  identifier    object  identifier  oi  return    object  identifier  of  oi  get  catalog  name  oi  get  database  name  normalize  name  oi  get  object  name  public    optional    object  identifier  get  identifier  return    optional  of  nullable  object  identifier  public    optional    string  get  simple  name  return    optional  of  nullable  function  name    list  of  the  component  names  of  this  function  identifier  public    list    string  to  list  if  object  identifier  null  return  object  identifier  to  list  else  if  function  name  null  return    collections  singleton  list  function  name  else  throw  new    illegal  state  exception  function  name  and  object  identifier  are  both  null  which  should  never  happen    returns  a  string  that  summarizes  this  instance  for  printing  to  a  console  or  log  public    string  as  summary  string  if  object  identifier  null  return    string  join  object  identifier  get  catalog  name  object  identifier  get  database  name  object  identifier  get  object  name  else  return  function  name    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    function  identifier  that    function  identifier  o  return    objects  equals  object  identifier  that  object  identifier    objects  equals  function  name  that  function  name    override  public  int  hash  code  return    objects  hash  object  identifier  function  name    override  public    string  to  string  return  as  summary  string  
public  evolving  public  enum    function  kind  scalar  table  async  table  aggregate  table  aggregate  other  
public  evolving  public  enum    function  requirement    requirement  that  an  aggregate  function  can  only  be  applied  in  an  over  window  over  window  only  
public  evolving  public  abstract  class    scalar  function  extends    user  defined  function    returns  the  result  type  of  the  evaluation  method  with  a  given  signature  deprecated    this  method  uses  the  old  type  system  and  is  based  on  the  old  reflective  extraction  logic    the  method  will  be  removed  in  future  versions  and  is  only  called  when  using  the  deprecated  code    table  environment  register  function  method    the  new  reflective  extraction  logic  possibly  enriched  with  link    data  type  hint  and  link    function  hint  should  be  powerful  enough  to  cover  most  use  cases    for  advanced  users  it  is  possible  to  override  link    user  defined  function  get  type  inference    data  type  factory    deprecated  public    type  information  get  result  type    class  signature  return  null    returns  link    type  information  about  the  operands  of  the  evaluation  method  with  a  given  signature  deprecated    this  method  uses  the  old  type  system  and  is  based  on  the  old  reflective  extraction  logic    the  method  will  be  removed  in  future  versions  and  is  only  called  when  using  the  deprecated  code    table  environment  register  function  method    the  new  reflective  extraction  logic  possibly  enriched  with  link    data  type  hint  and  link    function  hint  should  be  powerful  enough  to  cover  most  use  cases    for  advanced  users  it  is  possible  to  override  link    user  defined  function  get  type  inference    data  type  factory    deprecated  public    type  information  get  parameter  types    class  signature  final    type  information  types  new    type  information  signature  length  for  int  i    i  signature  length  i  try  types  i    type  extractor  get  for  class  signature  i  catch    invalid  types  exception  e  throw  new    validation  exception    parameter  types  of  scalar  function  this  get  class  get  canonical  name  cannot  be  automatically  determined    please  provide  type  information  manually  return  types    override  public  final    function  kind  get  kind  return    function  kind  scalar    override  public    type  inference  get  type  inference    data  type  factory  type  factory  return    type  inference  extractor  for  scalar  function  type  factory  get  class  
public  evolving  public  abstract  class    table  aggregate  function  t  acc  extends    user  defined  aggregate  function  t  acc    collects  a  record  and  forwards  it    the  collector  can  output  retract  messages  with  the  retract  method    note  only  use  it  in  code  emit  update  with  retract  public  interface    retractable  collector  t  extends    collector  t    retract  a  record  param  record    the  record  to  retract  void  retract  t  record    override  public  final    function  kind  get  kind  return    function  kind  table  aggregate    override  public    type  inference  get  type  inference    data  type  factory  type  factory  throw  new    table  exception    table  aggregate  functions  are  not  updated  to  the  new  type  system  yet  
public  evolving  public  abstract  class    table  function  t  extends    user  defined  function    the  code  generated  collector  used  to  emit  rows  private    collector  t  collector    internal  use    sets  the  current  collector  public  final  void  set  collector    collector  t  collector  this  collector  collector    returns  the  result  type  of  the  evaluation  method  deprecated    this  method  uses  the  old  type  system  and  is  based  on  the  old  reflective  extraction  logic    the  method  will  be  removed  in  future  versions  and  is  only  called  when  using  the  deprecated  code    table  environment  register  function  method    the  new  reflective  extraction  logic  possibly  enriched  with  link    data  type  hint  and  link    function  hint  should  be  powerful  enough  to  cover  most  use  cases    for  advanced  users  it  is  possible  to  override  link    user  defined  function  get  type  inference    data  type  factory    deprecated  public    type  information  t  get  result  type  return  null    returns  link    type  information  about  the  operands  of  the  evaluation  method  with  a  given  signature  deprecated    this  method  uses  the  old  type  system  and  is  based  on  the  old  reflective  extraction  logic    the  method  will  be  removed  in  future  versions  and  is  only  called  when  using  the  deprecated  code    table  environment  register  function  method    the  new  reflective  extraction  logic  possibly  enriched  with  link    data  type  hint  and  link    function  hint  should  be  powerful  enough  to  cover  most  use  cases    for  advanced  users  it  is  possible  to  override  link    user  defined  function  get  type  inference    data  type  factory    deprecated  public    type  information  get  parameter  types    class  signature  final    type  information  types  new    type  information  signature  length  for  int  i    i  signature  length  i  try  types  i    type  extractor  get  for  class  signature  i  catch    invalid  types  exception  e  throw  new    validation  exception    parameter  types  of  table  function  this  get  class  get  canonical  name  cannot  be  automatically  determined    please  provide  type  information  manually  return  types    emits  an  implicit  or  explicit  output  row  p    if  null  is  emitted  as  an  explicit  row  it  will  be  skipped  by  the  runtime    for  implicit  rows  the  row  s  field  will  be  null  param  row  the  output  row  protected  final  void  collect  t  row  collector  collect  row    override  public  final    function  kind  get  kind  return    function  kind  table    override    suppress  warnings  unchecked  rawtypes  public    type  inference  get  type  inference    data  type  factory  type  factory  return    type  inference  extractor  for  table  function  type  factory    class  get  class  
public  evolving  public  abstract  class    temporal  table  function  extends    table  function    row  
public  evolving  public  abstract  class    user  defined  aggregate  function  t  acc  extends    user  defined  function    creates  and  initializes  the  accumulator  for  this  link    user  defined  aggregate  function    the  accumulator  is  used  to  keep  the  aggregated  values  which  are  needed  to  compute  an  aggregation  result  return  the  accumulator  with  the  initial  value  public  abstract  acc  create  accumulator    returns  the  link    type  information  of  the  link    user  defined  aggregate  function  s  result  return    the  link    type  information  of  the  link    user  defined  aggregate  function  s  result  or  code  null  code  if  the  result  type  should  be  automatically  inferred  public    type  information  t  get  result  type  return  null    returns  the  link    type  information  of  the  link    user  defined  aggregate  function  s  accumulator  return    the  link    type  information  of  the  link    user  defined  aggregate  function  s  accumulator  or  code  null  code  if  the  accumulator  type  should  be  automatically  inferred  public    type  information  acc  get  accumulator  type  return  null  
public  evolving  public  abstract  class    user  defined  function  implements    function  definition    serializable    returns  a  unique  serialized  representation  for  this  function  public  final    string  function  identifier  final    string  md5    encoding  utils  hex    encoding  utils  md5    encoding  utils  encode  object  to  string  this  return  get  class  get  name  replace  concat  concat  md5    setup  method  for  user  defined  function    it  can  be  used  for  initialization  work    by  default  this  method  does  nothing  public  void  open    function  context  context  throws    exception  do  nothing    tear  down  method  for  user  defined  function    it  can  be  used  for  clean  up  work    by  default  this  method  does  nothing  public  void  close  throws    exception  do  nothing  inherit  doc  p    the  type  inference  for  user  defined  functions  is  automatically  extracted  using  reflection    it  does  this  by  analyzing  implementation  methods  such  as  code  eval  or  accumulate  and  the  generic  parameters  of  a  function  class  if  present    if  the  reflective  information  is  not  sufficient  it  can  be  supported  and  enriched  with  link    data  type  hint  and  link    function  hint  annotations  p    note    overriding  this  method  is  only  recommended  for  advanced  users    if  a  custom  type  inference  is  specified  it  is  the  responsibility  of  the  implementer  to  make  sure  that  the  output  of  the  type  inference  process  matches  with  the  implementation  method  p    the  implementation  method  must  comply  with  each  link    data  type  get  conversion  class  returned  by  the  type  inference    for  example  if  code    data  types  timestamp    bridged  to  java  sql    timestamp  class  is  an  expected  argument  type  the  method  must  accept  a  call  code  eval  java  sql    timestamp  p    regular    java  calling  semantics  including  type  widening  and  autoboxing  are  applied  when  calling  an  implementation  method  which  means  that  the  signature  can  be  code  eval  java  lang    object  p    the  runtime  will  take  care  of  converting  the  data  to  the  data  format  specified  by  the  link    data  type  get  conversion  class  coming  from  the  type  inference  logic    override  public  abstract    type  inference  get  type  inference    data  type  factory  type  factory    returns  the  name  of  the  udf  that  is  used  for  plan  explanation  and  logging    override  public    string  to  string  return  get  class  get  simple  name  
public  evolving  public  interface    module    list  names  of  all  functions  in  this  module  return  a  set  of  function  names  default    set    string  list  functions  return    collections  empty  set    get  an  optional  of  link    function  definition  by  a  given  name  param  name  name  of  the  link    function  definition  return  an  optional  function  definition  default    optional    function  definition  get  function  definition    string  name  return    optional  empty  user  defined  types  operators  rules  etc  
public  evolving  public  final  class    column  stats  number  of  distinct  values  private  final    long  ndv  number  of  nulls  private  final    long  null  count  average  length  of  column  values  private  final    double  avg  len  max  length  of  column  values  private  final    integer  max  len    deprecated  because  not  well  supported  comparable  type  e  g  link  java  util    date  link  java  sql    timestamp    deprecated  private  final    number  max  value  max  value  of  column  values  null  if  the  value  is  unknown  or  not  comparable  private  final    comparable  max    deprecated  because  not  well  supported  comparable  type  e  g  link  java  util    date  link  java  sql    timestamp    deprecated  private  final    number  min  value  min  value  of  column  values  null  if  the  value  is  unknown  or  not  comparable  private  final    comparable  min    deprecated  because    number  type  max  min  is  not  well  supported  comparable  type  e  g  link  java  util    date  link  java  sql    timestamp  please  use  link    column  stats    builder  to  construct    column  stats  instance    deprecated  public    column  stats    long  ndv    long  null  count    double  avg  len    integer  max  len    number  max    number  min  this  ndv  ndv  this  null  count  null  count  this  avg  len  avg  len  this  max  len  max  len  this  max  value  max  this  min  value  min  this  max  null  this  min  null    private  because  to  avoid  cannot  resolve  constructor  error  please  use  link    column  stats    builder  to  construct    column  stats  instance  could  change  to  public  if  the  deprecated  constructor  is  removed  in  the  future  private    column  stats    long  ndv    long  null  count    double  avg  len    integer  max  len    comparable  max    comparable  min  this  ndv  ndv  this  null  count  null  count  this  avg  len  avg  len  this  max  len  max  len  this  max  max  this  min  min  this  max  value  null  this  min  value  null  public    long  get  ndv  return  ndv  public    long  get  null  count  return  null  count  public    double  get  avg  len  return  avg  len  public    integer  get  max  len  return  max  len    deprecated  because    number  type  max  min  is  not  well  supported  comparable  type  e  g  link  java  util    date  link  java  sql    timestamp  p    returns  null  if  this  instance  is  constructed  by  link    column  stats    builder    deprecated  public    number  get  max  value  return  max  value    returns  null  if  this  instance  is  constructed  by  link    column  stats    column  stats    long    long    double    integer    number    number  public    comparable  get  max  return  max    deprecated  because    number  type  max  min  is  not  well  supported  comparable  type  e  g  link  java  util    date  link  java  sql    timestamp  p    returns  null  if  this  instance  is  constructed  by  link    column  stats    builder    deprecated  public    number  get  min  value  return  min  value    returns  null  if  this  instance  is  constructed  by  link    column  stats    column  stats    long    long    double    integer    number    number  public    comparable  get  min  return  min  public    string  to  string    list    string  column  stats  new    array  list  if  ndv  null  column  stats  add  ndv  ndv  if  null  count  null  column  stats  add  null  count  null  count  if  avg  len  null  column  stats  add  avg  len  avg  len  if  max  len  null  column  stats  add  max  len  max  len  if  max  null  column  stats  add  max  max  if  max  value  null  column  stats  add  max  max  value  if  min  null  column  stats  add  min  min  if  min  value  null  column  stats  add  min  min  value    string  column  stats  str    string  join  column  stats  return    column  stats  column  stats  str    create  a  deep  copy  of  this  instance  return  a  deep  copy  public    column  stats  copy  if  max  value  null  min  value  null  return  new    column  stats  this  ndv  this  null  count  this  avg  len  this  max  len  this  max  value  this  min  value  else  return  new    column  stats  this  ndv  this  null  count  this  avg  len  this  max  len  this  max  this  min    merges  two  column  stats    when  the  stats  are  unknown  whatever  the  other  are  we  need  return  unknown  stats    the  unknown  definition  for  column  stats  is  null  param  other    the  other  column  stats  to  merge  return    the  merged  column  stats  public    column  stats  merge    column  stats  other    long  ndv  combine  if  non  null    long  sum  this  ndv  other  ndv    long  null  count  combine  if  non  null    long  sum  this  null  count  other  null  count    double  avg  len  combine  if  non  null  a1  a2  a1  a2    this  avg  len  other  avg  len    integer  max  len  combine  if  non  null    math  max  this  max  len  other  max  len    number  max  value  combine  if  non  null  n1  n2  n1  double  value  n2  double  value  n1  n2  this  max  value  other  max  value    number  min  value  combine  if  non  null  n1  n2  n1  double  value  n2  double  value  n1  n2  this  min  value  other  min  value    suppress  warnings  unchecked    comparable  max  combine  if  non  null  c1  c2    comparable  c1  compare  to  c2    c1  c2  this  max  other  max    suppress  warnings  unchecked    comparable  min  combine  if  non  null  c1  c2    comparable  c1  compare  to  c2    c1  c2  this  min  other  min  if  max  null  min  null  return  new    column  stats  ndv  null  count  avg  len  max  len  max  min  else  return  new    column  stats  ndv  null  count  avg  len  max  len  max  value  min  value    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    column  stats  that    column  stats  o  return    objects  equals  ndv  that  ndv    objects  equals  null  count  that  null  count    objects  equals  avg  len  that  avg  len    objects  equals  max  len  that  max  len    objects  equals  max  value  that  max  value    objects  equals  max  that  max    objects  equals  min  value  that  min  value    objects  equals  min  that  min    override  public  int  hash  code  return    objects  hash  ndv  null  count  avg  len  max  len  max  value  max  min  value  min  private  static  t  t  combine  if  non  null    binary  operator  t  op  t  t1  t  t2  if  t1  null  t2  null  return  null  return  op  apply  t1  t2    column  stats  builder  public  static  class    builder  private    long  ndv  null  private    long  null  count  null  private    double  avg  len  null  private    integer  max  len  null  private    comparable  max  private    comparable  min  public  static    builder  builder  return  new    builder  public    builder  set  ndv    long  ndv  this  ndv  ndv  return  this  public    builder  set  null  count    long  null  count  this  null  count  null  count  return  this  public    builder  set  avg  len    double  avg  len  this  avg  len  avg  len  return  this  public    builder  set  max  len    integer  max  len  this  max  len  max  len  return  this  public    builder  set  max    comparable  max  this  max  max  return  this  public    builder  set  min    comparable  min  this  min  min  return  this  public    column  stats  build  return  new    column  stats  ndv  null  count  avg  len  max  len  max  min  
public  evolving  public  final  class    table  stats    unknown  definition  for  table  stats    unknown  link  row  count  is      unknown  link  col  stats  is  not  exist  in  map  public  static  final    table  stats  unknown  new    table  stats    new    hash  map  cardinality  of  table  private  final  long  row  count  col  stats  statistics  of  table  columns  private  final    map    string    column  stats  col  stats  public    table  stats  long  row  count  this  row  count  new    hash  map  public    table  stats  long  row  count    map    string    column  stats  col  stats  this  row  count  row  count  this  col  stats  col  stats  public  long  get  row  count  return  row  count  public    map    string    column  stats  get  column  stats  return  col  stats    create  a  deep  copy  of  this  instance  return  a  deep  copy  public    table  stats  copy    table  stats  copy  new    table  stats  this  row  count  for    map    entry    string    column  stats  entry  this  col  stats  entry  set  copy  col  stats  put  entry  get  key  entry  get  value  copy  return  copy    merges  two  table  stats    when  the  stats  are  unknown  whatever  the  other  are  we  need  return  unknown  stats    see  link  unknown  param  other    the  other  table  stats  to  merge  return    the  merged  table  stats    nonnull  public    table  stats  merge    table  stats  other    map    string    column  stats  col  stats  new    hash  map  for    map    entry    string    column  stats  entry  this  col  stats  entry  set    string  col  entry  get  key    column  stats  stats  entry  get  value    column  stats  other  stats  other  col  stats  get  col  if  other  stats  null  col  stats  put  col  stats  merge  other  stats  return  new    table  stats  this  row  count    other  row  count    this  row  count  other  row  count  unknown  row  count  col  stats    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    table  stats  that    table  stats  o  return  row  count  that  row  count    objects  equals  col  stats  that  col  stats    override  public  int  hash  code  return    objects  hash  row  count  col  stats    override  public    string  to  string  return    table  stats  row  count  row  count  col  stats  col  stats  
public  evolving  public  interface    table  sink  t    returns  the  data  type  consumed  by  this  link    table  sink  return    the  data  type  expected  by  this  link    table  sink  default    data  type  get  consumed  data  type  final    type  information  t  legacy  type  get  output  type  if  legacy  type  null  throw  new    table  exception    table  sink  does  not  implement  a  consumed  data  type  return  from  legacy  info  to  data  type  legacy  type  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    it  is  recommended  to  use  link  get  consumed  data  type  instead  which  uses  the  new  type  system  based  on  link    data  types    please  make  sure  to  use  either  the  old  or  the  new  type  system  consistently  to  avoid  unintended  behavior    see  the  website  documentation  for  more  information    deprecated  default    type  information  t  get  output  type  return  null    returns  the  schema  of  the  consumed  table  return    the  link    table  schema  of  the  consumed  table  default    table  schema  get  table  schema  final    string  field  names  get  field  names  final    type  information  legacy  field  types  get  field  types  if  field  names  null  legacy  field  types  null  throw  new    table  exception    table  sink  does  not  implement  a  table  schema  return  new    table  schema  field  names  legacy  field  types  deprecated    use  the  field  names  of  link  get  table  schema  instead    deprecated  default    string  get  field  names  return  null  deprecated    use  the  field  types  of  link  get  table  schema  instead    deprecated  default    type  information  get  field  types  return  null    returns  a  copy  of  this  link    table  sink  configured  with  the  field  names  and  types  of  the  table  to  emit  param  field  names    the  field  names  of  the  table  to  emit  param  field  types    the  field  types  of  the  table  to  emit  return  a  copy  of  this  link    table  sink  configured  with  the  field  names  and  types  of  the  table  to  emit  deprecated    this  method  will  be  dropped  in  future  versions    it  is  recommended  to  pass  a  static  schema  when  instantiating  the  sink  instead    deprecated    table  sink  t  configure    string  field  names    type  information  field  types  
public  evolving  public  interface    defined  field  mapping    returns  the  mapping  for  the  fields  of  the  link    table  source  s  link    table  schema  to  the  fields  of  its  produced  link    data  type  p    the  mapping  is  done  based  on  field  names  e  g  a  mapping  name  f1  maps  the  schema  field  name  to  the  field  f1  of  the  produced  data  type  for  example  in  this  case  the  second  field  of  a  link  org  apache  flink  api  java  tuple    tuple  p    the  returned  mapping  must  map  all  fields  except  proctime  and  rowtime  fields  to  the  produced  data  type    it  can  also  provide  a  mapping  for  fields  which  are  not  in  the  link    table  schema  to  make  fields  in  the  physical  link    data  type  accessible  for  a  code    timestamp  extractor  return  a  mapping  from  link    table  schema  fields  to  link    data  type  fields  or  null  if  no  mapping  is  necessary    nullable    map    string    string  get  field  mapping  
public  evolving  public  interface    defined  proctime  attribute    returns  the  name  of  a  processing  time  attribute  or  null  if  no  processing  time  attribute  is  present  p    the  referenced  attribute  must  be  present  in  the  link    table  schema  of  the  link    table  source  and  of  type  link    types  sql  timestamp    nullable    string  get  proctime  attribute  
public  evolving  public  interface    defined  rowtime  attributes    returns  a  list  of  link    rowtime  attribute  descriptor  for  all  rowtime  attributes  of  the  table  p    all  referenced  attributes  must  be  present  in  the  link    table  schema  of  the  link    table  source  and  of  type  link    types  sql  timestamp  return  a  list  of  link    rowtime  attribute  descriptor    list    rowtime  attribute  descriptor  get  rowtime  attribute  descriptors  
public  evolving  public  interface    field  computer  t    returns  the  names  of  all  fields  that  the  expression  of  the  field  computer  accesses  return    an  array  with  the  names  of  all  accessed  fields    string  get  argument  fields    returns  the  result  type  of  the  expression  return    the  result  type  of  the  expression    type  information  t  get  return  type    validates  that  the  fields  that  the  expression  references  have  the  correct  types  param  argument  field  types    the  types  of  the  physical  input  fields  void  validate  argument  fields    type  information  argument  field  types    returns  the  link    expression  that  computes  the  value  of  the  field  param  field  accesses    field  access  expressions  for  the  argument  fields  return    the  expression  to  extract  the  timestamp  from  the  link    table  source  return  type    expression  get  expression    resolved  field  reference  field  accesses  
public  evolving  public  interface    nested  fields  projectable  table  source  t    creates  a  copy  of  the  link    table  source  that  projects  its  output  to  the  given  field  indexes    the  field  indexes  relate  to  the  physical  produced  data  type  link    table  source  get  produced  data  type  and  not  to  the  table  schema  link    table  source  get  table  schema  of  the  link    table  source  p    the  table  schema  link    table  source  get  table  schema  of  the  link    table  source  copy  must  not  be  modified  by  this  method  but  only  the  produced  data  type  link    table  source  get  produced  data  type  and  the  produced  code    data  set  code    batch  table  source  get  data  set  or  code    data  stream  code    stream  table  source  get  data  stream    the  produced  data  type  may  only  be  changed  by  removing  or  reordering  first  level  fields    the  type  of  the  first  level  fields  must  not  be  changed  p    if  the  link    table  source  implements  the  link    defined  field  mapping  interface  it  might  be  necessary  to  adjust  the  mapping  as  well  p    the  code  nested  fields  parameter  contains  all  nested  fields  that  are  accessed  by  the  query    this  information  can  be  used  to  only  read  and  set  the  accessed  fields    non  accessed  fields  may  be  left  empty  set  to  null  or  to  a  default  value  p    this  method  is  called  with  parameters  as  shown  in  the  example  below  pre  code  schema  table  schema  id  student  school  city  tuition  age  name  teacher  age  name  query  select  id  student  school  city  student  age  teacher  parameters  fields  field        nested  fields  school  city  age  pre  p  important    this  method  must  return  a  true  copy  and  must  not  modify  the  original  table  source  object  param  fields    the  indexes  of  the  fields  to  return  param  nested  fields    the  paths  of  all  nested  fields  which  are  accessed  by  the  query    all  other  nested  fields  may  be  empty  return  a  copy  of  the  link    table  source  that  projects  its  output    table  source  t  project  nested  fields  int  fields    string  nested  fields  
public  evolving  public  interface    projectable  table  source  t    creates  a  copy  of  the  link    table  source  that  projects  its  output  to  the  given  field  indexes    the  field  indexes  relate  to  the  physical  poduced  data  type  link    table  source  get  produced  data  type  and  not  to  the  table  schema  link    table  source  get  table  schema  of  the  link    table  source  p    the  table  schema  link    table  source  get  table  schema  of  the  link    table  source  copy  must  not  be  modified  by  this  method  but  only  the  produced  data  type  link    table  source  get  produced  data  type  and  the  produced  code    data  set  code    batch  table  source  get  data  set  or  code    data  stream  code    stream  table  source  get  data  stream  p    if  the  link    table  source  implements  the  link    defined  field  mapping  interface  it  might  be  necessary  to  adjust  the  mapping  as  well  p  important    this  method  must  return  a  true  copy  and  must  not  modify  the  original  table  source  object  param  fields    the  indexes  of  the  fields  to  return  return  a  copy  of  the  link    table  source  that  projects  its  output    table  source  t  project  fields  int  fields  
public  evolving  public  interface    table  source  t    returns  the  link    data  type  for  the  produced  data  of  the  link    table  source  return    the  data  type  of  the  returned  code    data  set  or  code    data  stream  default    data  type  get  produced  data  type  final    type  information  t  legacy  type  get  return  type  if  legacy  type  null  throw  new    table  exception    table  source  does  not  implement  a  produced  data  type  return  from  legacy  info  to  data  type  legacy  type  deprecated    this  method  will  be  removed  in  future  versions  as  it  uses  the  old  type  system    it  is  recommended  to  use  link  get  produced  data  type  instead  which  uses  the  new  type  system  based  on  link    data  types    please  make  sure  to  use  either  the  old  or  the  new  type  system  consistently  to  avoid  unintended  behavior    see  the  website  documentation  for  more  information    deprecated  default    type  information  t  get  return  type  return  null    returns  the  schema  of  the  produced  table  return    the  link    table  schema  of  the  produced  table  deprecated    table  schema  is  a  logical  description  of  a  table  and  should  not  be  part  of  the  physical    table  source    define  schema  when  registering  a    table  either  in  ddl  or  in  code    table  environment  connect    deprecated    table  schema  get  table  schema    describes  the  table  source  return  a    string  explaining  the  link    table  source  default    string  explain  source  return    table  connector  utils  generate  runtime  name  get  class  get  table  schema  get  field  names  
public  evolving  public  abstract  class    timestamp  extractor  implements    field  computer    long    serializable    descriptor    override  public    type  information    long  get  return  type  return    types  long    this  method  is  a  default  implementation  that  uses  java  serialization  and  it  is  discouraged    all  implementation  should  provide  a  more  specific  set  of  properties    override  public    map    string    string  to  properties    map    string    string  properties  new    hash  map  properties  put    rowtime  rowtime  timestamps  type    rowtime  rowtime  timestamps  type  value  custom  properties  put    rowtime  rowtime  timestamps  class  this  get  class  get  name  properties  put    rowtime  rowtime  timestamps  serialized    encoding  utils  encode  object  to  string  this  return  properties  
public  evolving  public  final  class    preserve  watermarks  extends    watermark  strategy  private  static  final  long  serial  version  u  i  d  1  l  public  static  final    preserve  watermarks  instance  new    preserve  watermarks    override  public  boolean  equals    object  obj  return  obj  instanceof    preserve  watermarks    override  public  int  hash  code  return    preserve  watermarks  class  hash  code    override  public    map    string    string  to  properties    map    string    string  map  new    hash  map  map  put    rowtime  rowtime  watermarks  type    rowtime  rowtime  watermarks  type  value  from  source  return  map  
public  evolving  public  abstract  class    watermark  strategy  implements    serializable    descriptor    this  method  is  a  default  implementation  that  uses  java  serialization  and  it  is  discouraged    all  implementation  should  provide  a  more  specific  set  of  properties    override  public    map    string    string  to  properties    map    string    string  properties  new    hash  map  properties  put    rowtime  rowtime  watermarks  type    rowtime  rowtime  watermarks  type  value  custom  properties  put    rowtime  rowtime  watermarks  class  this  get  class  get  name  properties  put    rowtime  rowtime  watermarks  serialized    encoding  utils  encode  object  to  string  this  return  properties  
public  evolving  public  interface    abstract  data  type  t  extends    abstract  data  type  t    adds  a  hint  that  null  values  are  not  expected  in  the  data  for  this  type  return  a  new  reconfigured  data  type  instance  t  not  null    adds  a  hint  that  null  values  are  expected  in  the  data  for  this  type  default  behavior  p    this  method  exists  for  explicit  declaration  of  the  default  behavior  or  for  invalidation  of  a  previous  call  to  link  not  null  return  a  new  reconfigured  data  type  instance  t  nullable    adds  a  hint  that  data  should  be  represented  using  the  given  class  when  entering  or  leaving  the  table  ecosystem  p  a  supported  conversion  class  depends  on  the  logical  type  and  its  nullability  property  p    please  see  the  implementation  of  link    logical  type  supports  input  conversion    class  link    logical  type  supports  output  conversion    class  or  the  documentation  for  more  information  about  supported  conversions  return  a  new  reconfigured  data  type  instance  t  bridged  to    class  new  conversion  class  
public  evolving  public  final  class    atomic  data  type  extends    data  type  public    atomic  data  type    logical  type  logical  type    nullable    class  conversion  class  super  logical  type  conversion  class  public    atomic  data  type    logical  type  logical  type  super  logical  type  null    override  public    data  type  not  null  return  new    atomic  data  type  logical  type  copy  false  conversion  class    override  public    data  type  nullable  return  new    atomic  data  type  logical  type  copy  true  conversion  class    override  public    data  type  bridged  to    class  new  conversion  class  return  new    atomic  data  type  logical  type    preconditions  check  not  null  new  conversion  class    new  conversion  class  must  not  be  null    override  public    list    data  type  get  children  return    collections  empty  list    override  public  r  r  accept    data  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    collection  data  type  extends    data  type  private  final    data  type  element  data  type  public    collection  data  type    logical  type  logical  type    nullable    class  conversion  class    data  type  element  data  type  super  logical  type  ensure  array  conversion  class  logical  type  element  data  type  conversion  class  this  element  data  type    preconditions  check  not  null  element  data  type    element  data  type  must  not  be  null  public    collection  data  type    logical  type  logical  type    data  type  element  data  type  this  logical  type  null  element  data  type  public    data  type  get  element  data  type  return  element  data  type    override  public    data  type  not  null  return  new    collection  data  type  logical  type  copy  false  conversion  class  element  data  type    override  public    data  type  nullable  return  new    collection  data  type  logical  type  copy  true  conversion  class  element  data  type    override  public    data  type  bridged  to    class  new  conversion  class  return  new    collection  data  type  logical  type    preconditions  check  not  null  new  conversion  class    new  conversion  class  must  not  be  null  ensure  element  conversion  class  element  data  type  new  conversion  class    override  public    list    data  type  get  children  return    collections  singleton  list  element  data  type    override  public  r  r  accept    data  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    collection  data  type  that    collection  data  type  o  return  element  data  type  equals  that  element  data  type    override  public  int  hash  code  return    objects  hash  super  hash  code  element  data  type  private  static    class  ensure  array  conversion  class    logical  type  logical  type    data  type  element  data  type    nullable    class  clazz  arrays  are  a  special  case  because  their  default  conversion  class  depends  on  the  conversion  class  of  the  element  type  if  logical  type  get  type  root    logical  type  root  array  clazz  null  return    array  new  instance  element  data  type  get  conversion  class    get  class  return  clazz  private    data  type  ensure  element  conversion  class    data  type  element  data  type    class  clazz  arrays  are  a  special  case  because  their  element  conversion  class  depends  on  the  outer  conversion  class  if  logical  type  get  type  root    logical  type  root  array  clazz  is  array  return  element  data  type  bridged  to  clazz  get  component  type  return  element  data  type  
public  evolving  public  abstract  class    data  type  implements    abstract  data  type    data  type    serializable  protected  final    logical  type  logical  type  protected  final    class  conversion  class    data  type    logical  type  logical  type    nullable    class  conversion  class  this  logical  type    preconditions  check  not  null  logical  type    logical  type  must  not  be  null  this  conversion  class  perform  early  class  validation  logical  type  ensure  conversion  class  logical  type  conversion  class    returns  the  corresponding  logical  type  return  a  parameterized  instance  of  link    logical  type  public    logical  type  get  logical  type  return  logical  type    returns  the  corresponding  conversion  class  for  representing  values    if  no  conversion  class  was  defined  manually  the  default  conversion  defined  by  the  logical  type  is  used  see    logical  type  get  default  conversion  return  the  expected  conversion  class  public    class  get  conversion  class  return  conversion  class  public  abstract    list    data  type  get  children  public  abstract  r  r  accept    data  type  visitor  r  visitor    override  public    string  to  string  return  logical  type  to  string    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    data  type  data  type    data  type  o  return  logical  type  equals  data  type  logical  type  conversion  class  equals  data  type  conversion  class    override  public  int  hash  code  return    objects  hash  logical  type  conversion  class    this  method  should  catch  the  most  common  errors    however  another  validation  is  required  in  deeper  layers  as  we  don  t  know  whether  the  data  type  is  used  for  input  or  output  declaration  private  static  c    class  c  perform  early  class  validation    logical  type  logical  type    class  c  candidate  if  candidate  null  logical  type  supports  input  conversion  candidate  logical  type  supports  output  conversion  candidate  throw  new    validation  exception    string  format    logical  type  s  does  not  support  a  conversion  from  or  to  class  s  logical  type  as  summary  string  candidate  get  name  return  candidate  private  static    class  ensure  conversion  class    logical  type  logical  type    nullable    class  clazz  if  clazz  null  return  logical  type  get  default  conversion  return  clazz  
public  evolving  public  interface    data  type  visitor  r  r  visit    atomic  data  type  atomic  data  type  r  visit    collection  data  type  collection  data  type  r  visit    fields  data  type  fields  data  type  r  visit    key  value  data  type  key  value  data  type  
public  evolving  public  final  class    fields  data  type  extends    data  type  private  final    list    data  type  field  data  types  public    fields  data  type    logical  type  logical  type    nullable    class  conversion  class    list    data  type  field  data  types  super  logical  type  conversion  class  this  field  data  types    preconditions  check  not  null  field  data  types    field  data  types  must  not  be  null  public    fields  data  type    logical  type  logical  type    list    data  type  field  data  types  this  logical  type  null  field  data  types    override  public    data  type  not  null  return  new    fields  data  type  logical  type  copy  false  conversion  class  field  data  types    override  public    data  type  nullable  return  new    fields  data  type  logical  type  copy  true  conversion  class  field  data  types    override  public    data  type  bridged  to    class  new  conversion  class  return  new    fields  data  type  logical  type    preconditions  check  not  null  new  conversion  class    new  conversion  class  must  not  be  null  field  data  types    override  public    list    data  type  get  children  return  field  data  types    override  public  r  r  accept    data  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    fields  data  type  that    fields  data  type  o  return  field  data  types  equals  that  field  data  types    override  public  int  hash  code  return    objects  hash  super  hash  code  field  data  types  
public  evolving  public  interface    argument  count    enables  custom  validation  of  argument  counts  after  link  get  min  count  and  link  get  max  count  have  been  validated  param  count  total  number  of  arguments  including  each  argument  for  a  vararg  function  call  boolean  is  valid  count  int  count    returns  the  minimum  number  of  argument  inclusive  that  a  function  can  take  p  link    optional  empty  if  such  a  lower  bound  is  not  defined    optional    integer  get  min  count    returns  the  maximum  number  of  argument  inclusive  that  a  function  can  take  p  link    optional  empty  if  such  an  upper  bound  is  not  defined    optional    integer  get  max  count  
public  evolving  public  interface    argument  type  strategy    main  logic  for  inferring  and  validating  an  argument    returns  the  data  type  that  is  valid  for  the  given  call    if  the  returned  type  differs  from  link    call  context  get  argument  data  types  at  code  argument  pos  a  casting  operation  can  be  inserted    an  empty  result  means  that  the  given  input  type  could  not  be  inferred  param  call  context  provides  details  about  the  function  call  param  argument  pos  argument  index  in  the  link    call  context  param  throw  on  failure  whether  this  function  is  allowed  to  throw  an  link    validation  exception  with  a  meaningful  exception  in  case  the  inference  is  not  successful  or  if  this  function  should  simply  return  an  empty  result  return  three  state  result  for  either  true  same  data  type  as  argument  true  but  argument  must  be  casted  to  returned  data  type  or  false  no  inferred  data  type  could  be  found  see    call  context  new  validation  error    string    object    optional    data  type  infer  argument  type    call  context  call  context  int  argument  pos  boolean  throw  on  failure    returns  a  summary  of  the  function  s  expected  argument  at  code  argument  pos  param  function  definition  the  function  definition  that  defines  the  function  currently  being  called  param  argument  pos  the  position  within  the  function  call  for  which  the  signature  should  be  retrieved    signature    argument  get  expected  argument    function  definition  function  definition  int  argument  pos  
public  evolving  public  interface    call  context    enables  to  lookup  types  in  a  catalog  and  resolve  raw  types    data  type  factory  get  data  type  factory    returns  the  function  definition  that  defines  the  function  currently  being  called    function  definition  get  function  definition    returns  whether  the  argument  at  the  given  position  is  a  value  literal  boolean  is  argument  literal  int  pos    returns  code  true  if  the  argument  at  the  given  position  is  a  literal  and  code  null  code  false  otherwise  p    use  link  is  argument  literal  int  before  to  check  if  the  argument  is  actually  a  literal  boolean  is  argument  null  int  pos    returns  the  literal  value  of  the  argument  at  the  given  position  given  that  the  argument  is  a  literal  is  not  null  and  can  be  expressed  as  an  instance  of  the  provided  class  p    it  supports  conversions  to  default  conversion  classes  of  link    logical  type    logical  types    this  method  should  not  be  called  with  other  classes  p    use  link  is  argument  literal  int  before  to  check  if  the  argument  is  actually  a  literal  t    optional  t  get  argument  value  int  pos    class  t  clazz    returns  the  function  s  name  usually  referencing  the  function  in  a  catalog  p    note    the  name  is  meant  for  debugging  purposes  only    string  get  name    returns  a  resolved  list  of  the  call  s  argument  types    it  also  includes  a  type  for  every  argument  in  a  vararg  function  call    list    data  type  get  argument  data  types    returns  the  inferred  output  data  type  of  the  function  call  p    it  does  this  by  inferring  the  input  argument  data  type  using  link    argument  type  strategy  infer  argument  type    call  context  int  boolean  of  a  wrapping  call  if  available  where  this  function  call  is  an  argument    for  example  code  takes  string  this  function  null  would  lead  to  a  link    data  types  string  because  the  wrapping  call  expects  a  string  argument    optional    data  type  get  output  data  type    creates  a  validation  error  for  exiting  the  type  inference  process  with  a  meaningful  exception  default    validation  exception  new  validation  error    string  message    object  args  return  new    validation  exception    string  format  message  args  
public  evolving  public  interface    input  type  strategy    initial  input  validation  based  on  the  number  of  arguments    argument  count  get  argument  count    main  logic  for  inferring  and  validating  the  input  arguments    returns  a  list  of  argument  data  types  that  are  valid  for  the  given  call    if  the  returned  types  differ  from  link    call  context  get  argument  data  types  a  casting  operation  can  be  inserted    an  empty  result  means  that  the  given  input  is  invalid  param  call  context  provides  details  about  the  function  call  param  throw  on  failure  whether  this  function  is  allowed  to  throw  an  link    validation  exception  with  a  meaningful  exception  in  case  the  inference  is  not  successful  or  if  this  function  should  simply  return  an  empty  result  return  three  state  result  for  either  true  same  data  types  as  arguments  true  but  arguments  must  be  casted  to  returned  data  types  or  false  no  inferred  data  types  could  be  found  see    call  context  new  validation  error    string    object    optional    list    data  type  infer  input  types    call  context  call  context  boolean  throw  on  failure    returns  a  summary  of  the  function  s  expected  signatures  param  definition  the  function  definition  that  defines  the  function  currently  being  called    list    signature  get  expected  signatures    function  definition  definition  
public  evolving  public  final  class    signature  private  final    list    argument  arguments  private    signature    list    argument  arguments  this  arguments    preconditions  check  not  null  arguments    argument  must  not  be  null    creates  an  immutable  instance  of  link    signature  public  static    signature  of    argument  arguments  return  new    signature    arrays  as  list  arguments    creates  an  immutable  instance  of  link    signature  public  static    signature  of    list    argument  arguments  return  new    signature  arguments  public    list    argument  get  arguments  return  arguments    representation  of  a  single  argument  in  a  signature  p    the  type  is  represented  as  link    string  in  order  to  also  express  type  families  or  varargs  public  static  final  class    argument  private  final    nullable    string  name  private  final    string  type  private    argument    nullable    string  name    string  type  this  name  name  this  type    preconditions  check  not  null  type    returns  an  instance  of  link    argument  public  static    argument  of    string  name    string  type  return  new    argument    preconditions  check  not  null  name    name  must  not  be  null  type    returns  an  instance  of  link    argument  public  static    argument  of    string  type  return  new    argument  null  type  public    optional    string  get  name  return    optional  of  nullable  name  public    string  get  type  return  type  
public  evolving  public  final  class    type  inference  private  final    nullable    list    string  named  arguments  private  final    nullable    list    data  type  typed  arguments  private  final    input  type  strategy  input  type  strategy  private  final    nullable    type  strategy  accumulator  type  strategy  private  final    type  strategy  output  type  strategy  private    type  inference    nullable    list    string  named  arguments    nullable    list    data  type  typed  arguments    input  type  strategy  input  type  strategy    nullable    type  strategy  accumulator  type  strategy    type  strategy  output  type  strategy  this  named  arguments  named  arguments  this  typed  arguments  typed  arguments  this  input  type  strategy  input  type  strategy  this  accumulator  type  strategy  accumulator  type  strategy  this  output  type  strategy  output  type  strategy  if  named  arguments  null  typed  arguments  null  named  arguments  size  typed  arguments  size  throw  new    illegal  argument  exception    string  format    mismatch  between  typed  arguments  d  and  named  argument  d  named  arguments  size  typed  arguments  size    builder  for  configuring  and  creating  instances  of  link    type  inference  public  static    type  inference    builder  new  builder  return  new    type  inference    builder  public    optional    list    string  get  named  arguments  return    optional  of  nullable  named  arguments  public    optional    list    data  type  get  typed  arguments  return    optional  of  nullable  typed  arguments  public    input  type  strategy  get  input  type  strategy  return  input  type  strategy  public    optional    type  strategy  get  accumulator  type  strategy  return    optional  of  nullable  accumulator  type  strategy  public    type  strategy  get  output  type  strategy  return  output  type  strategy    builder  for  configuring  and  creating  instances  of  link    type  inference  public  static  class    builder  private    nullable    list    string  named  arguments  private    nullable    list    data  type  typed  arguments  private    input  type  strategy  input  type  strategy    input  type  strategies  wildcard  private    nullable    type  strategy  accumulator  type  strategy  private    nullable    type  strategy  output  type  strategy  public    builder  default  constructor  to  allow  a  fluent  definition    sets  the  list  of  argument  names  for  specifying  a  fixed  not  overloaded  not  vararg  input  signature  explicitly  p    this  information  is  useful  for  sql  s  concept  of  named  arguments  using  the  assignment  operator  e  g  code  func  max      the  names  are  used  for  reordering  the  call  s  arguments  to  the  formal  argument  order  of  the  function  public    builder  named  arguments    list    string  argument  names  this  named  arguments    preconditions  check  not  null  argument  names    list  of  argument  names  must  not  be  null  return  this  see  named  arguments    list  public    builder  named  arguments    string  argument  names  return  named  arguments    arrays  as  list  argument  names    sets  the  list  of  argument  types  for  specifying  a  fixed  not  overloaded  not  vararg  input  signature  explicitly  p    this  information  is  useful  for  optional  arguments  with  default  value    in  particular  the  number  of  arguments  that  need  to  be  filled  with  a  default  value  and  their  types  is  important  public    builder  typed  arguments    list    data  type  argument  types  this  typed  arguments    preconditions  check  not  null  argument  types    list  of  argument  types  must  not  be  null  return  this  see  typed  arguments    list  public    builder  typed  arguments    data  type  argument  types  return  typed  arguments    arrays  as  list  argument  types    sets  the  strategy  for  inferring  and  validating  input  arguments  in  a  function  call  p  a  link    input  type  strategies  wildcard  strategy  function  is  assumed  by  default  public    builder  input  type  strategy    input  type  strategy  input  type  strategy  this  input  type  strategy    preconditions  check  not  null  input  type  strategy    input  type  strategy  must  not  be  null  return  this    sets  the  strategy  for  inferring  the  intermediate  accumulator  data  type  of  a  function  call  public    builder  accumulator  type  strategy    type  strategy  accumulator  type  strategy  this  accumulator  type  strategy    preconditions  check  not  null  accumulator  type  strategy    accumulator  type  strategy  must  not  be  null  return  this    sets  the  strategy  for  inferring  the  final  output  data  type  of  a  function  call  p    required  public    builder  output  type  strategy    type  strategy  output  type  strategy  this  output  type  strategy    preconditions  check  not  null  output  type  strategy    output  type  strategy  must  not  be  null  return  this  public    type  inference  build  return  new    type  inference  named  arguments  typed  arguments  input  type  strategy  accumulator  type  strategy    preconditions  check  not  null  output  type  strategy    output  type  strategy  must  not  be  null  
public  evolving  public  interface    type  strategy    infers  a  type  from  the  given  function  call    optional    data  type  infer  type    call  context  call  context  
public  evolving  public  interface    type  transformation    transforms  the  given  data  type  to  a  different  data  type    data  type  transform    data  type  type  to  transform  
public  evolving  public  final  class    key  value  data  type  extends    data  type  private  final    data  type  key  data  type  private  final    data  type  value  data  type  public    key  value  data  type    logical  type  logical  type    nullable    class  conversion  class    data  type  key  data  type    data  type  value  data  type  super  logical  type  conversion  class  this  key  data  type    preconditions  check  not  null  key  data  type    key  data  type  must  not  be  null  this  value  data  type    preconditions  check  not  null  value  data  type    value  data  type  must  not  be  null  public    key  value  data  type    logical  type  logical  type    data  type  key  data  type    data  type  value  data  type  this  logical  type  null  key  data  type  value  data  type  public    data  type  get  key  data  type  return  key  data  type  public    data  type  get  value  data  type  return  value  data  type    override  public    data  type  not  null  return  new    key  value  data  type  logical  type  copy  false  conversion  class  key  data  type  value  data  type    override  public    data  type  nullable  return  new    key  value  data  type  logical  type  copy  true  conversion  class  key  data  type  value  data  type    override  public    data  type  bridged  to    class  new  conversion  class  return  new    key  value  data  type  logical  type    preconditions  check  not  null  new  conversion  class    new  conversion  class  must  not  be  null  key  data  type  value  data  type    override  public    list    data  type  get  children  return    arrays  as  list  key  data  type  value  data  type    override  public  r  r  accept    data  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    key  value  data  type  that    key  value  data  type  o  return  key  data  type  equals  that  key  data  type  value  data  type  equals  that  value  data  type    override  public  int  hash  code  return    objects  hash  super  hash  code  key  data  type  value  data  type  
public  evolving  public  final  class    array  type  extends    logical  type  public  static  final    string  format  array  s  private  static  final    set    string  input  output  conversion  conversion  set    list  class  get  name    array  data  class  get  name  private  final    logical  type  element  type  public    array  type  boolean  is  nullable    logical  type  element  type  super  is  nullable    logical  type  root  array  this  element  type    preconditions  check  not  null  element  type    element  type  must  not  be  null  public    array  type    logical  type  element  type  this  true  element  type  public    logical  type  get  element  type  return  element  type    override  public    logical  type  copy  boolean  is  nullable  return  new    array  type  is  nullable  element  type  copy    override  public    string  as  summary  string  return  with  nullability  format  element  type  as  summary  string    override  public    string  as  serializable  string  return  with  nullability  format  element  type  as  serializable  string    override  public  boolean  supports  input  conversion    class  clazz  if    list  class  is  assignable  from  clazz  return  true  if  input  output  conversion  contains  clazz  get  name  return  true  if  clazz  is  array  return  false  return  element  type  supports  input  conversion  clazz  get  component  type    override  public  boolean  supports  output  conversion    class  clazz  if  input  output  conversion  contains  clazz  get  name  return  true  if  clazz  is  array  return  false  return  element  type  supports  output  conversion  clazz  get  component  type    override  public    class  get  default  conversion  return    array  new  instance  element  type  get  default  conversion    get  class    override  public    list    logical  type  get  children  return    collections  singleton  list  element  type    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    array  type  array  type    array  type  o  return  element  type  equals  array  type  element  type    override  public  int  hash  code  return    objects  hash  super  hash  code  element  type  
public  evolving  public  final  class    big  int  type  extends    logical  type  public  static  final  int  precision    private  static  final    string  format  bigint  private  static  final    set    string  null  output  conversion  conversion  set    long  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set    long  class  get  name  long  class  get  name  private  static  final    class  default  conversion    long  class  public    big  int  type  boolean  is  nullable  super  is  nullable    logical  type  root  bigint  public    big  int  type  this  true    override  public    logical  type  copy  boolean  is  nullable  return  new    big  int  type  is  nullable    override  public    string  as  serializable  string  return  with  nullability  format    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    binary  type  extends    logical  type  public  static  final  int  empty  literal  length    public  static  final  int  min  length    public  static  final  int  max  length    integer  max  value  public  static  final  int  default  length    private  static  final    string  format  binary  d  private  static  final    class  input  output  conversion  byte  class  private  static  final    class  default  conversion  byte  class  private  final  int  length  public    binary  type  boolean  is  nullable  int  length  super  is  nullable    logical  type  root  binary  if  length  min  length  throw  new    validation  exception    string  format    binary  string  length  must  be  between  d  and  d  both  inclusive  min  length  max  length  this  length  length  public    binary  type  int  length  this  true  length  public    binary  type  this  default  length    helper  constructor  for  link  of  empty  literal  and  link  copy  boolean  private    binary  type  int  length  boolean  is  nullable  super  is  nullable    logical  type  root  binary  this  length  length    the  sql  standard  defines  that  character  string  literals  are  allowed  to  be  zero  length  strings  i  e  to  contain  no  characters  even  though  it  is  not  permitted  to  declare  a  type  that  is  zero    for  consistent  behavior  the  same  logic  applies  to  binary  strings  p    this  method  enables  this  special  kind  of  binary  string  p    zero  length  binary  strings  have  no  serializable  string  representation  public  static    binary  type  of  empty  literal  return  new    binary  type  empty  literal  length  false  public  int  get  length  return  length    override  public    logical  type  copy  boolean  is  nullable  return  new    binary  type  length  is  nullable    override  public    string  as  serializable  string  if  length  empty  literal  length  throw  new    table  exception    zero  length  binary  strings  have  no  serializable  string  representation  return  with  nullability  format  length    override  public    string  as  summary  string  return  with  nullability  format  length    override  public  boolean  supports  input  conversion    class  clazz  return  input  output  conversion  clazz    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  clazz    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    binary  type  that    binary  type  o  return  length  that  length    override  public  int  hash  code  return    objects  hash  super  hash  code  length  
public  evolving  public  final  class    boolean  type  extends    logical  type  private  static  final    string  format  boolean  private  static  final    set    string  null  output  conversion  conversion  set    boolean  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set    boolean  class  get  name  boolean  class  get  name  private  static  final    class  default  conversion    boolean  class  public    boolean  type  boolean  is  nullable  super  is  nullable    logical  type  root  boolean  public    boolean  type  this  true    override  public    logical  type  copy  boolean  is  nullable  return  new    boolean  type  is  nullable    override  public    string  as  serializable  string  return  with  nullability  format    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    char  type  extends    logical  type  public  static  final  int  empty  literal  length    public  static  final  int  min  length    public  static  final  int  max  length    integer  max  value  public  static  final  int  default  length    private  static  final    string  format  char  d  private  static  final    set    string  input  output  conversion  conversion  set    string  class  get  name  byte  class  get  name    string  data  class  get  name  private  static  final    class  default  conversion    string  class  private  final  int  length  public    char  type  boolean  is  nullable  int  length  super  is  nullable    logical  type  root  char  if  length  min  length  throw  new    validation  exception    string  format    character  string  length  must  be  between  d  and  d  both  inclusive  min  length  max  length  this  length  length  public    char  type  int  length  this  true  length  public    char  type  this  default  length    helper  constructor  for  link  of  empty  literal  and  link  copy  boolean  private    char  type  int  length  boolean  is  nullable  super  is  nullable    logical  type  root  char  this  length  length    the  sql  standard  defines  that  character  string  literals  are  allowed  to  be  zero  length  strings  i  e  to  contain  no  characters  even  though  it  is  not  permitted  to  declare  a  type  that  is  zero  p    this  method  enables  this  special  kind  of  character  string  p    zero  length  character  strings  have  no  serializable  string  representation  public  static    char  type  of  empty  literal  return  new    char  type  empty  literal  length  false  public  int  get  length  return  length    override  public    logical  type  copy  boolean  is  nullable  return  new    char  type  length  is  nullable    override  public    string  as  serializable  string  if  length  empty  literal  length  throw  new    table  exception    zero  length  character  strings  have  no  serializable  string  representation  return  with  nullability  format  length    override  public    string  as  summary  string  return  with  nullability  format  length    override  public  boolean  supports  input  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    char  type  char  type    char  type  o  return  length  char  type  length    override  public  int  hash  code  return    objects  hash  super  hash  code  length  
public  evolving  public  final  class    date  type  extends    logical  type  private  static  final    string  format  date  private  static  final    set    string  null  output  conversion  conversion  set  java  sql    date  class  get  name  java  time    local  date  class  get  name    integer  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set  java  sql    date  class  get  name  java  time    local  date  class  get  name    integer  class  get  name  int  class  get  name  private  static  final    class  default  conversion  java  time    local  date  class  public    date  type  boolean  is  nullable  super  is  nullable    logical  type  root  date  public    date  type  this  true    override  public    logical  type  copy  boolean  is  nullable  return  new    date  type  is  nullable    override  public    string  as  serializable  string  return  with  nullability  format    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    day  time  interval  type  extends    logical  type  public  static  final  int  min  day  precision    public  static  final  int  max  day  precision    public  static  final  int  default  day  precision    public  static  final  int  min  fractional  precision    public  static  final  int  max  fractional  precision    public  static  final  int  default  fractional  precision    private  static  final    string  day  format  interval  day    d  private  static  final    string  day  to  hour  format  interval  day    d  to  hour  private  static  final    string  day  to  minute  format  interval  day    d  to  minute  private  static  final    string  day  to  second  format  interval  day    d  to  second    d  private  static  final    string  hour  format  interval  hour  private  static  final    string  hour  to  minute  format  interval  hour  to  minute  private  static  final    string  hour  to  second  format  interval  hour  to  second    d  private  static  final    string  minute  format  interval  minute  private  static  final    string  minute  to  second  format  interval  minute  to  second    d  private  static  final    string  second  format  interval  second    d  private  static  final    set    string  null  output  conversion  conversion  set  java  time    duration  class  get  name    long  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set  java  time    duration  class  get  name    long  class  get  name  long  class  get  name  private  static  final    class  default  conversion  java  time    duration  class    supported  resolutions  of  this  type  p    note    the  order  of  this  enum  reflects  the  granularity  from  coarse  to  fine  public  enum    day  time  resolution  day  day  to  hour  day  to  minute  day  to  second  hour  hour  to  minute  hour  to  second  minute  minute  to  second  second  private  final    day  time  resolution  resolution  private  final  int  day  precision  private  final  int  fractional  precision  public    day  time  interval  type  boolean  is  nullable    day  time  resolution  resolution  int  day  precision  int  fractional  precision  super  is  nullable    logical  type  root  interval  day  time    preconditions  check  not  null  resolution  if  needs  default  day  precision  resolution  day  precision  default  day  precision  throw  new    validation  exception    string  format    day  precision  of  sub  day  intervals  must  be  equal  to  the  default  precision  d  default  day  precision  if  needs  default  fractional  precision  resolution  fractional  precision  default  fractional  precision  throw  new    validation  exception    string  format    fractional  precision  of  super  second  intervals  must  be  equal  to  the  default  precision  d  default  fractional  precision  if  day  precision  min  day  precision  day  precision  max  day  precision  throw  new    validation  exception    string  format    day  precision  of  day  time  intervals  must  be  between  d  and  d  both  inclusive  min  day  precision  max  day  precision  if  fractional  precision  min  fractional  precision  fractional  precision  max  fractional  precision  throw  new    validation  exception    string  format    fractional  precision  of  day  time  intervals  must  be  between  d  and  d  both  inclusive  min  fractional  precision  max  fractional  precision  this  resolution  resolution  this  day  precision  day  precision  this  fractional  precision  fractional  precision  public    day  time  interval  type    day  time  resolution  resolution  int  day  precision  int  fractional  precision  this  true  resolution  day  precision  fractional  precision  public    day  time  interval  type    day  time  resolution  resolution  this  resolution  default  day  precision  default  fractional  precision  public    day  time  resolution  get  resolution  return  resolution  public  int  get  day  precision  return  day  precision  public  int  get  fractional  precision  return  fractional  precision    override  public    logical  type  copy  boolean  is  nullable  return  new    day  time  interval  type  is  nullable  resolution  day  precision  fractional  precision    override  public    string  as  serializable  string  return  with  nullability  get  resolution  format  day  precision  fractional  precision    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    day  time  interval  type  that    day  time  interval  type  o  return  day  precision  that  day  precision  fractional  precision  that  fractional  precision  resolution  that  resolution    override  public  int  hash  code  return    objects  hash  super  hash  code  resolution  day  precision  fractional  precision  private  boolean  needs  default  day  precision    day  time  resolution  resolution  switch  resolution  case  hour  case  hour  to  minute  case  hour  to  second  case  minute  case  minute  to  second  case  second  return  true  default  return  false  private  boolean  needs  default  fractional  precision    day  time  resolution  resolution  switch  resolution  case  day  case  day  to  hour  case  day  to  minute  case  hour  case  hour  to  minute  case  minute  return  true  default  return  false  private    string  get  resolution  format  switch  resolution  case  day  return  day  format  case  day  to  hour  return  day  to  hour  format  case  day  to  minute  return  day  to  minute  format  case  day  to  second  return  day  to  second  format  case  hour  return  hour  format  case  hour  to  minute  return  hour  to  minute  format  case  hour  to  second  return  hour  to  second  format  case  minute  return  minute  format  case  minute  to  second  return  minute  to  second  format  case  second  return  second  format  default  throw  new    unsupported  operation  exception  
public  evolving  public  final  class    decimal  type  extends    logical  type  public  static  final  int  min  precision    public  static  final  int  max  precision    public  static  final  int  default  precision    public  static  final  int  min  scale    public  static  final  int  default  scale    private  static  final    string  format  decimal  d  d  private  static  final    set    string  input  output  conversion  conversion  set    big  decimal  class  get  name    decimal  data  class  get  name  private  static  final    class  default  conversion    big  decimal  class  private  final  int  precision  private  final  int  scale  public    decimal  type  boolean  is  nullable  int  precision  int  scale  super  is  nullable    logical  type  root  decimal  if  precision  min  precision  precision  max  precision  throw  new    validation  exception    string  format    decimal  precision  must  be  between  d  and  d  both  inclusive  min  precision  max  precision  if  scale  min  scale  scale  precision  throw  new    validation  exception    string  format    decimal  scale  must  be  between  d  and  the  precision  d  both  inclusive  min  scale  precision  this  precision  precision  this  scale  scale  public    decimal  type  int  precision  int  scale  this  true  precision  scale  public    decimal  type  int  precision  this  precision  default  scale  public    decimal  type  this  default  precision  public  int  get  precision  return  precision  public  int  get  scale  return  scale    override  public    logical  type  copy  boolean  is  nullable  return  new    decimal  type  is  nullable  precision  scale    override  public    string  as  serializable  string  return  with  nullability  format  precision  scale    override  public  boolean  supports  input  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    decimal  type  that    decimal  type  o  return  precision  that  precision  scale  that  scale    override  public  int  hash  code  return    objects  hash  super  hash  code  precision  scale  
public  evolving  public  final  class    distinct  type  extends    user  defined  type  a  builder  for  a  link    distinct  type    intended  for  future  extensibility  public  static  final  class    builder  private  final    object  identifier  object  identifier  private  final    logical  type  source  type  private    nullable    string  description  public    builder    object  identifier  object  identifier    logical  type  source  type  this  object  identifier    preconditions  check  not  null  object  identifier    object  identifier  must  not  be  null  this  source  type    preconditions  check  not  null  source  type    source  type  must  not  be  null    preconditions  check  argument  source  type  get  type  root  get  families  contains    logical  type  family  user  defined    source  type  must  not  be  a  user  defined  type  public    builder  description    string  description  this  description    preconditions  check  not  null  description    description  must  not  be  null  return  this  public    distinct  type  build  return  new    distinct  type  object  identifier  source  type  description  private  final    logical  type  source  type  private    distinct  type    object  identifier  object  identifier    logical  type  source  type    nullable    string  description  super  source  type  is  nullable    logical  type  root  distinct  type  object  identifier  true  description  this  source  type    preconditions  check  not  null  source  type    source  type  must  not  be  null    creates  a  builder  for  a  link    distinct  type  public  static    distinct  type    builder  new  builder    object  identifier  object  identifier    logical  type  source  type  return  new    distinct  type    builder  object  identifier  source  type  public    logical  type  get  source  type  return  source  type    override  public    logical  type  copy  boolean  is  nullable  return  new    distinct  type  get  object  identifier  or  else  throw    illegal  state  exception  new  source  type  copy  is  nullable  get  description  or  else  null    override  public  boolean  supports  input  conversion    class  clazz  return  source  type  supports  input  conversion  clazz    override  public  boolean  supports  output  conversion    class  clazz  return  source  type  supports  output  conversion  clazz    override  public    class  get  default  conversion  return  source  type  get  default  conversion    override  public    list    logical  type  get  children  return    collections  singleton  list  source  type    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    distinct  type  that    distinct  type  o  return  source  type  equals  that  source  type    override  public  int  hash  code  return    objects  hash  super  hash  code  source  type  
public  evolving  public  final  class    double  type  extends    logical  type  public  static  final  int  precision    adopted  from    calcite  private  static  final    string  format  double  private  static  final    set    string  null  output  conversion  conversion  set    double  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set    double  class  get  name  double  class  get  name  private  static  final    class  default  conversion    double  class  public    double  type  boolean  is  nullable  super  is  nullable    logical  type  root  double  public    double  type  this  true    override  public    logical  type  copy  boolean  is  nullable  return  new    double  type  is  nullable    override  public    string  as  serializable  string  return  with  nullability  format    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    float  type  extends    logical  type  public  static  final  int  precision    adopted  from    calcite  private  static  final    string  format  float  private  static  final    set    string  null  output  conversion  conversion  set    float  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set    float  class  get  name  float  class  get  name  private  static  final    class  default  conversion    float  class  public    float  type  boolean  is  nullable  super  is  nullable    logical  type  root  float  public    float  type  this  true    override  public    logical  type  copy  boolean  is  nullable  return  new    float  type  is  nullable    override  public    string  as  serializable  string  return  with  nullability  format    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    int  type  extends    logical  type  public  static  final  int  precision    private  static  final    string  format  int  private  static  final    set    string  null  output  conversion  conversion  set    integer  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set    integer  class  get  name  int  class  get  name  private  static  final    class  default  conversion    integer  class  public    int  type  boolean  is  nullable  super  is  nullable    logical  type  root  integer  public    int  type  this  true    override  public    logical  type  copy  boolean  is  nullable  return  new    int  type  is  nullable    override  public    string  as  serializable  string  return  with  nullability  format    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    local  zoned  timestamp  type  extends    logical  type  public  static  final  int  min  precision    timestamp  type  min  precision  public  static  final  int  max  precision    timestamp  type  max  precision  public  static  final  int  default  precision    timestamp  type  default  precision  private  static  final    string  format  timestamp  d  with  local  time  zone  private  static  final    set    string  null  output  conversion  conversion  set  java  time    instant  class  get  name    integer  class  get  name    long  class  get  name    timestamp  data  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set  java  time    instant  class  get  name    integer  class  get  name  int  class  get  name    long  class  get  name  long  class  get  name  private  static  final    class  default  conversion  java  time    instant  class  private  final    timestamp  kind  kind  private  final  int  precision    internal  constructor  that  allows  attaching  additional  metadata  about  time  attribute  properties    the  additional  metadata  does  not  affect  equality  or  serializability  p    use  link  get  kind  for  comparing  this  metadata    internal  public    local  zoned  timestamp  type  boolean  is  nullable    timestamp  kind  kind  int  precision  super  is  nullable    logical  type  root  timestamp  with  local  time  zone  if  precision  min  precision  precision  max  precision  throw  new    validation  exception    string  format    timestamp  with  local  time  zone  precision  must  be  between  d  and  d  both  inclusive  min  precision  max  precision  this  kind  kind  this  precision  precision  public    local  zoned  timestamp  type  boolean  is  nullable  int  precision  this  is  nullable    timestamp  kind  regular  precision  public    local  zoned  timestamp  type  int  precision  this  true  precision  public    local  zoned  timestamp  type  this  default  precision    internal  public    timestamp  kind  get  kind  return  kind  public  int  get  precision  return  precision    override  public    logical  type  copy  boolean  is  nullable  return  new    local  zoned  timestamp  type  is  nullable  kind  precision    override  public    string  as  serializable  string  return  with  nullability  format  precision    override  public    string  as  summary  string  if  kind    timestamp  kind  regular  return    string  format  s  s  as  serializable  string  kind  return  as  serializable  string    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    local  zoned  timestamp  type  that    local  zoned  timestamp  type  o  return  precision  that  precision    override  public  int  hash  code  return    objects  hash  super  hash  code  precision  
public  evolving  public  abstract  class    logical  type  implements    serializable  private  final  boolean  is  nullable  private  final    logical  type  root  type  root  public    logical  type  boolean  is  nullable    logical  type  root  type  root  this  is  nullable  is  nullable  this  type  root    preconditions  check  not  null  type  root    returns  whether  a  value  of  this  type  can  be  code  null  public  boolean  is  nullable  return  is  nullable    returns  the  root  of  this  type    it  is  an  essential  description  without  additional  parameters  public    logical  type  root  get  type  root  return  type  root    returns  a  deep  copy  of  this  type  with  possibly  different  nullability  param  is  nullable  the  intended  nullability  of  the  copied  type  return  a  deep  copy  public  abstract    logical  type  copy  boolean  is  nullable    returns  a  deep  copy  of  this  type    it  requires  an  implementation  of  link  copy  boolean  return  a  deep  copy  public  final    logical  type  copy  return  copy  is  nullable    returns  a  string  that  fully  serializes  this  instance    the  serialized  string  can  be  used  for  transmitting  or  persisting  a  type  p    see  link    logical  type  parser  for  the  reverse  operation  return  detailed  string  for  transmission  or  persistence  public  abstract    string  as  serializable  string    returns  a  string  that  summarizes  this  type  for  printing  to  a  console    an  implementation  might  shorten  long  names  or  skips  very  specific  properties  p    use  link  as  serializable  string  for  a  type  string  that  fully  serializes  this  instance  return  summary  string  of  this  type  for  debugging  purposes  public    string  as  summary  string  return  as  serializable  string    returns  whether  an  instance  of  the  given  class  can  be  represented  as  a  value  of  this  logical  type  when  entering  the  table  ecosystem    this  method  helps  for  the  interoperability  between  jvm  based  languages  and  the  relational  type  system  p  a  supported  conversion  directly  maps  an  input  class  to  a  logical  type  without  loss  of  precision  or  type  widening  p    for  example  code  java  lang    long  or  code  long  can  be  used  as  input  for  code  bigint  independent  of  the  set  nullability  param  clazz  input  class  to  be  converted  into  this  logical  type  return  flag  that  indicates  if  instances  of  this  class  can  be  used  as  input  into  the  table  ecosystem  see  get  default  conversion  public  abstract  boolean  supports  input  conversion    class  clazz    returns  whether  a  value  of  this  logical  type  can  be  represented  as  an  instance  of  the  given  class  when  leaving  the  table  ecosystem    this  method  helps  for  the  interoperability  between  jvm  based  languages  and  the  relational  type  system  p  a  supported  conversion  directly  maps  a  logical  type  to  an  output  class  without  loss  of  precision  or  type  widening  p    for  example  code  java  lang    long  or  code  long  can  be  used  as  output  for  code  bigint  if  the  type  is  not  nullable    if  the  type  is  nullable  only  code  java  lang    long  can  represent  this  param  clazz  output  class  to  be  converted  from  this  logical  type  return  flag  that  indicates  if  instances  of  this  class  can  be  used  as  output  from  the  table  ecosystem  see  get  default  conversion  public  abstract  boolean  supports  output  conversion    class  clazz    returns  the  default  conversion  class  a  value  of  this  logical  type  is  expected  to  be  an  instance  of  the  given  class  when  entering  or  is  represented  as  an  instance  of  the  given  class  when  leaving  the  table  ecosystem  if  no  other  conversion  has  been  specified  p    for  example  code  java  lang    long  is  the  default  input  and  output  for  code  bigint  return  default  class  to  represent  values  of  this  logical  type  see  supports  input  conversion    class  see  supports  output  conversion    class  public  abstract    class  get  default  conversion  public  abstract    list    logical  type  get  children  public  abstract  r  r  accept    logical  type  visitor  r  visitor    override  public    string  to  string  return  as  summary  string    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    logical  type  that    logical  type  o  return  is  nullable  that  is  nullable  type  root  that  type  root    override  public  int  hash  code  return    objects  hash  is  nullable  type  root  protected    string  with  nullability    string  format    object  params  if  is  nullable  return    string  format  format  not  null  params  return    string  format  format  params  protected  static    set    string  conversion  set    string  elements  return  new    hash  set    arrays  as  list  elements  
public  evolving  public  enum    logical  type  family  predefined  constructed  user  defined  character  string  binary  string  numeric  integer  numeric  exact  numeric  approximate  numeric  datetime  time  timestamp  interval  collection  extension  
public  evolving  public  enum    logical  type  root  char    logical  type  family  predefined    logical  type  family  character  string  varchar    logical  type  family  predefined    logical  type  family  character  string  boolean    logical  type  family  predefined  binary    logical  type  family  predefined    logical  type  family  binary  string  varbinary    logical  type  family  predefined    logical  type  family  binary  string  decimal    logical  type  family  predefined    logical  type  family  numeric    logical  type  family  exact  numeric  tinyint    logical  type  family  predefined    logical  type  family  numeric    logical  type  family  integer  numeric    logical  type  family  exact  numeric  smallint    logical  type  family  predefined    logical  type  family  numeric    logical  type  family  integer  numeric    logical  type  family  exact  numeric  integer    logical  type  family  predefined    logical  type  family  numeric    logical  type  family  integer  numeric    logical  type  family  exact  numeric  bigint    logical  type  family  predefined    logical  type  family  numeric    logical  type  family  integer  numeric    logical  type  family  exact  numeric  float    logical  type  family  predefined    logical  type  family  numeric    logical  type  family  approximate  numeric  double    logical  type  family  predefined    logical  type  family  numeric    logical  type  family  approximate  numeric  date    logical  type  family  predefined    logical  type  family  datetime  time  without  time  zone    logical  type  family  predefined    logical  type  family  datetime    logical  type  family  time  timestamp  without  time  zone    logical  type  family  predefined    logical  type  family  datetime    logical  type  family  timestamp  timestamp  with  time  zone    logical  type  family  predefined    logical  type  family  datetime    logical  type  family  timestamp  timestamp  with  local  time  zone    logical  type  family  predefined    logical  type  family  datetime    logical  type  family  timestamp    logical  type  family  extension  interval  year  month    logical  type  family  predefined    logical  type  family  interval  interval  day  time    logical  type  family  predefined    logical  type  family  interval  array    logical  type  family  constructed    logical  type  family  collection  multiset    logical  type  family  constructed    logical  type  family  collection  map    logical  type  family  constructed    logical  type  family  extension  row    logical  type  family  constructed  distinct  type    logical  type  family  user  defined  structured  type    logical  type  family  user  defined  null    logical  type  family  extension  raw    logical  type  family  extension  symbol    logical  type  family  extension  unresolved    logical  type  family  extension  private  final    set    logical  type  family  families    logical  type  root    logical  type  family  first  family    logical  type  family  other  families  this  families    collections  unmodifiable  set    enum  set  of  first  family  other  families  public    set    logical  type  family  get  families  return  families  
public  evolving  public  interface    logical  type  visitor  r  r  visit    char  type  char  type  r  visit    var  char  type  var  char  type  r  visit    boolean  type  boolean  type  r  visit    binary  type  binary  type  r  visit    var  binary  type  var  binary  type  r  visit    decimal  type  decimal  type  r  visit    tiny  int  type  tiny  int  type  r  visit    small  int  type  small  int  type  r  visit    int  type  int  type  r  visit    big  int  type  big  int  type  r  visit    float  type  float  type  r  visit    double  type  double  type  r  visit    date  type  date  type  r  visit    time  type  time  type  r  visit    timestamp  type  timestamp  type  r  visit    zoned  timestamp  type  zoned  timestamp  type  r  visit    local  zoned  timestamp  type  local  zoned  timestamp  type  r  visit    year  month  interval  type  year  month  interval  type  r  visit    day  time  interval  type  day  time  interval  type  r  visit    array  type  array  type  r  visit    multiset  type  multiset  type  r  visit    map  type  map  type  r  visit    row  type  row  type  r  visit    distinct  type  distinct  type  r  visit    structured  type  structured  type  r  visit    null  type  null  type  r  visit    raw  type  raw  type  r  visit    symbol  type  symbol  type  r  visit    logical  type  other  
public  evolving  public  final  class    map  type  extends    logical  type  public  static  final    string  format  map  s  s  private  static  final    set    string  input  output  conversion  conversion  set    map  class  get  name    map  data  class  get  name  private  static  final    class  default  conversion    map  class  private  final    logical  type  key  type  private  final    logical  type  value  type  public    map  type  boolean  is  nullable    logical  type  key  type    logical  type  value  type  super  is  nullable    logical  type  root  map  this  key  type    preconditions  check  not  null  key  type    key  type  must  not  be  null  this  value  type    preconditions  check  not  null  value  type    value  type  must  not  be  null  public    map  type    logical  type  key  type    logical  type  value  type  this  true  key  type  value  type  public    logical  type  get  key  type  return  key  type  public    logical  type  get  value  type  return  value  type    override  public    logical  type  copy  boolean  is  nullable  return  new    map  type  is  nullable  key  type  copy  value  type  copy    override  public    string  as  summary  string  return  with  nullability  format  key  type  as  summary  string  value  type  as  summary  string    override  public    string  as  serializable  string  return  with  nullability  format  key  type  as  serializable  string  value  type  as  serializable  string    override  public  boolean  supports  input  conversion    class  clazz  if    map  class  is  assignable  from  clazz  return  true  return  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  unmodifiable  list    arrays  as  list  key  type  value  type    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    map  type  map  type    map  type  o  return  key  type  equals  map  type  key  type  value  type  equals  map  type  value  type    override  public  int  hash  code  return    objects  hash  super  hash  code  key  type  value  type  
public  evolving  public  final  class    multiset  type  extends    logical  type  public  static  final    string  format  multiset  s  private  static  final    set    string  input  output  conversion  conversion  set    map  class  get  name    map  data  class  get  name  private  static  final    class  default  conversion    map  class  private  final    logical  type  element  type  public    multiset  type  boolean  is  nullable    logical  type  element  type  super  is  nullable    logical  type  root  multiset  this  element  type    preconditions  check  not  null  element  type    element  type  must  not  be  null  public    multiset  type    logical  type  element  type  this  true  element  type  public    logical  type  get  element  type  return  element  type    override  public    logical  type  copy  boolean  is  nullable  return  new    multiset  type  is  nullable  element  type  copy    override  public    string  as  summary  string  return  with  nullability  format  element  type  as  summary  string    override  public    string  as  serializable  string  return  with  nullability  format  element  type  as  serializable  string    override  public  boolean  supports  input  conversion    class  clazz  if    map  class  is  assignable  from  clazz  return  true  return  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  singleton  list  element  type    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    multiset  type  that    multiset  type  o  return  element  type  equals  that  element  type    override  public  int  hash  code  return    objects  hash  super  hash  code  element  type  
public  evolving  public  final  class    null  type  extends    logical  type  private  static  final    string  format  null  private  static  final    class  input  conversion    object  class  private  static  final    class  default  conversion    object  class  public    null  type  super  true    logical  type  root  null    override  public    logical  type  copy  boolean  is  nullable  if  is  nullable  throw  new    table  exception    the  nullability  of  a  null  type  cannot  be  disabled  because  the  type  must  always  be  able  to  contain  a  null  value  return  new    null  type    override  public    string  as  serializable  string  return  format    override  public  boolean  supports  input  conversion    class  clazz  return  input  conversion  equals  clazz    override  public  boolean  supports  output  conversion    class  clazz  any  nullable  class  is  supported  return  clazz  is  primitive    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    raw  type  t  extends    logical  type  public  static  final    string  format  raw  s  s  private  static  final    set    string  input  output  conversion  conversion  set  byte  class  get  name    raw  value  data  class  get  name  private  final    class  t  clazz  private  final    type  serializer  t  serializer  private  transient    string  serializer  string  public    raw  type  boolean  is  nullable    class  t  clazz    type  serializer  t  serializer  super  is  nullable    logical  type  root  raw  this  clazz    preconditions  check  not  null  clazz    class  must  not  be  null  this  serializer    preconditions  check  not  null  serializer    serializer  must  not  be  null  public    raw  type    class  t  clazz    type  serializer  t  serializer  this  true  clazz  serializer  public    class  t  get  originating  class  return  clazz  public    type  serializer  t  get  type  serializer  return  serializer    override  public    logical  type  copy  boolean  is  nullable  return  new    raw  type  is  nullable  clazz  serializer  duplicate    override  public    string  as  summary  string  return  with  nullability  format  clazz  get  name    override  public    string  as  serializable  string  return  with  nullability  format  clazz  get  name  get  serializer  string    override  public  boolean  supports  input  conversion    class  clazz  return  this  clazz  is  assignable  from  clazz  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  clazz  is  assignable  from  this  clazz  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  clazz    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    raw  type  raw  type    raw  type  o  return  clazz  equals  raw  type  clazz  serializer  equals  raw  type  serializer    override  public  int  hash  code  return    objects  hash  super  hash  code  clazz  serializer    restores  a  raw  type  from  the  components  of  a  serialized  string  representation    suppress  warnings  unchecked  rawtypes  public  static    raw  type  restore    class  loader  class  loader    string  class  name    string  serializer  string  try  final    class  clazz    class  for  name  class  name  true  class  loader  final  byte  bytes    encoding  utils  decode  base64  to  bytes  serializer  string  final    data  input  deserializer  input  deserializer  new    data  input  deserializer  bytes  final    type  serializer  snapshot  snapshot    type  serializer  snapshot  read  versioned  snapshot  input  deserializer  class  loader  return    raw  type  new    raw  type  clazz  snapshot  restore  serializer  catch    throwable  t  throw  new    validation  exception    string  format    unable  to  restore  the  raw  type  of  class  s  with  serializer  snapshot  s  class  name  serializer  string  t    returns  the  serialized  link    type  serializer  snapshot  in    base64  encoding  of  this  raw  type  public    string  get  serializer  string  if  serializer  string  null  final    data  output  serializer  output  serializer  new    data  output  serializer    try    type  serializer  snapshot  write  versioned  snapshot  output  serializer  serializer  snapshot  configuration  serializer  string    encoding  utils  encode  bytes  to  base64  output  serializer  get  copy  of  buffer  return  serializer  string  catch    exception  e  throw  new    table  exception    string  format    unable  to  generate  a  string  representation  of  the  serializer  snapshot  of  s  describing  the  class  s  for  the  raw  type  serializer  get  class  get  name  clazz  to  string  e  return  serializer  string  
public  evolving  public  final  class    row  type  extends    logical  type  public  static  final    string  format  row  s  private  static  final    set    string  input  output  conversion  conversion  set    row  class  get  name    row  data  class  get  name  private  static  final    class  default  conversion    row  class    describes  a  field  of  a  link    row  type  public  static  final  class    row  field  implements    serializable  public  static  final    string  field  format  with  description  s  s  s  public  static  final    string  field  format  no  description  s  s  private  final    string  name  private  final    logical  type  type  private  final    nullable    string  description  public    row  field    string  name    logical  type  type    nullable    string  description  this  name    preconditions  check  not  null  name    field  name  must  not  be  null  this  type    preconditions  check  not  null  type    field  type  must  not  be  null  this  description  description  public    row  field    string  name    logical  type  type  this  name  type  null  public    string  get  name  return  name  public    logical  type  get  type  return  type  public    optional    string  get  description  return    optional  of  nullable  description  public    row  field  copy  return  new    row  field  name  type  copy  description  public    string  as  summary  string  return  format  string  type  as  summary  string  true  public    string  as  serializable  string  return  format  string  type  as  serializable  string  false    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    row  field  row  field    row  field  o  return  name  equals  row  field  name  type  equals  row  field  type    objects  equals  description  row  field  description    override  public  int  hash  code  return    objects  hash  name  type  description  private    string  format  string    string  type  string  boolean  exclude  description  if  description  null  return    string  format  field  format  no  description  escape  identifier  name  type  string  else  if  exclude  description  return    string  format  field  format  with  description  escape  identifier  name  type  string  else  return    string  format  field  format  with  description  escape  identifier  name  type  string  escape  single  quotes  description  private  final    list    row  field  fields  public    row  type  boolean  is  nullable    list    row  field  fields  super  is  nullable    logical  type  root  row  this  fields    collections  unmodifiable  list  new    array  list    preconditions  check  not  null  fields    fields  must  not  be  null  validate  fields  fields  public    row  type    list    row  field  fields  this  true  fields  public    list    row  field  get  fields  return  fields  public    list    string  get  field  names  return  fields  stream  map    row  field  get  name  collect    collectors  to  list  public    logical  type  get  type  at  int  i  return  fields  get  i  get  type  public  int  get  field  count  return  fields  size  public  int  get  field  index    string  field  name  for  int  i    i  fields  size  i  if  fields  get  i  get  name  equals  field  name  return  i  return      override  public    logical  type  copy  boolean  is  nullable  return  new    row  type  is  nullable  fields  stream  map    row  field  copy  collect    collectors  to  list    override  public    string  as  summary  string  return  with  nullability  format  fields  stream  map    row  field  as  summary  string  collect    collectors  joining    override  public    string  as  serializable  string  return  with  nullability  format  fields  stream  map    row  field  as  serializable  string  collect    collectors  joining    override  public  boolean  supports  input  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  unmodifiable  list  fields  stream  map    row  field  get  type  collect    collectors  to  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    row  type  row  type    row  type  o  return  fields  equals  row  type  fields    override  public  int  hash  code  return    objects  hash  super  hash  code  fields  private  static  void  validate  fields    list    row  field  fields  final    list    string  field  names  fields  stream  map  f  f  name  collect    collectors  to  list  if  field  names  stream  any  match    string  utils  is  null  or  whitespace  only  throw  new    validation  exception    field  names  must  contain  at  least  one  non  whitespace  character  final    set    string  duplicates  field  names  stream  filter  n    collections  frequency  field  names  n    collect    collectors  to  set  if  duplicates  is  empty  throw  new    validation  exception    string  format    field  names  must  be  unique    found  duplicates  s  duplicates  public  static    row  type  of    logical  type  types  return  of  true  types  public  static    row  type  of  boolean  is  nullable    logical  type  types  final    list    row  field  fields  new    array  list  for  int  i    i  types  length  i  fields  add  new    row  field  f  i  types  i  return  new    row  type  is  nullable  fields  public  static    row  type  of    logical  type  types    string  names    list    row  field  fields  new    array  list  for  int  i    i  types  length  i  fields  add  new    row  field  names  i  types  i  return  new    row  type  fields  
public  evolving  public  final  class    small  int  type  extends    logical  type  public  static  final  int  precision    private  static  final    string  format  smallint  private  static  final    set    string  null  output  conversion  conversion  set    short  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set    short  class  get  name  short  class  get  name  private  static  final    class  default  conversion    short  class  public    small  int  type  boolean  is  nullable  super  is  nullable    logical  type  root  smallint  public    small  int  type  this  true    override  public    logical  type  copy  boolean  is  nullable  return  new    small  int  type  is  nullable    override  public    string  as  serializable  string  return  with  nullability  format    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving  public  final  class    structured  type  extends    user  defined  type  private  static  final    set    string  input  output  conversion  conversion  set    row  class  get  name    row  data  class  get  name  private  static  final    class  fallback  conversion    row  class    defines  an  attribute  of  a  link    structured  type  public  static  final  class    structured  attribute  implements    serializable  private  final    string  name  private  final    logical  type  type  private  final    nullable    string  description  public    structured  attribute    string  name    logical  type  type    nullable    string  description  this  name    preconditions  check  not  null  name    attribute  name  must  not  be  null  this  type    preconditions  check  not  null  type    attribute  type  must  not  be  null  this  description  description  public    structured  attribute    string  name    logical  type  type  this  name  type  null  public    string  get  name  return  name  public    logical  type  get  type  return  type  public    optional    string  get  description  return    optional  of  nullable  description  public    structured  attribute  copy  return  new    structured  attribute  name  type  copy  description    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false    structured  attribute  that    structured  attribute  o  return  name  equals  that  name  type  equals  that  type    objects  equals  description  that  description    override  public  int  hash  code  return    objects  hash  name  type  description    defines  equality  properties  for  scalar  evaluation  public  enum    structured  comparision  equals  true  false  full  true  true  none  false  false  private  final  boolean  equality  private  final  boolean  comparison    structured  comparision  boolean  equality  boolean  comparison  this  equality  equality  this  comparison  comparison  public  boolean  is  equality  return  equality  public  boolean  is  comparison  return  comparison  a  builder  for  a  link    structured  type    intended  for  future  extensibility  public  static  final  class    builder  private  final    nullable    object  identifier  object  identifier  private  final    nullable    class  implementation  class  private    list    structured  attribute  attributes  new    array  list  private  boolean  is  nullable  true  private  boolean  is  final  true  private  boolean  is  instantiable  true  private    structured  comparision  comparision    structured  comparision  none  private    nullable    structured  type  super  type  private    nullable    string  description  public    builder    class  implementation  class  this  object  identifier  null  this  implementation  class    preconditions  check  not  null  implementation  class    implementation  class  must  not  be  null  public    builder    object  identifier  object  identifier  this  object  identifier    preconditions  check  not  null  object  identifier    object  identifier  must  not  be  null  this  implementation  class  null  public    builder    object  identifier  object  identifier    class  implementation  class  this  object  identifier    preconditions  check  not  null  object  identifier    object  identifier  must  not  be  null  this  implementation  class    preconditions  check  not  null  implementation  class    implementation  class  must  not  be  null  public    builder  attributes    list    structured  attribute  attributes  this  attributes    collections  unmodifiable  list  new    array  list    preconditions  check  not  null  attributes    attributes  must  not  be  null  return  this  public    builder  set  nullable  boolean  is  nullable  this  is  nullable  is  nullable  return  this  public    builder  description    string  description  this  description    preconditions  check  not  null  description    description  must  not  be  null  return  this  public    builder  set  final  boolean  is  final  this  is  final  is  final  return  this  public    builder  set  instantiable  boolean  is  instantiable  this  is  instantiable  is  instantiable  return  this  public    builder  comparision    structured  comparision  comparision  this  comparision    preconditions  check  not  null  comparision    comparision  must  not  be  null  return  this  public    builder  super  type    structured  type  super  type  this  super  type    preconditions  check  not  null  super  type    super  type  must  not  be  null  return  this  public    structured  type  build  return  new    structured  type  is  nullable  object  identifier  attributes  is  final  is  instantiable  comparision  super  type  description  implementation  class  private  final    list    structured  attribute  attributes  private  final  boolean  is  instantiable  private  final    structured  comparision  comparision  private  final    nullable    structured  type  super  type  private  final    nullable    class  implementation  class  private    structured  type  boolean  is  nullable    object  identifier  object  identifier    list    structured  attribute  attributes  boolean  is  final  boolean  is  instantiable    structured  comparision  comparision    nullable    structured  type  super  type    nullable    string  description    nullable    class  implementation  class  super  is  nullable    logical  type  root  structured  type  object  identifier  is  final  description    preconditions  check  argument  object  identifier  null  implementation  class  null    an  identifier  is  missing  this  attributes  attributes  this  is  instantiable  is  instantiable  this  comparision  comparision  this  super  type  super  type  this  implementation  class  implementation  class    creates  a  builder  for  a  link    structured  type  that  has  been  stored  in  a  catalog  and  is  identified  by  an  link    object  identifier  public  static    structured  type    builder  new  builder    object  identifier  object  identifier  return  new    structured  type    builder  object  identifier    creates  a  builder  for  a  link    structured  type  that  has  been  stored  in  a  catalog  and  is  identified  by  an  link    object  identifier    the  optional  implementation  class  defines  supported  conversions  public  static    structured  type    builder  new  builder    object  identifier  object  identifier    class  implementation  class  return  new    structured  type    builder  object  identifier  implementation  class    creates  a  builder  for  a  link    structured  type  that  is  not  stored  in  a  catalog  and  is  identified  by  an  implementation  link    class  public  static    structured  type    builder  new  builder    class  implementation  class  return  new    structured  type    builder  implementation  class  public    list    structured  attribute  get  attributes  return  attributes  public  boolean  is  instantiable  return  is  instantiable  public    structured  comparision  get  comparision  return  comparision  public    optional    structured  type  get  super  type  return    optional  of  nullable  super  type  public    optional    class  get  implementation  class  return    optional  of  nullable  implementation  class    override  public    logical  type  copy  boolean  is  nullable  return  new    structured  type  is  nullable  get  object  identifier  or  else  null  attributes  is  final  is  instantiable  comparision  super  type  null  null    structured  type  super  type  copy  get  description  or  else  null  implementation  class    override  public    string  as  summary  string  if  get  object  identifier  is  present  return  as  serializable  string  assert  implementation  class  null  we  use  class  to  make  it  visible  that  this  type  is  unregistered  and  not  confuse  it  with  catalog  types  return  implementation  class  get  name    override  public  boolean  supports  input  conversion    class  clazz  return  implementation  class  null  implementation  class  is  assignable  from  clazz  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz    structured  type  current  type  this  while  current  type  null  if  current  type  implementation  class  null  clazz  is  assignable  from  current  type  implementation  class  return  true  current  type  current  type  super  type  return  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  if  implementation  class  null  return  implementation  class  return  fallback  conversion    override  public    list    logical  type  get  children  final    list    logical  type  children  new    array  list  add  super  fields  first  if  super  type  null  children  add  all  super  type  get  children  then  specific  fields  children  add  all  attributes  stream  map    structured  attribute  get  type  collect    collectors  to  list  return    collections  unmodifiable  list  children    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    structured  type  that    structured  type  o  return  is  instantiable  that  is  instantiable  attributes  equals  that  attributes  comparision  that  comparision    objects  equals  super  type  that  super  type    objects  equals  implementation  class  that  implementation  class    override  public  int  hash  code  return    objects  hash  super  hash  code  attributes  is  instantiable  comparision  super  type  implementation  class  
public  evolving  public  final  class    symbol  type  t  extends    table  symbol  extends    logical  type  private  static  final    string  format  symbol  s  private  final    class  t  symbol  class  public    symbol  type  boolean  is  nullable    class  t  symbol  class  super  is  nullable    logical  type  root  symbol  this  symbol  class    preconditions  check  not  null  symbol  class    symbol  class  must  not  be  null  public    symbol  type    class  t  symbol  class  this  true  symbol  class    override  public    logical  type  copy  boolean  is  nullable  return  new    symbol  type  is  nullable  symbol  class    override  public    string  as  summary  string  return  with  nullability  format  symbol  class  get  name    override  public    string  as  serializable  string  throw  new    table  exception  a  symbol  type  has  no  serializable  string  representation    override  public  boolean  supports  input  conversion    class  clazz  return  symbol  class  equals  clazz    override  public  boolean  supports  output  conversion    class  clazz  return  symbol  class  equals  clazz    override  public    class  get  default  conversion  return  symbol  class    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    symbol  type  that    symbol  type  o  return  symbol  class  equals  that  symbol  class    override  public  int  hash  code  return    objects  hash  super  hash  code  symbol  class  
public  evolving  public  final  class    timestamp  type  extends    logical  type  public  static  final  int  min  precision    public  static  final  int  max  precision    public  static  final  int  default  precision    private  static  final    string  format  timestamp  d  private  static  final    set    string  input  output  conversion  conversion  set  java  sql    timestamp  class  get  name  java  time    local  date  time  class  get  name    timestamp  data  class  get  name  private  static  final    class  default  conversion  java  time    local  date  time  class  private  final    timestamp  kind  kind  private  final  int  precision    internal  constructor  that  allows  attaching  additional  metadata  about  time  attribute  properties    the  additional  metadata  does  not  affect  equality  or  serializability  p    use  link  get  kind  for  comparing  this  metadata    internal  public    timestamp  type  boolean  is  nullable    timestamp  kind  kind  int  precision  super  is  nullable    logical  type  root  timestamp  without  time  zone  if  precision  min  precision  precision  max  precision  throw  new    validation  exception    string  format    timestamp  precision  must  be  between  d  and  d  both  inclusive  min  precision  max  precision  this  kind  kind  this  precision  precision  public    timestamp  type  boolean  is  nullable  int  precision  this  is  nullable    timestamp  kind  regular  precision  public    timestamp  type  int  precision  this  true  precision  public    timestamp  type  this  default  precision    internal  public    timestamp  kind  get  kind  return  kind  public  int  get  precision  return  precision    override  public    logical  type  copy  boolean  is  nullable  return  new    timestamp  type  is  nullable  kind  precision    override  public    string  as  serializable  string  return  with  nullability  format  precision    override  public    string  as  summary  string  if  kind    timestamp  kind  regular  return    string  format  s  s  as  serializable  string  kind  return  as  serializable  string    override  public  boolean  supports  input  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    timestamp  type  that    timestamp  type  o  return  precision  that  precision    override  public  int  hash  code  return    objects  hash  super  hash  code  precision  
public  evolving  public  final  class    time  type  extends    logical  type  public  static  final  int  min  precision    public  static  final  int  max  precision    public  static  final  int  default  precision    private  static  final    string  format  time  d  private  static  final    set    string  null  output  conversion  conversion  set  java  sql    time  class  get  name  java  time    local  time  class  get  name    integer  class  get  name    long  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set  java  sql    time  class  get  name  java  time    local  time  class  get  name    integer  class  get  name  int  class  get  name    long  class  get  name  long  class  get  name  private  static  final    class  default  conversion  java  time    local  time  class  private  final  int  precision  public    time  type  boolean  is  nullable  int  precision  super  is  nullable    logical  type  root  time  without  time  zone  if  precision  min  precision  precision  max  precision  throw  new    validation  exception    string  format    time  precision  must  be  between  d  and  d  both  inclusive  min  precision  max  precision  this  precision  precision  public    time  type  int  precision  this  true  precision  public    time  type  this  default  precision  public  int  get  precision  return  precision    override  public    logical  type  copy  boolean  is  nullable  return  new    time  type  is  nullable  precision    override  public    string  as  serializable  string  return  with  nullability  format  precision    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    time  type  time  type    time  type  o  return  precision  time  type  precision    override  public  int  hash  code  return    objects  hash  super  hash  code  precision  
public  evolving  public  final  class    tiny  int  type  extends    logical  type  public  static  final  int  precision    private  static  final    string  format  tinyint  private  static  final    set    string  null  output  conversion  conversion  set    byte  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set    byte  class  get  name  byte  class  get  name  private  static  final    class  default  conversion    byte  class  public    tiny  int  type  boolean  is  nullable  super  is  nullable    logical  type  root  tinyint  public    tiny  int  type  this  true    override  public    logical  type  copy  boolean  is  nullable  return  new    tiny  int  type  is  nullable    override  public    string  as  serializable  string  return  with  nullability  format    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this  
public  evolving    deprecated  public  final  class    type  information  raw  type  t  extends    logical  type  private  static  final    string  format  raw  s  private  static  final    set    string  input  output  conversion  conversion  set  byte  class  get  name    raw  value  data  class  get  name  private  static  final    type  information  default  type  info    types  generic    object  class  private  final    type  information  t  type  info  public    type  information  raw  type  boolean  is  nullable    type  information  t  type  info  super  is  nullable    logical  type  root  raw  this  type  info    preconditions  check  not  null  type  info    type  information  must  not  be  null  public    type  information  raw  type    type  information  t  type  info  this  true  type  info    suppress  warnings  unchecked  public    type  information  raw  type  this  true    type  information  t  default  type  info  public    type  information  t  get  type  information  return  type  info    internal  public    raw  type  t  resolve    execution  config  config  return  new    raw  type  is  nullable  type  info  get  type  class  type  info  create  serializer  config    override  public    logical  type  copy  boolean  is  nullable  return  new    type  information  raw  type  is  nullable  type  info  we  must  assume  immutability  here    override  public    string  as  summary  string  return  with  nullability  format  type  info  get  type  class  get  name    override  public    string  as  serializable  string  throw  new    table  exception  a  raw  type  backed  by  type  information  has  no  serializable  string  representation    it  needs  to  be  resolved  into  a  proper  raw  type    override  public  boolean  supports  input  conversion    class  clazz  return  type  info  get  type  class  is  assignable  from  clazz  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  clazz  is  assignable  from  type  info  get  type  class  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  type  info  get  type  class    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    type  information  raw  type  that    type  information  raw  type  o  return  type  info  equals  that  type  info    override  public  int  hash  code  return    objects  hash  super  hash  code  type  info  
public  evolving  public  final  class    unresolved  user  defined  type  extends    logical  type  private  final    unresolved  identifier  unresolved  identifier  public    unresolved  user  defined  type  boolean  is  nullable    unresolved  identifier  unresolved  identifier  super  is  nullable    logical  type  root  unresolved  this  unresolved  identifier    preconditions  check  not  null  unresolved  identifier    type  identifier  must  not  be  null  public    unresolved  user  defined  type    unresolved  identifier  unresolved  identifier  this  true  unresolved  identifier  public    unresolved  identifier  get  unresolved  identifier  return  unresolved  identifier    override  public    logical  type  copy  boolean  is  nullable  return  new    unresolved  user  defined  type  is  nullable  unresolved  identifier    override  public    string  as  summary  string  return  with  nullability  unresolved  identifier  as  summary  string    override  public    string  as  serializable  string  throw  new    table  exception    an  unresolved  user  defined  type  has  no  serializable  string  representation    it  needs  to  be  resolved  into  a  proper  user  defined  type    override  public  boolean  supports  input  conversion    class  clazz  throw  new    table  exception    an  unresolved  user  defined  type  does  not  support  any  input  conversion    override  public  boolean  supports  output  conversion    class  clazz  throw  new    table  exception    an  unresolved  user  defined  type  does  not  support  any  output  conversion    override  public    class  get  default  conversion  throw  new    table  exception    an  unresolved  user  defined  type  has  no  default  conversion    override  public    list    logical  type  get  children  throw  new    table  exception    an  unresolved  user  defined  type  cannot  return  children    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    unresolved  user  defined  type  that    unresolved  user  defined  type  o  return  unresolved  identifier  equals  that  unresolved  identifier    override  public  int  hash  code  return    objects  hash  super  hash  code  unresolved  identifier  
public  evolving  public  abstract  class    user  defined  type  extends    logical  type  private  final    nullable    object  identifier  object  identifier  private  final  boolean  is  final  private  final    nullable    string  description    user  defined  type  boolean  is  nullable    logical  type  root  type  root    nullable    object  identifier  object  identifier  boolean  is  final    nullable    string  description  super  is  nullable  type  root  this  object  identifier  object  identifier  this  is  final  is  final  this  description  description  public    optional    object  identifier  get  object  identifier  return    optional  of  nullable  object  identifier  public  boolean  is  final  return  is  final  public    optional    string  get  description  return    optional  of  nullable  description    override  public    string  as  serializable  string  if  object  identifier  null  throw  new    table  exception    an  unregistered  user  defined  type  has  no  serializable  string  representation  return  with  nullability  object  identifier  as  serializable  string    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    user  defined  type  that    user  defined  type  o  return  is  final  that  is  final    objects  equals  object  identifier  that  object  identifier    objects  equals  description  that  description    override  public  int  hash  code  return    objects  hash  super  hash  code  object  identifier  is  final  description  
public  evolving  public  final  class    logical  type  parser    parses  a  type  string    all  types  will  be  fully  resolved  except  for  link    unresolved  user  defined  type  s  param  type  string  a  string  like  row  field1  int  field2  boolean  param  class  loader  class  loader  for  loading  classes  of  the  raw  type  throws    validation  exception  in  case  of  parsing  errors  public  static    logical  type  parse    string  type  string    class  loader  class  loader  final    list    token  tokens  tokenize  type  string  final    token  parser  converter  new    token  parser  type  string  tokens  class  loader  return  converter  parse  tokens    parses  a  type  string    all  types  will  be  fully  resolved  except  for  link    unresolved  user  defined  type  s  param  type  string  a  string  like  row  field1  int  field2  boolean  throws    validation  exception  in  case  of  parsing  errors  public  static    logical  type  parse    string  type  string  return  parse  type  string    thread  current  thread  get  context  class  loader    tokenizer  private  static  final  char  char  begin  subtype  private  static  final  char  char  end  subtype  private  static  final  char  char  begin  parameter  private  static  final  char  char  end  parameter  private  static  final  char  char  list  separator  private  static  final  char  char  string  private  static  final  char  char  identifier  private  static  final  char  char  dot  private  static  boolean  is  delimiter  char  character  return    character  is  whitespace  character  character  char  begin  subtype  character  char  end  subtype  character  char  begin  parameter  character  char  end  parameter  character  char  list  separator  character  char  dot  private  static  boolean  is  digit  char  c  return  c    c    private  static    list    token  tokenize    string  chars  final    list    token  tokens  new    array  list  final    string  builder  builder  new    string  builder  for  int  cursor    cursor  chars  length  cursor  char  cur  char  chars  char  at  cursor  switch  cur  char  case  char  begin  subtype  tokens  add  new    token    token  type  begin  subtype  cursor    character  to  string  char  begin  subtype  break  case  char  end  subtype  tokens  add  new    token    token  type  end  subtype  cursor    character  to  string  char  end  subtype  break  case  char  begin  parameter  tokens  add  new    token    token  type  begin  parameter  cursor    character  to  string  char  begin  parameter  break  case  char  end  parameter  tokens  add  new    token    token  type  end  parameter  cursor    character  to  string  char  end  parameter  break  case  char  list  separator  tokens  add  new    token    token  type  list  separator  cursor    character  to  string  char  list  separator  break  case  char  dot  tokens  add  new    token    token  type  identifier  separator  cursor    character  to  string  char  dot  break  case  char  string  builder  set  length    cursor  consume  escaped  builder  chars  cursor  char  string  tokens  add  new    token    token  type  literal  string  cursor  builder  to  string  break  case  char  identifier  builder  set  length    cursor  consume  escaped  builder  chars  cursor  char  identifier  tokens  add  new    token    token  type  identifier  cursor  builder  to  string  break  default  if    character  is  whitespace  cur  char  continue  if  is  digit  cur  char  builder  set  length    cursor  consume  int  builder  chars  cursor  tokens  add  new    token    token  type  literal  int  cursor  builder  to  string  break  builder  set  length    cursor  consume  identifier  builder  chars  cursor  final    string  token  builder  to  string  final    string  normalized  token  token  to  upper  case  if  keywords  contains  normalized  token  tokens  add  new    token    token  type  keyword  cursor  normalized  token  else  tokens  add  new    token    token  type  identifier  cursor  token  return  tokens  private  static  int  consume  escaped    string  builder  builder    string  chars  int  cursor  char  delimiter  skip  delimiter  cursor  for  chars  length  cursor  cursor  final  char  cur  char  chars  char  at  cursor  if  cur  char  delimiter  cursor    chars  length  chars  char  at  cursor    delimiter  escaping  of  the  escaping  char  e  g    hello    world  cursor  builder  append  cur  char  else  if  cur  char  delimiter  break  else  builder  append  cur  char  return  cursor  private  static  int  consume  int    string  builder  builder    string  chars  int  cursor  for  chars  length  cursor  is  digit  chars  char  at  cursor  cursor  builder  append  chars  char  at  cursor  return  cursor    private  static  int  consume  identifier    string  builder  builder    string  chars  int  cursor  for  cursor  chars  length  is  delimiter  chars  char  at  cursor  cursor  builder  append  chars  char  at  cursor  return  cursor    private  enum    token  type  e  g  row  begin  subtype  e  g  row  end  subtype  e  g  char  begin  parameter  e  g  char  end  parameter  e  g  row  int  list  separator  e  g  row  name  int    comment  literal  string  char    literal  int  e  g  char  or  to  keyword  e  g  row  name  or  my  catalog  my  database  identifier  e  g  my  catalog  my  database  identifier  separator  private  enum    keyword  char  varchar  string  boolean  binary  varbinary  bytes  decimal  numeric  dec  tinyint  smallint  int  integer  bigint  float  double  precision  date  time  with  without  local  zone  timestamp  interval  year  month  day  hour  minute  second  to  array  multiset  map  row  null  raw  legacy  not  private  static  final    set    string  keywords    stream  of    keyword  values  map  k  k  to  string  to  upper  case  collect    collectors  to  set  private  static  class    token  public  final    token  type  type  public  final  int  cursor  position  public  final    string  value  public    token    token  type  type  int  cursor  position    string  value  this  type  type  this  cursor  position  cursor  position  this  value  value    token    parsing  private  static  class    token  parser  private  final    string  input  string  private  final    list    token  tokens  private  final    class  loader  class  loader  private  int  last  valid  token  private  int  current  token  public    token  parser    string  input  string    list    token  tokens    class  loader  class  loader  this  input  string  input  string  this  tokens  tokens  this  class  loader  class  loader  this  last  valid  token    this  current  token    private    logical  type  parse  tokens  final    logical  type  type  parse  type  with  nullability  if  has  remaining  tokens  next  token  throw  parsing  error    unexpected  token  token  value  return  type  private  int  last  cursor  if  last  valid  token    return    return  tokens  get  last  valid  token  cursor  position    private    validation  exception  parsing  error    string  cause    nullable    throwable  t  return  new    validation  exception    string  format    could  not  parse  type  at  position  d  s  n    input  type  string  s  last  cursor  cause  input  string  t  private    validation  exception  parsing  error    string  cause  return  parsing  error  cause  null  private  boolean  has  remaining  tokens  return  current  token    tokens  size  private    token  token  return  tokens  get  current  token  private  int  token  as  int  try  return    integer  parse  int  token  value  catch    number  format  exception  e  throw  parsing  error    invalid  integer  value  e  private    keyword  token  as  keyword  return    keyword  value  of  token  value  private    string  token  as  string  return  token  value  private  void  next  token  this  current  token  if  current  token  tokens  size  throw  parsing  error    unexpected  end  last  valid  token  this  current  token    private  void  next  token    token  type  type  next  token  final    token  token  token  if  token  type  type  throw  parsing  error  type  name  expected  but  was  token  type  private  void  next  token    keyword  keyword  next  token    token  type  keyword  final    token  token  token  if  keyword  equals    keyword  value  of  token  value  throw  parsing  error    keyword  keyword  expected  but  was  token  value  private  boolean  has  next  token    token  type  types  if  current  token  types  length    tokens  size  return  false  for  int  i    i  types  length  i  final    token  look  ahead  tokens  get  current  token  i    if  look  ahead  type  types  i  return  false  return  true  private  boolean  has  next  token    keyword  keywords  if  current  token  keywords  length    tokens  size  return  false  for  int  i    i  keywords  length  i  final    token  look  ahead  tokens  get  current  token  i    if  look  ahead  type    token  type  keyword  keywords  i    keyword  value  of  look  ahead  value  return  false  return  true  private  boolean  parse  nullability  not  null  if  has  next  token    keyword  not    keyword  null  next  token    keyword  not  next  token    keyword  null  return  false  explicit  null  else  if  has  next  token    keyword  null  next  token    keyword  null  return  true  implicit  null  return  true  private    logical  type  parse  type  with  nullability  final    logical  type  logical  type  if  has  next  token    token  type  identifier  logical  type  parse  type  by  identifier  copy  parse  nullability  else  logical  type  parse  type  by  keyword  copy  parse  nullability  special  case  suffix  notation  for  array  and  multiset  types  if  has  next  token    keyword  array  next  token    keyword  array  return  new    array  type  logical  type  copy  parse  nullability  else  if  has  next  token    keyword  multiset  next  token    keyword  multiset  return  new    multiset  type  logical  type  copy  parse  nullability  return  logical  type  private    logical  type  parse  type  by  keyword  next  token    token  type  keyword  switch  token  as  keyword  case  char  return  parse  char  type  case  varchar  return  parse  var  char  type  case  string  return  new    var  char  type    var  char  type  max  length  case  boolean  return  new    boolean  type  case  binary  return  parse  binary  type  case  varbinary  return  parse  var  binary  type  case  bytes  return  new    var  binary  type    var  binary  type  max  length  case  decimal  case  numeric  case  dec  return  parse  decimal  type  case  tinyint  return  new    tiny  int  type  case  smallint  return  new    small  int  type  case  int  case  integer  return  new    int  type  case  bigint  return  new    big  int  type  case  float  return  new    float  type  case  double  return  parse  double  type  case  date  return  new    date  type  case  time  return  parse  time  type  case  timestamp  return  parse  timestamp  type  case  interval  return  parse  interval  type  case  array  return  parse  array  type  case  multiset  return  parse  multiset  type  case  map  return  parse  map  type  case  row  return  parse  row  type  case  null  return  new    null  type  case  raw  return  parse  raw  type  case  legacy  return  parse  legacy  type  default  throw  parsing  error    unsupported  type  token  value  private    logical  type  parse  type  by  identifier  next  token    token  type  identifier    list    string  parts  new    array  list  parts  add  token  as  string  if  has  next  token    token  type  identifier  separator  next  token    token  type  identifier  separator  next  token    token  type  identifier  parts  add  token  as  string  if  has  next  token    token  type  identifier  separator  next  token    token  type  identifier  separator  next  token    token  type  identifier  parts  add  token  as  string  final    string  identifier  parts    stream  of  last  part  parts    last  part  parts    last  part  parts    filter    objects  non  null  to  array    string  new  return  new    unresolved  user  defined  type    unresolved  identifier  of  identifier  parts  private    nullable    string  last  part    list    string  parts  int  inverse  pos  final  int  pos  parts  size  inverse  pos    if  pos    return  null  return  parts  get  pos  private  int  parse  string  type  explicit  length  if  has  next  token    token  type  begin  parameter  next  token    token  type  begin  parameter  next  token    token  type  literal  int  final  int  length  token  as  int  next  token    token  type  end  parameter  return  length  implicit  length  return    private    logical  type  parse  char  type  final  int  length  parse  string  type  if  length    return  new    char  type  else  return  new    char  type  length  private    logical  type  parse  var  char  type  final  int  length  parse  string  type  if  length    return  new    var  char  type  else  return  new    var  char  type  length  private    logical  type  parse  binary  type  final  int  length  parse  string  type  if  length    return  new    binary  type  else  return  new    binary  type  length  private    logical  type  parse  var  binary  type  final  int  length  parse  string  type  if  length    return  new    var  binary  type  else  return  new    var  binary  type  length  private    logical  type  parse  decimal  type  int  precision    decimal  type  default  precision  int  scale    decimal  type  default  scale  if  has  next  token    token  type  begin  parameter  next  token    token  type  begin  parameter  next  token    token  type  literal  int  precision  token  as  int  if  has  next  token    token  type  list  separator  next  token    token  type  list  separator  next  token    token  type  literal  int  scale  token  as  int  next  token    token  type  end  parameter  return  new    decimal  type  precision  scale  private    logical  type  parse  double  type  if  has  next  token    keyword  precision  next  token    keyword  precision  return  new    double  type  private    logical  type  parse  time  type  int  precision  parse  optional  precision    time  type  default  precision  if  has  next  token    keyword  without  next  token    keyword  without  next  token    keyword  time  next  token    keyword  zone  return  new    time  type  precision  private    logical  type  parse  timestamp  type  int  precision  parse  optional  precision    timestamp  type  default  precision  if  has  next  token    keyword  without  next  token    keyword  without  next  token    keyword  time  next  token    keyword  zone  else  if  has  next  token    keyword  with  next  token    keyword  with  if  has  next  token    keyword  local  next  token    keyword  local  next  token    keyword  time  next  token    keyword  zone  return  new    local  zoned  timestamp  type  precision  else  next  token    keyword  time  next  token    keyword  zone  return  new    zoned  timestamp  type  precision  return  new    timestamp  type  precision  private    logical  type  parse  interval  type  next  token    token  type  keyword  switch  token  as  keyword  case  year  case  month  return  parse  year  month  interval  type  case  day  case  hour  case  minute  case  second  return  parse  day  time  interval  type  default  throw  parsing  error    invalid  interval  resolution  private    logical  type  parse  year  month  interval  type  int  year  precision    year  month  interval  type  default  precision  switch  token  as  keyword  case  year  year  precision  parse  optional  precision  year  precision  if  has  next  token    keyword  to  next  token    keyword  to  next  token    keyword  month  return  new    year  month  interval  type    year  month  resolution  year  to  month  year  precision  return  new    year  month  interval  type    year  month  resolution  year  year  precision  case  month  return  new    year  month  interval  type    year  month  resolution  month  year  precision  default  throw  parsing  error    invalid  year  month  interval  resolution  private    logical  type  parse  day  time  interval  type  int  day  precision    day  time  interval  type  default  day  precision  int  fractional  precision    day  time  interval  type  default  fractional  precision  switch  token  as  keyword  case  day  day  precision  parse  optional  precision  day  precision  if  has  next  token    keyword  to  next  token    keyword  to  next  token    token  type  keyword  switch  token  as  keyword  case  hour  return  new    day  time  interval  type    day  time  resolution  day  to  hour  day  precision  fractional  precision  case  minute  return  new    day  time  interval  type    day  time  resolution  day  to  minute  day  precision  fractional  precision  case  second  fractional  precision  parse  optional  precision  fractional  precision  return  new    day  time  interval  type    day  time  resolution  day  to  second  day  precision  fractional  precision  default  throw  parsing  error    invalid  day  time  interval  resolution  return  new    day  time  interval  type    day  time  resolution  day  day  precision  fractional  precision  case  hour  if  has  next  token    keyword  to  next  token    keyword  to  next  token    token  type  keyword  switch  token  as  keyword  case  minute  return  new    day  time  interval  type    day  time  resolution  hour  to  minute  day  precision  fractional  precision  case  second  fractional  precision  parse  optional  precision  fractional  precision  return  new    day  time  interval  type    day  time  resolution  hour  to  second  day  precision  fractional  precision  default  throw  parsing  error    invalid  day  time  interval  resolution  return  new    day  time  interval  type    day  time  resolution  hour  day  precision  fractional  precision  case  minute  if  has  next  token    keyword  to  next  token    keyword  to  next  token    keyword  second  fractional  precision  parse  optional  precision  fractional  precision  return  new    day  time  interval  type    day  time  resolution  minute  to  second  day  precision  fractional  precision  return  new    day  time  interval  type    day  time  resolution  minute  day  precision  fractional  precision  case  second  fractional  precision  parse  optional  precision  fractional  precision  return  new    day  time  interval  type    day  time  resolution  second  day  precision  fractional  precision  default  throw  parsing  error    invalid  day  time  interval  resolution  private  int  parse  optional  precision  int  default  precision  int  precision  default  precision  if  has  next  token    token  type  begin  parameter  next  token    token  type  begin  parameter  next  token    token  type  literal  int  precision  token  as  int  next  token    token  type  end  parameter  return  precision  private    logical  type  parse  array  type  next  token    token  type  begin  subtype  final    logical  type  element  type  parse  type  with  nullability  next  token    token  type  end  subtype  return  new    array  type  element  type  private    logical  type  parse  multiset  type  next  token    token  type  begin  subtype  final    logical  type  element  type  parse  type  with  nullability  next  token    token  type  end  subtype  return  new    multiset  type  element  type  private    logical  type  parse  map  type  next  token    token  type  begin  subtype  final    logical  type  key  type  parse  type  with  nullability  next  token    token  type  list  separator  final    logical  type  value  type  parse  type  with  nullability  next  token    token  type  end  subtype  return  new    map  type  key  type  value  type  private    logical  type  parse  row  type    list    row  type    row  field  fields  sql  standard  notation  if  has  next  token    token  type  begin  parameter  next  token    token  type  begin  parameter  fields  parse  row  fields    token  type  end  parameter  next  token    token  type  end  parameter  else  next  token    token  type  begin  subtype  fields  parse  row  fields    token  type  end  subtype  next  token    token  type  end  subtype  return  new    row  type  fields  private    list    row  type    row  field  parse  row  fields    token  type  end  token    list    row  type    row  field  fields  new    array  list  boolean  is  first  true  while  has  next  token  end  token  if  is  first  is  first  false  else  next  token    token  type  list  separator  next  token    token  type  identifier  final    string  name  token  as  string  final    logical  type  type  parse  type  with  nullability  if  has  next  token    token  type  literal  string  next  token    token  type  literal  string  final    string  description  token  as  string  fields  add  new    row  type    row  field  name  type  description  else  fields  add  new    row  type    row  field  name  type  return  fields    suppress  warnings  unchecked  private    logical  type  parse  raw  type  next  token    token  type  begin  parameter  next  token    token  type  literal  string  final    string  class  name  token  as  string  next  token    token  type  list  separator  next  token    token  type  literal  string  final    string  serializer  string  token  as  string  next  token    token  type  end  parameter  return    raw  type  restore  class  loader  class  name  serializer  string    suppress  warnings  unchecked  private    logical  type  parse  legacy  type  next  token    token  type  begin  parameter  next  token    token  type  literal  string  final    string  root  string  token  as  string  next  token    token  type  list  separator  next  token    token  type  literal  string  final    string  type  info  string  token  as  string  next  token    token  type  end  parameter  try  final    logical  type  root  root    logical  type  root  value  of  root  string  final    type  information  type  info    type  string  utils  read  type  info  type  info  string  return  new    legacy  type  information  type  root  type  info  catch    throwable  t  throw  parsing  error    unable  to  restore  the    legacy  type  of  type  info  string  with  type  root  root  string  t  
public  evolving  public  final  class    var  binary  type  extends    logical  type  public  static  final  int  empty  literal  length    public  static  final  int  min  length    public  static  final  int  max  length    integer  max  value  public  static  final  int  default  length    private  static  final    string  format  varbinary  d  private  static  final    string  max  format  bytes  private  static  final    class  input  output  conversion  byte  class  private  static  final    class  default  conversion  byte  class  private  final  int  length  public    var  binary  type  boolean  is  nullable  int  length  super  is  nullable    logical  type  root  varbinary  if  length  min  length  throw  new    validation  exception    string  format    variable  binary  string  length  must  be  between  d  and  d  both  inclusive  min  length  max  length  this  length  length  public    var  binary  type  int  length  this  true  length  public    var  binary  type  this  default  length    helper  constructor  for  link  of  empty  literal  and  link  copy  boolean  private    var  binary  type  int  length  boolean  is  nullable  super  is  nullable    logical  type  root  varbinary  this  length  length    the  sql  standard  defines  that  character  string  literals  are  allowed  to  be  zero  length  strings  i  e  to  contain  no  characters  even  though  it  is  not  permitted  to  declare  a  type  that  is  zero    for  consistent  behavior  the  same  logic  applies  to  binary  strings    this  has  also  implications  on  variable  length  binary  strings  during  type  inference  because  any  fixed  length  binary  string  should  be  convertible  to  a  variable  length  one  p    this  method  enables  this  special  kind  of  binary  string  p    zero  length  binary  strings  have  no  serializable  string  representation  public  static    var  binary  type  of  empty  literal  return  new    var  binary  type  empty  literal  length  false  public  int  get  length  return  length    override  public    logical  type  copy  boolean  is  nullable  return  new    var  binary  type  length  is  nullable    override  public    string  as  serializable  string  if  length  empty  literal  length  throw  new    table  exception    zero  length  binary  strings  have  no  serializable  string  representation  return  with  nullability  format  length    override  public    string  as  summary  string  if  length  max  length  return  with  nullability  max  format  return  with  nullability  format  length    override  public  boolean  supports  input  conversion    class  clazz  return  input  output  conversion  clazz    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  clazz    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    var  binary  type  that    var  binary  type  o  return  length  that  length    override  public  int  hash  code  return    objects  hash  super  hash  code  length  
public  evolving  public  final  class    var  char  type  extends    logical  type  public  static  final  int  empty  literal  length    public  static  final  int  min  length    public  static  final  int  max  length    integer  max  value  public  static  final  int  default  length    private  static  final    string  format  varchar  d  private  static  final    string  max  format  string  private  static  final    set    string  input  output  conversion  conversion  set    string  class  get  name  byte  class  get  name    string  data  class  get  name  private  static  final    class  default  conversion    string  class  private  final  int  length  public    var  char  type  boolean  is  nullable  int  length  super  is  nullable    logical  type  root  varchar  if  length  min  length  throw  new    validation  exception    string  format    variable  character  string  length  must  be  between  d  and  d  both  inclusive  min  length  max  length  this  length  length  public    var  char  type  int  length  this  true  length  public    var  char  type  this  default  length    helper  constructor  for  link  of  empty  literal  and  link  copy  boolean  private    var  char  type  int  length  boolean  is  nullable  super  is  nullable    logical  type  root  varchar  this  length  length    the  sql  standard  defines  that  character  string  literals  are  allowed  to  be  zero  length  strings  i  e  to  contain  no  characters  even  though  it  is  not  permitted  to  declare  a  type  that  is  zero    this  has  also  implications  on  variable  length  character  strings  during  type  inference  because  any  fixed  length  character  string  should  be  convertible  to  a  variable  length  one  p    this  method  enables  this  special  kind  of  character  string  p    zero  length  character  strings  have  no  serializable  string  representation  public  static    var  char  type  of  empty  literal  return  new    var  char  type  empty  literal  length  false  public  int  get  length  return  length    override  public    logical  type  copy  boolean  is  nullable  return  new    var  char  type  length  is  nullable    override  public    string  as  serializable  string  if  length  empty  literal  length  throw  new    table  exception    zero  length  character  strings  have  no  serializable  string  representation  return  with  nullability  format  length    override  public    string  as  summary  string  if  length  max  length  return  with  nullability  max  format  return  with  nullability  format  length    override  public  boolean  supports  input  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    var  char  type  that    var  char  type  o  return  length  that  length    override  public  int  hash  code  return    objects  hash  super  hash  code  length  
public  evolving  public  final  class    year  month  interval  type  extends    logical  type  public  static  final  int  min  precision    public  static  final  int  max  precision    public  static  final  int  default  precision    private  static  final    string  year  format  interval  year  d  private  static  final    string  year  to  month  format  interval  year  d  to  month  private  static  final    string  month  format  interval  month  private  static  final    set    string  null  output  conversion  conversion  set  java  time    period  class  get  name    integer  class  get  name  private  static  final    set    string  not  null  input  output  conversion  conversion  set  java  time    period  class  get  name    integer  class  get  name  int  class  get  name  private  static  final    class  default  conversion  java  time    period  class    supported  resolutions  of  this  type  p    note    the  order  of  this  enum  reflects  the  granularity  from  coarse  to  fine  public  enum    year  month  resolution  year  year  to  month  month  private  final    year  month  resolution  resolution  private  final  int  year  precision  public    year  month  interval  type  boolean  is  nullable    year  month  resolution  resolution  int  year  precision  super  is  nullable    logical  type  root  interval  year  month    preconditions  check  not  null  resolution  if  resolution    year  month  resolution  month  year  precision  default  precision  throw  new    validation  exception    string  format    year  precision  of  sub  year  intervals  must  be  equal  to  the  default  precision  d  default  precision  if  year  precision  min  precision  year  precision  max  precision  throw  new    validation  exception    string  format    year  precision  of  year  month  intervals  must  be  between  d  and  d  both  inclusive  min  precision  max  precision  this  resolution  resolution  this  year  precision  year  precision  public    year  month  interval  type    year  month  resolution  resolution  int  year  precision  this  true  resolution  year  precision  public    year  month  interval  type    year  month  resolution  resolution  this  resolution  default  precision  public    year  month  resolution  get  resolution  return  resolution  public  int  get  year  precision  return  year  precision    override  public    logical  type  copy  boolean  is  nullable  return  new    year  month  interval  type  is  nullable  resolution  year  precision    override  public    string  as  serializable  string  return  with  nullability  get  resolution  format  year  precision    override  public  boolean  supports  input  conversion    class  clazz  return  not  null  input  output  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  if  is  nullable  return  null  output  conversion  contains  clazz  get  name  return  not  null  input  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    year  month  interval  type  that    year  month  interval  type  o  return  year  precision  that  year  precision  resolution  that  resolution    override  public  int  hash  code  return    objects  hash  super  hash  code  resolution  year  precision  private    string  get  resolution  format  switch  resolution  case  year  return  year  format  case  year  to  month  return  year  to  month  format  case  month  return  month  format  default  throw  new    unsupported  operation  exception  
public  evolving  public  final  class    zoned  timestamp  type  extends    logical  type  public  static  final  int  min  precision    timestamp  type  min  precision  public  static  final  int  max  precision    timestamp  type  max  precision  public  static  final  int  default  precision    timestamp  type  default  precision  private  static  final    string  format  timestamp  d  with  time  zone  private  static  final    set    string  input  conversion  conversion  set  java  time    zoned  date  time  class  get  name  java  time    offset  date  time  class  get  name  private  static  final    set    string  output  conversion  conversion  set  java  time    offset  date  time  class  get  name  private  static  final    class  default  conversion  java  time    offset  date  time  class  private  final    timestamp  kind  kind  private  final  int  precision    internal  constructor  that  allows  attaching  additional  metadata  about  time  attribute  properties    the  additional  metadata  does  not  affect  equality  or  serializability  p    use  link  get  kind  for  comparing  this  metadata    internal  public    zoned  timestamp  type  boolean  is  nullable    timestamp  kind  kind  int  precision  super  is  nullable    logical  type  root  timestamp  with  time  zone  if  precision  min  precision  precision  max  precision  throw  new    validation  exception    string  format    timestamp  with  time  zone  precision  must  be  between  d  and  d  both  inclusive  min  precision  max  precision  this  kind  kind  this  precision  precision  public    zoned  timestamp  type  boolean  is  nullable  int  precision  this  is  nullable    timestamp  kind  regular  precision  public    zoned  timestamp  type  int  precision  this  true  precision  public    zoned  timestamp  type  this  default  precision    internal  public    timestamp  kind  get  kind  return  kind  public  int  get  precision  return  precision    override  public    logical  type  copy  boolean  is  nullable  return  new    zoned  timestamp  type  is  nullable  kind  precision    override  public    string  as  serializable  string  return  with  nullability  format  precision    override  public    string  as  summary  string  if  kind    timestamp  kind  regular  return    string  format  s  s  as  serializable  string  kind  return  as  serializable  string    override  public  boolean  supports  input  conversion    class  clazz  return  input  conversion  contains  clazz  get  name    override  public  boolean  supports  output  conversion    class  clazz  return  output  conversion  contains  clazz  get  name    override  public    class  get  default  conversion  return  default  conversion    override  public    list    logical  type  get  children  return    collections  empty  list    override  public  r  r  accept    logical  type  visitor  r  visitor  return  visitor  visit  this    override  public  boolean  equals    object  o  if  this  o  return  true  if  o  null  get  class  o  get  class  return  false  if  super  equals  o  return  false    zoned  timestamp  type  that    zoned  timestamp  type  o  return  precision  that  precision    override  public  int  hash  code  return    objects  hash  super  hash  code  precision  
public  evolving  public  final  class    unresolved  data  type  implements    abstract  data  type    unresolved  data  type  private  static  final    string  format  s  indicates  that  this  is  an  unresolved  type  private  final    nullable    boolean  is  nullable  private  final    nullable    class  conversion  class  private  final    supplier    string  description  private  final    function    data  type  factory    data  type  resolution  factory  private    unresolved  data  type    nullable    boolean  is  nullable    nullable    class  conversion  class    supplier    string  description    function    data  type  factory    data  type  resolution  factory  this  is  nullable  is  nullable  this  conversion  class  conversion  class  this  description  description  this  resolution  factory  resolution  factory  public    unresolved  data  type    supplier    string  description    function    data  type  factory    data  type  resolution  factory  this  null  null  description  resolution  factory    converts  this  instance  to  a  resolved  link    data  type  possibly  enriched  with  additional  nullability  and  conversion  class  information  public    data  type  to  data  type    data  type  factory  factory    data  type  resolved  data  type  resolution  factory  apply  factory  if  is  nullable    boolean  true  resolved  data  type  resolved  data  type  nullable  else  if  is  nullable    boolean  false  resolved  data  type  resolved  data  type  not  null  if  conversion  class  null  resolved  data  type  resolved  data  type  bridged  to  conversion  class  return  resolved  data  type    override  public    unresolved  data  type  not  null  return  new    unresolved  data  type  false  conversion  class  description  resolution  factory    override  public    unresolved  data  type  nullable  return  new    unresolved  data  type  true  conversion  class  description  resolution  factory    override  public    unresolved  data  type  bridged  to    class  new  conversion  class  return  new    unresolved  data  type  is  nullable  new  conversion  class  description  resolution  factory    override  public    string  to  string  return    string  format  format  description  get  
public  evolving  public  class    sorted  map  type  info  k  v  extends    abstract  map  type  info  k  v    sorted  map  k  v  private  static  final  long  serial  version  u  i  d  1  l    the  comparator  for  the  keys  in  the  map  private  final    comparator  k  comparator  public    sorted  map  type  info    type  information  k  key  type  info    type  information  v  value  type  info    comparator  k  comparator  super  key  type  info  value  type  info    preconditions  check  not  null  comparator    the  comparator  cannot  be  null  this  comparator  comparator  public    sorted  map  type  info    class  k  key  class    class  v  value  class    comparator  k  comparator  super  key  class  value  class    preconditions  check  not  null  comparator    the  comparator  cannot  be  null  this  comparator  comparator  public    sorted  map  type  info    class  k  key  class    class  v  value  class  super  key  class  value  class    preconditions  check  argument    comparable  class  is  assignable  from  key  class    the  key  class  must  be  comparable  when  no  comparator  is  given  this  comparator  new    comparable  comparator    suppress  warnings  unchecked    override  public    class    sorted  map  k  v  get  type  class  return    class    sorted  map  k  v    class    sorted  map  class    override  public    type  serializer    sorted  map  k  v  create  serializer    execution  config  config    type  serializer  k  key  type  serializer  key  type  info  create  serializer  config    type  serializer  v  value  type  serializer  value  type  info  create  serializer  config  return  new    sorted  map  serializer  comparator  key  type  serializer  value  type  serializer    override  public  boolean  can  equal    object  obj  return  null  obj  get  class  obj  get  class    override  public  boolean  equals    object  o  if  super  equals  o  return  false    sorted  map  type  info  that    sorted  map  type  info  o  return  comparator  equals  that  comparator    override  public  int  hash  code  int  result  super  hash  code  result    result  comparator  hash  code  return  result    override  public    string  to  string  return    sorted  map  type  info  comparator  comparator  key  type  info  get  key  type  info  value  type  info  get  value  type  info    the  default  comparator  for  comparable  types  private  static  class    comparable  comparator  k  implements    comparator  k    serializable  private  static  final  long  serial  version  u  i  d  1  l    suppress  warnings  unchecked  public  int  compare  k  obj1  k  obj2  return    comparable  k  obj1  compare  to  obj2    override  public  boolean  equals    object  o  return  o  this  o  null  o  get  class  get  class    override  public  int  hash  code  return    comparable  comparator  hash  code  
public  evolving    suppress  warnings  unused  public  class    alert  sink  implements    sink  function    alert  private  static  final  long  serial  version  u  i  d  1  l  private  static  final    logger  log    logger  factory  get  logger    alert  sink  class    override  public  void  invoke    alert  value    context  context  log  info  value  to  string  
